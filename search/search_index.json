{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"autoscaler/","text":"https://speakerdeck.com/oracle4engineer/kubernetes-autoscale-deep-dive","title":"Index"},{"location":"aws-eks/aws-load-balancer-controller/","text":"aws-load-balancer-controller \u53c2\u8003 https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/","title":"aws-load-balancer-controller"},{"location":"aws-eks/aws-load-balancer-controller/#aws-load-balancer-controller","text":"","title":"aws-load-balancer-controller"},{"location":"aws-eks/aws-load-balancer-controller/#_1","text":"https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/","title":"\u53c2\u8003"},{"location":"failure_stories/","text":"Failure Test Failure Stories https://k8s.af/ https://codeberg.org/hjacobs/kubernetes-failure-stories Node Node - OOM Killer \u53c2\u8003 k8s.af \u304b\u3089 https://www.bluematador.com/blog/post-mortem-kubernetes-node-oom https://www.scsk.jp/sp/sysdig/blog/sysdig_monitor/kubernetes_oomcpu.html https://blog.mosuke.tech/entry/2020/03/31/kubernetes-resource/ Impact Memory\u304cNode\u4e0a\u9650\u306b\u9054\u3057\u305f\u5834\u5408OOM Killer\u304cPod\u3092\u5f37\u5236\u505c\u6b62\u3059\u308b \u5f37\u5236\u505c\u6b62\u3059\u308bPod\u306fQoS Class\u306e\u512a\u5148\u5ea6\u3067\u6c7a\u307e\u308b ( \u53c2\u8003 ) Note: The kubelet also sets an oom_score_adj value of -997 for containers in Pods that have system-node-critical Priority \u540c\u3058QoS Class(Burstable) \u306e\u5834\u5408\u306fPod\u306erequest\u3057\u305f\u30e1\u30e2\u30ea\u30fc\u91cf\u3068\u30ce\u30fc\u30c9\u306e\u30ad\u30e3\u30d1\u30b7\u30c6\u30a3\u306e\u5272\u5408\u306b\u3088\u3063\u3066\u30b9\u30b3\u30a2\u4ed8\u3051\u3055\u308c\u308b Node - \u505c\u6b62 \u53c2\u8003 Goldstine\u7814\u7a76\u6240: Kubernetes\u306e\u30ce\u30fc\u30c9\u969c\u5bb3\u6642\u306ePod\u306e\u52d5\u304d\u306b\u3064\u3044\u3066\u306e\u691c\u8a3c Impact ReplicaSet Pod\u306e\u518d\u914d\u7f6e\u304c\u884c\u308f\u308c\u308b node_lifecycle_controller Kubelet\u304cNode\u60c5\u5831\u3092\u66f4\u65b0\u3057\u306a\u304f\u306a\u3063\u305f\u3053\u3068\u3092\u691c\u77e5\u3057\u3066Node\u306eStatus\u3092\u5909\u66f4 key: node.kubernetes.io/unreachable \u306eTaint\u3092\u4ed8\u4e0e Pod\u306e\u518d\u914d\u7f6e Pod\u304c\u4f5c\u6210\u3055\u308c\u308b\u6642\u306bDefaultTolerationSeconds AdmissionController\u306b\u3088\u3063\u3066\u4ee5\u4e0b\u306etolerations\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u3044\u308b node.kubernetes.io/not-ready:NoExecute node.kubernetes.io/unreachable:NoExecute \u3053\u308c\u3089\u306etolerations\u304c tolerationSeconds: 300 \u3092\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001300\u79d2\u7d4c\u904e\u5f8c\u306bNode\u304c\u5fa9\u65e7\u305b\u305a node.kubernetes.io/unreachable Taint\u304c\u5916\u308c\u306a\u3044\u5834\u5408Pod\u304cEviction\uff08\u5f37\u5236\u9000\u53bb\uff09\u3055\u308c\u5225\u306e\u30ce\u30fc\u30c9\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u308b https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaulttolerationseconds \u5225\u30ce\u30fc\u30c9\u3078\u306e\u518d\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u307e\u3067\u6700\u59275\u5206\u306e\u30bf\u30a4\u30e0\u30e9\u30b0\u304c\u767a\u751f\u3059\u308b\u5834\u5408\u304c\u3042\u308b DefaultTolerationSeconds","title":"Failure Stories"},{"location":"failure_stories/#failure-test","text":"","title":"Failure Test"},{"location":"failure_stories/#failure-stories","text":"https://k8s.af/ https://codeberg.org/hjacobs/kubernetes-failure-stories","title":"Failure Stories"},{"location":"failure_stories/#node","text":"","title":"Node"},{"location":"failure_stories/#node-oom-killer","text":"","title":"Node - OOM Killer"},{"location":"failure_stories/#_1","text":"k8s.af \u304b\u3089 https://www.bluematador.com/blog/post-mortem-kubernetes-node-oom https://www.scsk.jp/sp/sysdig/blog/sysdig_monitor/kubernetes_oomcpu.html https://blog.mosuke.tech/entry/2020/03/31/kubernetes-resource/","title":"\u53c2\u8003"},{"location":"failure_stories/#impact","text":"Memory\u304cNode\u4e0a\u9650\u306b\u9054\u3057\u305f\u5834\u5408OOM Killer\u304cPod\u3092\u5f37\u5236\u505c\u6b62\u3059\u308b \u5f37\u5236\u505c\u6b62\u3059\u308bPod\u306fQoS Class\u306e\u512a\u5148\u5ea6\u3067\u6c7a\u307e\u308b ( \u53c2\u8003 ) Note: The kubelet also sets an oom_score_adj value of -997 for containers in Pods that have system-node-critical Priority \u540c\u3058QoS Class(Burstable) \u306e\u5834\u5408\u306fPod\u306erequest\u3057\u305f\u30e1\u30e2\u30ea\u30fc\u91cf\u3068\u30ce\u30fc\u30c9\u306e\u30ad\u30e3\u30d1\u30b7\u30c6\u30a3\u306e\u5272\u5408\u306b\u3088\u3063\u3066\u30b9\u30b3\u30a2\u4ed8\u3051\u3055\u308c\u308b","title":"Impact"},{"location":"failure_stories/#node-","text":"","title":"Node - \u505c\u6b62"},{"location":"failure_stories/#_2","text":"Goldstine\u7814\u7a76\u6240: Kubernetes\u306e\u30ce\u30fc\u30c9\u969c\u5bb3\u6642\u306ePod\u306e\u52d5\u304d\u306b\u3064\u3044\u3066\u306e\u691c\u8a3c","title":"\u53c2\u8003"},{"location":"failure_stories/#impact_1","text":"ReplicaSet Pod\u306e\u518d\u914d\u7f6e\u304c\u884c\u308f\u308c\u308b node_lifecycle_controller Kubelet\u304cNode\u60c5\u5831\u3092\u66f4\u65b0\u3057\u306a\u304f\u306a\u3063\u305f\u3053\u3068\u3092\u691c\u77e5\u3057\u3066Node\u306eStatus\u3092\u5909\u66f4 key: node.kubernetes.io/unreachable \u306eTaint\u3092\u4ed8\u4e0e Pod\u306e\u518d\u914d\u7f6e Pod\u304c\u4f5c\u6210\u3055\u308c\u308b\u6642\u306bDefaultTolerationSeconds AdmissionController\u306b\u3088\u3063\u3066\u4ee5\u4e0b\u306etolerations\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u3044\u308b node.kubernetes.io/not-ready:NoExecute node.kubernetes.io/unreachable:NoExecute \u3053\u308c\u3089\u306etolerations\u304c tolerationSeconds: 300 \u3092\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001300\u79d2\u7d4c\u904e\u5f8c\u306bNode\u304c\u5fa9\u65e7\u305b\u305a node.kubernetes.io/unreachable Taint\u304c\u5916\u308c\u306a\u3044\u5834\u5408Pod\u304cEviction\uff08\u5f37\u5236\u9000\u53bb\uff09\u3055\u308c\u5225\u306e\u30ce\u30fc\u30c9\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u308b https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaulttolerationseconds \u5225\u30ce\u30fc\u30c9\u3078\u306e\u518d\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u307e\u3067\u6700\u59275\u5206\u306e\u30bf\u30a4\u30e0\u30e9\u30b0\u304c\u767a\u751f\u3059\u308b\u5834\u5408\u304c\u3042\u308b DefaultTolerationSeconds","title":"Impact"},{"location":"kubernetes_dashboard/","text":"kubernetes dashboard \u53c2\u8003 https://github.com/kubernetes/dashboard https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/","title":"kubernetes dashboard"},{"location":"kubernetes_dashboard/#kubernetes-dashboard","text":"","title":"kubernetes dashboard"},{"location":"kubernetes_dashboard/#_1","text":"https://github.com/kubernetes/dashboard https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/","title":"\u53c2\u8003"},{"location":"pod_security/","text":"Pod https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/","title":"Index"},{"location":"pod_security/#pod","text":"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/","title":"Pod"},{"location":"setup/01_setup_RaspberryPi/","text":"Raspberry Pi \u69cb\u6210 \u8cfc\u5165\u3057\u305f\u3082\u306e Raspberry Pi 4 Model B/2GB x3 GeeekPi Raspberry Pi4 \u30af\u30e9\u30b9\u30bf\u30fc\u30b1\u30fc\u30b9(\u51b7\u5374\u30d5\u30a1\u30f3\u4ed8\u304d) x1 Anker PowerPort I PD - 1 PD & 4 PowerIQ x1 Amazon\u30d9\u30fc\u30b7\u30c3\u30af HDMI\u30b1\u30fc\u30d6\u30eb 0.9m (\u30bf\u30a4\u30d7A\u30aa\u30b9 - \u30de\u30a4\u30af\u30ed\u30bf\u30a4\u30d7D\u30aa\u30b9) x1 Samsung EVO Plus 32GB microSDHC x3 Anker USB Type C \u30b1\u30fc\u30d6\u30eb PowerLine USB-C & USB-A 3.0 \u30b1\u30fc\u30d6\u30eb x3","title":"01. Raspberry Pi \u69cb\u6210"},{"location":"setup/01_setup_RaspberryPi/#raspberry-pi","text":"","title":"Raspberry Pi \u69cb\u6210"},{"location":"setup/01_setup_RaspberryPi/#_1","text":"Raspberry Pi 4 Model B/2GB x3 GeeekPi Raspberry Pi4 \u30af\u30e9\u30b9\u30bf\u30fc\u30b1\u30fc\u30b9(\u51b7\u5374\u30d5\u30a1\u30f3\u4ed8\u304d) x1 Anker PowerPort I PD - 1 PD & 4 PowerIQ x1 Amazon\u30d9\u30fc\u30b7\u30c3\u30af HDMI\u30b1\u30fc\u30d6\u30eb 0.9m (\u30bf\u30a4\u30d7A\u30aa\u30b9 - \u30de\u30a4\u30af\u30ed\u30bf\u30a4\u30d7D\u30aa\u30b9) x1 Samsung EVO Plus 32GB microSDHC x3 Anker USB Type C \u30b1\u30fc\u30d6\u30eb PowerLine USB-C & USB-A 3.0 \u30b1\u30fc\u30d6\u30eb x3","title":"\u8cfc\u5165\u3057\u305f\u3082\u306e"},{"location":"setup/02_setup_ubuntu20-04-LTS/","text":"OS Setup UbuntuServer20.04.2LTS Setup GeeekPi\u30b1\u30fc\u30b9\u306e\u7d44\u307f\u7acb\u3066 & \u7d50\u7dda & \u8d77\u52d5\u78ba\u8a8d Raspberry Pi Imager \u3067SD\u30ab\u30fc\u30c9\u306bUbuntuServer20.04.2LTS(64bit) \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Raspberry Pi 4\u3078SD\u30ab\u30fc\u30c9\u3092\u633f\u5165\u3057\u8d77\u52d5 wifi\u8a2d\u5b9a sudo vim /etc/netplan/50-cloud-init.yaml # config check sudo netplan --debug try sudo netplan --debug generate # \u9069\u7528 sudo netplan --debug apply /etc/netplan/50-cloud-init.yaml (master\u306e\u5834\u5408) network: ethernets: eth0: dhcp4: true optional: true version: 2 wifis: wlan0: optional: true dhcp4: false addresses: - 192.168.10.50/24 gateway4: 192.168.10.1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 search: [] access-points: \"<SSID\u540d>\": password: \"<\u30d1\u30b9\u30ef\u30fc\u30c9>\" package\u66f4\u65b0 sudo apt update sudo apt upgrade -y \u65e5\u672c\u8a9e\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u5909\u66f4\u3057\u518d\u8d77\u52d5 sudo dpkg-reconfigure keyboard-configuration sudo reboot Generic 105-key (Intl) PC \u3092\u9078\u629e Japanese \u3092\u9078\u629e Japanese \u3092\u9078\u629e The default for the keyboard layout \u3092\u9078\u629e No compose key \u3092\u9078\u629e LOCALE sudo apt install -y language-pack-ja sudo update-locale LANG=ja_JP.UTF-8 \u30db\u30b9\u30c8\u540d e.g. k8s-master name=k8s-master echo ${name} | sudo tee /etc/hostname sudo sed -i -e 's/127.0.1.1.*/127.0.1.1\\t'$name'/' /etc/hosts","title":"02. OS Install"},{"location":"setup/02_setup_ubuntu20-04-LTS/#os-setup","text":"","title":"OS Setup"},{"location":"setup/02_setup_ubuntu20-04-LTS/#ubuntuserver20042lts-setup","text":"GeeekPi\u30b1\u30fc\u30b9\u306e\u7d44\u307f\u7acb\u3066 & \u7d50\u7dda & \u8d77\u52d5\u78ba\u8a8d Raspberry Pi Imager \u3067SD\u30ab\u30fc\u30c9\u306bUbuntuServer20.04.2LTS(64bit) \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Raspberry Pi 4\u3078SD\u30ab\u30fc\u30c9\u3092\u633f\u5165\u3057\u8d77\u52d5 wifi\u8a2d\u5b9a sudo vim /etc/netplan/50-cloud-init.yaml # config check sudo netplan --debug try sudo netplan --debug generate # \u9069\u7528 sudo netplan --debug apply /etc/netplan/50-cloud-init.yaml (master\u306e\u5834\u5408) network: ethernets: eth0: dhcp4: true optional: true version: 2 wifis: wlan0: optional: true dhcp4: false addresses: - 192.168.10.50/24 gateway4: 192.168.10.1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 search: [] access-points: \"<SSID\u540d>\": password: \"<\u30d1\u30b9\u30ef\u30fc\u30c9>\" package\u66f4\u65b0 sudo apt update sudo apt upgrade -y \u65e5\u672c\u8a9e\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u5909\u66f4\u3057\u518d\u8d77\u52d5 sudo dpkg-reconfigure keyboard-configuration sudo reboot Generic 105-key (Intl) PC \u3092\u9078\u629e Japanese \u3092\u9078\u629e Japanese \u3092\u9078\u629e The default for the keyboard layout \u3092\u9078\u629e No compose key \u3092\u9078\u629e LOCALE sudo apt install -y language-pack-ja sudo update-locale LANG=ja_JP.UTF-8 \u30db\u30b9\u30c8\u540d e.g. k8s-master name=k8s-master echo ${name} | sudo tee /etc/hostname sudo sed -i -e 's/127.0.1.1.*/127.0.1.1\\t'$name'/' /etc/hosts","title":"UbuntuServer20.04.2LTS Setup"},{"location":"setup/03_common_settings/","text":"swap\u3092\u7121\u52b9\u306b\u3059\u308b swap\u304c\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ free -h total used free shared buff/cache available Mem: 1.8Gi 54Mi 1.5Gi 8.0Mi 244Mi 1.7Gi Swap: 99Mi 0B 99Mi swap\u3092\u7121\u52b9\u306b\u8a2d\u5b9a\u3059\u308b sudo swapoff --all sudo systemctl stop dphys-swapfile sudo systemctl disable dphys-swapfile swap\u304c\u7121\u52b9\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b $ free -h total used free shared buff/cache available Mem: 1.8Gi 57Mi 1.5Gi 8.0Mi 251Mi 1.7Gi Swap: 0B 0B 0B $ systemctl status dphys-swapfile \u25cf dphys-swapfile.service - dphys-swapfile - set up, mount/unmount, and delete a swap file Loaded: loaded (/lib/systemd/system/dphys-swapfile.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:dphys-swapfile(8) 12\u6708 30 20:48:54 k8s-master1 systemd[1]: Starting dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 30 20:48:55 k8s-master1 dphys-swapfile[330]: want /var/swap=100MByte, checking existing: keeping it 12\u6708 30 20:48:55 k8s-master1 systemd[1]: Started dphys-swapfile - set up, mount/unmount, and delete a swap file. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopping dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 31 06:57:57 k8s-master1 systemd[1]: dphys-swapfile.service: Succeeded. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopped dphys-swapfile - set up, mount/unmount, and delete a swap file. cgroupfs \u306ememory\u3092\u6709\u52b9\u306b\u3059\u308b kernel\u306eboot option\u306b cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory \u3092\u8ffd\u8a18\u3059\u308b sudo vim /boot/firmware/cmdline.txt cmdline.txt \u306e\u6709\u52b9\u884c\u306e\u78ba\u8a8d $ sudo sed -e 's/\\s/\\n/g' /boot/firmware/cmdline.txt console=serial0,115200 console=tty1 root=PARTUUID=fb7271c3-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait quiet splash plymouth.ignore-serial-consoles cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory $ sudo reboot $ cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 9 1 1 cpu 5 34 1 cpuacct 5 34 1 blkio 10 34 1 memory 8 80 1 devices 4 34 1 freezer 7 1 1 net_cls 2 1 1 perf_event 6 1 1 net_prio 2 1 1 pids 3 39 1 CRI-O \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o kernel module\u306eload overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module overlay iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module br_netfilter cat <<EOF | sudo tee /etc/modules-load.d/crio.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter kernel parameter\u306eset iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf # https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF sudo sysctl --system system\u8d77\u52d5\u6642\u306b kernel parameter \u3092\u518d\u8aad\u307f\u8fbc\u307f\u3055\u305b\u308b kube-proxy\u306b\u3066\u5fc5\u8981\u306akernel parameter\u8a2d\u5b9a(kubelet\u8a2d\u5b9a\u624b\u9806\u306b\u3066\u5f8c\u8ff0) \u304ciptables\u8d77\u52d5\u6642\u306ekernel module load\u3067\u4e0a\u66f8\u304d\u3055\u308c\u308b\u305f\u3081 \u5229\u7528\u74b0\u5883\u304c /etc/sysconfig/iptables-config \u3092\u5229\u7528\u53ef\u80fd\u306a\u3089 IPTABLES_MODULES_UNLOAD=\"no\" \u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u672c\u624b\u9806\u306f\u4e0d\u8981\u3067\u3059 egrep \"sysctl\\s+--system\" /etc/rc.local > /dev/null || sudo bash -c \"echo \\\"sysctl --system\\\" >> /etc/rc.local\" egrep \"sysctl\\s+--system\" /etc/rc.local CRI-O\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.21/xUbuntu_20.04/arm64/ VERSION=1.21 OS=xUbuntu_20.04 sudo bash -c \"echo \\\"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\" sudo bash -c \"echo \\\"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list\" curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key add - curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key add - sudo apt update sudo apt install -y cri-o cri-o-runc sudo apt install -y conntrack storage driver\u3092 overlay2 \u3078\u5909\u66f4\u3059\u308b sudo vim /etc/containers/storage.conf sudo vim /etc/crio/crio.conf /etc/crio/crio.conf \u3078graph driver\u8a2d\u5b9a\u3092\u5165\u308c\u308b podman\u3084buildah\u3067build\u3057\u305flocal image\u3092\u53c2\u7167\u3059\u308b\u305f\u3081 [crio] \u30bb\u30af\u30b7\u30e7\u30f3\u306b\u5165\u308c\u308b graphroot = \"/var/lib/containers/storage\" /etc/containers/storage.conf [storage] driver = \"overlay2\" runroot = \"/run/containers/storage\" graphroot = \"/var/lib/containers/storage\" [storage.options] additionalimagestores = [ ] [storage.options.overlay] mountopt = \"nodev\" [storage.options.thinpool] /etc/crio/crio.conf [crio] storage_driver = \"overlay2\" graphroot = \"/var/lib/containers/storage\" log_dir = \"/var/log/crio/pods\" version_file = \"/var/run/crio/version\" version_file_persist = \"/var/lib/crio/version\" clean_shutdown_file = \"/var/lib/crio/clean.shutdown\" [crio.api] listen = \"/var/run/crio/crio.sock\" stream_address = \"127.0.0.1\" stream_port = \"0\" stream_enable_tls = false stream_idle_timeout = \"\" stream_tls_cert = \"\" stream_tls_key = \"\" stream_tls_ca = \"\" grpc_max_send_msg_size = 16777216 grpc_max_recv_msg_size = 16777216 [crio.runtime] no_pivot = false decryption_keys_path = \"/etc/crio/keys/\" conmon = \"\" conmon_cgroup = \"system.slice\" conmon_env = [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", ] default_env = [ ] seccomp_profile = \"\" seccomp_use_default_when_empty = false apparmor_profile = \"crio-default\" irqbalance_config_file = \"/etc/sysconfig/irqbalance\" cgroup_manager = \"systemd\" separate_pull_cgroup = \"\" default_capabilities = [ \"CHOWN\", \"DAC_OVERRIDE\", \"FSETID\", \"FOWNER\", \"SETGID\", \"SETUID\", \"SETPCAP\", \"NET_BIND_SERVICE\", \"KILL\", ] default_sysctls = [ ] additional_devices = [ ] hooks_dir = [ \"/usr/share/containers/oci/hooks.d\", ]pids_limit = 1024 log_size_max = -1 log_to_journald = false container_exits_dir = \"/var/run/crio/exits\" container_attach_socket_dir = \"/var/run/crio\" bind_mount_prefix = \"\" read_only = false log_level = \"info\" log_filter = \"\" uid_mappings = \"\" gid_mappings = \"\" ctr_stop_timeout = 30 drop_infra_ctr = false infra_ctr_cpuset = \"\" namespaces_dir = \"/var/run\" pinns_path = \"\" default_runtime = \"runc\" [crio.runtime.runtimes.runc] runtime_path = \"\" runtime_type = \"oci\" runtime_root = \"/run/runc\" allowed_annotations = [ \"io.containers.trace-syscall\", ] [crio.image] default_transport = \"docker://\" global_auth_file = \"\" pause_image = \"k8s.gcr.io/pause:3.2\" pause_image_auth_file = \"\" pause_command = \"/pause\" signature_policy = \"\" image_volumes = \"mkdir\" big_files_temporary_dir = \"\" [crio.network] network_dir = \"/etc/cni/net.d/\" plugin_dirs = [ \"/opt/cni/bin/\", ] [crio.metrics] enable_metrics = false metrics_port = 9090 metrics_socket = \"\" crio\u3092\u518d\u8d77\u52d5\u3059\u308b sudo systemctl daemon-reload sudo systemctl restart crio CLI TOOL(buildah, cri-tools) buildah https://github.com/containers/buildah/blob/master/install.md sudo apt-get -qq -y install buildah cri-tools(crictl) \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md VERSION=\"v1.22.0\" ARCH=\"arm64\" DOWNLOAD_URL=\"https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-${VERSION}-linux-${ARCH}.tar.gz\" curl -L ${DOWNLOAD_URL} | sudo tar -zxvf -C /usr/local/bin ls -l /usr/local/bin podman \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(optional) https://podman.io/getting-started/installation sudo apt-get -y install podman sudo rm -f /etc/cni/net.d/87-podman-bridge.conflist CNI Plugin\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://github.com/containernetworking/plugins VERSION=\"v1.0.0\" ARCH=\"arm64\" DOWNLOAD_URL=\"https://github.com/containernetworking/plugins/releases/download/${VERSION}/cni-plugins-linux-${ARCH}-${VERSION}.tgz\" curl -L ${DOWNLOAD_URL} | sudo tar -zxvf -C /opt/cni/bin ls -l /opt/cni/bin/ kubectl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://kubernetes.io/ja/docs/tasks/tools/install-kubectl/","title":"03. master/node\u3067\u5171\u901a\u624b\u9806"},{"location":"setup/03_common_settings/#swap","text":"swap\u304c\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ free -h total used free shared buff/cache available Mem: 1.8Gi 54Mi 1.5Gi 8.0Mi 244Mi 1.7Gi Swap: 99Mi 0B 99Mi swap\u3092\u7121\u52b9\u306b\u8a2d\u5b9a\u3059\u308b sudo swapoff --all sudo systemctl stop dphys-swapfile sudo systemctl disable dphys-swapfile swap\u304c\u7121\u52b9\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b $ free -h total used free shared buff/cache available Mem: 1.8Gi 57Mi 1.5Gi 8.0Mi 251Mi 1.7Gi Swap: 0B 0B 0B $ systemctl status dphys-swapfile \u25cf dphys-swapfile.service - dphys-swapfile - set up, mount/unmount, and delete a swap file Loaded: loaded (/lib/systemd/system/dphys-swapfile.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:dphys-swapfile(8) 12\u6708 30 20:48:54 k8s-master1 systemd[1]: Starting dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 30 20:48:55 k8s-master1 dphys-swapfile[330]: want /var/swap=100MByte, checking existing: keeping it 12\u6708 30 20:48:55 k8s-master1 systemd[1]: Started dphys-swapfile - set up, mount/unmount, and delete a swap file. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopping dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 31 06:57:57 k8s-master1 systemd[1]: dphys-swapfile.service: Succeeded. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopped dphys-swapfile - set up, mount/unmount, and delete a swap file.","title":"swap\u3092\u7121\u52b9\u306b\u3059\u308b"},{"location":"setup/03_common_settings/#cgroupfs-memory","text":"kernel\u306eboot option\u306b cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory \u3092\u8ffd\u8a18\u3059\u308b sudo vim /boot/firmware/cmdline.txt cmdline.txt \u306e\u6709\u52b9\u884c\u306e\u78ba\u8a8d $ sudo sed -e 's/\\s/\\n/g' /boot/firmware/cmdline.txt console=serial0,115200 console=tty1 root=PARTUUID=fb7271c3-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait quiet splash plymouth.ignore-serial-consoles cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory $ sudo reboot $ cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 9 1 1 cpu 5 34 1 cpuacct 5 34 1 blkio 10 34 1 memory 8 80 1 devices 4 34 1 freezer 7 1 1 net_cls 2 1 1 perf_event 6 1 1 net_prio 2 1 1 pids 3 39 1","title":"cgroupfs \u306ememory\u3092\u6709\u52b9\u306b\u3059\u308b"},{"location":"setup/03_common_settings/#cri-o","text":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o kernel module\u306eload overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module overlay iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module br_netfilter cat <<EOF | sudo tee /etc/modules-load.d/crio.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter kernel parameter\u306eset iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf # https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF sudo sysctl --system system\u8d77\u52d5\u6642\u306b kernel parameter \u3092\u518d\u8aad\u307f\u8fbc\u307f\u3055\u305b\u308b kube-proxy\u306b\u3066\u5fc5\u8981\u306akernel parameter\u8a2d\u5b9a(kubelet\u8a2d\u5b9a\u624b\u9806\u306b\u3066\u5f8c\u8ff0) \u304ciptables\u8d77\u52d5\u6642\u306ekernel module load\u3067\u4e0a\u66f8\u304d\u3055\u308c\u308b\u305f\u3081 \u5229\u7528\u74b0\u5883\u304c /etc/sysconfig/iptables-config \u3092\u5229\u7528\u53ef\u80fd\u306a\u3089 IPTABLES_MODULES_UNLOAD=\"no\" \u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u672c\u624b\u9806\u306f\u4e0d\u8981\u3067\u3059 egrep \"sysctl\\s+--system\" /etc/rc.local > /dev/null || sudo bash -c \"echo \\\"sysctl --system\\\" >> /etc/rc.local\" egrep \"sysctl\\s+--system\" /etc/rc.local CRI-O\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.21/xUbuntu_20.04/arm64/ VERSION=1.21 OS=xUbuntu_20.04 sudo bash -c \"echo \\\"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\" sudo bash -c \"echo \\\"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list\" curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key add - curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key add - sudo apt update sudo apt install -y cri-o cri-o-runc sudo apt install -y conntrack storage driver\u3092 overlay2 \u3078\u5909\u66f4\u3059\u308b sudo vim /etc/containers/storage.conf sudo vim /etc/crio/crio.conf /etc/crio/crio.conf \u3078graph driver\u8a2d\u5b9a\u3092\u5165\u308c\u308b podman\u3084buildah\u3067build\u3057\u305flocal image\u3092\u53c2\u7167\u3059\u308b\u305f\u3081 [crio] \u30bb\u30af\u30b7\u30e7\u30f3\u306b\u5165\u308c\u308b graphroot = \"/var/lib/containers/storage\" /etc/containers/storage.conf [storage] driver = \"overlay2\" runroot = \"/run/containers/storage\" graphroot = \"/var/lib/containers/storage\" [storage.options] additionalimagestores = [ ] [storage.options.overlay] mountopt = \"nodev\" [storage.options.thinpool] /etc/crio/crio.conf [crio] storage_driver = \"overlay2\" graphroot = \"/var/lib/containers/storage\" log_dir = \"/var/log/crio/pods\" version_file = \"/var/run/crio/version\" version_file_persist = \"/var/lib/crio/version\" clean_shutdown_file = \"/var/lib/crio/clean.shutdown\" [crio.api] listen = \"/var/run/crio/crio.sock\" stream_address = \"127.0.0.1\" stream_port = \"0\" stream_enable_tls = false stream_idle_timeout = \"\" stream_tls_cert = \"\" stream_tls_key = \"\" stream_tls_ca = \"\" grpc_max_send_msg_size = 16777216 grpc_max_recv_msg_size = 16777216 [crio.runtime] no_pivot = false decryption_keys_path = \"/etc/crio/keys/\" conmon = \"\" conmon_cgroup = \"system.slice\" conmon_env = [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", ] default_env = [ ] seccomp_profile = \"\" seccomp_use_default_when_empty = false apparmor_profile = \"crio-default\" irqbalance_config_file = \"/etc/sysconfig/irqbalance\" cgroup_manager = \"systemd\" separate_pull_cgroup = \"\" default_capabilities = [ \"CHOWN\", \"DAC_OVERRIDE\", \"FSETID\", \"FOWNER\", \"SETGID\", \"SETUID\", \"SETPCAP\", \"NET_BIND_SERVICE\", \"KILL\", ] default_sysctls = [ ] additional_devices = [ ] hooks_dir = [ \"/usr/share/containers/oci/hooks.d\", ]pids_limit = 1024 log_size_max = -1 log_to_journald = false container_exits_dir = \"/var/run/crio/exits\" container_attach_socket_dir = \"/var/run/crio\" bind_mount_prefix = \"\" read_only = false log_level = \"info\" log_filter = \"\" uid_mappings = \"\" gid_mappings = \"\" ctr_stop_timeout = 30 drop_infra_ctr = false infra_ctr_cpuset = \"\" namespaces_dir = \"/var/run\" pinns_path = \"\" default_runtime = \"runc\" [crio.runtime.runtimes.runc] runtime_path = \"\" runtime_type = \"oci\" runtime_root = \"/run/runc\" allowed_annotations = [ \"io.containers.trace-syscall\", ] [crio.image] default_transport = \"docker://\" global_auth_file = \"\" pause_image = \"k8s.gcr.io/pause:3.2\" pause_image_auth_file = \"\" pause_command = \"/pause\" signature_policy = \"\" image_volumes = \"mkdir\" big_files_temporary_dir = \"\" [crio.network] network_dir = \"/etc/cni/net.d/\" plugin_dirs = [ \"/opt/cni/bin/\", ] [crio.metrics] enable_metrics = false metrics_port = 9090 metrics_socket = \"\" crio\u3092\u518d\u8d77\u52d5\u3059\u308b sudo systemctl daemon-reload sudo systemctl restart crio","title":"CRI-O \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/03_common_settings/#cli-toolbuildah-cri-tools","text":"buildah https://github.com/containers/buildah/blob/master/install.md sudo apt-get -qq -y install buildah cri-tools(crictl) \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md VERSION=\"v1.22.0\" ARCH=\"arm64\" DOWNLOAD_URL=\"https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-${VERSION}-linux-${ARCH}.tar.gz\" curl -L ${DOWNLOAD_URL} | sudo tar -zxvf -C /usr/local/bin ls -l /usr/local/bin podman \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(optional) https://podman.io/getting-started/installation sudo apt-get -y install podman sudo rm -f /etc/cni/net.d/87-podman-bridge.conflist","title":"CLI TOOL(buildah, cri-tools)"},{"location":"setup/03_common_settings/#cni-plugin","text":"https://github.com/containernetworking/plugins VERSION=\"v1.0.0\" ARCH=\"arm64\" DOWNLOAD_URL=\"https://github.com/containernetworking/plugins/releases/download/${VERSION}/cni-plugins-linux-${ARCH}-${VERSION}.tgz\" curl -L ${DOWNLOAD_URL} | sudo tar -zxvf -C /opt/cni/bin ls -l /opt/cni/bin/","title":"CNI Plugin\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/03_common_settings/#kubectl","text":"https://kubernetes.io/ja/docs/tasks/tools/install-kubectl/","title":"kubectl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/04_creation_certificate/","text":"\u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210 \u624b\u9806 cfssl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u8a3c\u660e\u66f8\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e cfssl \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b https://qiita.com/iaoiui/items/fc2ea829498402d4a8e3 https://coreos.com/os/docs/latest/generate-self-signed-certificates.html sudo apt install golang-cfssl CA(\u8a8d\u8a3c\u5c40) \u4f5c\u6210 cat << EOF > ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF cat << EOF > ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Sample\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d ca-key.pem ca.pem \u8a3c\u660e\u66f8\u306e\u4f5c\u6210 \u7ba1\u7406\u8005\u30e6\u30fc\u30b6 \u8a3c\u660e\u66f8 cat << EOF > admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d admin-key.pem admin.pem kubelet\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 EXTERNAL_IP master\u30b5\u30fc\u30d0\u306eIP\u30a2\u30c9\u30ec\u30b9 master\u304c\u8907\u6570\u30b5\u30fc\u30d0\u69cb\u6210\u306e\u5834\u5408\u306f\u4e0a\u4f4d\u306eLB IP for instance in k8s-master k8s-node1 k8s-node2; do cat << EOF > ${instance}-csr.json { \"CN\": \"system:node:${instance}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF EXTERNAL_IP=192.168.10.50 cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${instance},${EXTERNAL_IP} \\ -profile=kubernetes \\ ${instance}-csr.json | cfssljson -bare ${instance} done \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d k8s-node1-key.pem k8s-node1.pem k8s-node2-key.pem k8s-node2.pem kube-proxy\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 cat << EOF > kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-proxy-key.pem kube-proxy.pem kube-controller-manage\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 cat << EOF > kube-controller-manager-csr.json { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-controller-manager-key.pem kube-controller-manager.pem kube-scheduler\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 cat << EOF > kube-scheduler-csr.json { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-scheduler-key.pem kube-scheduler.pem kube-apiserver\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8 10.32.0.1 Cluster IP KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local cat << EOF > kubernetes-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=10.32.0.1,192.168.10.50,192.168.10.51,192.168.10.52,127.0.0.1,${KUBERNETES_HOSTNAMES} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kubernetes-key.pem kubernetes.pem service-account\u306e\u8a3c\u660e\u66f8 cat << EOF > service-account-csr.json { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d service-account-key.pem service-account.pem \u8a3c\u660e\u66f8\u3092master/node\u3078\u30b3\u30d4\u30fc\u3059\u308b master node \u53c2\u8003\u6587\u732e https://kubernetes.io/ja/docs/setup/best-practices/certificates/ https://kubernetes.io/ja/docs/concepts/cluster-administration/certificates/ https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf","title":"04. \u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210"},{"location":"setup/04_creation_certificate/#tls","text":"","title":"\u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210"},{"location":"setup/04_creation_certificate/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/04_creation_certificate/#cfssl","text":"\u8a3c\u660e\u66f8\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e cfssl \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b https://qiita.com/iaoiui/items/fc2ea829498402d4a8e3 https://coreos.com/os/docs/latest/generate-self-signed-certificates.html sudo apt install golang-cfssl","title":"cfssl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/04_creation_certificate/#ca","text":"cat << EOF > ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF cat << EOF > ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Sample\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d ca-key.pem ca.pem","title":"CA(\u8a8d\u8a3c\u5c40) \u4f5c\u6210"},{"location":"setup/04_creation_certificate/#_2","text":"","title":"\u8a3c\u660e\u66f8\u306e\u4f5c\u6210"},{"location":"setup/04_creation_certificate/#_3","text":"cat << EOF > admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d admin-key.pem admin.pem","title":"\u7ba1\u7406\u8005\u30e6\u30fc\u30b6 \u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kubelet","text":"EXTERNAL_IP master\u30b5\u30fc\u30d0\u306eIP\u30a2\u30c9\u30ec\u30b9 master\u304c\u8907\u6570\u30b5\u30fc\u30d0\u69cb\u6210\u306e\u5834\u5408\u306f\u4e0a\u4f4d\u306eLB IP for instance in k8s-master k8s-node1 k8s-node2; do cat << EOF > ${instance}-csr.json { \"CN\": \"system:node:${instance}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF EXTERNAL_IP=192.168.10.50 cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${instance},${EXTERNAL_IP} \\ -profile=kubernetes \\ ${instance}-csr.json | cfssljson -bare ${instance} done \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d k8s-node1-key.pem k8s-node1.pem k8s-node2-key.pem k8s-node2.pem","title":"kubelet\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-proxy","text":"cat << EOF > kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-proxy-key.pem kube-proxy.pem","title":"kube-proxy\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-controller-manage","text":"cat << EOF > kube-controller-manager-csr.json { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-controller-manager-key.pem kube-controller-manager.pem","title":"kube-controller-manage\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-scheduler","text":"cat << EOF > kube-scheduler-csr.json { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-scheduler-key.pem kube-scheduler.pem","title":"kube-scheduler\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-apiserver","text":"10.32.0.1 Cluster IP KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local cat << EOF > kubernetes-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=10.32.0.1,192.168.10.50,192.168.10.51,192.168.10.52,127.0.0.1,${KUBERNETES_HOSTNAMES} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kubernetes-key.pem kubernetes.pem","title":"kube-apiserver\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#service-account","text":"cat << EOF > service-account-csr.json { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d service-account-key.pem service-account.pem","title":"service-account\u306e\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#masternode","text":"master node","title":"\u8a3c\u660e\u66f8\u3092master/node\u3078\u30b3\u30d4\u30fc\u3059\u308b"},{"location":"setup/04_creation_certificate/#_4","text":"https://kubernetes.io/ja/docs/setup/best-practices/certificates/ https://kubernetes.io/ja/docs/concepts/cluster-administration/certificates/ https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/05_creating_config/","text":"\u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210 Controll Plane\u3068Node\u306e\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e .kubeconfig \u3092\u4f5c\u6210\u3059\u308b \u624b\u9806 kubelet KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 for instance in k8s-master k8s-node1 k8s-node2; do kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\ --client-certificate=${instance}.pem \\ --client-key=${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done kube-proxy KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig kube-controller-manager KUBE_API_SERVER_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig kube-scheduler KUBE_API_SERVER_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig admin KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig sudo mkdir -p /var/lib/kubernetes/ sudo cp admin.kubeconfig /var/lib/kubernetes/admin.kubeconfig \u53c2\u8003\u8cc7\u6599 https://github.com/kelseyhightower/kubernetes/blob/master/docs/05-kubernetes-configuration-files.md https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf https://h3poteto.hatenablog.com/entry/2020/08/20/180552 kubectl config set-cluster https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-cluster/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-cluster-em- kubectl config set-credentials https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-credentials/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-credentials-em- kubectl config set-context https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-context/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-context-em-","title":"05. \u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210"},{"location":"setup/05_creating_config/#kubeconfig","text":"Controll Plane\u3068Node\u306e\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e .kubeconfig \u3092\u4f5c\u6210\u3059\u308b","title":"\u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210"},{"location":"setup/05_creating_config/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/05_creating_config/#kubelet","text":"KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 for instance in k8s-master k8s-node1 k8s-node2; do kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\ --client-certificate=${instance}.pem \\ --client-key=${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done","title":"kubelet"},{"location":"setup/05_creating_config/#kube-proxy","text":"KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig","title":"kube-proxy"},{"location":"setup/05_creating_config/#kube-controller-manager","text":"KUBE_API_SERVER_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig","title":"kube-controller-manager"},{"location":"setup/05_creating_config/#kube-scheduler","text":"KUBE_API_SERVER_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig","title":"kube-scheduler"},{"location":"setup/05_creating_config/#admin","text":"KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig sudo mkdir -p /var/lib/kubernetes/ sudo cp admin.kubeconfig /var/lib/kubernetes/admin.kubeconfig","title":"admin"},{"location":"setup/05_creating_config/#_2","text":"https://github.com/kelseyhightower/kubernetes/blob/master/docs/05-kubernetes-configuration-files.md https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf https://h3poteto.hatenablog.com/entry/2020/08/20/180552 kubectl config set-cluster https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-cluster/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-cluster-em- kubectl config set-credentials https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-credentials/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-credentials-em- kubectl config set-context https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-context/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-context-em-","title":"\u53c2\u8003\u8cc7\u6599"},{"location":"setup/06_master/01_bootstrapping_kubelet/","text":"bootstrapping kubelet(master/worker \u5171\u901a) kubelet \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002 worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206 Reserve Compute Resources for System Daemons https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9 = Node resource - system-reserved - kube-reserved - eviction-threshold \u3089\u3057\u3044 name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi \u624b\u9806 kubelet \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 VERSION=\"v1.22.0\" ARCH=\"arm64\" sudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet sudo chmod +x /usr/bin/kubelet kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" sudo install -o root -g root -m 755 -d /etc/kubelet.d sudo install -o root -g root -m 755 -d /var/lib/kubernetes sudo install -o root -g root -m 755 -d /var/lib/kubelet sudo cp ca.pem /var/lib/kubernetes/ sudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/ sudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig /var/lib/kubelet/kubelet-config.yaml \u3092\u4f5c\u6210\u3059\u308b clusterDNS \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b podCIDR \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 # https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ staticPodPath: /etc/kubelet.d # kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f # - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b # - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981 authentication: anonymous: enabled: true webhook: enabled: false cacheTTL: \"2m\" x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" # kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a # - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow # - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981 authorization: mode: AlwaysAllow clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" podCIDR: \"10.200.0.0/24\" resolvConf: \"/etc/resolv.conf\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${host}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\" # Reserve Compute Resources for System Daemons # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # # Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044 # # system-reserved # - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b # # kube-reserved # - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b enforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"] cgroupsPerQOS: true cgroupDriver: systemd cgroupRoot: / systemCgroups: /systemd/system.slice systemReservedCgroup: /systemd/system.slice systemReserved: cpu: 256m memory: 256Mi runtimeCgroups: /kube.slice/crio.service kubeletCgroups: /kube.slice/kubelet.service kubeReservedCgroup: /kube.slice kubeReserved: cpu: 256m memory: 256Mi EOF /etc/systemd/system/kubelet.service \u3092\u914d\u7f6e cat << 'EOF' | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=crio.service Requires=crio.service [Service] Restart=on-failure RestartSec=5 ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice ExecStart=/usr/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --container-runtime=remote \\ --container-runtime-endpoint=/var/run/crio/crio.sock \\ --register-node=true \\ --v=2 [Install] WantedBy=multi-user.target EOF kubelet.service \u3092\u8d77\u52d5 sudo systemctl enable kubelet.service sudo systemctl start kubelet.service \u30a8\u30e9\u30fc\u4e8b\u4f8b cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408 kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist kubelet \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( --v 10 ) cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice] \u5bfe\u5fdc kubelet.service \u306e ExecStartPre \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f \u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy kubeconfig \u306e\u8a3c\u660e\u66f8\u306e CN \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b 360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\" kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: k8s-master \u304c\u6b63\u3057\u3044\u306e\u306b CN = system:node:k8s-node1 \u3068\u306a\u3063\u3066\u3044\u305f root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master Node \u30ea\u30bd\u30fc\u30b9\u306e spec.podCIDR \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044 \u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044 flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f... kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u30ed\u30b0 Set node k8s-node1 PodCIDR to [10.200.0.0/24] \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8 kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b --allocate-node-cidrs=true \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist range_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24] ttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\" controller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}] controller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes node_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\" controller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1 node_lifecycle_controller.go:1429] Initializing eviction metric for zone: node_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp. event.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\" node_lifecycle_controller.go:1245] Controller detected that zone is now in state Normal. Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044 I0214 07:03:56.822586 1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\" F0214 07:03:56.822637 1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication goroutine 1 [running]: https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication CNI Plugin\u3092 /etc/cni/net.d \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044 kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized CNI Plugin\u3092 /etc/cni/net.d \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b https://github.com/containernetworking/plugins/releases Kubelet cannot determine CPU online state sysinfo.go:203] Nodes topology is not available, providing CPU topology sysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory gce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:72] Cannot read number of physical cores correctly, number of cores set to 0 machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:86] Cannot read number of sockets correctly, number of sockets set to 0 container_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name: \u65e2\u77e5\u3089\u3057\u3044 https://github.com/kubernetes/kubernetes/issues/95039 \u53c2\u8003 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go https://cyberagent.ai/blog/tech/4036/ kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b https://downloadkubernetes.com/ https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/ Node Authorization https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822 https://kubernetes.io/docs/reference/access-authn-authz/node/ https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ static pod https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/policy/pod-security-policy/ https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver PodSecurityPolicy \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf( false \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f true \u306b\u76f4\u3059) https://github.com/kubernetes/kubernetes/issues/70952","title":"01. bootstrapping kubelet"},{"location":"setup/06_master/01_bootstrapping_kubelet/#bootstrapping-kubeletmasterworker","text":"kubelet \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002","title":"bootstrapping kubelet(master/worker \u5171\u901a)"},{"location":"setup/06_master/01_bootstrapping_kubelet/#worker-node","text":"Reserve Compute Resources for System Daemons https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9 = Node resource - system-reserved - kube-reserved - eviction-threshold \u3089\u3057\u3044 name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi","title":"worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubelet","text":"VERSION=\"v1.22.0\" ARCH=\"arm64\" sudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet sudo chmod +x /usr/bin/kubelet","title":"kubelet \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeconfig","text":"# host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" sudo install -o root -g root -m 755 -d /etc/kubelet.d sudo install -o root -g root -m 755 -d /var/lib/kubernetes sudo install -o root -g root -m 755 -d /var/lib/kubelet sudo cp ca.pem /var/lib/kubernetes/ sudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/ sudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig","title":"kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e"},{"location":"setup/06_master/01_bootstrapping_kubelet/#varlibkubeletkubelet-configyaml","text":"clusterDNS \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b podCIDR \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 # https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ staticPodPath: /etc/kubelet.d # kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f # - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b # - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981 authentication: anonymous: enabled: true webhook: enabled: false cacheTTL: \"2m\" x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" # kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a # - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow # - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981 authorization: mode: AlwaysAllow clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" podCIDR: \"10.200.0.0/24\" resolvConf: \"/etc/resolv.conf\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${host}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\" # Reserve Compute Resources for System Daemons # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # # Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044 # # system-reserved # - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b # # kube-reserved # - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b enforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"] cgroupsPerQOS: true cgroupDriver: systemd cgroupRoot: / systemCgroups: /systemd/system.slice systemReservedCgroup: /systemd/system.slice systemReserved: cpu: 256m memory: 256Mi runtimeCgroups: /kube.slice/crio.service kubeletCgroups: /kube.slice/kubelet.service kubeReservedCgroup: /kube.slice kubeReserved: cpu: 256m memory: 256Mi EOF","title":"/var/lib/kubelet/kubelet-config.yaml \u3092\u4f5c\u6210\u3059\u308b"},{"location":"setup/06_master/01_bootstrapping_kubelet/#etcsystemdsystemkubeletservice","text":"cat << 'EOF' | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=crio.service Requires=crio.service [Service] Restart=on-failure RestartSec=5 ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice ExecStart=/usr/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --container-runtime=remote \\ --container-runtime-endpoint=/var/run/crio/crio.sock \\ --register-node=true \\ --v=2 [Install] WantedBy=multi-user.target EOF","title":"/etc/systemd/system/kubelet.service \u3092\u914d\u7f6e"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeletservice","text":"sudo systemctl enable kubelet.service sudo systemctl start kubelet.service","title":"kubelet.service \u3092\u8d77\u52d5"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_2","text":"","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cgroup","text":"kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist kubelet \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( --v 10 ) cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice] \u5bfe\u5fdc kubelet.service \u306e ExecStartPre \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice","title":"cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cgroupsystemreserved-memory-size","text":"\u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy","title":"cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeconfig-cn-node","text":"360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\" kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: k8s-master \u304c\u6b63\u3057\u3044\u306e\u306b CN = system:node:k8s-node1 \u3068\u306a\u3063\u3066\u3044\u305f root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master","title":"kubeconfig \u306e\u8a3c\u660e\u66f8\u306e CN \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b"},{"location":"setup/06_master/01_bootstrapping_kubelet/#node-specpodcidr-cidr","text":"\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044 flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f... kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u30ed\u30b0 Set node k8s-node1 PodCIDR to [10.200.0.0/24] \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8 kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b --allocate-node-cidrs=true \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist range_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24] ttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\" controller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}] controller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes node_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\" controller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1 node_lifecycle_controller.go:1429] Initializing eviction metric for zone: node_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp. event.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\" node_lifecycle_controller.go:1245] Controller detected that zone is now in state Normal.","title":"Node \u30ea\u30bd\u30fc\u30b9\u306e spec.podCIDR \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044"},{"location":"setup/06_master/01_bootstrapping_kubelet/#webhook-authentication","text":"I0214 07:03:56.822586 1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\" F0214 07:03:56.822637 1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication goroutine 1 [running]: https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication","title":"Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cni-plugin-etccninetd-cni-plugin","text":"kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized CNI Plugin\u3092 /etc/cni/net.d \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b https://github.com/containernetworking/plugins/releases","title":"CNI Plugin\u3092 /etc/cni/net.d \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubelet-cannot-determine-cpu-online-state","text":"sysinfo.go:203] Nodes topology is not available, providing CPU topology sysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory gce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:72] Cannot read number of physical cores correctly, number of cores set to 0 machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:86] Cannot read number of sockets correctly, number of sockets set to 0 container_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name: \u65e2\u77e5\u3089\u3057\u3044 https://github.com/kubernetes/kubernetes/issues/95039","title":"Kubelet cannot determine CPU online state"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_3","text":"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go https://cyberagent.ai/blog/tech/4036/ kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b https://downloadkubernetes.com/ https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/ Node Authorization https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822 https://kubernetes.io/docs/reference/access-authn-authz/node/ https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ static pod https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/policy/pod-security-policy/ https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver PodSecurityPolicy \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf( false \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f true \u306b\u76f4\u3059) https://github.com/kubernetes/kubernetes/issues/70952","title":"\u53c2\u8003"},{"location":"setup/06_master/02_bootstrapping_etcd/","text":"bootstrapping etcd coreos\u304cetcd docker image \u3092\u63d0\u4f9b \u3057\u3066\u3044\u307e\u3059\u304c\u3001Raspberry Pi\u306b\u642d\u8f09\u3055\u308c\u3066\u3044\u308bARM CPU\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3 armv8(64bit) \u3067\u5229\u7528\u53ef\u80fd\u306aimage\u306f\u63d0\u4f9b\u3057\u3066\u3044\u306a\u3044\u305f\u3081\u3001image\u3092build\u3057\u307e\u3059\u3002 \u624b\u9806 Dockerfile_etcd.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_etcd.armhf cat << 'EOF' > Dockerfile_etcd.armhf FROM arm64v8/ubuntu:bionic AS etcd-builder RUN set -ex \\ && apt update \\ && apt install -y git tar zip \\ && apt clean RUN set -ex \\ && curl -L https://golang.org/dl/go1.17.linux-arm64.tar.gz | tar -zxvf -C /usr/local RUN set -ex \\ && git clone https://github.com/etcd-io/etcd.git /tmp/etcd\\ && cd /tmp/etcd \\ && PATH=$PATH:/usr/local/go/bin:~/go/bin ./build FROM arm64v8/ubuntu:bionic COPY --from=etcd-builder /tmp/etcd/bin/etcd /usr/local/bin/ COPY --from=etcd-builder /tmp/etcd/bin/etcdctl /usr/local/bin/ RUN set -ex \\ && apt update \\ && apt clean \\ && install -o root -g root -m 700 -d /var/lib/etcd \\ && install -o root -g root -m 644 -d /etc/etcd COPY ca.pem /etc/etcd/ COPY kubernetes-key.pem /etc/etcd/ COPY kubernetes.pem /etc/etcd/ ENV ETCD_UNSUPPORTED_ARCH=arm64 EXPOSE 2379 2380 ENTRYPOINT [\"/usr/local/bin/etcd\"] EOF image build sudo buildah bud -t k8s-etcd --file=Dockerfile_etcd.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/etcd.yaml cat << EOF | sudo tee /etc/kubelet.d/etcd.yaml --- apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.10.50:2379 name: etcd namespace: kube-system labels: tier: control-plane component: etcd spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: etcd image: localhost/k8s-etcd:latest imagePullPolicy: IfNotPresent env: - name: ETCD_UNSUPPORTED_ARCH value: \"arm64\" resources: requests: cpu: 0.5 memory: \"384Mi\" limits: cpu: 1 memory: \"384Mi\" command: - /usr/local/bin/etcd - --advertise-client-urls=https://192.168.10.50:2379,https://192.168.10.50:2380 - --listen-client-urls=https://0.0.0.0:2379 - --initial-advertise-peer-urls=https://192.168.10.50:2380 - --listen-peer-urls=https://0.0.0.0:2380 - --name=etcd0 - --cert-file=/etc/etcd/kubernetes.pem - --key-file=/etc/etcd/kubernetes-key.pem - --peer-cert-file=/etc/etcd/kubernetes.pem - --peer-key-file=/etc/etcd/kubernetes-key.pem - --trusted-ca-file=/etc/etcd/ca.pem - --peer-trusted-ca-file=/etc/etcd/ca.pem - --peer-client-cert-auth - --client-cert-auth - --initial-cluster-token=etcd-cluster-1 - --initial-cluster=etcd0=https://192.168.10.50:2380 - --initial-cluster-state=new EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name etcd CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 72f58248ec087 6e8b8110dc13cfe61d75f867a22c39766a397989413570500f51dedf94be7a12 25 seconds ago Running etcd 0 206c5b952097a \u53c2\u8003\u6587\u732e https://etcd.io/docs/v2/docker_guide/ https://quay.io/repository/coreos/etcd?tag=latest&tab=tags https://github.com/etcd-io/etcd","title":"02. bootstrapping etcd"},{"location":"setup/06_master/02_bootstrapping_etcd/#bootstrapping-etcd","text":"coreos\u304cetcd docker image \u3092\u63d0\u4f9b \u3057\u3066\u3044\u307e\u3059\u304c\u3001Raspberry Pi\u306b\u642d\u8f09\u3055\u308c\u3066\u3044\u308bARM CPU\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3 armv8(64bit) \u3067\u5229\u7528\u53ef\u80fd\u306aimage\u306f\u63d0\u4f9b\u3057\u3066\u3044\u306a\u3044\u305f\u3081\u3001image\u3092build\u3057\u307e\u3059\u3002","title":"bootstrapping etcd"},{"location":"setup/06_master/02_bootstrapping_etcd/#_1","text":"Dockerfile_etcd.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_etcd.armhf cat << 'EOF' > Dockerfile_etcd.armhf FROM arm64v8/ubuntu:bionic AS etcd-builder RUN set -ex \\ && apt update \\ && apt install -y git tar zip \\ && apt clean RUN set -ex \\ && curl -L https://golang.org/dl/go1.17.linux-arm64.tar.gz | tar -zxvf -C /usr/local RUN set -ex \\ && git clone https://github.com/etcd-io/etcd.git /tmp/etcd\\ && cd /tmp/etcd \\ && PATH=$PATH:/usr/local/go/bin:~/go/bin ./build FROM arm64v8/ubuntu:bionic COPY --from=etcd-builder /tmp/etcd/bin/etcd /usr/local/bin/ COPY --from=etcd-builder /tmp/etcd/bin/etcdctl /usr/local/bin/ RUN set -ex \\ && apt update \\ && apt clean \\ && install -o root -g root -m 700 -d /var/lib/etcd \\ && install -o root -g root -m 644 -d /etc/etcd COPY ca.pem /etc/etcd/ COPY kubernetes-key.pem /etc/etcd/ COPY kubernetes.pem /etc/etcd/ ENV ETCD_UNSUPPORTED_ARCH=arm64 EXPOSE 2379 2380 ENTRYPOINT [\"/usr/local/bin/etcd\"] EOF image build sudo buildah bud -t k8s-etcd --file=Dockerfile_etcd.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/etcd.yaml cat << EOF | sudo tee /etc/kubelet.d/etcd.yaml --- apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.10.50:2379 name: etcd namespace: kube-system labels: tier: control-plane component: etcd spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: etcd image: localhost/k8s-etcd:latest imagePullPolicy: IfNotPresent env: - name: ETCD_UNSUPPORTED_ARCH value: \"arm64\" resources: requests: cpu: 0.5 memory: \"384Mi\" limits: cpu: 1 memory: \"384Mi\" command: - /usr/local/bin/etcd - --advertise-client-urls=https://192.168.10.50:2379,https://192.168.10.50:2380 - --listen-client-urls=https://0.0.0.0:2379 - --initial-advertise-peer-urls=https://192.168.10.50:2380 - --listen-peer-urls=https://0.0.0.0:2380 - --name=etcd0 - --cert-file=/etc/etcd/kubernetes.pem - --key-file=/etc/etcd/kubernetes-key.pem - --peer-cert-file=/etc/etcd/kubernetes.pem - --peer-key-file=/etc/etcd/kubernetes-key.pem - --trusted-ca-file=/etc/etcd/ca.pem - --peer-trusted-ca-file=/etc/etcd/ca.pem - --peer-client-cert-auth - --client-cert-auth - --initial-cluster-token=etcd-cluster-1 - --initial-cluster=etcd0=https://192.168.10.50:2380 - --initial-cluster-state=new EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name etcd CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 72f58248ec087 6e8b8110dc13cfe61d75f867a22c39766a397989413570500f51dedf94be7a12 25 seconds ago Running etcd 0 206c5b952097a","title":"\u624b\u9806"},{"location":"setup/06_master/02_bootstrapping_etcd/#_2","text":"https://etcd.io/docs/v2/docker_guide/ https://quay.io/repository/coreos/etcd?tag=latest&tab=tags https://github.com/etcd-io/etcd","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/","text":"bootstrapping kube-apiserver \u624b\u9806 Dockerfile_kube-apiserver.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-apiserver.armhf cat << 'EOF' > Dockerfile_kube-apiserver.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget --quiet -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-apiserver \\ && chmod +x /usr/bin/kube-apiserver \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config \\ && install -o root -g root -m 755 -d /etc/kubernetes/webhook COPY ca.pem \\ ca-key.pem \\ kubernetes-key.pem \\ kubernetes.pem \\ service-account-key.pem \\ service-account.pem \\ encryption-config.yaml \\ /var/lib/kubernetes/ COPY authorization-config.yaml /etc/kubernetes/webhook/ EXPOSE 6443 ENTRYPOINT [\"/usr/bin/kube-apiserver\"] EOF encryption-provider-config \u3092\u4f5c\u6210\u3059\u308b --encryption-provider-config \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u3066secret\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u6697\u53f7\u5316\u3059\u308b\u305f\u3081\u306e\u9375\u3092\u5b9a\u7fa9\u3059\u308b https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#encrypting-your-data https://access.redhat.com/documentation/ja-jp/openshift_container_platform/3.11/html/cluster_administration/admin-guide-encrypting-data-at-datastore encryption-config.yaml ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat << EOF > encryption-config.yaml --- kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF webbhook config\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b --authorization-webhook-config-file \u3067\u6307\u5b9a\u3059\u308b\u30d5\u30a1\u30a4\u30eb authorization-config.yaml KUBE_API_SERVER_ADDRESS=192.168.10.50 cat << EOF > authorization-config.yaml --- apiVersion: v1 # kind of the API object kind: Config # clusters refers to the remote service. clusters: - name: kubernetes cluster: certificate-authority: /var/lib/kubernetes/ca.pem # CA for verifying the remote service. server: https://${KUBE_API_SERVER_ADDRESS}:6443/authenticate # URL of remote service to query. Must use 'https'. # users refers to the API server's webhook configuration. users: - name: api-server-webhook user: client-certificate: /var/lib/kubernetes/kubernetes.pem # cert for the webhook plugin to use client-key: /var/lib/kubernetes/kubernetes-key.pem # key matching the cert # kubeconfig files require a context. Provide one for the API server. current-context: webhook contexts: - context: cluster: kubernetes user: api-server-webhook name: webhook EOF image build sudo buildah bud -t k8s-kube-apiserver --file=Dockerfile_kube-apiserver.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/kube-api-server.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-api-server.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system annotations: seccomp.security.alpha.kubernetes.io/pod: runtime/default labels: tier: control-plane component: kube-apiserver spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-apiserver image: localhost/k8s-kube-apiserver:latest imagePullPolicy: IfNotPresent resources: requests: memory: \"512Mi\" limits: memory: \"1024Mi\" command: - /usr/bin/kube-apiserver - --advertise-address=192.168.10.50 - --allow-privileged=true - --anonymous-auth=false - --apiserver-count=1 - --audit-log-maxage=30 - --audit-log-maxbackup=3 - --audit-log-maxsize=100 - --audit-log-path=/var/log/audit.log - --authorization-mode=Node,RBAC,Webhook - --authorization-webhook-config-file=/etc/kubernetes/webhook/authorization-config.yaml - --authentication-token-webhook-cache-ttl=2m - --authentication-token-webhook-version=v1 - --bind-address=0.0.0.0 - --client-ca-file=/var/lib/kubernetes/ca.pem - --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,RuntimeClass - --etcd-cafile=/var/lib/kubernetes/ca.pem - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem - --etcd-servers=https://192.168.10.50:2379 - --event-ttl=1h - --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml - --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem - --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem - --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem - --kubelet-https=true - --runtime-config=authentication.k8s.io/v1beta1=true - --feature-gates=APIPriorityAndFairness=false - --service-account-key-file=/var/lib/kubernetes/service-account.pem - --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem - --service-account-issuer=api - --service-account-api-audiences=api - --service-cluster-ip-range=10.32.0.0/24 - --service-node-port-range=30000-32767 - --tls-cert-file=/var/lib/kubernetes/kubernetes.pem - --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem - --http2-max-streams-per-connection=3000 - --max-requests-inflight=3000 - --max-mutating-requests-inflight=1000 - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-apiserver CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 82c371fd9d99e 83e685a0b921ef5dd91eb3cdf208ba70690c1dd7decfc39bb3903be6ede752e6 24 seconds ago Running kube-apiserver 0 6af4d1b99fa37 master node\u306bPod\u304cschedule\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u308b taint\u3092\u8a2d\u5b9a\u3059\u308b https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/ https://kubernetes.io/ja/docs/concepts/scheduling-eviction/taint-and-toleration/ kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule $ kubectl get node k8s-master -o=jsonpath='{.spec.taints}' [{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\"}] \u30a8\u30e9\u30fc\u4e8b\u4f8b failed creating mandatory flowcontrol settings: failed getting mandatory FlowSchema exempt due to the server was unable to return a response in the time allotted, but may still be processing the request https://github.com/kubernetes/kubernetes/issues/97525#issuecomment-753022219 kube-apiserver\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u4ee5\u4e0b2\u3064\u3092\u4ed8\u52a0\u3059\u308b --feature-gates=APIPriorityAndFairness=false --runtime-config=flowcontrol.apiserver.k8s.io/v1beta1=false failed to run Kubelet: no client provided, cannot use webhook authentication kubelet\u304cWebhook\u8a8d\u8a3c\u3092\u671f\u5f85\u3057\u3066\u3044\u308b\u306e\u306bkube-api-server\u3067Webhook\u8a8d\u8a3c\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408 Webhook\u8a8d\u8a3c\u3092\u6709\u52b9\u306b\u3059\u308b https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication Failed creating a mirror pod for \"kube-scheduler-k8s-master_kube-system(a4a914cd05761a5a4335e2510ca075aa)\": pods \"kube-scheduler-k8s-master\" is forbidden: PodSecurityPolicy: no providers available to validate pod request StaticPod\u3092\u8d77\u52d5\u3057\u305f\u969b\u306bkubelet\u304b\u3089kube-apiserver\u3078mirror pod\u60c5\u5831\u3092\u767b\u9332\u3057\u3088\u3046\u3068\u3057\u3066PodSecurityPolicy\u306b\u3088\u308a\u62d2\u5426\u3055\u308c\u305f \u53c2\u8003\u6587\u732e https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/ old default\u306ePodSecurityPolicy(PSP)\u3092\u4f5c\u6210\u3059\u308b staticPod \u3092\u4f5c\u6210\u3059\u308b\u969b\u306bkubelet\u304b\u3089mirror pod\u4f5c\u6210\u30ea\u30af\u30a8\u30b9\u30c8\u304c\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3059 ( \u53c2\u8003 ) PSP / ClusterRole / ClusterRoleBinding cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: annotations: apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default' name: default spec: # allowedCapabilities: [] # default set of capabilities are implicitly allowed allowedCapabilities: - '*' # - NET_ADMIN # - NET_RAW # - SYS_ADMIN fsGroup: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 hostIPC: true hostNetwork: true hostPID: true privileged: true allowPrivilegeEscalation: true readOnlyRootFilesystem: true runAsUser: rule: 'MustRunAsNonRoot' seLinux: rule: 'RunAsNonRoot' supplementalGroups: rule: 'RunAsNonRoot' ranges: # Forbid adding the root group. - min: 1 max: 65535 volumes: - 'configMap' - 'downwardAPI' - 'emptyDir' - 'persistentVolumeClaim' - 'projected' - 'secret' - 'hostPath' hostNetwork: true runAsUser: rule: 'RunAsAny' seLinux: rule: 'RunAsAny' supplementalGroups: rule: 'RunAsAny' fsGroup: rule: 'RunAsAny' --- # Cluster role which grants access to the default pod security policy apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: default-psp rules: - apiGroups: - policy resourceNames: - default resources: - podsecuritypolicies verbs: - use --- # Cluster role binding for default pod security policy granting all authenticated users access apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default-psp roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: default-psp subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:authenticated EOF $ cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f - <\u7701\u7565> podsecuritypolicy.policy/default created clusterrole.rbac.authorization.k8s.io/default-psp created clusterrolebinding.rbac.authorization.k8s.io/default-psp created","title":"03. bootstrapping kube-apiserver"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#bootstrapping-kube-apiserver","text":"","title":"bootstrapping kube-apiserver"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_1","text":"Dockerfile_kube-apiserver.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-apiserver.armhf cat << 'EOF' > Dockerfile_kube-apiserver.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget --quiet -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-apiserver \\ && chmod +x /usr/bin/kube-apiserver \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config \\ && install -o root -g root -m 755 -d /etc/kubernetes/webhook COPY ca.pem \\ ca-key.pem \\ kubernetes-key.pem \\ kubernetes.pem \\ service-account-key.pem \\ service-account.pem \\ encryption-config.yaml \\ /var/lib/kubernetes/ COPY authorization-config.yaml /etc/kubernetes/webhook/ EXPOSE 6443 ENTRYPOINT [\"/usr/bin/kube-apiserver\"] EOF encryption-provider-config \u3092\u4f5c\u6210\u3059\u308b --encryption-provider-config \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u3066secret\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u6697\u53f7\u5316\u3059\u308b\u305f\u3081\u306e\u9375\u3092\u5b9a\u7fa9\u3059\u308b https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#encrypting-your-data https://access.redhat.com/documentation/ja-jp/openshift_container_platform/3.11/html/cluster_administration/admin-guide-encrypting-data-at-datastore encryption-config.yaml ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat << EOF > encryption-config.yaml --- kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF webbhook config\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b --authorization-webhook-config-file \u3067\u6307\u5b9a\u3059\u308b\u30d5\u30a1\u30a4\u30eb authorization-config.yaml KUBE_API_SERVER_ADDRESS=192.168.10.50 cat << EOF > authorization-config.yaml --- apiVersion: v1 # kind of the API object kind: Config # clusters refers to the remote service. clusters: - name: kubernetes cluster: certificate-authority: /var/lib/kubernetes/ca.pem # CA for verifying the remote service. server: https://${KUBE_API_SERVER_ADDRESS}:6443/authenticate # URL of remote service to query. Must use 'https'. # users refers to the API server's webhook configuration. users: - name: api-server-webhook user: client-certificate: /var/lib/kubernetes/kubernetes.pem # cert for the webhook plugin to use client-key: /var/lib/kubernetes/kubernetes-key.pem # key matching the cert # kubeconfig files require a context. Provide one for the API server. current-context: webhook contexts: - context: cluster: kubernetes user: api-server-webhook name: webhook EOF image build sudo buildah bud -t k8s-kube-apiserver --file=Dockerfile_kube-apiserver.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/kube-api-server.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-api-server.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system annotations: seccomp.security.alpha.kubernetes.io/pod: runtime/default labels: tier: control-plane component: kube-apiserver spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-apiserver image: localhost/k8s-kube-apiserver:latest imagePullPolicy: IfNotPresent resources: requests: memory: \"512Mi\" limits: memory: \"1024Mi\" command: - /usr/bin/kube-apiserver - --advertise-address=192.168.10.50 - --allow-privileged=true - --anonymous-auth=false - --apiserver-count=1 - --audit-log-maxage=30 - --audit-log-maxbackup=3 - --audit-log-maxsize=100 - --audit-log-path=/var/log/audit.log - --authorization-mode=Node,RBAC,Webhook - --authorization-webhook-config-file=/etc/kubernetes/webhook/authorization-config.yaml - --authentication-token-webhook-cache-ttl=2m - --authentication-token-webhook-version=v1 - --bind-address=0.0.0.0 - --client-ca-file=/var/lib/kubernetes/ca.pem - --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,RuntimeClass - --etcd-cafile=/var/lib/kubernetes/ca.pem - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem - --etcd-servers=https://192.168.10.50:2379 - --event-ttl=1h - --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml - --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem - --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem - --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem - --kubelet-https=true - --runtime-config=authentication.k8s.io/v1beta1=true - --feature-gates=APIPriorityAndFairness=false - --service-account-key-file=/var/lib/kubernetes/service-account.pem - --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem - --service-account-issuer=api - --service-account-api-audiences=api - --service-cluster-ip-range=10.32.0.0/24 - --service-node-port-range=30000-32767 - --tls-cert-file=/var/lib/kubernetes/kubernetes.pem - --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem - --http2-max-streams-per-connection=3000 - --max-requests-inflight=3000 - --max-mutating-requests-inflight=1000 - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-apiserver CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 82c371fd9d99e 83e685a0b921ef5dd91eb3cdf208ba70690c1dd7decfc39bb3903be6ede752e6 24 seconds ago Running kube-apiserver 0 6af4d1b99fa37 master node\u306bPod\u304cschedule\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u308b taint\u3092\u8a2d\u5b9a\u3059\u308b https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/ https://kubernetes.io/ja/docs/concepts/scheduling-eviction/taint-and-toleration/ kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule $ kubectl get node k8s-master -o=jsonpath='{.spec.taints}' [{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\"}]","title":"\u624b\u9806"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_2","text":"failed creating mandatory flowcontrol settings: failed getting mandatory FlowSchema exempt due to the server was unable to return a response in the time allotted, but may still be processing the request https://github.com/kubernetes/kubernetes/issues/97525#issuecomment-753022219 kube-apiserver\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u4ee5\u4e0b2\u3064\u3092\u4ed8\u52a0\u3059\u308b --feature-gates=APIPriorityAndFairness=false --runtime-config=flowcontrol.apiserver.k8s.io/v1beta1=false failed to run Kubelet: no client provided, cannot use webhook authentication kubelet\u304cWebhook\u8a8d\u8a3c\u3092\u671f\u5f85\u3057\u3066\u3044\u308b\u306e\u306bkube-api-server\u3067Webhook\u8a8d\u8a3c\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408 Webhook\u8a8d\u8a3c\u3092\u6709\u52b9\u306b\u3059\u308b https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication Failed creating a mirror pod for \"kube-scheduler-k8s-master_kube-system(a4a914cd05761a5a4335e2510ca075aa)\": pods \"kube-scheduler-k8s-master\" is forbidden: PodSecurityPolicy: no providers available to validate pod request StaticPod\u3092\u8d77\u52d5\u3057\u305f\u969b\u306bkubelet\u304b\u3089kube-apiserver\u3078mirror pod\u60c5\u5831\u3092\u767b\u9332\u3057\u3088\u3046\u3068\u3057\u3066PodSecurityPolicy\u306b\u3088\u308a\u62d2\u5426\u3055\u308c\u305f","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_3","text":"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#old","text":"default\u306ePodSecurityPolicy(PSP)\u3092\u4f5c\u6210\u3059\u308b staticPod \u3092\u4f5c\u6210\u3059\u308b\u969b\u306bkubelet\u304b\u3089mirror pod\u4f5c\u6210\u30ea\u30af\u30a8\u30b9\u30c8\u304c\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3059 ( \u53c2\u8003 ) PSP / ClusterRole / ClusterRoleBinding cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: annotations: apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default' name: default spec: # allowedCapabilities: [] # default set of capabilities are implicitly allowed allowedCapabilities: - '*' # - NET_ADMIN # - NET_RAW # - SYS_ADMIN fsGroup: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 hostIPC: true hostNetwork: true hostPID: true privileged: true allowPrivilegeEscalation: true readOnlyRootFilesystem: true runAsUser: rule: 'MustRunAsNonRoot' seLinux: rule: 'RunAsNonRoot' supplementalGroups: rule: 'RunAsNonRoot' ranges: # Forbid adding the root group. - min: 1 max: 65535 volumes: - 'configMap' - 'downwardAPI' - 'emptyDir' - 'persistentVolumeClaim' - 'projected' - 'secret' - 'hostPath' hostNetwork: true runAsUser: rule: 'RunAsAny' seLinux: rule: 'RunAsAny' supplementalGroups: rule: 'RunAsAny' fsGroup: rule: 'RunAsAny' --- # Cluster role which grants access to the default pod security policy apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: default-psp rules: - apiGroups: - policy resourceNames: - default resources: - podsecuritypolicies verbs: - use --- # Cluster role binding for default pod security policy granting all authenticated users access apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default-psp roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: default-psp subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:authenticated EOF $ cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f - <\u7701\u7565> podsecuritypolicy.policy/default created clusterrole.rbac.authorization.k8s.io/default-psp created clusterrolebinding.rbac.authorization.k8s.io/default-psp created","title":"old"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/","text":"bootstrapping kube-controller-manager \u624b\u9806 Dockerfile_kube-controller-manager.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-controller-manager.armhf cat << 'EOF' > Dockerfile_kube-controller-manager.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-controller-manager \\ && chmod +x /usr/bin/kube-controller-manager \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY ca.pem \\ ca-key.pem \\ service-account-key.pem \\ kube-controller-manager.kubeconfig \\ /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-controller-manager\"] EOF image build sudo buildah bud -t k8s-kube-controller-manager --file=Dockerfile_kube-controller-manager.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b --allocate-node-cidrs=true Node resource\u306e spec.podCIDR \u3078CIDR\u304c\u8a2d\u5b9a\u3055\u308c\u308b kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' spec.podCIDR \u306e\u5024\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044node instance\u3067\u306fCNI Plugin(flannel)\u304c\u6b63\u5e38\u52d5\u4f5c\u3057\u306a\u304b\u3063\u305f /etc/kubelet.d/kube-controller-manager.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-controller-manager.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-controller-manager namespace: kube-system labels: tier: control-plane component: kube-controller-manager spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-controller-manager image: localhost/k8s-kube-controller-manager:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" memory: \"128Mi\" limits: cpu: \"384m\" memory: \"128Mi\" command: - /usr/bin/kube-controller-manager - --bind-address=0.0.0.0 - --cluster-cidr=10.200.0.0/16 - --allocate-node-cidrs=true - --node-cidr-mask-size=24 - --cluster-name=kubernetes - --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem - --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem - --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig - --leader-elect=false - --root-ca-file=/var/lib/kubernetes/ca.pem - --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem - --service-cluster-ip-range=10.32.0.0/24 - --use-service-account-credentials=true - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-controller-manager CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a72cec7323686 4ada5d332b2c795b6333b8b6c538491dec96fb80f81b600359615651725b0ccf 20 seconds ago Running kube-controller-manager 0 526d7f2e9d3cb \u30a8\u30e9\u30fc\u4e8b\u4f8b Client.Timeout\u3092\u8d85\u3048\u305f\u305f\u3081\u3001kube-control-manager\u3068kube-scheduler\u304c\u30ed\u30c3\u30af\u3092\u53d6\u5f97\u3067\u304d\u306a\u3044 \u767a\u751f\u3057\u305f\u3089\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u518d\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067\u56de\u5fa9\u3059\u308b kube-apiserver\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u304c\u4e0a\u304c\u308b\u3068\u767a\u751f\u3057\u6613\u304f\u306a\u308b E0325 11:08:47.205570 1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://192.168.10.50:6443/apis/coordination.k8s.io/v1/namespaces/kube- system/leases/kube-controller-manager?timeout=10s\": context deadline exceeded I0325 11:08:47.205695 1 leaderelection.go:278] failed to renew lease kube-system/kube-controller-manager: timed out waiting for the condition F0325 11:08:47.205929 1 controllermanager.go:294] leaderelection lost \u53c2\u8003\u6587\u732e https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/ node(flannel)\u306e Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned \u30a8\u30e9\u30fc\u306b\u95a2\u3057\u3066 https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/ https://devops.stackexchange.com/questions/5898/how-to-get-kubernetes-pod-network-cidr","title":"04. bootstrapping kube-controller-manager"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#bootstrapping-kube-controller-manager","text":"","title":"bootstrapping kube-controller-manager"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_1","text":"Dockerfile_kube-controller-manager.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-controller-manager.armhf cat << 'EOF' > Dockerfile_kube-controller-manager.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-controller-manager \\ && chmod +x /usr/bin/kube-controller-manager \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY ca.pem \\ ca-key.pem \\ service-account-key.pem \\ kube-controller-manager.kubeconfig \\ /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-controller-manager\"] EOF image build sudo buildah bud -t k8s-kube-controller-manager --file=Dockerfile_kube-controller-manager.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b --allocate-node-cidrs=true Node resource\u306e spec.podCIDR \u3078CIDR\u304c\u8a2d\u5b9a\u3055\u308c\u308b kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' spec.podCIDR \u306e\u5024\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044node instance\u3067\u306fCNI Plugin(flannel)\u304c\u6b63\u5e38\u52d5\u4f5c\u3057\u306a\u304b\u3063\u305f /etc/kubelet.d/kube-controller-manager.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-controller-manager.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-controller-manager namespace: kube-system labels: tier: control-plane component: kube-controller-manager spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-controller-manager image: localhost/k8s-kube-controller-manager:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" memory: \"128Mi\" limits: cpu: \"384m\" memory: \"128Mi\" command: - /usr/bin/kube-controller-manager - --bind-address=0.0.0.0 - --cluster-cidr=10.200.0.0/16 - --allocate-node-cidrs=true - --node-cidr-mask-size=24 - --cluster-name=kubernetes - --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem - --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem - --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig - --leader-elect=false - --root-ca-file=/var/lib/kubernetes/ca.pem - --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem - --service-cluster-ip-range=10.32.0.0/24 - --use-service-account-credentials=true - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-controller-manager CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a72cec7323686 4ada5d332b2c795b6333b8b6c538491dec96fb80f81b600359615651725b0ccf 20 seconds ago Running kube-controller-manager 0 526d7f2e9d3cb","title":"\u624b\u9806"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_2","text":"Client.Timeout\u3092\u8d85\u3048\u305f\u305f\u3081\u3001kube-control-manager\u3068kube-scheduler\u304c\u30ed\u30c3\u30af\u3092\u53d6\u5f97\u3067\u304d\u306a\u3044 \u767a\u751f\u3057\u305f\u3089\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u518d\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067\u56de\u5fa9\u3059\u308b kube-apiserver\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u304c\u4e0a\u304c\u308b\u3068\u767a\u751f\u3057\u6613\u304f\u306a\u308b E0325 11:08:47.205570 1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://192.168.10.50:6443/apis/coordination.k8s.io/v1/namespaces/kube- system/leases/kube-controller-manager?timeout=10s\": context deadline exceeded I0325 11:08:47.205695 1 leaderelection.go:278] failed to renew lease kube-system/kube-controller-manager: timed out waiting for the condition F0325 11:08:47.205929 1 controllermanager.go:294] leaderelection lost","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_3","text":"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/ node(flannel)\u306e Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned \u30a8\u30e9\u30fc\u306b\u95a2\u3057\u3066 https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/ https://devops.stackexchange.com/questions/5898/how-to-get-kubernetes-pod-network-cidr","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/","text":"bootstrapping kube-scheduler \u624b\u9806 Dockerfile_kube-scheduler.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-scheduler.armhf cat << 'EOF' > Dockerfile_kube-scheduler.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-scheduler \\ && chmod +x /usr/bin/kube-scheduler \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY kube-scheduler.yaml /etc/kubernetes/config/ COPY kube-scheduler.kubeconfig /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-scheduler\"] EOF kube-scheduler\u306econfig\u751f\u6210 k8s 1.19.0 \u3067 KubeSchedulerConfiguration \u304c beta\u306bupdate\u3055\u308c\u3066\u3044\u307e\u3059 https://qiita.com/everpeace/items/7dbf14773db82e765370 kube-scheduler.yaml cat << EOF > kube-scheduler.yaml --- apiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: false EOF image build sudo buildah bud -t k8s-kube-scheduler --file=Dockerfile_kube-scheduler.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/kube-scheduler.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-scheduler.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-scheduler namespace: kube-system labels: tier: control-plane component: kube-scheduler spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-scheduler image: localhost/k8s-kube-scheduler:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" memory: \"128Mi\" limits: cpu: \"384m\" memory: \"128Mi\" command: - /usr/bin/kube-scheduler - --config=/etc/kubernetes/config/kube-scheduler.yaml - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-scheduler CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a19648dec2d54 70e852515b3c74175bb3ad4855287cb81101921b2b1f5a890fa4ebd0eeeee684 15 seconds ago Running kube-scheduler 0 da1d0572bc2b1 \u53c2\u8003\u6587\u732e https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/","title":"05. bootstrapping kube-scheduler"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/#bootstrapping-kube-scheduler","text":"","title":"bootstrapping kube-scheduler"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/#_1","text":"Dockerfile_kube-scheduler.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-scheduler.armhf cat << 'EOF' > Dockerfile_kube-scheduler.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-scheduler \\ && chmod +x /usr/bin/kube-scheduler \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY kube-scheduler.yaml /etc/kubernetes/config/ COPY kube-scheduler.kubeconfig /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-scheduler\"] EOF kube-scheduler\u306econfig\u751f\u6210 k8s 1.19.0 \u3067 KubeSchedulerConfiguration \u304c beta\u306bupdate\u3055\u308c\u3066\u3044\u307e\u3059 https://qiita.com/everpeace/items/7dbf14773db82e765370 kube-scheduler.yaml cat << EOF > kube-scheduler.yaml --- apiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: false EOF image build sudo buildah bud -t k8s-kube-scheduler --file=Dockerfile_kube-scheduler.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/kube-scheduler.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-scheduler.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-scheduler namespace: kube-system labels: tier: control-plane component: kube-scheduler spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-scheduler image: localhost/k8s-kube-scheduler:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" memory: \"128Mi\" limits: cpu: \"384m\" memory: \"128Mi\" command: - /usr/bin/kube-scheduler - --config=/etc/kubernetes/config/kube-scheduler.yaml - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-scheduler CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a19648dec2d54 70e852515b3c74175bb3ad4855287cb81101921b2b1f5a890fa4ebd0eeeee684 15 seconds ago Running kube-scheduler 0 da1d0572bc2b1","title":"\u624b\u9806"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/#_2","text":"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/","text":"kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b kubectl \u3084\u4ed6Client tool\u3067\u306fkube-apiserver\u3078\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u6295\u3052\u307e\u3059\u3002 kube-apiserver \u3067\u306fetcd\u306b\u683c\u7d0d\u3055\u308c\u305f\u60c5\u5831\u3092\u57fa\u306b\u5404worker node(\u306e kubelet ) \u3068\u3084\u308a\u3068\u308a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002(\u4f8b\u3048\u3070exec,top,logs\u306a\u3069) kube-apiserver \u304b\u3089 kubelet \u306e\u5fc5\u8981\u306a\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u3092\u4ed8\u4e0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 \u624b\u9806 ClusterRole system:kube-apiserver-to-kubelet \u3092\u4f5c\u6210 rbac.authorization.kubernetes.io/autoupdate annotations \u8d77\u52d5\u3059\u308b\u305f\u3073\u306b\u3001API\u30b5\u30fc\u30d0\u30fc\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRole\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a29\u9650\u3067\u66f4\u65b0\u3057\u3001 \u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRoleBinding\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308bsubjects\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u8aa4\u3063\u305f\u5909\u66f4\u3092\u30af\u30e9\u30b9\u30bf\u304c\u4fee\u5fa9\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001 \u65b0\u3057\u3044Kubernetes\u30ea\u30ea\u30fc\u30b9\u3067\u6a29\u9650\u3068subjects\u304c\u5909\u66f4\u3055\u308c\u3066\u3082\u3001 Role\u3068RoleBinding\u3092\u6700\u65b0\u306e\u72b6\u614b\u306b\u4fdd\u3064\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 kubernetes.io/bootstrapping: rbac-defaults labels k8s\u306e\u65e2\u5b9a\u30af\u30e9\u30b9\u30bf\u30ed\u30fc\u30eb\u3068\u65e2\u5b9a\u30ed\u30fc\u30eb\u30d0\u30a4\u30f3\u30c9\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3059 cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:kube-apiserver-to-kubelet annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF Kubernetes \u30e6\u30fc\u30b6\u3078 system:kube-apiserver-to-kubelet ClusterRole\u3092\u7d10\u4ed8\u3051\u308b roleRef \u3067\u7d10\u4ed8\u3051\u305fRole\u3092\u6307\u5b9a\u3059\u308b subjects \u3067Role\u3092\u7d10\u4ed8\u3051\u308bAccount\u3092\u6307\u5b9a\u3059\u308b cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: Kubernetes EOF \u3053\u306e\u7d10\u4ed8\u3051\u304c\u6b63\u3057\u304f\u306a\u3044\u3001\u3082\u3057\u304f\u306f\u672a\u8a2d\u5b9a\u306e\u5834\u5408\u3001token\u4ed8\u304d\u3067kubectl\u3092\u5229\u7528\u3057\u305f\u5834\u5408\u306b\u4ee5\u4e0b\u30a8\u30e9\u30fc\u3068\u306a\u308b Error from server (Forbidden): Forbidden (user=Kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-proxy) \u53c2\u8003\u8cc7\u6599 https://kubernetes.io/ja/docs/reference/access-authn-authz/rbac/ https://qiita.com/sheepland/items/67a5bb9b19d8686f389d","title":"06. kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/#kube-apiserver-kubelet","text":"kubectl \u3084\u4ed6Client tool\u3067\u306fkube-apiserver\u3078\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u6295\u3052\u307e\u3059\u3002 kube-apiserver \u3067\u306fetcd\u306b\u683c\u7d0d\u3055\u308c\u305f\u60c5\u5831\u3092\u57fa\u306b\u5404worker node(\u306e kubelet ) \u3068\u3084\u308a\u3068\u308a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002(\u4f8b\u3048\u3070exec,top,logs\u306a\u3069) kube-apiserver \u304b\u3089 kubelet \u306e\u5fc5\u8981\u306a\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u3092\u4ed8\u4e0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002","title":"kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/#_1","text":"ClusterRole system:kube-apiserver-to-kubelet \u3092\u4f5c\u6210 rbac.authorization.kubernetes.io/autoupdate annotations \u8d77\u52d5\u3059\u308b\u305f\u3073\u306b\u3001API\u30b5\u30fc\u30d0\u30fc\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRole\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a29\u9650\u3067\u66f4\u65b0\u3057\u3001 \u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRoleBinding\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308bsubjects\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u8aa4\u3063\u305f\u5909\u66f4\u3092\u30af\u30e9\u30b9\u30bf\u304c\u4fee\u5fa9\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001 \u65b0\u3057\u3044Kubernetes\u30ea\u30ea\u30fc\u30b9\u3067\u6a29\u9650\u3068subjects\u304c\u5909\u66f4\u3055\u308c\u3066\u3082\u3001 Role\u3068RoleBinding\u3092\u6700\u65b0\u306e\u72b6\u614b\u306b\u4fdd\u3064\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 kubernetes.io/bootstrapping: rbac-defaults labels k8s\u306e\u65e2\u5b9a\u30af\u30e9\u30b9\u30bf\u30ed\u30fc\u30eb\u3068\u65e2\u5b9a\u30ed\u30fc\u30eb\u30d0\u30a4\u30f3\u30c9\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3059 cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:kube-apiserver-to-kubelet annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF Kubernetes \u30e6\u30fc\u30b6\u3078 system:kube-apiserver-to-kubelet ClusterRole\u3092\u7d10\u4ed8\u3051\u308b roleRef \u3067\u7d10\u4ed8\u3051\u305fRole\u3092\u6307\u5b9a\u3059\u308b subjects \u3067Role\u3092\u7d10\u4ed8\u3051\u308bAccount\u3092\u6307\u5b9a\u3059\u308b cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: Kubernetes EOF \u3053\u306e\u7d10\u4ed8\u3051\u304c\u6b63\u3057\u304f\u306a\u3044\u3001\u3082\u3057\u304f\u306f\u672a\u8a2d\u5b9a\u306e\u5834\u5408\u3001token\u4ed8\u304d\u3067kubectl\u3092\u5229\u7528\u3057\u305f\u5834\u5408\u306b\u4ee5\u4e0b\u30a8\u30e9\u30fc\u3068\u306a\u308b Error from server (Forbidden): Forbidden (user=Kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-proxy)","title":"\u624b\u9806"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/#_2","text":"https://kubernetes.io/ja/docs/reference/access-authn-authz/rbac/ https://qiita.com/sheepland/items/67a5bb9b19d8686f389d","title":"\u53c2\u8003\u8cc7\u6599"},{"location":"setup/06_master/07_controller_health_check/","text":"Kubernetes API \u306e\u30d8\u30eb\u30b9\u30c1\u30a7\u30c3\u30af \u624b\u9806 \u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u8d77\u52d5\u78ba\u8a8d kubectl get pods -n kube-system \u5b9f\u884c\u4f8b $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE etcd-k8s-master 1/1 Running 0 5m56s kube-apiserver-k8s-master 1/1 Running 0 6m7s kube-controller-manager-k8s-master 1/1 Running 0 4m2s kube-scheduler-k8s-master 1/1 Running 0 2m48s master node\u4e0a\u306eresource\u78ba\u8a8d kubectl get nodes kubectl describe node <pod_name> \u5b9f\u884c\u4f8b $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready <none> 7m57s v1.20.1 $ kubectl describe node k8s-master Name: k8s-master Roles: <none> Labels: beta.kubernetes.io/arch=arm64 beta.kubernetes.io/os=linux kubernetes.io/arch=arm64 kubernetes.io/hostname=k8s-master kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Sat, 17 Apr 2021 15:13:42 +0000 Taints: node-role.kubernetes.io/master:NoSchedule Unschedulable: false Lease: HolderIdentity: k8s-master AcquireTime: <unset> RenewTime: Sat, 17 Apr 2021 16:34:29 +0000 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:52 +0000 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192.168.10.50 Hostname: k8s-master Capacity: cpu: 4 ephemeral-storage: 30459624Ki memory: 1892528Ki pods: 110 Allocatable: cpu: 3400m ephemeral-storage: 28071589432 memory: 1380528Ki pods: 110 System Info: Machine ID: 58f6de70444c4198b56b30122b6c77dc System UUID: 58f6de70444c4198b56b30122b6c77dc Boot ID: 79af3428-cf70-4189-a447-0b917a035a42 Kernel Version: 5.4.0-1032-raspi OS Image: Ubuntu 20.04.2 LTS Operating System: linux Architecture: arm64 Container Runtime Version: cri-o://1.20.2 Kubelet Version: v1.20.1 Kube-Proxy Version: v1.20.1 PodCIDR: 10.200.0.0/24 PodCIDRs: 10.200.0.0/24 Non-terminated Pods: (4 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system etcd-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-apiserver-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-controller-manager-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 76m kube-system kube-scheduler-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 75m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1200m (35%) 2600m (76%) memory 768Mi (56%) 1280Mi (94%) ephemeral-storage 0 (0%) 0 (0%) Events: <none> health checks kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067 --anonymous-auth=false \u3092\u4ed8\u52a0\u3057\u3066\u3044\u308b\u305f\u3081 https://localhost:6443 \u3078\u306eanonymous\u30a2\u30ab\u30a6\u30f3\u30c8\u3067\u306e\u78ba\u8a8d\u306f\u884c\u308f\u305a\u306b kubectl \u3067\u78ba\u8a8d\u3059\u308b API endpoints for health kubectl get --raw='/readyz?verbose' \u5b9f\u884c\u4f8b $ kubectl get --raw='/readyz?verbose' [+]ping ok [+]log ok [+]etcd ok [+]informer-sync ok [+]poststarthook/start-kube-apiserver-admission-initializer ok [+]poststarthook/generic-apiserver-start-informers ok [+]poststarthook/max-in-flight-filter ok [+]poststarthook/start-apiextensions-informers ok [+]poststarthook/start-apiextensions-controllers ok [+]poststarthook/crd-informer-synced ok [+]poststarthook/bootstrap-controller ok [+]poststarthook/rbac/bootstrap-roles ok [+]poststarthook/scheduling/bootstrap-system-priority-classes ok [+]poststarthook/priority-and-fairness-config-producer ok [+]poststarthook/start-cluster-authentication-info-controller ok [+]poststarthook/start-kube-aggregator-informers ok [+]poststarthook/apiservice-registration-controller ok [+]poststarthook/apiservice-status-available-controller ok [+]poststarthook/kube-apiserver-autoregistration ok [+]autoregister-completion ok [+]poststarthook/apiservice-openapi-controller ok [+]shutdown ok readyz check passed Individual health checks kubectl get --raw='/livez/etcd' \u5b9f\u884c\u4f8b $ kubectl get --raw='/livez/etcd' ok $ kubectl get --raw='/livez/poststarthook/start-apiextensions-controllers' ok \u53c2\u8003\u8cc7\u6599 https://kubernetes.io/docs/reference/using-api/health-checks/","title":"07. Kubernetes API \u306e\u30d8\u30eb\u30b9\u30c1\u30a7\u30c3\u30af"},{"location":"setup/06_master/07_controller_health_check/#kubernetes-api","text":"","title":"Kubernetes API \u306e\u30d8\u30eb\u30b9\u30c1\u30a7\u30c3\u30af"},{"location":"setup/06_master/07_controller_health_check/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/06_master/07_controller_health_check/#_2","text":"kubectl get pods -n kube-system \u5b9f\u884c\u4f8b $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE etcd-k8s-master 1/1 Running 0 5m56s kube-apiserver-k8s-master 1/1 Running 0 6m7s kube-controller-manager-k8s-master 1/1 Running 0 4m2s kube-scheduler-k8s-master 1/1 Running 0 2m48s","title":"\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u8d77\u52d5\u78ba\u8a8d"},{"location":"setup/06_master/07_controller_health_check/#master-noderesource","text":"kubectl get nodes kubectl describe node <pod_name> \u5b9f\u884c\u4f8b $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready <none> 7m57s v1.20.1 $ kubectl describe node k8s-master Name: k8s-master Roles: <none> Labels: beta.kubernetes.io/arch=arm64 beta.kubernetes.io/os=linux kubernetes.io/arch=arm64 kubernetes.io/hostname=k8s-master kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Sat, 17 Apr 2021 15:13:42 +0000 Taints: node-role.kubernetes.io/master:NoSchedule Unschedulable: false Lease: HolderIdentity: k8s-master AcquireTime: <unset> RenewTime: Sat, 17 Apr 2021 16:34:29 +0000 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:52 +0000 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192.168.10.50 Hostname: k8s-master Capacity: cpu: 4 ephemeral-storage: 30459624Ki memory: 1892528Ki pods: 110 Allocatable: cpu: 3400m ephemeral-storage: 28071589432 memory: 1380528Ki pods: 110 System Info: Machine ID: 58f6de70444c4198b56b30122b6c77dc System UUID: 58f6de70444c4198b56b30122b6c77dc Boot ID: 79af3428-cf70-4189-a447-0b917a035a42 Kernel Version: 5.4.0-1032-raspi OS Image: Ubuntu 20.04.2 LTS Operating System: linux Architecture: arm64 Container Runtime Version: cri-o://1.20.2 Kubelet Version: v1.20.1 Kube-Proxy Version: v1.20.1 PodCIDR: 10.200.0.0/24 PodCIDRs: 10.200.0.0/24 Non-terminated Pods: (4 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system etcd-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-apiserver-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-controller-manager-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 76m kube-system kube-scheduler-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 75m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1200m (35%) 2600m (76%) memory 768Mi (56%) 1280Mi (94%) ephemeral-storage 0 (0%) 0 (0%) Events: <none>","title":"master node\u4e0a\u306eresource\u78ba\u8a8d"},{"location":"setup/06_master/07_controller_health_check/#health-checks","text":"kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067 --anonymous-auth=false \u3092\u4ed8\u52a0\u3057\u3066\u3044\u308b\u305f\u3081 https://localhost:6443 \u3078\u306eanonymous\u30a2\u30ab\u30a6\u30f3\u30c8\u3067\u306e\u78ba\u8a8d\u306f\u884c\u308f\u305a\u306b kubectl \u3067\u78ba\u8a8d\u3059\u308b API endpoints for health kubectl get --raw='/readyz?verbose' \u5b9f\u884c\u4f8b $ kubectl get --raw='/readyz?verbose' [+]ping ok [+]log ok [+]etcd ok [+]informer-sync ok [+]poststarthook/start-kube-apiserver-admission-initializer ok [+]poststarthook/generic-apiserver-start-informers ok [+]poststarthook/max-in-flight-filter ok [+]poststarthook/start-apiextensions-informers ok [+]poststarthook/start-apiextensions-controllers ok [+]poststarthook/crd-informer-synced ok [+]poststarthook/bootstrap-controller ok [+]poststarthook/rbac/bootstrap-roles ok [+]poststarthook/scheduling/bootstrap-system-priority-classes ok [+]poststarthook/priority-and-fairness-config-producer ok [+]poststarthook/start-cluster-authentication-info-controller ok [+]poststarthook/start-kube-aggregator-informers ok [+]poststarthook/apiservice-registration-controller ok [+]poststarthook/apiservice-status-available-controller ok [+]poststarthook/kube-apiserver-autoregistration ok [+]autoregister-completion ok [+]poststarthook/apiservice-openapi-controller ok [+]shutdown ok readyz check passed Individual health checks kubectl get --raw='/livez/etcd' \u5b9f\u884c\u4f8b $ kubectl get --raw='/livez/etcd' ok $ kubectl get --raw='/livez/poststarthook/start-apiextensions-controllers' ok","title":"health checks"},{"location":"setup/06_master/07_controller_health_check/#_3","text":"https://kubernetes.io/docs/reference/using-api/health-checks/","title":"\u53c2\u8003\u8cc7\u6599"},{"location":"setup/07_worker/01_bootstrapping_kubelet/","text":"bootstrapping kubelet(master/worker \u5171\u901a) kubelet \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002 worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206 Reserve Compute Resources for System Daemons https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9 = Node resource - system-reserved - kube-reserved - eviction-threshold \u3089\u3057\u3044 name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi \u624b\u9806 kubelet \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 VERSION=\"v1.22.0\" ARCH=\"arm64\" sudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet sudo chmod +x /usr/bin/kubelet kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" sudo install -o root -g root -m 755 -d /etc/kubelet.d sudo install -o root -g root -m 755 -d /var/lib/kubernetes sudo install -o root -g root -m 755 -d /var/lib/kubelet sudo cp ca.pem /var/lib/kubernetes/ sudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/ sudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig /var/lib/kubelet/kubelet-config.yaml \u3092\u4f5c\u6210\u3059\u308b clusterDNS \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b podCIDR \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 # https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ staticPodPath: /etc/kubelet.d # kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f # - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b # - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981 authentication: anonymous: enabled: true webhook: enabled: false cacheTTL: \"2m\" x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" # kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a # - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow # - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981 authorization: mode: AlwaysAllow clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" podCIDR: \"10.200.0.0/24\" resolvConf: \"/etc/resolv.conf\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${host}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\" # Reserve Compute Resources for System Daemons # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # # Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044 # # system-reserved # - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b # # kube-reserved # - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b enforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"] cgroupsPerQOS: true cgroupDriver: systemd cgroupRoot: / systemCgroups: /systemd/system.slice systemReservedCgroup: /systemd/system.slice systemReserved: cpu: 256m memory: 256Mi runtimeCgroups: /kube.slice/crio.service kubeletCgroups: /kube.slice/kubelet.service kubeReservedCgroup: /kube.slice kubeReserved: cpu: 256m memory: 256Mi EOF /etc/systemd/system/kubelet.service \u3092\u914d\u7f6e cat << 'EOF' | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=crio.service Requires=crio.service [Service] Restart=on-failure RestartSec=5 ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice ExecStart=/usr/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --container-runtime=remote \\ --container-runtime-endpoint=/var/run/crio/crio.sock \\ --register-node=true \\ --v=2 [Install] WantedBy=multi-user.target EOF kubelet.service \u3092\u8d77\u52d5 sudo systemctl enable kubelet.service sudo systemctl start kubelet.service \u30a8\u30e9\u30fc\u4e8b\u4f8b cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408 kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist kubelet \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( --v 10 ) cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice] \u5bfe\u5fdc kubelet.service \u306e ExecStartPre \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f \u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy kubeconfig \u306e\u8a3c\u660e\u66f8\u306e CN \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b 360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\" kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: k8s-master \u304c\u6b63\u3057\u3044\u306e\u306b CN = system:node:k8s-node1 \u3068\u306a\u3063\u3066\u3044\u305f root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master Node \u30ea\u30bd\u30fc\u30b9\u306e spec.podCIDR \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044 \u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044 flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f... kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u30ed\u30b0 Set node k8s-node1 PodCIDR to [10.200.0.0/24] \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8 kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b --allocate-node-cidrs=true \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist range_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24] ttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\" controller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}] controller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes node_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\" controller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1 node_lifecycle_controller.go:1429] Initializing eviction metric for zone: node_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp. event.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\" node_lifecycle_controller.go:1245] Controller detected that zone is now in state Normal. Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044 I0214 07:03:56.822586 1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\" F0214 07:03:56.822637 1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication goroutine 1 [running]: https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication CNI Plugin\u3092 /etc/cni/net.d \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044 kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized CNI Plugin\u3092 /etc/cni/net.d \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b https://github.com/containernetworking/plugins/releases Kubelet cannot determine CPU online state sysinfo.go:203] Nodes topology is not available, providing CPU topology sysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory gce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:72] Cannot read number of physical cores correctly, number of cores set to 0 machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:86] Cannot read number of sockets correctly, number of sockets set to 0 container_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name: \u65e2\u77e5\u3089\u3057\u3044 https://github.com/kubernetes/kubernetes/issues/95039 \u53c2\u8003 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go https://cyberagent.ai/blog/tech/4036/ kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b https://downloadkubernetes.com/ https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/ Node Authorization https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822 https://kubernetes.io/docs/reference/access-authn-authz/node/ https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ static pod https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/policy/pod-security-policy/ https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver PodSecurityPolicy \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf( false \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f true \u306b\u76f4\u3059) https://github.com/kubernetes/kubernetes/issues/70952","title":"01. bootstrapping kubelet"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#bootstrapping-kubeletmasterworker","text":"kubelet \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002","title":"bootstrapping kubelet(master/worker \u5171\u901a)"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#worker-node","text":"Reserve Compute Resources for System Daemons https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9 = Node resource - system-reserved - kube-reserved - eviction-threshold \u3089\u3057\u3044 name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi","title":"worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubelet","text":"VERSION=\"v1.22.0\" ARCH=\"arm64\" sudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet sudo chmod +x /usr/bin/kubelet","title":"kubelet \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeconfig","text":"# host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" sudo install -o root -g root -m 755 -d /etc/kubelet.d sudo install -o root -g root -m 755 -d /var/lib/kubernetes sudo install -o root -g root -m 755 -d /var/lib/kubelet sudo cp ca.pem /var/lib/kubernetes/ sudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/ sudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig","title":"kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#varlibkubeletkubelet-configyaml","text":"clusterDNS \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b podCIDR \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 # https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ staticPodPath: /etc/kubelet.d # kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f # - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b # - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981 authentication: anonymous: enabled: true webhook: enabled: false cacheTTL: \"2m\" x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" # kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a # - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow # - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981 authorization: mode: AlwaysAllow clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" podCIDR: \"10.200.0.0/24\" resolvConf: \"/etc/resolv.conf\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${host}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\" # Reserve Compute Resources for System Daemons # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # # Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044 # # system-reserved # - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b # # kube-reserved # - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b enforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"] cgroupsPerQOS: true cgroupDriver: systemd cgroupRoot: / systemCgroups: /systemd/system.slice systemReservedCgroup: /systemd/system.slice systemReserved: cpu: 256m memory: 256Mi runtimeCgroups: /kube.slice/crio.service kubeletCgroups: /kube.slice/kubelet.service kubeReservedCgroup: /kube.slice kubeReserved: cpu: 256m memory: 256Mi EOF","title":"/var/lib/kubelet/kubelet-config.yaml \u3092\u4f5c\u6210\u3059\u308b"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#etcsystemdsystemkubeletservice","text":"cat << 'EOF' | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=crio.service Requires=crio.service [Service] Restart=on-failure RestartSec=5 ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice ExecStart=/usr/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --container-runtime=remote \\ --container-runtime-endpoint=/var/run/crio/crio.sock \\ --register-node=true \\ --v=2 [Install] WantedBy=multi-user.target EOF","title":"/etc/systemd/system/kubelet.service \u3092\u914d\u7f6e"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeletservice","text":"sudo systemctl enable kubelet.service sudo systemctl start kubelet.service","title":"kubelet.service \u3092\u8d77\u52d5"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_2","text":"","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cgroup","text":"kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist kubelet \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( --v 10 ) cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice] \u5bfe\u5fdc kubelet.service \u306e ExecStartPre \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice","title":"cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cgroupsystemreserved-memory-size","text":"\u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy","title":"cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeconfig-cn-node","text":"360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\" kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: k8s-master \u304c\u6b63\u3057\u3044\u306e\u306b CN = system:node:k8s-node1 \u3068\u306a\u3063\u3066\u3044\u305f root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master","title":"kubeconfig \u306e\u8a3c\u660e\u66f8\u306e CN \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#node-specpodcidr-cidr","text":"\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044 flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f... kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u30ed\u30b0 Set node k8s-node1 PodCIDR to [10.200.0.0/24] \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8 kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b --allocate-node-cidrs=true \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist range_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24] ttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\" controller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}] controller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes node_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\" controller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1 node_lifecycle_controller.go:1429] Initializing eviction metric for zone: node_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp. event.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\" node_lifecycle_controller.go:1245] Controller detected that zone is now in state Normal.","title":"Node \u30ea\u30bd\u30fc\u30b9\u306e spec.podCIDR \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#webhook-authentication","text":"I0214 07:03:56.822586 1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\" F0214 07:03:56.822637 1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication goroutine 1 [running]: https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication","title":"Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cni-plugin-etccninetd-cni-plugin","text":"kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized CNI Plugin\u3092 /etc/cni/net.d \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b https://github.com/containernetworking/plugins/releases","title":"CNI Plugin\u3092 /etc/cni/net.d \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubelet-cannot-determine-cpu-online-state","text":"sysinfo.go:203] Nodes topology is not available, providing CPU topology sysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory gce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:72] Cannot read number of physical cores correctly, number of cores set to 0 machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:86] Cannot read number of sockets correctly, number of sockets set to 0 container_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name: \u65e2\u77e5\u3089\u3057\u3044 https://github.com/kubernetes/kubernetes/issues/95039","title":"Kubelet cannot determine CPU online state"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_3","text":"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go https://cyberagent.ai/blog/tech/4036/ kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b https://downloadkubernetes.com/ https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/ Node Authorization https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822 https://kubernetes.io/docs/reference/access-authn-authz/node/ https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ static pod https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/policy/pod-security-policy/ https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver PodSecurityPolicy \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf( false \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f true \u306b\u76f4\u3059) https://github.com/kubernetes/kubernetes/issues/70952","title":"\u53c2\u8003"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/","text":"bootstrapping kube-proxy kube-proxy \u3068\u306f kube-proxy \u3068\u306f\u5404worker node\u3067\u52d5\u4f5c\u3059\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30d7\u30ed\u30ad\u30b7\u3092\u5b9f\u73fe\u3059\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3067\u3059\u3002\u5177\u4f53\u7684\u306b\u306f Sertvice \u30ea\u30bd\u30fc\u30b9\u3067\u4f5c\u6210\u3055\u308c\u308bCluster IP\u3084Node Port\u306e\u7ba1\u7406\u3068\u305d\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30c6\u30fc\u30d6\u30eb\u306e\u7ba1\u7406\u3001\u307e\u305fnginx ingress controller\u3092\u5229\u7528\u3057\u305fIngress\u30ea\u30bd\u30fc\u30b9\u3067\u306fPod\u3078\u306e\u8ca0\u8377\u5206\u6563\u306bkube-proxy\u3092\u6d3b\u7528\u6307\u5b9a\u305f\u308a\u3059\u308b\u305d\u3046\u3067\u3059( With NGINX, we\u2019ll use the DNS name or virtual IP address to identify the service, and rely on kube-proxy to perform the internal load-balancing across the pool of pods. ) \u624b\u9806 Dockerfile_kube-proxy.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-proxy.armhf cat << 'EOF' > Dockerfile_kube-proxy.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-proxy \\ && chmod +x /usr/bin/kube-proxy \\ && install -o root -g root -m 755 -d /var/lib/kube-proxy \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY kube-proxy-config.yaml /var/lib/kube-proxy/kube-proxy-config.yaml COPY kube-proxy.kubeconfig /var/lib//kube-proxy/kubeconfig ENTRYPOINT [\"/usr/bin/kube-proxy\"] EOF kube-proxy\u306econfig\u751f\u6210 10.200.0.0/16 \u306f kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3 --cluster-cidr \u3067\u6307\u5b9a\u3057\u305f\u5024 cluster_cidr=\"10.200.0.0/16\" cat << EOF > kube-proxy-config.yaml --- kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"${cluster_cidr}\" EOF image build sudo buildah bud \\ -f Dockerfile_kube-proxy.armhf \\ -t k8s-kube-proxy kernel parameter cat << EOF >> /etc/sysctl.d/kubelet.conf # kube-proxy net.ipv4.conf.all.route_localnet = 1 net.netfilter.nf_conntrack_max = 131072 net.netfilter.nf_conntrack_tcp_timeout_established = 86400 net.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600 EOF sudo sysctl --system cat << EOF >> /etc/modprobe.d/kube-proxy.conf options nf_conntrack hashsize=32768 EOF sudo exec /sbin/modprobe nf_conntrack hashsize=32768 pod manifests\u3092 /etc/kubernetes/manifests/ \u3078\u4f5c\u6210\u3059\u308b /etc/kubernetes/manifests/kube-proxy.yaml sudo mkdir -p /etc/kubernetes/manifests cat << EOF | sudo tee /etc/kubernetes/manifests/kube-proxy.yaml --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-proxy namespace: kube-system labels: component: kube-proxy # TODO # master node\u306baddon-manager\u3092\u5c0e\u5165\u3057\u305f\u3089\u30b3\u30e1\u30f3\u30c8\u5916\u3059 # addonmanager.kubernetes.io/mode=Reconcile spec: selector: matchLabels: name: kube-proxy # https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#performing-a-rolling-update updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: # template \u4ee5\u4e0b\u306fpod templates # (apiVersion\u3084kind\u3092\u3082\u305f\u306a\u3044\u3053\u3068\u3092\u9664\u3044\u3066\u306f\u3001Pod\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3068\u540c\u3058\u30b9\u30ad\u30fc\u30de) # https://kubernetes.io/ja/docs/concepts/workloads/controllers/daemonset/ metadata: labels: name: kube-proxy annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-proxy image: localhost/k8s-kube-proxy:latest securityContext: capabilities: add: - SYS_ADMIN - NET_ADMIN - NET_RAW command: - /usr/bin/kube-proxy - --config=/var/lib/kube-proxy/kube-proxy-config.yaml imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" volumeMounts: - name: conntrack-command mountPath: /usr/sbin/conntrack - name: iptables-command mountPath: /usr/sbin/iptables - name: iptables-restore-command mountPath: /usr/sbin/iptables-restore - name: iptables-save-command mountPath: /usr/sbin/iptables-save - name: xtables-lock-file mountPath: /run/xtables.lock - name: usr-lib-dir mountPath: /usr/lib - name: lib-dir mountPath: /lib - name: sys-dir mountPath: /sys volumes: - name: conntrack-command hostPath: path: /usr/sbin/conntrack - name: iptables-command hostPath: path: /usr/sbin/iptables - name: iptables-restore-command hostPath: path: /usr/sbin/iptables-restore - name: iptables-save-command hostPath: path: /usr/sbin/iptables-save - name: xtables-lock-file hostPath: path: /run/xtables.lock - name: usr-lib-dir hostPath: path: /usr/lib - name: lib-dir hostPath: path: /lib - name: sys-dir hostPath: path: /sys EOF pod\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b kubectl apply -f /etc/kubernetes/manifests/kube-proxy.yaml","title":"02. bootstrapping kube-proxy"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/#bootstrapping-kube-proxy","text":"","title":"bootstrapping kube-proxy"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/#kube-proxy","text":"kube-proxy \u3068\u306f\u5404worker node\u3067\u52d5\u4f5c\u3059\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30d7\u30ed\u30ad\u30b7\u3092\u5b9f\u73fe\u3059\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3067\u3059\u3002\u5177\u4f53\u7684\u306b\u306f Sertvice \u30ea\u30bd\u30fc\u30b9\u3067\u4f5c\u6210\u3055\u308c\u308bCluster IP\u3084Node Port\u306e\u7ba1\u7406\u3068\u305d\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30c6\u30fc\u30d6\u30eb\u306e\u7ba1\u7406\u3001\u307e\u305fnginx ingress controller\u3092\u5229\u7528\u3057\u305fIngress\u30ea\u30bd\u30fc\u30b9\u3067\u306fPod\u3078\u306e\u8ca0\u8377\u5206\u6563\u306bkube-proxy\u3092\u6d3b\u7528\u6307\u5b9a\u305f\u308a\u3059\u308b\u305d\u3046\u3067\u3059( With NGINX, we\u2019ll use the DNS name or virtual IP address to identify the service, and rely on kube-proxy to perform the internal load-balancing across the pool of pods. )","title":"kube-proxy \u3068\u306f"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/#_1","text":"Dockerfile_kube-proxy.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-proxy.armhf cat << 'EOF' > Dockerfile_kube-proxy.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-proxy \\ && chmod +x /usr/bin/kube-proxy \\ && install -o root -g root -m 755 -d /var/lib/kube-proxy \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY kube-proxy-config.yaml /var/lib/kube-proxy/kube-proxy-config.yaml COPY kube-proxy.kubeconfig /var/lib//kube-proxy/kubeconfig ENTRYPOINT [\"/usr/bin/kube-proxy\"] EOF kube-proxy\u306econfig\u751f\u6210 10.200.0.0/16 \u306f kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3 --cluster-cidr \u3067\u6307\u5b9a\u3057\u305f\u5024 cluster_cidr=\"10.200.0.0/16\" cat << EOF > kube-proxy-config.yaml --- kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"${cluster_cidr}\" EOF image build sudo buildah bud \\ -f Dockerfile_kube-proxy.armhf \\ -t k8s-kube-proxy kernel parameter cat << EOF >> /etc/sysctl.d/kubelet.conf # kube-proxy net.ipv4.conf.all.route_localnet = 1 net.netfilter.nf_conntrack_max = 131072 net.netfilter.nf_conntrack_tcp_timeout_established = 86400 net.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600 EOF sudo sysctl --system cat << EOF >> /etc/modprobe.d/kube-proxy.conf options nf_conntrack hashsize=32768 EOF sudo exec /sbin/modprobe nf_conntrack hashsize=32768 pod manifests\u3092 /etc/kubernetes/manifests/ \u3078\u4f5c\u6210\u3059\u308b /etc/kubernetes/manifests/kube-proxy.yaml sudo mkdir -p /etc/kubernetes/manifests cat << EOF | sudo tee /etc/kubernetes/manifests/kube-proxy.yaml --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-proxy namespace: kube-system labels: component: kube-proxy # TODO # master node\u306baddon-manager\u3092\u5c0e\u5165\u3057\u305f\u3089\u30b3\u30e1\u30f3\u30c8\u5916\u3059 # addonmanager.kubernetes.io/mode=Reconcile spec: selector: matchLabels: name: kube-proxy # https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#performing-a-rolling-update updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: # template \u4ee5\u4e0b\u306fpod templates # (apiVersion\u3084kind\u3092\u3082\u305f\u306a\u3044\u3053\u3068\u3092\u9664\u3044\u3066\u306f\u3001Pod\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3068\u540c\u3058\u30b9\u30ad\u30fc\u30de) # https://kubernetes.io/ja/docs/concepts/workloads/controllers/daemonset/ metadata: labels: name: kube-proxy annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-proxy image: localhost/k8s-kube-proxy:latest securityContext: capabilities: add: - SYS_ADMIN - NET_ADMIN - NET_RAW command: - /usr/bin/kube-proxy - --config=/var/lib/kube-proxy/kube-proxy-config.yaml imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" volumeMounts: - name: conntrack-command mountPath: /usr/sbin/conntrack - name: iptables-command mountPath: /usr/sbin/iptables - name: iptables-restore-command mountPath: /usr/sbin/iptables-restore - name: iptables-save-command mountPath: /usr/sbin/iptables-save - name: xtables-lock-file mountPath: /run/xtables.lock - name: usr-lib-dir mountPath: /usr/lib - name: lib-dir mountPath: /lib - name: sys-dir mountPath: /sys volumes: - name: conntrack-command hostPath: path: /usr/sbin/conntrack - name: iptables-command hostPath: path: /usr/sbin/iptables - name: iptables-restore-command hostPath: path: /usr/sbin/iptables-restore - name: iptables-save-command hostPath: path: /usr/sbin/iptables-save - name: xtables-lock-file hostPath: path: /run/xtables.lock - name: usr-lib-dir hostPath: path: /usr/lib - name: lib-dir hostPath: path: /lib - name: sys-dir hostPath: path: /sys EOF pod\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b kubectl apply -f /etc/kubernetes/manifests/kube-proxy.yaml","title":"\u624b\u9806"},{"location":"setup/08_flannel/bootstrapping_flannel/","text":"bootstrapping flannel \u53c2\u8003\u6587\u732e https://github.com/flannel-io/flannel/ https://github.com/flannel-io/flannel/blob/master/Documentation/troubleshooting.md /opt/bin/flanneld \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3 https://github.com/flannel-io/flannel/blob/master/main.go#L110-L132 \u624b\u9806 flannel k8s manifests\u3092\u516c\u5f0f\u304b\u3089\u53d6\u5f97\u3059\u308b master\u30d6\u30e9\u30f3\u30c1\u304b\u3089\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u304c2021/03/06 \u6642\u70b9\u3067\u306f release tag v0.13.1-rc2 \u306e\u5185\u5bb9 sudo curl -o /etc/kubernetes/manifests/kube-flannel.yml -sSL https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml manifests\u3092\u4fee\u6b63\u3059\u308b net-conf.json \u306e Network \u3092controller-manager\u306e --cluster-cidr \u3067\u6307\u5b9a\u3057\u305f\u5024\u306b\u5909\u66f4\u3059\u308b etcd\u306b\u7528\u3044\u305fcertificate\u3084admin\u306ekubeconfig\u3092kube-flannel\u30b3\u30f3\u30c6\u30ca\u3078bind mount\u3059\u308b /opt/bin/flanneld \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3 --kubeconfig-file --etcd-endpoints --etcd-prefix --etcd-keyfile --etcd-certfile --etcd-cafile --v sudo vim /etc/kubernetes/manifests/kube-flannel.yml \u4fee\u6b63\u5f8c\u306e /etc/kubernetes/manifests/kube-flannel.yml --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \"/etc/cni/net.d\" - pathPrefix: \"/etc/kube-flannel\" - pathPrefix: \"/run/flannel\" readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN', 'NET_RAW'] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unused in CaaSP rule: 'RunAsAny' --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel rules: - apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged'] - apiGroups: - \"\" resources: - pods verbs: - get - apiGroups: - \"\" resources: - nodes verbs: - list - watch - apiGroups: - \"\" resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } net-conf.json: | { \"Network\": \"10.200.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.13.1-rc2 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.13.1-rc2 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.14.0 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig - --etcd-endpoints=https://192.168.10.50:4001 - --etcd-prefix=/coreos.com/network - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem - --etcd-cafile=/var/lib/kubernetes/ca.pem - --v=10 resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: false capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ - name: var-lib-kubernetes-dir mountPath: /var/lib/kubernetes/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: var-lib-kubernetes-dir hostPath: path: /var/lib/kubernetes - name: flannel-cfg configMap: name: kube-flannel-cfg flannel Pod\u3092\u30c7\u30d7\u30ed\u30a4 kubectl apply -f /etc/kubernetes/manifests/kube-flannel.yml \u30a8\u30e9\u30fc\u4e8b\u4f8b flannel\u304c\u53c2\u7167\u3059\u308bkubeconfig\u304c\u6b63\u3057\u304f\u306a\u3044 flannel-io/flannel Documentation/kube-flannel.yml \u305d\u306e\u307e\u307e\u3060\u3068\u767a\u751f\u3057\u305f Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. error creating inClusterConfig, falling back to default config: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory Failed to create SubnetManager: fail to create kubernetes config: invalid configuration: no configuration has been provided, try setting KUBERNETES_MASTER environment variable Failed to create SubnetManager: fail to create kubernetes config: stat \"/var/lib/kubernetes/admin.kubeconfig\": no such file or directory --kubeconfig-file \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u305fkubeconfig path\u304c\u6b63\u3057\u304f\u306a\u3044 \u79c1\u306e\u30b1\u30fc\u30b9\u3067\u306f --kubeconfig-file=\"/var/lib/kubernetes/admin.kubeconfig\" \u3068\u3057\u3066\u3044\u308b\u3068\u30c0\u30d6\u30eb\u30af\u30a9\u30fc\u30c8(\") \u304c\u30d1\u30b9\u306b\u542b\u307e\u308c\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6c17\u4ed8\u304f\u306e\u306b\u6642\u9593\u304b\u304b\u308a\u307e\u3057\u305f(\u6b63\u3057\u304f\u306f --kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig ) Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned Node\u30ea\u30bd\u30fc\u30b9\u306e .spec.podCIDR \u304c\u767b\u9332\u3055\u308c\u3066\u3044\u306a\u3044 \u78ba\u8a8d\u65b9\u6cd5 kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u8a2d\u5b9a\u4e0d\u5099\u306e\u53ef\u80fd\u6027 \u4ee5\u4e0b\u8a2d\u5b9a\u5909\u66f4\u5f8c\u3001kubelet\u306e\u8d77\u52d5\u524d\u306b kubectl delete node <NODE> \u3092\u5b9f\u884c\u3059\u308b https://github.com/flannel-io/flannel/issues/728#issuecomment-325347810 podCIDR\u5272\u308a\u5f53\u3066\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044 --cluster-cidr=<CIDR> --allocate-node-cidrs=true https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/ --service-cluster-ip-range=<CIDR> \u3068 (flannnel\u306e) --pod-cidr=<CIDR> \u304c\u88ab\u3063\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b (kube-controller-manager \u304c\u6b63\u3057\u3044\u5834\u5408) Node\u30ea\u30bd\u30fc\u30b9\u306e .spec.podCIDR \u3092\u624b\u52d5\u8a2d\u5b9a\u3057\u3066\u56de\u907f\u3059\u308b kubectl patch node <NODE_NAME> -p '{\"spec\":{\"podCIDR\":\"<SUBNET>\"}}' Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again. kubectl delete pod kube-flannel-... \u3067 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9 flannel.1 \u304c\u958b\u653e\u3055\u308c\u306a\u3044 https://github.com/flannel-io/flannel/issues/1060 \u53e4\u3044 flannel.1 \u3092\u524a\u9664 ip a sudo ip link delete flannel.1","title":"08. bootstrapping flannel"},{"location":"setup/08_flannel/bootstrapping_flannel/#bootstrapping-flannel","text":"","title":"bootstrapping flannel"},{"location":"setup/08_flannel/bootstrapping_flannel/#_1","text":"https://github.com/flannel-io/flannel/ https://github.com/flannel-io/flannel/blob/master/Documentation/troubleshooting.md /opt/bin/flanneld \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3 https://github.com/flannel-io/flannel/blob/master/main.go#L110-L132","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/08_flannel/bootstrapping_flannel/#_2","text":"flannel k8s manifests\u3092\u516c\u5f0f\u304b\u3089\u53d6\u5f97\u3059\u308b master\u30d6\u30e9\u30f3\u30c1\u304b\u3089\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u304c2021/03/06 \u6642\u70b9\u3067\u306f release tag v0.13.1-rc2 \u306e\u5185\u5bb9 sudo curl -o /etc/kubernetes/manifests/kube-flannel.yml -sSL https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml manifests\u3092\u4fee\u6b63\u3059\u308b net-conf.json \u306e Network \u3092controller-manager\u306e --cluster-cidr \u3067\u6307\u5b9a\u3057\u305f\u5024\u306b\u5909\u66f4\u3059\u308b etcd\u306b\u7528\u3044\u305fcertificate\u3084admin\u306ekubeconfig\u3092kube-flannel\u30b3\u30f3\u30c6\u30ca\u3078bind mount\u3059\u308b /opt/bin/flanneld \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3 --kubeconfig-file --etcd-endpoints --etcd-prefix --etcd-keyfile --etcd-certfile --etcd-cafile --v sudo vim /etc/kubernetes/manifests/kube-flannel.yml \u4fee\u6b63\u5f8c\u306e /etc/kubernetes/manifests/kube-flannel.yml --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \"/etc/cni/net.d\" - pathPrefix: \"/etc/kube-flannel\" - pathPrefix: \"/run/flannel\" readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN', 'NET_RAW'] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unused in CaaSP rule: 'RunAsAny' --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel rules: - apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged'] - apiGroups: - \"\" resources: - pods verbs: - get - apiGroups: - \"\" resources: - nodes verbs: - list - watch - apiGroups: - \"\" resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } net-conf.json: | { \"Network\": \"10.200.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.13.1-rc2 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.13.1-rc2 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.14.0 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig - --etcd-endpoints=https://192.168.10.50:4001 - --etcd-prefix=/coreos.com/network - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem - --etcd-cafile=/var/lib/kubernetes/ca.pem - --v=10 resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: false capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ - name: var-lib-kubernetes-dir mountPath: /var/lib/kubernetes/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: var-lib-kubernetes-dir hostPath: path: /var/lib/kubernetes - name: flannel-cfg configMap: name: kube-flannel-cfg flannel Pod\u3092\u30c7\u30d7\u30ed\u30a4 kubectl apply -f /etc/kubernetes/manifests/kube-flannel.yml","title":"\u624b\u9806"},{"location":"setup/08_flannel/bootstrapping_flannel/#_3","text":"flannel\u304c\u53c2\u7167\u3059\u308bkubeconfig\u304c\u6b63\u3057\u304f\u306a\u3044 flannel-io/flannel Documentation/kube-flannel.yml \u305d\u306e\u307e\u307e\u3060\u3068\u767a\u751f\u3057\u305f Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. error creating inClusterConfig, falling back to default config: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory Failed to create SubnetManager: fail to create kubernetes config: invalid configuration: no configuration has been provided, try setting KUBERNETES_MASTER environment variable Failed to create SubnetManager: fail to create kubernetes config: stat \"/var/lib/kubernetes/admin.kubeconfig\": no such file or directory --kubeconfig-file \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u305fkubeconfig path\u304c\u6b63\u3057\u304f\u306a\u3044 \u79c1\u306e\u30b1\u30fc\u30b9\u3067\u306f --kubeconfig-file=\"/var/lib/kubernetes/admin.kubeconfig\" \u3068\u3057\u3066\u3044\u308b\u3068\u30c0\u30d6\u30eb\u30af\u30a9\u30fc\u30c8(\") \u304c\u30d1\u30b9\u306b\u542b\u307e\u308c\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6c17\u4ed8\u304f\u306e\u306b\u6642\u9593\u304b\u304b\u308a\u307e\u3057\u305f(\u6b63\u3057\u304f\u306f --kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig ) Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned Node\u30ea\u30bd\u30fc\u30b9\u306e .spec.podCIDR \u304c\u767b\u9332\u3055\u308c\u3066\u3044\u306a\u3044 \u78ba\u8a8d\u65b9\u6cd5 kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u8a2d\u5b9a\u4e0d\u5099\u306e\u53ef\u80fd\u6027 \u4ee5\u4e0b\u8a2d\u5b9a\u5909\u66f4\u5f8c\u3001kubelet\u306e\u8d77\u52d5\u524d\u306b kubectl delete node <NODE> \u3092\u5b9f\u884c\u3059\u308b https://github.com/flannel-io/flannel/issues/728#issuecomment-325347810 podCIDR\u5272\u308a\u5f53\u3066\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044 --cluster-cidr=<CIDR> --allocate-node-cidrs=true https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/ --service-cluster-ip-range=<CIDR> \u3068 (flannnel\u306e) --pod-cidr=<CIDR> \u304c\u88ab\u3063\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b (kube-controller-manager \u304c\u6b63\u3057\u3044\u5834\u5408) Node\u30ea\u30bd\u30fc\u30b9\u306e .spec.podCIDR \u3092\u624b\u52d5\u8a2d\u5b9a\u3057\u3066\u56de\u907f\u3059\u308b kubectl patch node <NODE_NAME> -p '{\"spec\":{\"podCIDR\":\"<SUBNET>\"}}' Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again. kubectl delete pod kube-flannel-... \u3067 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9 flannel.1 \u304c\u958b\u653e\u3055\u308c\u306a\u3044 https://github.com/flannel-io/flannel/issues/1060 \u53e4\u3044 flannel.1 \u3092\u524a\u9664 ip a sudo ip link delete flannel.1","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/09_coredns/bootstrapping_coredns/","text":"bootstrapping CoreDNS CoreDNS \u3068\u306f CoreDNS \u306f CNCF\u306egraduated project \u3068\u3057\u3066\u30db\u30b9\u30c8\u3055\u308c\u3066\u3044\u308bDNS\u30b5\u30fc\u30d0\u3067\u3001 Kubernetes 1.13 \u4ee5\u964d\u306b\u3066\u30c7\u30d5\u30a9\u30eb\u30c8DNS\u30b5\u30fc\u30d0\u3068\u3057\u3066\u63a1\u7528 \u3055\u308c\u3066\u304a\u308a\u3001Cluster\u5185\u3067\u306eService\u30ea\u30bd\u30fc\u30b9\u306e\u540d\u524d\u89e3\u6c7a\u306b\u5229\u7528\u3057\u3066\u3044\u307e\u3059 ( Kubernetes DNS-Based Service Discovery )\u3002 AWS Load Balancer Controller \u306a\u3069Cluster\u5916\u3078\u30a8\u30f3\u30c9\u30dd\u30a4\u30f3\u30c8\u3092\u516c\u958b\u3059\u308b\u5834\u5408\u306f\u5225\u9014\u5916\u90e8DNS\u30b5\u30fc\u30d0(Route53\u306a\u3069)\u3092\u5229\u7528\u3057\u307e\u3059(CoreDNS\u3092Cluster\u5916\u90e8\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd)\u3002 CoreDNS\u306f\u3042\u3089\u3086\u308b\u51e6\u7406\u3092Plugin\u3068\u3057\u3066\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002CoreDNS\u5358\u4f53\u306fDNS\u30af\u30a8\u30ea\u30fc\u3092\u89e3\u91c8\u3057\u3066\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb( ./Corefile )\u306b\u8a18\u8ff0\u3055\u308c\u305fPlugin\u306b\u51e6\u7406\u3092\u53d7\u3051\u6e21\u3057\u307e\u3059\u3002( \u53c2\u8003 ) kubernetes plugin \u306f Kubernetes DNS-Based Service Discovery Specification. \u306e\u5b9f\u88c5\u3067\u3059\u3002 Corefile \u3067kubernetes plugin\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u5229\u7528\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u8a2d\u5b9a\u306f kubernetes/coredns.yaml.sed \u306e\u5185\u5bb9\u3067\u3059\u3002 fallthrough \u3068\u306fNXDOMAIN(\u4e0d\u5728\u5fdc\u7b54)\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408\u306b\u51e6\u7406\u3092\u4e0b\u6d41\u306ePlugin\u306b\u6e21\u3057\u3066\u304f\u308c\u307e\u3059(\u3053\u306e\u8a2d\u5b9a\u3067\u306f\u9006\u5f15\u304d\u3067NXDOMAIN\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408)\u3002 .:53 { kubernetes cluster.local in-addr.arpa ip6.arpa { fallthrough in-addr.arpa ip6.arpa } } \u53c2\u8003\u6587\u732e https://coredns.io/ https://github.com/coredns/coredns https://github.com/coredns/deployment https://github.com/coredns/helm https://engineer.retty.me/entry/2020/12/15/161544 https://www.netone.co.jp/knowledge-center/netone-blog/20191226-1/ https://www.scsk.jp/sp/sysdig/blog/container_monitoring/coredns.html \u624b\u9806 coredns k8s manifests\u3092 \u516c\u5f0f \u304b\u3089\u53d6\u5f97\u3059\u308b git clone https://github.com/coredns/deployment.git coredns_deployment cd coredns_deployment/kubernetes/ bash deploy.sh -i 10.32.0.10 -s -t coredns.yaml.sed | kubectl apply -f - CoreDNS\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u53d6\u5f97\u306b\u3064\u3044\u3066 Corefile \u3067\u4ee5\u4e0b\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u304a\u304f\u3053\u3068\u3067Prometheus\u7528\u306b 9153/TCP \u3067\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u516c\u958b\u3067\u304d\u307e\u3059 .:53 { prometheus :9153 } \u30a8\u30e9\u30fc\u4e8b\u4f8b kubectl get pods -n kube-system \u3067\u3044\u3064\u307e\u3067\u7d4c\u3063\u3066\u3082 ContainerCreating \u306e\u307e\u307e kubectl describe pod -n kube-system <POD_ID> Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"<CONTAINER_ID>\" network for pod \"<POD_ID>\": networkPlugin cni failed to set up pod \"<<POD_NAME>\" network: failed to Statfs \"/proc/15875/ns/net\": no such file or directory, failed to clean up sandbox container \"<CONTAINER_ID>\" network for pod \"<POD_ID>\": networkPlugin cni failed to teardown pod \"<POD_NAME>\" network: neither iptables nor ip6tables usable] controller-manager https://github.com/kubernetes/kubernetes/blob/v1.20.2/pkg/controller/endpointslice/utils.go#L407-L415 couldn't find ipfamilies for headless service: kube-system/kube-dns. This could happen if controller manager is connected to an old apiserver that does not support ip families yet. EndpointSlices for this Service will use IPv4 as the IP Family based on familyOf(ClusterIP:10.32.0.10). $ kubectl get service -n kube-system -o jsonpath='{.items[*].spec.clusterIP}' 10.32.0.10 Warning FailedCreatePodSandBox 3m12s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8 s_coredns-675db8b7cc-vxvmv_kube-system_a2446fa3-70e1-41f0-be93-595853692eac_0(9e46ece4c71d1919d3dfc318d0dca65fac540b491f5044e40c076fe996c1ccf3): failed to set bridge addr: \"cni0\" already has an IP address different from 10.200.1.1/24 .:53 [INFO] plugin/reload: Running configuration MD5 = b0741fcbd8bd79287446297caa87f7a1 CoreDNS-1.8.4 linux/arm64, go1.16.4, 053c4d5 [FATAL] plugin/loop: Loop (127.0.0.1:54310 -> :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting . Query: \"HINFO 7207295349162295100.1640133564250428921.\"","title":"09. bootstrapping coredns"},{"location":"setup/09_coredns/bootstrapping_coredns/#bootstrapping-coredns","text":"","title":"bootstrapping CoreDNS"},{"location":"setup/09_coredns/bootstrapping_coredns/#coredns","text":"CoreDNS \u306f CNCF\u306egraduated project \u3068\u3057\u3066\u30db\u30b9\u30c8\u3055\u308c\u3066\u3044\u308bDNS\u30b5\u30fc\u30d0\u3067\u3001 Kubernetes 1.13 \u4ee5\u964d\u306b\u3066\u30c7\u30d5\u30a9\u30eb\u30c8DNS\u30b5\u30fc\u30d0\u3068\u3057\u3066\u63a1\u7528 \u3055\u308c\u3066\u304a\u308a\u3001Cluster\u5185\u3067\u306eService\u30ea\u30bd\u30fc\u30b9\u306e\u540d\u524d\u89e3\u6c7a\u306b\u5229\u7528\u3057\u3066\u3044\u307e\u3059 ( Kubernetes DNS-Based Service Discovery )\u3002 AWS Load Balancer Controller \u306a\u3069Cluster\u5916\u3078\u30a8\u30f3\u30c9\u30dd\u30a4\u30f3\u30c8\u3092\u516c\u958b\u3059\u308b\u5834\u5408\u306f\u5225\u9014\u5916\u90e8DNS\u30b5\u30fc\u30d0(Route53\u306a\u3069)\u3092\u5229\u7528\u3057\u307e\u3059(CoreDNS\u3092Cluster\u5916\u90e8\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd)\u3002 CoreDNS\u306f\u3042\u3089\u3086\u308b\u51e6\u7406\u3092Plugin\u3068\u3057\u3066\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002CoreDNS\u5358\u4f53\u306fDNS\u30af\u30a8\u30ea\u30fc\u3092\u89e3\u91c8\u3057\u3066\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb( ./Corefile )\u306b\u8a18\u8ff0\u3055\u308c\u305fPlugin\u306b\u51e6\u7406\u3092\u53d7\u3051\u6e21\u3057\u307e\u3059\u3002( \u53c2\u8003 ) kubernetes plugin \u306f Kubernetes DNS-Based Service Discovery Specification. \u306e\u5b9f\u88c5\u3067\u3059\u3002 Corefile \u3067kubernetes plugin\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u5229\u7528\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u8a2d\u5b9a\u306f kubernetes/coredns.yaml.sed \u306e\u5185\u5bb9\u3067\u3059\u3002 fallthrough \u3068\u306fNXDOMAIN(\u4e0d\u5728\u5fdc\u7b54)\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408\u306b\u51e6\u7406\u3092\u4e0b\u6d41\u306ePlugin\u306b\u6e21\u3057\u3066\u304f\u308c\u307e\u3059(\u3053\u306e\u8a2d\u5b9a\u3067\u306f\u9006\u5f15\u304d\u3067NXDOMAIN\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408)\u3002 .:53 { kubernetes cluster.local in-addr.arpa ip6.arpa { fallthrough in-addr.arpa ip6.arpa } }","title":"CoreDNS \u3068\u306f"},{"location":"setup/09_coredns/bootstrapping_coredns/#_1","text":"https://coredns.io/ https://github.com/coredns/coredns https://github.com/coredns/deployment https://github.com/coredns/helm https://engineer.retty.me/entry/2020/12/15/161544 https://www.netone.co.jp/knowledge-center/netone-blog/20191226-1/ https://www.scsk.jp/sp/sysdig/blog/container_monitoring/coredns.html","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/09_coredns/bootstrapping_coredns/#_2","text":"coredns k8s manifests\u3092 \u516c\u5f0f \u304b\u3089\u53d6\u5f97\u3059\u308b git clone https://github.com/coredns/deployment.git coredns_deployment cd coredns_deployment/kubernetes/ bash deploy.sh -i 10.32.0.10 -s -t coredns.yaml.sed | kubectl apply -f -","title":"\u624b\u9806"},{"location":"setup/09_coredns/bootstrapping_coredns/#coredns_1","text":"Corefile \u3067\u4ee5\u4e0b\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u304a\u304f\u3053\u3068\u3067Prometheus\u7528\u306b 9153/TCP \u3067\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u516c\u958b\u3067\u304d\u307e\u3059 .:53 { prometheus :9153 }","title":"CoreDNS\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u53d6\u5f97\u306b\u3064\u3044\u3066"},{"location":"setup/09_coredns/bootstrapping_coredns/#_3","text":"kubectl get pods -n kube-system \u3067\u3044\u3064\u307e\u3067\u7d4c\u3063\u3066\u3082 ContainerCreating \u306e\u307e\u307e kubectl describe pod -n kube-system <POD_ID> Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"<CONTAINER_ID>\" network for pod \"<POD_ID>\": networkPlugin cni failed to set up pod \"<<POD_NAME>\" network: failed to Statfs \"/proc/15875/ns/net\": no such file or directory, failed to clean up sandbox container \"<CONTAINER_ID>\" network for pod \"<POD_ID>\": networkPlugin cni failed to teardown pod \"<POD_NAME>\" network: neither iptables nor ip6tables usable] controller-manager https://github.com/kubernetes/kubernetes/blob/v1.20.2/pkg/controller/endpointslice/utils.go#L407-L415 couldn't find ipfamilies for headless service: kube-system/kube-dns. This could happen if controller manager is connected to an old apiserver that does not support ip families yet. EndpointSlices for this Service will use IPv4 as the IP Family based on familyOf(ClusterIP:10.32.0.10). $ kubectl get service -n kube-system -o jsonpath='{.items[*].spec.clusterIP}' 10.32.0.10 Warning FailedCreatePodSandBox 3m12s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8 s_coredns-675db8b7cc-vxvmv_kube-system_a2446fa3-70e1-41f0-be93-595853692eac_0(9e46ece4c71d1919d3dfc318d0dca65fac540b491f5044e40c076fe996c1ccf3): failed to set bridge addr: \"cni0\" already has an IP address different from 10.200.1.1/24 .:53 [INFO] plugin/reload: Running configuration MD5 = b0741fcbd8bd79287446297caa87f7a1 CoreDNS-1.8.4 linux/arm64, go1.16.4, 053c4d5 [FATAL] plugin/loop: Loop (127.0.0.1:54310 -> :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting . Query: \"HINFO 7207295349162295100.1640133564250428921.\"","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/","text":"bootstrapping nginx ingress controller \u53c2\u8003 https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/ https://kubernetes.github.io/ingress-nginx/ https://github.com/nginxinc/kubernetes-ingress \u624b\u9806 \u69cb\u7bc9 https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/baremetal/deploy.yaml \u52d5\u4f5c\u78ba\u8a8d nginx IngressClass \u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl describe ingressclasses nginx Name: nginx Labels: app.kubernetes.io/component=controller app.kubernetes.io/instance=ingress-nginx app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=ingress-nginx app.kubernetes.io/version=1.0.0 helm.sh/chart=ingress-nginx-4.0.1 Annotations: <none> Controller: k8s.io/ingress-nginx Events: <none> Ingress\u3092\u4f5c\u6210\u3057\u3066worker node\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml sudo tee /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml << EOF > /dev/null --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test-deployment spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort ports: - name: node-port protocol: TCP port: 8080 targetPort: 80 nodePort: 30011 selector: app: nginx --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-test-ingress annotations: external-dns.alpha.kubernetes.io/hostname: <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> spec: ingressClassName: nginx defaultBackend: service: name: nginx-service port: number: 8080 rules: - http: paths: - path: / pathType: Prefix backend: service: name: nginx-service port: number: 8080 # external-dns.alpha.kubernetes.io/hostname annotations\u3092\u6307\u5b9a\u305b\u305arule\u6bce\u306b\u8a2d\u5b9a\u3059\u308b\u4f8b # - host: <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> # http: # paths: # - path: / # pathType: Prefix # backend: # service: # name: nginx-service # port: # number: 8080 EOF <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> \u306e\u7b87\u6240\u3092\u4fee\u6b63\u3059\u308b sudo vim /etc.kubernetes/manifests/04_nginx_ingress_controller.yaml \u30ea\u30bd\u30fc\u30b9\u4f5c\u6210 kubectl apply -f /etc.kubernetes/manifests/04_nginx_ingress_controller.yaml service\u3067node port(30011/TCP) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl describe services nginx-service Name: nginx-service Namespace: default Labels: <none> Annotations: <none> Selector: app=nginx Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.32.0.145 IPs: 10.32.0.145 Port: node-port 8080/TCP TargetPort: 80/TCP NodePort: node-port 30011/TCP Endpoints: 10.200.1.184:80 Session Affinity: None External Traffic Policy: Cluster Events: <none> ingress\u3067worker node\u306eIP\u30a2\u30c9\u30ec\u30b9(wlan0: 192.168.10.51 ) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl describe ingresses nginx-test-ingress Name: nginx-test-ingress Namespace: default Address: 192.168.10.51 Default backend: nginx-service:8080 (10.200.1.184:80) Rules: Host Path Backends ---- ---- -------- * / nginx-service:8080 (10.200.1.184:80) Annotations: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 5m39s (x7 over 25h) nginx-ingress-controller Scheduled for sync MacBookPro\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u3067curl\u306b\u3066\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d $ curl --include http://192.168.10.51:30011/ HTTP/1.1 200 OK Date: Tue, 21 Sep 2021 16:35:07 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Tue, 04 Dec 2018 14:44:49 GMT ETag: \"5c0692e1-264\" Accept-Ranges: bytes <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> \u30a8\u30e9\u30fc\u4e8b\u4f8b Error when getting IngressClass nginx: the server could not find the requested resource https://github.com/nginxinc/kubernetes-ingress/issues/1906 https://qiita.com/smallpalace/items/7a6844651d1d7b43b411 https://github.com/kubernetes/ingress-nginx/issues/7448 https://github.com/kubernetes/ingress-nginx/blob/3c0bfc1ca3eb48246b12e77d40bde1162633efae/deploy/static/provider/baremetal/deploy.yaml","title":"10. bootstrapping nginx ingress controller"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#bootstrapping-nginx-ingress-controller","text":"","title":"bootstrapping nginx ingress controller"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_1","text":"https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/ https://kubernetes.github.io/ingress-nginx/ https://github.com/nginxinc/kubernetes-ingress","title":"\u53c2\u8003"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_2","text":"","title":"\u624b\u9806"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_3","text":"https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/baremetal/deploy.yaml","title":"\u69cb\u7bc9"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_4","text":"","title":"\u52d5\u4f5c\u78ba\u8a8d"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#nginx-ingressclass","text":"$ kubectl describe ingressclasses nginx Name: nginx Labels: app.kubernetes.io/component=controller app.kubernetes.io/instance=ingress-nginx app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=ingress-nginx app.kubernetes.io/version=1.0.0 helm.sh/chart=ingress-nginx-4.0.1 Annotations: <none> Controller: k8s.io/ingress-nginx Events: <none>","title":"nginx IngressClass \u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#ingressworker-nodeip","text":"manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml sudo tee /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml << EOF > /dev/null --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test-deployment spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort ports: - name: node-port protocol: TCP port: 8080 targetPort: 80 nodePort: 30011 selector: app: nginx --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-test-ingress annotations: external-dns.alpha.kubernetes.io/hostname: <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> spec: ingressClassName: nginx defaultBackend: service: name: nginx-service port: number: 8080 rules: - http: paths: - path: / pathType: Prefix backend: service: name: nginx-service port: number: 8080 # external-dns.alpha.kubernetes.io/hostname annotations\u3092\u6307\u5b9a\u305b\u305arule\u6bce\u306b\u8a2d\u5b9a\u3059\u308b\u4f8b # - host: <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> # http: # paths: # - path: / # pathType: Prefix # backend: # service: # name: nginx-service # port: # number: 8080 EOF <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> \u306e\u7b87\u6240\u3092\u4fee\u6b63\u3059\u308b sudo vim /etc.kubernetes/manifests/04_nginx_ingress_controller.yaml \u30ea\u30bd\u30fc\u30b9\u4f5c\u6210 kubectl apply -f /etc.kubernetes/manifests/04_nginx_ingress_controller.yaml service\u3067node port(30011/TCP) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl describe services nginx-service Name: nginx-service Namespace: default Labels: <none> Annotations: <none> Selector: app=nginx Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.32.0.145 IPs: 10.32.0.145 Port: node-port 8080/TCP TargetPort: 80/TCP NodePort: node-port 30011/TCP Endpoints: 10.200.1.184:80 Session Affinity: None External Traffic Policy: Cluster Events: <none> ingress\u3067worker node\u306eIP\u30a2\u30c9\u30ec\u30b9(wlan0: 192.168.10.51 ) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl describe ingresses nginx-test-ingress Name: nginx-test-ingress Namespace: default Address: 192.168.10.51 Default backend: nginx-service:8080 (10.200.1.184:80) Rules: Host Path Backends ---- ---- -------- * / nginx-service:8080 (10.200.1.184:80) Annotations: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 5m39s (x7 over 25h) nginx-ingress-controller Scheduled for sync MacBookPro\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u3067curl\u306b\u3066\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d $ curl --include http://192.168.10.51:30011/ HTTP/1.1 200 OK Date: Tue, 21 Sep 2021 16:35:07 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Tue, 04 Dec 2018 14:44:49 GMT ETag: \"5c0692e1-264\" Accept-Ranges: bytes <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>","title":"Ingress\u3092\u4f5c\u6210\u3057\u3066worker node\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_5","text":"Error when getting IngressClass nginx: the server could not find the requested resource https://github.com/nginxinc/kubernetes-ingress/issues/1906 https://qiita.com/smallpalace/items/7a6844651d1d7b43b411 https://github.com/kubernetes/ingress-nginx/issues/7448 https://github.com/kubernetes/ingress-nginx/blob/3c0bfc1ca3eb48246b12e77d40bde1162633efae/deploy/static/provider/baremetal/deploy.yaml","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/11_external_dns/bootstrapping_external_dns/","text":"bootstrapping external-dns \u53c2\u8003\u60c5\u5831 https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/integrations/external_dns/ kubernetes 1.22 \u5bfe\u5fdc\u72b6\u6cc1\u306b\u3064\u3044\u3066 v0.10.0 \u3067\u5bfe\u5fdc\u6e08\u307f \u30aa\u30f3\u30d7\u30ec\u3067AWS Route53\u5411\u3051external-dns controller\u306e\u8d77\u52d5\u65b9\u6cd5\u306b\u3064\u3044\u3066 https://stackoverflow.com/questions/60267737/is-it-possible-to-use-aws-route-53-as-a-dns-provider-for-a-bare-metal-k8s-cluste https://github.com/kubernetes-sigs/external-dns/issues/539 \u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306eWAN IP\u30a2\u30c9\u30ec\u30b9\u3092DNS Provider\u306b\u901a\u77e5\u3057\u305f\u3044 https://github.com/kubernetes-sigs/external-dns/issues/1394 \u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306b\u304a\u3051\u308bPublic IP\u3092\u6255\u3044\u51fa\u3057\u3064\u3064route53\u3078\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3059\u308b k8s cluster\u5185\u90e8\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u306f\u306a\u304f\u3001worker node\u306einterface(wlan0)\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u308bIP\u30a2\u30c9\u30ec\u30b9(\u4fbf\u5b9c\u7684\u306bpublic ip\u3068\u4eee\u5b9a)\u3068\u540c\u3058\u30ec\u30f3\u30b8\u3067IP\u30a2\u30c9\u30ec\u30b9\u3092\u6255\u3044\u51fa\u3059\u3088\u3046\u306a\u69cb\u6210 \u3053\u308c\u306f\u5c06\u6765\u7528 metallb \u3067k8s cluster\u306e\u5916\u5074\u306bLoadBalancer\u3092\u4f5c\u6210\u3057Public IP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b https://blog.web-apps.tech/type-loadbalancer_by_metallb/ \u69cb\u7bc9\u624b\u9806 1. AWS IAM Policy\u4f5c\u6210 external-dns-controller-policy-document.json \u3092\u4f5c\u6210 external-dns-controller-policy-document.json ``` sudo tee external-dns-controller-policy-document.json << EOF > /dev/null { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\" ], \"Resource\": [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ListHostedZones\", \"route53:ListResourceRecordSets\" ], \"Resource\": [ \"*\" ] } ] } EOF ``` policy\u3092\u4f5c\u6210 aws iam create-policy --policy-name k8s-external-dns-policy --policy-document file://external-dns-controller-policy-document.json 2. AWS IAM User\u4f5c\u6210 user\u3092\u4f5c\u6210 aws iam create-user --user-name k8s-external-dns \u4f5c\u6210\u3057\u305fIAM Policy\u3092\u30a2\u30bf\u30c3\u30c1\u3059\u308b aws iam attach-user-policy --user-name k8s-external-dns --policy-arn arn:aws:iam::<AWS_ACCOUNT_ID>:policy/k8s-external-dns-policy \u4f5c\u6210\u3057\u305fIAM User\u306ecredential\u3092\u78ba\u8a8d\u3059\u308b(Deployments\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u74b0\u5883\u5909\u6570\u3068\u3057\u3066\u30bb\u30c3\u30c8\u3059\u308b) AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY 3. external-dns controller\u3092\u30c7\u30d7\u30ed\u30a4 point https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md \u3092\u30d9\u30fc\u30b9\u306bbare-metal\u5411\u3051\u306b\u4fee\u6b63 namespace\u306f kube-system aws credential\u306f k8s-external-dns iam user\u306e\u3082\u306e hosted_zone_id \u306fexternal-dns\u3067\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3055\u305b\u305f\u3044Route53 zone manifests\u306b\u4ee3\u5165\u3059\u308b\u5909\u6570\u3092\u5b9a\u7fa9 variable name description DOMAIN external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306e\u30c9\u30e1\u30a4\u30f3 HOSTED_ZONE_ID external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306eHosted Zone ID AWS_ACCESS_KEY_ID external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 AWS_SECRET_ACCESS_KEY external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 AWS_DEFAULT_REGION external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 DOMAIN=\"XXXXXXX.com\" HOSTED_ZONE_ID=\"XXXXXXX\" AWS_ACCESS_KEY_ID=\"XXXXXXX\" AWS_SECRET_ACCESS_KEY=\"XXXXXXX\" AWS_DEFAULT_REGION=\"XXXXXXX\" manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 /etc/kubernetes/manifests/external-dns.yaml ``` AWS_DEFAULT_REGION=\"ap-north-east-1\" sudo tee /etc/kubernetes/manifests/external-dns.yaml << EOF > /dev/null --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: external-dns rules: - apiGroups: [\"\"] resources: [\"services\",\"endpoints\",\"pods\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"extensions\",\"networking.k8s.io\"] resources: [\"ingresses\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"list\",\"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: external-dns-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: kube-system spec: strategy: type: Recreate selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: k8s.gcr.io/external-dns/external-dns:v0.10.0 env: - name: AWS_ACCESS_KEY_ID value: <k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_ACCESS_KEY_ID> - name: AWS_SECRET_ACCESS_KEY value: <k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_SECRET_ACCESS_KEY> - name: AWS_DEFAULT_REGION value: \"${AWS_DEFAULT_REGION}\" args: - --source=service - --source=ingress - --domain-filter=${DOMAIN} # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=<HOSTED_ZONE_ID> - --txt-prefix=prefix_ - --log-level=debug securityContext: fsGroup: 65534 # For ExternalDNS to be able to read Kubernetes and AWS token files EOF ``` \u30c7\u30d7\u30ed\u30a4 kubectl apply -f /etc/kubernetes/manifests/external-dns.yaml \u52d5\u4f5c\u78ba\u8a8d external-dns \u30b3\u30f3\u30c6\u30ca\u30ed\u30b0 \u30c7\u30d7\u30ed\u30a4\u6e08\u307fingress\u306ehostname\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3092\u78ba\u8a8d time=\"2021-09-26T04:59:24Z\" level=info msg=\"Instantiating new Kubernetes client\" time=\"2021-09-26T04:59:24Z\" level=debug msg=\"apiServerURL: \" time=\"2021-09-26T04:59:24Z\" level=debug msg=\"kubeConfig: \" time=\"2021-09-26T04:59:24Z\" level=info msg=\"Using inCluster-config based on serviceaccount-token\" time=\"2021-09-26T04:59:24Z\" level=info msg=\"Created Kubernetes client https://10.32.0.1:443\" time=\"2021-09-26T04:59:30Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/kube-dns\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/metrics-server\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/kubernetes\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/nginx-service\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller-admission\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN A 192.168.10.51 []]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com A [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com TXT [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:32Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>] were successfully updated\" route53 record set \u5bfe\u8c61\u306ehosted zone\u306bingress\u306ehost\u3067\u6307\u5b9a\u3057\u305fhostname\u3067\u30ec\u30b3\u30fc\u30c9\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ aws route53 list-resource-record-sets --output json --hosted-zone-id <HOSTED_ZONE_ID> | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\"))' [ { \"Name\": \"dev1.example.com.\", \"Type\": \"A\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"192.168.10.51\" } ] }, { \"Name\": \"dev1.example.com.\", \"Type\": \"TXT\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"\\\"heritage=external-dns,external-dns/owner=<HOSTED_ZONE_ID>,external-dns/resource=ingress/default/nginx-test-ingress\\\"\" } ] } ] \u767b\u9332\u3055\u308c\u305fA\u30ec\u30b3\u30fc\u30c9\u306ehostname\u304c\u540d\u524d\u89e3\u6c7a\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d k8s service\u30ea\u30bd\u30fc\u30b9\u304b\u3089\u898b\u308b\u3068node port\u306b\u5bfe\u3059\u308bnode address\u306f\u81ea\u5b85\u74b0\u5883\u306eWifi\u30eb\u30fc\u30bf\u3067\u6255\u3044\u51fa\u3059\u30ec\u30f3\u30b8(192.168.10.0/24)\u306a\u306e\u3067\u60f3\u5b9a\u901a\u308a $ dig +noall +answer dev1.example.com dev1.example.com. 283 IN A 192.168.10.51 Appendix Ingress\u30ea\u30bd\u30fc\u30b9\u3067\u4f7f\u7528\u53ef\u80fd\u306aannotations https://github.com/kubernetes-sigs/external-dns/blob/v0.10.0/source/source.go#L40-L68 annotations describe external-dns.alpha.kubernetes.io/controller \u8907\u6570\u306eDNS Controller\u304c\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u3069\u306eDNS Controller\u304c\u8cac\u4efb\u3092\u8ca0\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u628a\u63e1\u3059\u308b\u305f\u3081\u306b\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/hostname \u4f7f\u7528\u3059\u308b\u30db\u30b9\u30c8\u540d\u3092\u6307\u5b9a\u3059\u308b Service\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u3067\u6307\u5b9a\u3059\u308b Ingress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u304brule\u306ehost attr\u3067\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/access \u30d1\u30d6\u30ea\u30c3\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u3068\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u306e\u3069\u3061\u3089\u3092\u4f7f\u7528\u3059\u308b\u304b\u3092\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/target CNAME record\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\u306bCNAME record\u306evalue\u3068\u306a\u308b\u5024\u3092\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/ttl DNS record\u306eTTL\u3092\u6307\u5b9a\u3059\u308b(default: 300) external-dns.alpha.kubernetes.io/alias true \u3067ALIAS record\u3092\u4f5c\u6210\u3059\u308b external-dns.alpha.kubernetes.io/ingress-hostname-source Ingress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306bhostname\u306e\u6307\u5b9a\u65b9\u6cd5\u3092annotations\u304brule\u306ehost attr\u304b\u3092\u9650\u5b9a\u3067\u304d\u308b external-dns.alpha.kubernetes.io/internal-hostname Target IP\u30a2\u30c9\u30ec\u30b9\u3092Cluster IP\u30a2\u30c9\u30ec\u30b9\u3068\u3059\u308b\u5834\u5408\u306b\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/set-identifier AWS Route53\u306b\u304a\u3044\u3066DNS Name\u3068Type\u304c\u540c\u3058\u5834\u5408\u306b\u91cd\u307f\u4ed8\u3051\u306a\u3069\u306b\u3088\u308b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30dd\u30ea\u30b7\u30fc\u3092\u5b9a\u7fa9\u3059\u308b\u969b\u306e\u8b58\u5225\u5b50 Record ID\u3068\u306a\u308b\u5024 external-dns\u3067A\u30ec\u30b3\u30fc\u30c9\u3067\u306f\u306a\u304fCNAME\u30ec\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3059\u308b controller\u306e\u8d77\u52d5\u5f15\u6570\u306b --txt-prefix=<prefix\u6587\u5b57\u5217> \u3092\u8ffd\u52a0 \u6307\u5b9a\u3057\u305f\u6587\u5b57\u5217\u304cTXT record\u306e\u30ec\u30b3\u30fc\u30c9\u540d(A\u30ec\u30b3\u30fc\u30c9\u306e\u5834\u5408\u306fA\u30ec\u30b3\u30fc\u30c9\u3068\u540c\u540d)\u306eprefix\u3068\u3057\u3066\u8ffd\u52a0\u3055\u308c\u307e\u3059 CNAME\u30ec\u30b3\u30fc\u30c9\u306f(TXT\u30ec\u30b3\u30fc\u30c9\u3067\u3042\u3063\u3066\u3082)\u4ed6\u306e\u30ec\u30b3\u30fc\u30c9\u3068\u5171\u5b58\u3067\u304d\u306a\u3044\u4ed5\u69d8\u3067\u3059( RFC 1034\u30bb\u30af\u30b7\u30e7\u30f33.6.2 \u3001 RFC 1912\u30bb\u30af\u30b7\u30e7\u30f32.4 https://github.com/kubernetes-sigs/external-dns/blob/master/docs/faq.md#im-using-an-elb-with-txt-registry-but-the-cname-record-clashes-with-the-txt-record-how-to-avoid-this Ingress\u30ea\u30bd\u30fc\u30b9\u306eannotations\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/hostname \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eFQDN\u3092\u6307\u5b9a external-dns.alpha.kubernetes.io/target \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eValue\u3068\u306a\u308b\u53c2\u7167\u5148\u306eFQDN\u307e\u305f\u306fIP\u30a2\u30c9\u30ec\u30b9\u306a\u3069\u6307\u5b9a external-dns.alpha.kubernetes.io/hostname: dev1.example.com external-dns.alpha.kubernetes.io/target: alias1.example.com --txt-prefix=prefix_ \u3067\u52d5\u4f5c\u78ba\u8a8d Ingress\u30ea\u30bd\u30fc\u30b9 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-test-ingress annotations: external-dns.alpha.kubernetes.io/hostname: dev1.example.com external-dns.alpha.kubernetes.io/target: alias1.example.com spec: ingressClassName: nginx defaultBackend: service: name: nginx-service port: number: 8080 rules: - host: dev1.example.com http: paths: - path: / pathType: Prefix backend: service: name: nginx-service port: number: 8080 external-dns log time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN CNAME alias1.example.com [] dev1.example.com 0 IN CNAME alias1.example.com []]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Removing duplicate endpoint dev1.example.com 0 IN CNAME alias1.example.com []\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding prefix_dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE dev1.example.com CNAME [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE prefix_dev1.example.com TXT [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:19Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>] were successfully updated\" route53 \u30ec\u30b3\u30fc\u30c9\u78ba\u8a8d $ aws route53 list-resource-record-sets --output json --hosted-zone-id <HOSTED_ZONE_ID> | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\" or .Name == \"prefix_dev1.example.com.\"))' [ { \"Name\": \"prefix_dev1.example.com.\", \"Type\": \"TXT\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"\\\"heritage=external-dns,external-dns/owner=<HOSTED_ZONE_ID>,external-dns/resource=ingress/default/nginx-test-ingress\\\"\" } ] }, { \"Name\": \"dev1.example.com.\", \"Type\": \"CNAME\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"alias1.example.com\" } ] } ] $ dig +noall +answer dev1.example.com dev1.example.com. 300 IN CNAME alias1.example.com.","title":"11. bootstrapping external-dns"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#bootstrapping-external-dns","text":"","title":"bootstrapping external-dns"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_1","text":"https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/integrations/external_dns/","title":"\u53c2\u8003\u60c5\u5831"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#kubernetes-122","text":"v0.10.0 \u3067\u5bfe\u5fdc\u6e08\u307f","title":"kubernetes 1.22 \u5bfe\u5fdc\u72b6\u6cc1\u306b\u3064\u3044\u3066"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#aws-route53external-dns-controller","text":"https://stackoverflow.com/questions/60267737/is-it-possible-to-use-aws-route-53-as-a-dns-provider-for-a-bare-metal-k8s-cluste https://github.com/kubernetes-sigs/external-dns/issues/539","title":"\u30aa\u30f3\u30d7\u30ec\u3067AWS Route53\u5411\u3051external-dns controller\u306e\u8d77\u52d5\u65b9\u6cd5\u306b\u3064\u3044\u3066"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#wan-ipdns-provider","text":"https://github.com/kubernetes-sigs/external-dns/issues/1394","title":"\u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306eWAN IP\u30a2\u30c9\u30ec\u30b9\u3092DNS Provider\u306b\u901a\u77e5\u3057\u305f\u3044"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#public-iproute53","text":"k8s cluster\u5185\u90e8\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u306f\u306a\u304f\u3001worker node\u306einterface(wlan0)\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u308bIP\u30a2\u30c9\u30ec\u30b9(\u4fbf\u5b9c\u7684\u306bpublic ip\u3068\u4eee\u5b9a)\u3068\u540c\u3058\u30ec\u30f3\u30b8\u3067IP\u30a2\u30c9\u30ec\u30b9\u3092\u6255\u3044\u51fa\u3059\u3088\u3046\u306a\u69cb\u6210 \u3053\u308c\u306f\u5c06\u6765\u7528 metallb \u3067k8s cluster\u306e\u5916\u5074\u306bLoadBalancer\u3092\u4f5c\u6210\u3057Public IP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b https://blog.web-apps.tech/type-loadbalancer_by_metallb/","title":"\u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306b\u304a\u3051\u308bPublic IP\u3092\u6255\u3044\u51fa\u3057\u3064\u3064route53\u3078\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3059\u308b"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_2","text":"","title":"\u69cb\u7bc9\u624b\u9806"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#1-aws-iam-policy","text":"external-dns-controller-policy-document.json \u3092\u4f5c\u6210 external-dns-controller-policy-document.json ``` sudo tee external-dns-controller-policy-document.json << EOF > /dev/null { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\" ], \"Resource\": [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ListHostedZones\", \"route53:ListResourceRecordSets\" ], \"Resource\": [ \"*\" ] } ] } EOF ``` policy\u3092\u4f5c\u6210 aws iam create-policy --policy-name k8s-external-dns-policy --policy-document file://external-dns-controller-policy-document.json","title":"1. AWS IAM Policy\u4f5c\u6210"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#2-aws-iam-user","text":"user\u3092\u4f5c\u6210 aws iam create-user --user-name k8s-external-dns \u4f5c\u6210\u3057\u305fIAM Policy\u3092\u30a2\u30bf\u30c3\u30c1\u3059\u308b aws iam attach-user-policy --user-name k8s-external-dns --policy-arn arn:aws:iam::<AWS_ACCOUNT_ID>:policy/k8s-external-dns-policy \u4f5c\u6210\u3057\u305fIAM User\u306ecredential\u3092\u78ba\u8a8d\u3059\u308b(Deployments\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u74b0\u5883\u5909\u6570\u3068\u3057\u3066\u30bb\u30c3\u30c8\u3059\u308b) AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY","title":"2. AWS IAM User\u4f5c\u6210"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#3-external-dns-controller","text":"point https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md \u3092\u30d9\u30fc\u30b9\u306bbare-metal\u5411\u3051\u306b\u4fee\u6b63 namespace\u306f kube-system aws credential\u306f k8s-external-dns iam user\u306e\u3082\u306e hosted_zone_id \u306fexternal-dns\u3067\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3055\u305b\u305f\u3044Route53 zone manifests\u306b\u4ee3\u5165\u3059\u308b\u5909\u6570\u3092\u5b9a\u7fa9 variable name description DOMAIN external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306e\u30c9\u30e1\u30a4\u30f3 HOSTED_ZONE_ID external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306eHosted Zone ID AWS_ACCESS_KEY_ID external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 AWS_SECRET_ACCESS_KEY external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 AWS_DEFAULT_REGION external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 DOMAIN=\"XXXXXXX.com\" HOSTED_ZONE_ID=\"XXXXXXX\" AWS_ACCESS_KEY_ID=\"XXXXXXX\" AWS_SECRET_ACCESS_KEY=\"XXXXXXX\" AWS_DEFAULT_REGION=\"XXXXXXX\" manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 /etc/kubernetes/manifests/external-dns.yaml ``` AWS_DEFAULT_REGION=\"ap-north-east-1\" sudo tee /etc/kubernetes/manifests/external-dns.yaml << EOF > /dev/null --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: external-dns rules: - apiGroups: [\"\"] resources: [\"services\",\"endpoints\",\"pods\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"extensions\",\"networking.k8s.io\"] resources: [\"ingresses\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"list\",\"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: external-dns-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: kube-system spec: strategy: type: Recreate selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: k8s.gcr.io/external-dns/external-dns:v0.10.0 env: - name: AWS_ACCESS_KEY_ID value: <k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_ACCESS_KEY_ID> - name: AWS_SECRET_ACCESS_KEY value: <k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_SECRET_ACCESS_KEY> - name: AWS_DEFAULT_REGION value: \"${AWS_DEFAULT_REGION}\" args: - --source=service - --source=ingress - --domain-filter=${DOMAIN} # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=<HOSTED_ZONE_ID> - --txt-prefix=prefix_ - --log-level=debug securityContext: fsGroup: 65534 # For ExternalDNS to be able to read Kubernetes and AWS token files EOF ``` \u30c7\u30d7\u30ed\u30a4 kubectl apply -f /etc/kubernetes/manifests/external-dns.yaml","title":"3. external-dns controller\u3092\u30c7\u30d7\u30ed\u30a4"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_3","text":"external-dns \u30b3\u30f3\u30c6\u30ca\u30ed\u30b0 \u30c7\u30d7\u30ed\u30a4\u6e08\u307fingress\u306ehostname\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3092\u78ba\u8a8d time=\"2021-09-26T04:59:24Z\" level=info msg=\"Instantiating new Kubernetes client\" time=\"2021-09-26T04:59:24Z\" level=debug msg=\"apiServerURL: \" time=\"2021-09-26T04:59:24Z\" level=debug msg=\"kubeConfig: \" time=\"2021-09-26T04:59:24Z\" level=info msg=\"Using inCluster-config based on serviceaccount-token\" time=\"2021-09-26T04:59:24Z\" level=info msg=\"Created Kubernetes client https://10.32.0.1:443\" time=\"2021-09-26T04:59:30Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/kube-dns\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/metrics-server\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/kubernetes\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/nginx-service\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller-admission\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN A 192.168.10.51 []]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com A [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com TXT [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:32Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>] were successfully updated\" route53 record set \u5bfe\u8c61\u306ehosted zone\u306bingress\u306ehost\u3067\u6307\u5b9a\u3057\u305fhostname\u3067\u30ec\u30b3\u30fc\u30c9\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ aws route53 list-resource-record-sets --output json --hosted-zone-id <HOSTED_ZONE_ID> | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\"))' [ { \"Name\": \"dev1.example.com.\", \"Type\": \"A\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"192.168.10.51\" } ] }, { \"Name\": \"dev1.example.com.\", \"Type\": \"TXT\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"\\\"heritage=external-dns,external-dns/owner=<HOSTED_ZONE_ID>,external-dns/resource=ingress/default/nginx-test-ingress\\\"\" } ] } ] \u767b\u9332\u3055\u308c\u305fA\u30ec\u30b3\u30fc\u30c9\u306ehostname\u304c\u540d\u524d\u89e3\u6c7a\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d k8s service\u30ea\u30bd\u30fc\u30b9\u304b\u3089\u898b\u308b\u3068node port\u306b\u5bfe\u3059\u308bnode address\u306f\u81ea\u5b85\u74b0\u5883\u306eWifi\u30eb\u30fc\u30bf\u3067\u6255\u3044\u51fa\u3059\u30ec\u30f3\u30b8(192.168.10.0/24)\u306a\u306e\u3067\u60f3\u5b9a\u901a\u308a $ dig +noall +answer dev1.example.com dev1.example.com. 283 IN A 192.168.10.51","title":"\u52d5\u4f5c\u78ba\u8a8d"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#appendix","text":"","title":"Appendix"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#ingressannotations","text":"https://github.com/kubernetes-sigs/external-dns/blob/v0.10.0/source/source.go#L40-L68 annotations describe external-dns.alpha.kubernetes.io/controller \u8907\u6570\u306eDNS Controller\u304c\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u3069\u306eDNS Controller\u304c\u8cac\u4efb\u3092\u8ca0\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u628a\u63e1\u3059\u308b\u305f\u3081\u306b\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/hostname \u4f7f\u7528\u3059\u308b\u30db\u30b9\u30c8\u540d\u3092\u6307\u5b9a\u3059\u308b Service\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u3067\u6307\u5b9a\u3059\u308b Ingress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u304brule\u306ehost attr\u3067\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/access \u30d1\u30d6\u30ea\u30c3\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u3068\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u306e\u3069\u3061\u3089\u3092\u4f7f\u7528\u3059\u308b\u304b\u3092\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/target CNAME record\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\u306bCNAME record\u306evalue\u3068\u306a\u308b\u5024\u3092\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/ttl DNS record\u306eTTL\u3092\u6307\u5b9a\u3059\u308b(default: 300) external-dns.alpha.kubernetes.io/alias true \u3067ALIAS record\u3092\u4f5c\u6210\u3059\u308b external-dns.alpha.kubernetes.io/ingress-hostname-source Ingress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306bhostname\u306e\u6307\u5b9a\u65b9\u6cd5\u3092annotations\u304brule\u306ehost attr\u304b\u3092\u9650\u5b9a\u3067\u304d\u308b external-dns.alpha.kubernetes.io/internal-hostname Target IP\u30a2\u30c9\u30ec\u30b9\u3092Cluster IP\u30a2\u30c9\u30ec\u30b9\u3068\u3059\u308b\u5834\u5408\u306b\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/set-identifier AWS Route53\u306b\u304a\u3044\u3066DNS Name\u3068Type\u304c\u540c\u3058\u5834\u5408\u306b\u91cd\u307f\u4ed8\u3051\u306a\u3069\u306b\u3088\u308b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30dd\u30ea\u30b7\u30fc\u3092\u5b9a\u7fa9\u3059\u308b\u969b\u306e\u8b58\u5225\u5b50 Record ID\u3068\u306a\u308b\u5024","title":"Ingress\u30ea\u30bd\u30fc\u30b9\u3067\u4f7f\u7528\u53ef\u80fd\u306aannotations"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#external-dnsacname","text":"controller\u306e\u8d77\u52d5\u5f15\u6570\u306b --txt-prefix=<prefix\u6587\u5b57\u5217> \u3092\u8ffd\u52a0 \u6307\u5b9a\u3057\u305f\u6587\u5b57\u5217\u304cTXT record\u306e\u30ec\u30b3\u30fc\u30c9\u540d(A\u30ec\u30b3\u30fc\u30c9\u306e\u5834\u5408\u306fA\u30ec\u30b3\u30fc\u30c9\u3068\u540c\u540d)\u306eprefix\u3068\u3057\u3066\u8ffd\u52a0\u3055\u308c\u307e\u3059 CNAME\u30ec\u30b3\u30fc\u30c9\u306f(TXT\u30ec\u30b3\u30fc\u30c9\u3067\u3042\u3063\u3066\u3082)\u4ed6\u306e\u30ec\u30b3\u30fc\u30c9\u3068\u5171\u5b58\u3067\u304d\u306a\u3044\u4ed5\u69d8\u3067\u3059( RFC 1034\u30bb\u30af\u30b7\u30e7\u30f33.6.2 \u3001 RFC 1912\u30bb\u30af\u30b7\u30e7\u30f32.4 https://github.com/kubernetes-sigs/external-dns/blob/master/docs/faq.md#im-using-an-elb-with-txt-registry-but-the-cname-record-clashes-with-the-txt-record-how-to-avoid-this Ingress\u30ea\u30bd\u30fc\u30b9\u306eannotations\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/hostname \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eFQDN\u3092\u6307\u5b9a external-dns.alpha.kubernetes.io/target \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eValue\u3068\u306a\u308b\u53c2\u7167\u5148\u306eFQDN\u307e\u305f\u306fIP\u30a2\u30c9\u30ec\u30b9\u306a\u3069\u6307\u5b9a external-dns.alpha.kubernetes.io/hostname: dev1.example.com external-dns.alpha.kubernetes.io/target: alias1.example.com --txt-prefix=prefix_ \u3067\u52d5\u4f5c\u78ba\u8a8d Ingress\u30ea\u30bd\u30fc\u30b9 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-test-ingress annotations: external-dns.alpha.kubernetes.io/hostname: dev1.example.com external-dns.alpha.kubernetes.io/target: alias1.example.com spec: ingressClassName: nginx defaultBackend: service: name: nginx-service port: number: 8080 rules: - host: dev1.example.com http: paths: - path: / pathType: Prefix backend: service: name: nginx-service port: number: 8080 external-dns log time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN CNAME alias1.example.com [] dev1.example.com 0 IN CNAME alias1.example.com []]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Removing duplicate endpoint dev1.example.com 0 IN CNAME alias1.example.com []\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding prefix_dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE dev1.example.com CNAME [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE prefix_dev1.example.com TXT [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:19Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>] were successfully updated\" route53 \u30ec\u30b3\u30fc\u30c9\u78ba\u8a8d $ aws route53 list-resource-record-sets --output json --hosted-zone-id <HOSTED_ZONE_ID> | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\" or .Name == \"prefix_dev1.example.com.\"))' [ { \"Name\": \"prefix_dev1.example.com.\", \"Type\": \"TXT\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"\\\"heritage=external-dns,external-dns/owner=<HOSTED_ZONE_ID>,external-dns/resource=ingress/default/nginx-test-ingress\\\"\" } ] }, { \"Name\": \"dev1.example.com.\", \"Type\": \"CNAME\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"alias1.example.com\" } ] } ] $ dig +noall +answer dev1.example.com dev1.example.com. 300 IN CNAME alias1.example.com.","title":"external-dns\u3067A\u30ec\u30b3\u30fc\u30c9\u3067\u306f\u306a\u304fCNAME\u30ec\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3059\u308b"},{"location":"setup/12_metrics_server/bootstrapping_metrics_server/","text":"metrics server metrics-server \u3068\u306fkubelet\u3084kube-apiserver\u304b\u3089\u5404\u7a2e\u30ea\u30bd\u30fc\u30b9\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002 \u53ce\u96c6\u3057\u305f\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u60c5\u5831\u306fautoscaling( HPA \u3084 VPA )\u3092\u884c\u3046\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u53c2\u8003 https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/ https://github.com/kubernetes-sigs/metrics-server \u69cb\u7bc9\u624b\u9806 $ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml serviceaccount/metrics-server created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrole.rbac.authorization.k8s.io/system:metrics-server created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created service/metrics-server created deployment.apps/metrics-server created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created kubectl top pod \u306a\u3069\u3067\u4ee5\u4e0b\u30a8\u30e9\u30fc Error from server (ServiceUnavailable): the server is currently unable to handle the request (get pods.metrics.k8s.io) kube-apiserver \u3067\u4ee5\u4e0b\u30ed\u30b0 E0829 14:15:26.199732 1 controller.go:116] loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]] I0829 14:15:26.199769 1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.","title":"12. bootstrapping metrics_server"},{"location":"setup/12_metrics_server/bootstrapping_metrics_server/#metrics-server","text":"metrics-server \u3068\u306fkubelet\u3084kube-apiserver\u304b\u3089\u5404\u7a2e\u30ea\u30bd\u30fc\u30b9\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002 \u53ce\u96c6\u3057\u305f\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u60c5\u5831\u306fautoscaling( HPA \u3084 VPA )\u3092\u884c\u3046\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002","title":"metrics server"},{"location":"setup/12_metrics_server/bootstrapping_metrics_server/#_1","text":"https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/ https://github.com/kubernetes-sigs/metrics-server","title":"\u53c2\u8003"},{"location":"setup/12_metrics_server/bootstrapping_metrics_server/#_2","text":"$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml serviceaccount/metrics-server created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrole.rbac.authorization.k8s.io/system:metrics-server created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created service/metrics-server created deployment.apps/metrics-server created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created kubectl top pod \u306a\u3069\u3067\u4ee5\u4e0b\u30a8\u30e9\u30fc Error from server (ServiceUnavailable): the server is currently unable to handle the request (get pods.metrics.k8s.io) kube-apiserver \u3067\u4ee5\u4e0b\u30ed\u30b0 E0829 14:15:26.199732 1 controller.go:116] loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]] I0829 14:15:26.199769 1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.","title":"\u69cb\u7bc9\u624b\u9806"},{"location":"static_pod/","text":"8\u6708 31 12:57:50 k8s-master kubelet[1912]: E0831 12:57:50.411795 1912 kubelet.go:1635] Failed creating a mirror pod for \"etcd-k8s-master_kube-system(8fff3e1f31b52ebeb520767ae50cc739)\": pods \"etcd-k8s-master\" is forbidden: PodSecurityPolicy: no providers available to validate pod request 8\u6708 31 12:58:55 k8s-master kubelet[1912]: I0831 12:58:55.446865 1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-apiserver-k8s-master_kube-system(b2f9759c-9ffe-436c-9ebb-0b5f9c68a452)\" 8\u6708 31 12:58:58 k8s-master kubelet[1912]: I0831 12:58:58.433779 1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-scheduler-k8s-master_kube-system(b502a669-2571-41f7-a705-3aa194ade526)\" 8\u6708 31 12:59:00 k8s-master kubelet[1912]: I0831 12:59:00.432199 1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-controller-manager-k8s-master_kube-system(f834a62b-d802-40c5-90df-24a49a38efb6)\" 8\u6708 31 12:59:11 k8s-master kubelet[1912]: I0831 12:59:11.421459 1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"etcd-k8s-master_kube-system(e612e1ab-a907-4955-8b4c-97d5a37b11fa)\" 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849447 1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/etcd-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849581 1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-apiserver-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849654 1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-controller-manager-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849719 1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-scheduler-k8s-master\" status=Running","title":"Index"}]}