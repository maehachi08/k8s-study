{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"addons/metallb/","title":"MetalLB","text":""},{"location":"addons/metallb/#about-metallb","title":"About MetalLB","text":"<p>https://metallb.universe.tf/ https://github.com/metallb/metallb</p> <p>Bare Metal\u306a\u74b0\u5883\u3067LoadBlancer\u30b5\u30fc\u30d3\u30b9\u3092\u63d0\u4f9b\u3059\u308baddon\u3067\u3059\u3002</p> <p>CloudProvider\u304c\u63d0\u4f9b\u3059\u308bKubernetes\u30b5\u30fc\u30d3\u30b9\u3067\u306f\u5f53\u8a72CloudProvider\u306eLoadBlancer\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3067\u304d\u307e\u3059\u3002 AWS\u3067\u306f AWS Load Balancer Controller \u3092\u5c0e\u5165\u3059\u308b\u3053\u3068\u3067Ingress\u30ea\u30bd\u30fc\u30b9\u3067Elastic Load Balancer\u306e\u4f5c\u6210\u3092\u884c\u3046\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002</p> <p>Bare Metal\u306a\u74b0\u5883\u3067Nginx Ingress Controller\u3092\u5c0e\u5165\u3057\u305f\u5834\u5408\u3001Cluster\u5185\u306ePod Network\u306eIP\u30a2\u30c9\u30ec\u30b9\u304c\u30a2\u30b5\u30a4\u30f3\u3055\u308c\u307e\u3059\u3002</p> <p>\u79c1\u306fRaspberry pi(ubuntu server)\u3067Kubernetes Cluster\u3092\u69cb\u7bc9\u3057\u3066\u304a\u308a\u3001Node IP\u30a2\u30c9\u30ec\u30b9\u306f\u81ea\u5b85\u306eWiFi\u30eb\u30fc\u30bf(<code>192.168.3.0/24</code>)\u304b\u3089\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u3002 Cluster Cidr(<code>10.200.0.0/16</code>)\u3067\u306fMacBook\u306a\u3069Cluster\u5916\u306e\u30d6\u30e9\u30a6\u30b6\u30a2\u30af\u30bb\u30b9\u304c\u3067\u304d\u305a\u3001(NAT\u5909\u63db\u306a\u3069\u3092\u99c6\u4f7f\u3059\u308c\u3070\u53ef\u80fd\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c) \u5c11\u3005\u4e0d\u4fbf\u3067\u3059\u3002</p> <p>MetalLB\u3067\u306f <code>Service</code> \u30ea\u30bd\u30fc\u30b9\u3067 <code>type: LoadBalancer</code> \u3092\u6307\u5b9a\u53ef\u80fd\u3068\u3057\u3001\u304b\u3064<code>192.168.3.200/27</code> \u306a\u3069Kubernetes Cluster\u306e\u5916\u90e8\u306b\u516c\u958b\u53ef\u80fd\u306aIP\u30a2\u30c9\u30ec\u30b9\u306e\u5272\u308a\u5f53\u3066\u304c\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"addons/metallb/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://blog.framinal.life/entry/2020/04/16/022042</li> <li>https://garafu.blogspot.com/2019/06/install-metallb.html</li> </ul>"},{"location":"addons/metallb/#installation","title":"Installation","text":"<ul> <li>https://metallb.universe.tf/installation/</li> </ul>"},{"location":"addons/metallb/#configuration","title":"Configuration","text":"<ul> <li> <p>https://metallb.universe.tf/configuration/</p> <ul> <li><code>IPAddressPool</code><ul> <li><code>type: LoadBalancer</code> \u3092\u6307\u5b9a\u3057\u305fService\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306e\u30d7\u30fc\u30eb\u3092\u5b9a\u7fa9</li> </ul> </li> <li><code>L2Advertisement</code><ul> <li>IP\u30a2\u30c9\u30ec\u30b9\u306eAdvertisement\u3092\u884c\u3046k8s node\u3092\u6307\u5b9a\u3059\u308b</li> <li>https://metallb.universe.tf/configuration/_advanced_l2_configuration/</li> </ul> </li> </ul> <p>manifests</p> <pre><code>---\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: ip-pool\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.3.200-192.168.3.210\n\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: ip-pool-advertisement\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n  - ip-pool\n  nodeSelectors:\n  - matchLabels:\n      kubernetes.io/hostname: k8s-master\n  - matchLabels:\n      kubernetes.io/hostname: k8s-node1\n  - matchLabels:\n      kubernetes.io/hostname: k8s-node2\n</code></pre> </li> </ul>"},{"location":"addons/metallb/#kubernetes-dashboard-metallbip","title":"Kubernetes Dashboard \u3092MetalLB\u3067\u6255\u3044\u51fa\u3057\u305fIP\u30a2\u30c9\u30ec\u30b9\u3067\u30a2\u30af\u30bb\u30b9\u3057\u3066\u307f\u308b","text":"<ol> <li> <p>edit of kubernetes-dashboard manifests</p> <ul> <li>Service\u30ea\u30bd\u30fc\u30b9\u306eType\u3092 <code>LoadBalancer</code> \u306b\u5909\u66f4</li> <li><code>metallb.universe.tf/address-pool</code> annotations\u3092\u8ffd\u8a18</li> </ul> <pre><code>@@ -36,7 +36,10 @@\n     k8s-app: kubernetes-dashboard\n   name: kubernetes-dashboard\n   namespace: kubernetes-dashboard\n+  annotations:\n+    metallb.universe.tf/address-pool: ip-pool\n spec:\n+  type: LoadBalancer\n   ports:\n     - port: 443\n       targetPort: 8443\n</code></pre> </li> <li> <p>apply kubernetes-dashboard manifests     <pre><code>kubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard.yaml\n</code></pre></p> </li> <li> <p>check service and ingress</p> <p>service</p> <pre><code>$ kubectl describe svc -n kubernetes-dashboard kubernetes-dashboard\nName:                     kubernetes-dashboard\nNamespace:                kubernetes-dashboard\nLabels:                   k8s-app=kubernetes-dashboard\nAnnotations:              metallb.universe.tf/address-pool: ip-pool\nSelector:                 k8s-app=kubernetes-dashboard\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.32.0.177\nIPs:                      10.32.0.177\nLoadBalancer Ingress:     192.168.3.201\nPort:                     &lt;unset&gt;  443/TCP\nTargetPort:               8443/TCP\nNodePort:                 &lt;unset&gt;  30522/TCP\nEndpoints:                10.200.2.78:8443\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:\n  Type    Reason        Age                 From                Message\n  ----    ------        ----                ----                -------\n  Normal  IPAllocated   53m                 metallb-controller  Assigned IP [\"192.168.3.201\"]\n  Normal  nodeAssigned  50s (x34 over 53m)  metallb-speaker     announcing from node \"k8s-master\" with protocol \"layer2\"\n</code></pre> <p>ingress</p> <pre><code>$ kubectl describe ingress -n kubernetes-dashboard dashboard-ingress\nName:             dashboard-ingress\nLabels:           &lt;none&gt;\nNamespace:        kubernetes-dashboard\nAddress:          192.168.3.200\nIngress Class:    &lt;none&gt;\nDefault backend:  &lt;default&gt;\nTLS:\n  dashboard-secret-tls terminates k8s-dashboard.local\nRules:\n  Host                 Path  Backends\n  ----                 ----  --------\n  k8s-dashboard.local\n                       /   kubernetes-dashboard:443 (10.200.2.78:8443)\nAnnotations:           kubernetes.io/ingress.class: nginx\n                       nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n                       nginx.ingress.kubernetes.io/ssl-passthrough: true\nEvents:\n  Type    Reason  Age                From                      Message\n  ----    ------  ----               ----                      -------\n  Normal  Sync    53m (x2 over 54m)  nginx-ingress-controller  Scheduled for sync\n</code></pre> </li> <li> <p>Service\u306b\u5272\u308a\u5f53\u3066\u3089\u308c\u3066\u3044\u308b <code>192.168.3.201</code> \u3067\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> <ul> <li>NodePort\u306a\u3069\u306e\u6307\u5b9a\u306f\u4e0d\u8981\u306a\u306e\u3067\u30a2\u30af\u30bb\u30b9\u304c\u624b\u8efd</li> <li></li> </ul> </li> </ol>"},{"location":"addons/argo/argo-rollouts/about/","title":"About","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/</li> <li>https://github.com/argoproj/argo-rollouts</li> <li>https://techstep.hatenablog.com/entry/2020/10/13/084905</li> </ul> <p>Argo Rollouts\u306f\u3001<code>Blue/Green</code> \u3084 <code>Canary</code> \u3068\u3044\u3063\u305fdeploy strategies\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> <p>Kubernetes Deployment\u6a19\u6e96\u306edeploy strategies(<code>RollingUpdate</code>) \u3068\u6bd4\u8f03\u3057\u9ad8\u5ea6\u306a\u30c7\u30d7\u30ed\u30a4\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002 (\u53c2\u8003: Why Argo Rollouts?)</p>"},{"location":"addons/argo/argo-rollouts/about/#architecture","title":"Architecture","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/architecture/ </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#argo-rollout-controller","title":"<code>Argo Rollout Controller</code>","text":"<ul> <li><code>Rollout</code> resource type\u3092\u76e3\u8996\u3057\u5909\u66f4\u304c\u751f\u3058\u308b\u3068\u5b9a\u7fa9\u3068\u540c\u3058\u72b6\u614b\u306b\u3059\u308b\u5f79\u5272\u3092\u6301\u3064\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9</li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#rollout-resource","title":"<code>Rollout</code> resource","text":"<ul> <li><code>Argo Rollout</code> \u306eCustomResource<ul> <li>Deployment resource type\u3068\u307b\u307c\u4e92\u63db\u6027\u304c\u3042\u308a\u307e\u3059\u304c\u3001deployment strategies\u3068\u3057\u3066canary\u3084blue-green\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u305d\u308c\u3089\u306e\u305f\u3081\u306e\u30d5\u30a3\u30fc\u30eb\u30c9\u3092\u6301\u3064</li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#replicaset","title":"<code>ReplicaSet</code>","text":"<ul> <li>Kubernetes\u306e\u6a19\u6e96\u7684\u306aReplicaSet<ul> <li><code>Argo Rollout</code> \u304c\u4f5c\u6210\u30fb\u7ba1\u7406\u3057\u307e\u3059</li> <li>rollout\u3067\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u3082\u8ffd\u52a0\u3055\u308c\u307e\u3059</li> <li>\u4e0a\u56f3\u3067\u306f<code>Canary ReplicaSet</code> \u3084 <code>Stable ReplicaSet</code> \u3068\u8868\u8a18\u3055\u308c\u3066\u3044\u308b\u7b87\u6240<ul> <li><code>\u65e7version application</code>\u304c\u52d5\u4f5c\u3059\u308b<code>Stable ReplicaSet</code></li> <li>stable\u6607\u683c\u524d\u306e\u65b0version application\u304c\u52d5\u4f5c\u3059\u308b<code>Canary ReplicaSet</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#ingress-service","title":"<code>Ingress</code> / <code>Service</code>","text":"<ul> <li><code>Service</code>\u306b\u5bfe\u3059\u308btraffic\u3092\u3069\u306eReplicaSet\u3078routing\u3059\u308b\u306e\u304b\u3092selector\u3067\u7ba1\u7406\u3057\u307e\u3059</li> <li><code>Ingress</code> \u3068\u9023\u643a\u3059\u308b\u3053\u3068\u3067canary service\u306b\u5bfe\u3059\u308btraffic routing\u306e\u6bd4\u91cd\u3084routing rule\u3092\u5236\u5fa1\u3067\u304d\u307e\u3059<ul> <li><code>Service</code> \u306e\u307f\u306e\u5834\u5408\u3001stable \u3068 canary \u306ePod\u6570\u3067\u306e\u307ftraffic routing\u306e\u6bd4\u91cd\u3092\u5236\u5fa1\u3057\u307e\u3059</li> <li> <p><code>Ingress</code> \u306e\u5834\u5408\u3001\u30d1\u30fc\u30bb\u30f3\u30c6\u30fc\u30b8 / HTTP Header / Mirror \u3068\u3044\u3063\u305f\u624b\u6cd5\u3067\u306etraffic routing\u3092\u5b9f\u73fe\u3067\u304d\u307e\u3059</p> <p>Warning</p> <p>HTTP Header \u3084 Mirror\u306fIngress Controller\u306b\u3088\u3063\u3066\u30b5\u30dd\u30fc\u30c8\u72b6\u6cc1\u304c\u7570\u306a\u308a\u307e\u3059</p> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#analysistemplate-analysisrun","title":"<code>AnalysisTemplate</code> / <code>AnalysisRun</code>","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/analysis/</li> <li><code>Analysis</code> \u3068\u306f<code>Argo Rollout</code>\u304cdeploy\u4e2d\u306bMetricsProvider\u306b\u63a5\u7d9a\u3057\u7279\u5b9a\u306eMetrics\u306b\u5bfe\u3059\u308b\u95be\u5024\u3092\u8a2d\u5b9a\u3057\u3066\u304a\u304f\u3053\u3068\u3067\u65b0version application\u304c\u6b63\u5e38\u306b\u52d5\u3044\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3059\u308b\u4ed5\u7d44\u307f\u3067\u3059<ul> <li>Metrics\u304c\u826f\u597d\u306a\u5834\u5408\u306frollout\u306fdeploy\u3092\u7d99\u7d9a\u3057\u3001\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306fdeploy\u3092\u4e2d\u65ad\u3057rollback\u3057\u307e\u3059</li> </ul> </li> <li><code>Analysis</code> \u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306eCutomResource\u3068\u3057\u3066 <code>AnalysisTemplate</code> \u3068 <code>AnalysisRun</code> \u304c\u3042\u308a\u307e\u3059<ul> <li><code>AnalysisTemplate</code><ul> <li>MetricsProvider\u3084Analysis\u306b\u4f7f\u7528\u3059\u308bMetrics\u306b\u95a2\u3059\u308b\u8a2d\u5b9a</li> <li>Rollout Resource\u306b\u76f4\u63a5\u8a2d\u5b9a\u3001<code>AnalysisTemplate</code> \u3082\u3057\u304f\u306f <code>ClusterAnalysisTemplate</code> Resource\u3068\u3057\u3066\u5b9a\u7fa9\u3059\u308b</li> </ul> </li> <li><code>AnalysisRun</code><ul> <li><code>AnalysisTemplate</code> \u306e\u5b9f\u884c\u7d50\u679c</li> <li>\u7279\u5b9a\u306e <code>Rollout</code> Resource\u306bscope\u3055\u308c\u307e\u3059</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#deploy-strategies","title":"Deploy Strategies","text":"<p>rollout CRD\u306e <code>.spec.strategy</code> \u3067 <code>blueGreen</code> \u3082\u3057\u304f\u306f <code>canary</code> \u3092\u6307\u5b9a\u3057\u307e\u3059\u3002</p>"},{"location":"addons/argo/argo-rollouts/about/#bluegreen","title":"Blue/Green","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/bluegreen/</li> <li>rollout\u306eBlue/Green\u30c7\u30d7\u30ed\u30a4\u3067\u306f<code>activeService</code>\u3068<code>previewService</code>\u3068\u3044\u30462\u3064\u306eService\u3092\u6307\u5b9a\u3057\u307e\u3059<ul> <li><code>activeService</code> \u306f\u65e7version application ReplicaSet\u3078traffic\u3092routing\u3057\u307e\u3059</li> <li><code>previewService</code> \u306f\u65b0version application ReplicaSet\u3078traffic\u3092routing\u3057\u307e\u3059</li> </ul> </li> <li>rollout\u306e <code>.spec.template</code> \u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u65b0ReplicaSet\u3092\u4f5c\u6210\u3057\u307e\u3059<ul> <li><code>activeService</code> \u306btraffic\u304c\u6d41\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3059\u3050\u306b\u65b0ReplicaSet\u3078\u5207\u308a\u66ff\u3048\u3001\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u65b0ReplicaSet\u304c\u5229\u7528\u53ef\u80fd\u306b\u306a\u308b\u307e\u3067\u306f\u65e7ReplicaSet\u3078routing\u3057\u307e\u3059</li> <li>\u65b0ReplicaSet\u304c\u5229\u7528\u53ef\u80fd\u306b\u306a\u3063\u305f\u3089<code>activeService</code>\u3092\u65b0ReplicaSet\u3078routing\u3092\u5207\u308a\u66ff\u3048\u307e\u3059</li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#canary","title":"Canary","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/canary/</li> <li>rollout\u306eCanary\u30c7\u30d7\u30ed\u30a4\u3067\u306f\u65e7Service\u306b\u306f\u65e7version application\u306b\u5bfe\u3059\u308btraffic\u3092routing\u3057\u3064\u3064traffic\u3092\u5f90\u3005\u306b\u65b0Service\u306brouting\u3057\u307e\u3059<ul> <li>rollout CRD\u306e <code>spec.strategy.canary.steps</code> \u3067\u65b0Service\u3078traffic\u3092routing\u3059\u308b\u6bd4\u91cd\u3068\u79fb\u884c\u9593\u9694\u3092\u6307\u5b9a\u3057\u307e\u3059</li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#progressive-delivery","title":"Progressive Delivery","text":"<p><code>Progressive Delivery</code> \u3068\u306fContinuous Delivery\u3092\u767a\u5c55\u3055\u305b\u305f\u8003\u3048\u65b9\u3067\u3001canary deploy\u9014\u4e2d\u3067\u65b0version application\u3092\u89e3\u6790\u3057\u6b63\u5e38\u3067\u3042\u308c\u3070deploy\u3092\u7d99\u7d9a\u3001\u7570\u5e38\u3067\u3042\u308c\u3070\u65e7version application\u3078rollback\u3059\u308b\u3068\u3044\u3063\u305f\u8003\u3048\u65b9\u3084\u4ed5\u7d44\u307f\u3092\u6307\u3057\u307e\u3059\u3002Argo Rollouts\u3067\u306f <code>Analysis</code> \u6a5f\u80fd\u3092\u5229\u7528\u3057\u307e\u3059\u3002</p> <p>\u5f15\u7528: Leveling Up Your CD: Unlocking Progressive Delivery on Kubernetes \u2013 Daniel Thomson &amp; Jesse Suen, Intuit</p> <p></p> \u5f93\u6765\u306eCD <p></p> progressive delivery"},{"location":"addons/argo/argo-rollouts/about/#analysis","title":"Analysis","text":"<ul> <li>Rollout \u306e <code>sepc.strategy.canary.analysis</code></li> <li>blue/green strategy\u3067\u3082<code>Analysis</code> \u3092\u5229\u7528\u53ef\u80fd<ul> <li>\u65b0version application\u306eReplicaSet\u306escale\u304c\u5b8c\u4e86\u3057\u30c8\u30e9\u30d5\u30a3\u30c3\u30af\u3092\u65b0\u3057\u3044\u30d0\u30fc\u30b8\u30e7\u30f3\u306b\u5207\u308a\u66ff\u3048\u308b\u524d\u5f8c\u3067 <code>AnalysisRun</code> \u3092\u8d77\u52d5\u3067\u304d\u307e\u3059</li> <li>https://argoproj.github.io/argo-rollouts/features/analysis/#bluegreen-pre-promotion-analysis</li> <li>https://argoproj.github.io/argo-rollouts/features/analysis/#bluegreen-post-promotion-analysis</li> <li>https://aws.amazon.com/jp/blogs/architecture/use-amazon-eks-and-argo-rollouts-for-progressive-delivery/</li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#analysis_1","title":"Analysis\u306b\u95a2\u3059\u308b\u30ab\u30b9\u30bf\u30e0\u30ea\u30bd\u30fc\u30b9","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/analysis/#custom-resource-definitions<ul> <li><code>AnalysisTemplate</code><ul> <li>\u89e3\u6790\u65b9\u6cd5\u306b\u3064\u3044\u3066\u306e\u5b9a\u7fa9\u3057\u305f\u30ab\u30b9\u30bf\u30e0\u30ea\u30bd\u30fc\u30b9</li> <li>namespace\u3054\u3068</li> </ul> </li> <li><code>ClusterAnalysisTemplate</code><ul> <li><code>AnalysisTemplate</code> \u3068\u540c\u3058</li> <li>cluster wide</li> </ul> </li> <li><code>AnalysisRun</code><ul> <li><code>AnalysisTemplate</code> \u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3055\u308c\u305f\u3082\u306e</li> <li>Kubernetes\u306e <code>Job</code> \u30ea\u30bd\u30fc\u30b9\u3068\u4f3c\u3066\u3044\u3066\u6700\u7d42\u7684\u306b\u5b8c\u4e86\u3057\u307e\u3059</li> <li>\u5b9f\u884c\u7d50\u679c\u3068\u3057\u3066\u306f <code>Successful</code>\u3001<code>Failed</code>\u3001<code>Inconclusive</code> \u304c\u3042\u308a\u3001\u305d\u308c\u305e\u308c\u304crollout\u306edeploy\u304c\u7d99\u7d9a\u3001\u4e2d\u65ad\u3001\u307e\u305f\u306f\u4e00\u6642\u505c\u6b62\u3055\u308c\u308b\u304b\u306b\u5f71\u97ff\u3057\u307e\u3059\u3002</li> </ul> </li> <li><code>Experiment</code><ul> <li>\u5f8c\u8ff0</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#experimentation","title":"<code>experimentation</code>","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/experiment/</li> <li>1\u3064\u307e\u305f\u306f\u8907\u6570\u306eReplicaSet\u3092\u30a8\u30d5\u30a7\u30e1\u30e9\u30eb\u306a\u30ea\u30bd\u30fc\u30b9\u3068\u3057\u3066\u8d77\u52d5\u3055\u305b\u3001background\u3067AnalysisRun\u3092\u5b9f\u884c\u3055\u305b\u308b\u3053\u3068\u3067\u65b0version application\u306e\u6b63\u5e38\u6027\u78ba\u8a8d\u3092\u884c\u3048\u307e\u3059<ul> <li><code>Experiment</code> \u30671\u3064\u307e\u305f\u306f\u8907\u6570\u306eReplicaSet\u3068<code>AnalysisTemplate</code>\u306e\u6307\u5b9a\u3092\u884c\u3044\u307e\u3059</li> <li><code>Rollout</code>\u30ea\u30bd\u30fc\u30b9\u5185\u306esteps\u3068\u3057\u3066<code>experiment</code>\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059<ul> <li>https://argoproj.github.io/argo-rollouts/features/experiment/#integration-with-rollouts</li> <li>https://github.com/argoproj/argo-rollouts/blob/master/examples/rollout-experiment-step.yaml</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/basic_usage/","title":"Basic Usage","text":""},{"location":"addons/argo/argo-rollouts/basic_usage/#canary","title":"canary","text":""},{"location":"addons/argo/argo-rollouts/basic_usage/#manifests","title":"manifests","text":"<ul> <li>\u4eca\u56de\u306fService\u3084Ingress\u3092\u6307\u5b9a\u3057\u306a\u3044\u305f\u3081Pod\u6570\u306e\u5897\u6e1b\u3068\u306a\u308a\u307e\u3059</li> <li>canary \u3078\u306etraffic shift \u304c\u5206\u304b\u308a\u3084\u3059\u3044\u3088\u3046\u306b <code>.spec.replicas</code> \u306f <code>5</code> \u3068\u3057\u307e\u3059</li> <li> <p>\u4eca\u56de\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306astep\u3067canary traffic \u3078\u306e\u5272\u308a\u632f\u308a\u3092\u884c\u3044\u307e\u3059</p> # step 0 20%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b 1 60sec pause 2 80%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b 3 60sec pause 4 100%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b 5 canary \u3092 stable \u306b\u5207\u308a\u66ff\u3048 <p>argo-rollouts/nginx.yaml <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rollout-test-canary-nginx-conf\ndata:\n  nginx.conf: |\n    user nginx;\n    worker_processes  1;\n    error_log  /var/log/nginx/error.log;\n    events {\n      worker_connections  1024;\n    }\n    http {\n      server {\n          listen       80;\n          server_name  _;\n\n          location / {\n              root   html;\n              index  index.html index.htm;\n          }\n\n          location /nginx_status {\n              stub_status on;\n              access_log off;\n              allow 127.0.0.1;\n              deny all;\n          }\n\n      }\n    }\n\n---\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: rollout-test-canary-nginx\nspec:\n  strategy:\n    canary:\n      steps:\n      - setWeight: 20\n      - pause: {duration: 60}\n      - setWeight: 80\n      - pause: {duration: 60}\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 5\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n      labels:\n        app: nginx\n    spec:\n      volumes:\n      - name: nginx-conf\n        configMap:\n          name: rollout-test-canary-nginx-conf\n          items:\n            - key: nginx.conf\n              path: nginx.conf\n      containers:\n      - name: nginx\n        image: nginx:1.14.1\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx/nginx.conf\n          readOnly: true\n          name: nginx-conf\n          subPath: nginx.conf\n      - name: nginx-exporter\n        image: nginx/nginx-prometheus-exporter:0.11.0\n        args:\n          - -nginx.scrape-uri=http://localhost/nginx_status\n        ports:\n          - containerPort: 9113\n</code></pre> </p> </li> </ul>"},{"location":"addons/argo/argo-rollouts/basic_usage/#deploy","title":"\u521d\u56dedeploy","text":"<ol> <li> <p>deploy\u5f8c\u306e\u8d77\u52d5\u78ba\u8a8d     list <pre><code>$ kubectl argo rollouts list rollouts\nNAME                STRATEGY   STATUS        STEP  SET-WEIGHT  READY  DESIRED  UP-TO-DATE  AVAILABLE\nrollout-test-canary-nginx  Canary     Healthy       4/4   100         5/5    5        5           5\n</code></pre> </p> <p>get <pre><code>$ kubectl argo rollouts get rollout rollout-test-canary-nginx\nName:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        Canary\n  Step:          4/4\n  SetWeight:     100\n  ActualWeight:  100\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (stable)\n                 nginx:1.14.1 (stable)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                            KIND        STATUS     AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u2714 Healthy  2m58s\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2714 Healthy  2m58s  stable\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-47h6s  Pod         \u2714 Running  2m58s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u2714 Running  2m58s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-ph8pf  Pod         \u2714 Running  2m58s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-pw8md  Pod         \u2714 Running  2m58s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-rqbr5  Pod         \u2714 Running  2m58s  ready:2/2\n</code></pre> </p> </li> </ol>"},{"location":"addons/argo/argo-rollouts/basic_usage/#2deploy","title":"2\u56de\u76ee\u306edeploy","text":"<ol> <li><code>nginx:1.14.2</code> \u3078bump up\u3057\u3066deploy</li> <li> <p><code>kubectl argo rollouts get rollout rollout-test-canary-nginx -w</code> \u3067\u72b6\u614b\u9077\u79fb\u3092\u78ba\u8a8d     0. 20%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u25cc Progressing\nMessage:         more replicas need to be updated\nStrategy:        Canary\n  Step:          0/4\n  SetWeight:     20\n  ActualWeight:  0\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary, stable)\n                 nginx:1.14.1 (stable)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       1\n  Ready:         4\n  Available:     4\n\nNAME                                            KIND        STATUS         AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u25cc Progressing  4m57s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89            ReplicaSet  \u25cc Progressing  7s     canary\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb   Pod         \u2714 Running      6s     ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2714 Healthy      4m57s  stable\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u2714 Running      4m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-ph8pf  Pod         \u2714 Running      4m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-pw8md  Pod         \u2714 Running      4m57s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-rqbr5  Pod         \u2714 Running      4m57s  ready:2/2\n</code></pre> </p> <p>1. 60sec pause <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u0965 Paused\nMessage:         CanaryPauseStep\nStrategy:        Canary\n  Step:          1/4\n  SetWeight:     20\n  ActualWeight:  20\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary, stable)\n                 nginx:1.14.1 (stable)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       1\n  Ready:         5\n  Available:     5\n\nNAME                                            KIND        STATUS     AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u0965 Paused   4m57s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89            ReplicaSet  \u2714 Healthy  7s     canary\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb   Pod         \u2714 Running  6s     ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2714 Healthy  4m57s  stable\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u2714 Running  4m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-ph8pf  Pod         \u2714 Running  4m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-pw8md  Pod         \u2714 Running  4m57s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-rqbr5  Pod         \u2714 Running  4m57s  ready:2/2\n</code></pre> </p> <p>2. 80%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u25cc Progressing\nMessage:         more replicas need to be updated\nStrategy:        Canary\n  Step:          2/4\n  SetWeight:     80\n  ActualWeight:  25\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary, stable)\n                 nginx:1.14.1 (stable)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       4\n  Updated:       1\n  Ready:         4\n  Available:     4\n\nNAME                                            KIND        STATUS               AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u25cc Progressing        5m57s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89            ReplicaSet  \u25cc Progressing        67s    canary\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb   Pod         \u2714 Running            66s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-9dcqt   Pod         \u25cc Pending            0s     ready:0/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-cgf6w   Pod         \u25cc ContainerCreating  0s     ready:0/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-hjlrp   Pod         \u25cc Pending            0s     ready:0/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2714 Healthy            5m57s  stable\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u2714 Running            5m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-ph8pf  Pod         \u2714 Running            5m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-pw8md  Pod         \u25cc Terminating        5m57s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-rqbr5  Pod         \u2714 Running            5m57s  ready:2/2\n</code></pre> </p> <p>3. 60sec pause <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u0965 Paused\nMessage:         CanaryPauseStep\nStrategy:        Canary\n  Step:          3/4\n  SetWeight:     80\n  ActualWeight:  80\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary, stable)\n                 nginx:1.14.1 (stable)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       4\n  Ready:         5\n  Available:     5\n\nNAME                                            KIND        STATUS         AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u0965 Paused       6m14s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89            ReplicaSet  \u2714 Healthy      84s    canary\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb   Pod         \u2714 Running      83s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-9dcqt   Pod         \u2714 Running      17s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-cgf6w   Pod         \u2714 Running      17s    ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-hjlrp   Pod         \u2714 Running      17s    ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2714 Healthy      6m14s  stable\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u2714 Running      6m14s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-ph8pf  Pod         \u25cc Terminating  6m14s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-rqbr5  Pod         \u25cc Terminating  6m14s  ready:2/2\n</code></pre> </p> <p>4. 100%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u25cc Progressing\nMessage:         more replicas need to be updated\nStrategy:        Canary\n  Step:          4/4\n  SetWeight:     100\n  ActualWeight:  100\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary, stable)\n                 nginx:1.14.1 (stable)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       4\n  Ready:         5\n  Available:     5\n\nNAME                                            KIND        STATUS         AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u25cc Progressing  7m13s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89            ReplicaSet  \u25cc Progressing  2m23s  canary\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb   Pod         \u2714 Running      2m22s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-9dcqt   Pod         \u2714 Running      76s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-cgf6w   Pod         \u2714 Running      76s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-hjlrp   Pod         \u2714 Running      76s    ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-ct9xs   Pod         \u25cc Pending      0s     ready:0/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2022 ScaledDown   7m13s  stable\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u25cc Terminating  7m13s  ready:2/2\n\n\nsnip...\n\n\nName:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u25cc Progressing\nMessage:         updated replicas are still becoming available\nStrategy:        Canary\n  Step:          4/4\n  SetWeight:     100\n  ActualWeight:  100\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       5\n  Ready:         4\n  Available:     4\n\nNAME                                           KIND        STATUS         AGE    INFO\n\u27f3 rollout-test-canary-nginx                           Rollout     \u25cc Progressing  7m22s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89           ReplicaSet  \u25cc Progressing  2m32s  canary\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb  Pod         \u2714 Running      2m31s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-9dcqt  Pod         \u2714 Running      85s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-cgf6w  Pod         \u2714 Running      85s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-hjlrp  Pod         \u2714 Running      85s    ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-ct9xs  Pod         \u2714 Running      9s     ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8          ReplicaSet  \u2022 ScaledDown   7m22s  stable\n</code></pre> </p> <p>5. canary \u3092 stable \u306b\u5207\u308a\u66ff\u3048 <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        Canary\n  Step:          4/4\n  SetWeight:     100\n  ActualWeight:  100\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (stable)\n                 nginx:1.14.2 (stable)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                           KIND        STATUS        AGE    INFO\n\u27f3 rollout-test-canary-nginx                           Rollout     \u2714 Healthy     7m23s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89           ReplicaSet  \u2714 Healthy     2m33s  stable\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb  Pod         \u2714 Running     2m32s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-9dcqt  Pod         \u2714 Running     86s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-cgf6w  Pod         \u2714 Running     86s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-hjlrp  Pod         \u2714 Running     86s    ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-ct9xs  Pod         \u2714 Running     10s    ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8          ReplicaSet  \u2022 ScaledDown  7m23s\n</code></pre> </p> </li> </ol>"},{"location":"addons/argo/argo-rollouts/basic_usage/#bluegreen","title":"blue/green","text":""},{"location":"addons/argo/argo-rollouts/basic_usage/#manifests_1","title":"manifests","text":"<ul> <li> <p>Argo Rollout\u306e <code>blue/green</code> \u306fService\u304b\u3089traffic routing\u3059\u308bReplicaSet\u3092\u5207\u308a\u66ff\u3048\u308b\u3053\u3068\u3067\u5b9f\u73fe\u3057\u307e\u3059\u3002   \u305d\u306e\u305f\u3081\u3001<code>Service</code>\u3082\u4f75\u305b\u3066\u4f5c\u6210\u3057\u307e\u3059\u3002</p> <p>argo-rollouts/nginx_blue-green.yaml <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rollout-test-bluegreen-nginx-conf\ndata:\n  nginx.conf: |\n    user nginx;\n    worker_processes  1;\n    error_log  /var/log/nginx/error.log;\n    events {\n      worker_connections  1024;\n    }\n    http {\n      server {\n          listen       80;\n          server_name  _;\n\n          location / {\n              root   html;\n              index  index.html index.htm;\n          }\n\n          location /nginx_status {\n              stub_status on;\n              access_log off;\n              allow 127.0.0.1;\n              deny all;\n          }\n\n      }\n    }\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: rollout-test-bluegreen-active\nspec:\n  selector:\n    app: rollout-test-bluegreen-nginx\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: rollout-test-bluegreen-preview\nspec:\n  selector:\n    app: rollout-test-bluegreen-nginx\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n\n---\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: rollout-test-bluegreen-nginx\nspec:\n  strategy:\n    blueGreen:\n      activeService: rollout-test-bluegreen-active\n      previewService: rollout-test-bluegreen-preview\n\n  selector:\n    matchLabels:\n      app: rollout-test-bluegreen-nginx\n  replicas: 5\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n      labels:\n        app: rollout-test-bluegreen-nginx\n    spec:\n      volumes:\n      - name: nginx-conf\n        configMap:\n          name: rollout-test-bluegreen-nginx-conf\n          items:\n            - key: nginx.conf\n              path: nginx.conf\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx/nginx.conf\n          readOnly: true\n          name: nginx-conf\n          subPath: nginx.conf\n      - name: nginx-exporter\n        image: nginx/nginx-prometheus-exporter:0.11.0\n        args:\n          - -nginx.scrape-uri=http://localhost/nginx_status\n        ports:\n          - containerPort: 9113\n</code></pre> </p> <p>Warning</p> <ul> <li><code>Service</code>\u3088\u308a\u3082\u5148\u306b<code>Rollout</code>\u304c\u4f5c\u6210\u3055\u308c\u305f\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b<code>Degraded</code>\u3068\u306a\u308a\u307e\u3059\u306e\u3067\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059<ul> <li><code>Rollout</code> \u3088\u308a\u3082\u5148\u306b <code>Service</code> \u3092\u4f5c\u6210\u3059\u308b</li> <li>\u540c\u3058manifests\u30d5\u30a1\u30a4\u30eb\u306b\u8a18\u8f09\u3059\u308b\u5834\u5408\u3001\u4e0a\u304b\u3089\u9806\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3057\u3066\u3044\u304f\u305f\u3081\u8a18\u8f09\u9806\u3082\u6ce8\u610f\u3059\u308b\u3053\u3068</li> <li>https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/ <p>The resources will be created in the order they appear in the file. Therefore, it's best to specify the service first, since that will ensure the scheduler can spread the pods associated with the service as they are created by the controller(s), such as Deployment.</p> </li> </ul> </li> </ul> <pre><code>$ kubectl argo rollouts get rollout rollout-test-bluegreen-nginx\nName:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2716 Degraded\nMessage:         InvalidSpec: The Rollout \"rollout-test-bluegreen-nginx\" is invalid: spec.strategy.blueGreen.activeService: Invalid value: \"rollout-test-bluegreen-active\": service \"rollout-test-bl\nuegreen-active\" not found\nStrategy:        BlueGreen\nReplicas:\n  Desired:       5\n  Current:       0\n  Updated:       0\n  Ready:         0\n  Available:     0\n</code></pre> </li> </ul>"},{"location":"addons/argo/argo-rollouts/basic_usage/#deploy_1","title":"\u521d\u56dedeploy","text":"<ol> <li> <p>deploy\u5f8c\u306e\u8d77\u52d5\u78ba\u8a8d     get <pre><code>$ kubectl argo rollouts get rollout rollout-test-bluegreen-nginx\nName:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (stable, active)\n                 nginx:1.14.1 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS     AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy  16s\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2714 Healthy  16s  stable,active\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u2714 Running  15s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u2714 Running  15s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u2714 Running  15s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u2714 Running  15s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u2714 Running  15s  ready:2/2\n</code></pre> </p> <p>Service <pre><code>$ kubectl get service\nNAME                             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE\n\nsnip...\n\nrollout-test-bluegreen-active    ClusterIP   10.32.0.33    &lt;none&gt;        80/TCP           46s\nrollout-test-bluegreen-preview   ClusterIP   10.32.0.253   &lt;none&gt;        80/TCP           46s\n</code></pre> </p> </li> </ol>"},{"location":"addons/argo/argo-rollouts/basic_usage/#2deploy_1","title":"2\u56de\u76ee\u306edeploy","text":"<ol> <li><code>nginx:1.14.2</code> \u3078bump up\u3057\u3066deploy</li> <li> <p><code>kubectl argo rollouts get rollout rollout-test-bluegreen-nginx -w</code> \u3067\u72b6\u614b\u9077\u79fb\u3092\u78ba\u8a8d     1. \u65b0\u3057\u3044ReplicaSet\u304c\u4f5c\u6210\u3055\u308c\u3001Pod\u304c\u8d77\u52d5\u3059\u308b <pre><code>Name:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u25cc Progressing\nMessage:         active service cutover pending\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (active, preview, stable)\n                 nginx:1.14.1 (stable, active)\n                 nginx:1.14.2 (preview)\nReplicas:\n  Desired:       5\n  Current:       10\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS         AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u25cc Progressing  14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u25cc Progressing  13s  preview\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running      13s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running      13s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running      13s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running      12s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running      12s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2714 Healthy      14m  stable,active\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u2714 Running      14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u2714 Running      14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u2714 Running      14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u2714 Running      14m  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u2714 Running      14m  ready:2/2\n</code></pre> </p> <p>2. active service\u304c\u65b0\u3057\u3044ReplicaSet\u306b\u5411\u304d\u3001\u53e4\u3044ReplicaSet\u304cscale down\u3059\u308b\u307e\u3067scaleDownDelaySeconds` sec(default: 30) delay <pre><code>Name:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (active, stable)\n                 nginx:1.14.1\n                 nginx:1.14.2 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       10\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS     AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy  14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u2714 Healthy  14s  stable,active\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running  14s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running  14s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running  14s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running  13s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running  13s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2714 Healthy  14m  delay:28s\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u2714 Running  14m  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u2714 Running  14m  ready:2/2\n\n\nName:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (active, stable)\n                 nginx:1.14.1\n                 nginx:1.14.2 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       10\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS     AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy  14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u2714 Healthy  42s  stable,active\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running  42s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running  42s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running  42s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running  41s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running  41s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2714 Healthy  14m  delay:0s\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u2714 Running  14m  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u2714 Running  14m  ready:2/2\n</code></pre> </p> <p>4. <code>scaleDownDelaySeconds</code> sec\u304c\u7d4c\u904e <pre><code>Name:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (active, stable)\n                 nginx:1.14.1\n                 nginx:1.14.2 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       10\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS     AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy  14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u2714 Healthy  43s  stable,active\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running  43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running  43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running  43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running  42s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running  42s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2714 Healthy  14m  delay:passed\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u2714 Running  14m  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u2714 Running  14m  ready:2/2\n</code></pre> </p> <p>5. \u53e4\u3044ReplicaSet\u304cscale down\u3059\u308b <pre><code>Name:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (stable, active)\n                 nginx:1.14.2 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       10\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS         AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy      14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u2714 Healthy      43s  stable,active\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running      43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running      43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running      43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running      42s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running      42s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2022 ScaledDown   14m\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u25cc Terminating  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u25cc Terminating  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u25cc Terminating  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u25cc Terminating  14m  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u25cc Terminating  14m  ready:2/2\n</code></pre> </p> <p>6. \u53e4\u3044ReplicaSet\u306escale down\u304c\u5b8c\u4e86 <pre><code>Name:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (stable, active)\n                 nginx:1.14.2 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS        AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy     14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u2714 Healthy     52s  stable,active\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running     52s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running     52s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running     52s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running     51s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running     51s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2022 ScaledDown  14m\n</code></pre> </p> </li> </ol>"},{"location":"addons/argo/argo-rollouts/install/","title":"Install","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/installation/</li> <li>https://argoproj.github.io/argo-rollouts/features/helm/</li> <li> <p>https://github.com/argoproj/argo-helm/tree/main/charts/argo-rollouts</p> <p>Info</p> <p><code>.Values.keepCRDs</code> \u304c <code>default: True</code> \u306e\u305f\u3081 <code>helm uninstall</code> \u5b9f\u884c\u6642\u306b <code>CustomResourceDefinition</code> Resource\u306f\u6b8b\u308a\u307e\u3059</p> <p>refs Chart Values</p> <pre><code>$ kubectl get CustomResourceDefinition | grep argoproj.io\nanalysisruns.argoproj.io                         2023-01-29T08:45:21Z\nanalysistemplates.argoproj.io                    2023-01-29T08:45:20Z\nclusteranalysistemplates.argoproj.io             2023-01-29T08:45:21Z\nexperiments.argoproj.io                          2023-01-29T08:45:21Z\nrollouts.argoproj.io                             2023-01-29T08:45:21Z\n</code></pre> </li> </ul>"},{"location":"addons/argo/argo-rollouts/install/#controller-install","title":"controller install","text":"<ol> <li> <p>install with helm</p> <ul> <li>https://github.com/argoproj/argo-helm/tree/main/charts/argo-rollouts <pre><code>helm repo add argo https://argoproj.github.io/argo-helm\nhelm upgrade -i argo-rollouts argo/argo-rollouts --namespace argo-rollouts --create-namespace\n</code></pre></li> </ul> </li> <li> <p><code>argo-rollouts</code> controller\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> <p>deployment <pre><code>$ kubectl describe deployments -n argo-rollouts argo-rollouts\nName:               argo-rollouts\nNamespace:          argo-rollouts\nCreationTimestamp:  Sun, 29 Jan 2023 17:45:21 +0900\nLabels:             app.kubernetes.io/component=rollouts-controller\n                    app.kubernetes.io/instance=argo-rollouts\n                    app.kubernetes.io/managed-by=Helm\n                    app.kubernetes.io/name=argo-rollouts\n                    app.kubernetes.io/part-of=argo-rollouts\n                    app.kubernetes.io/version=v1.4.0\n                    helm.sh/chart=argo-rollouts-2.22.1\nAnnotations:        deployment.kubernetes.io/revision: 1\n                    meta.helm.sh/release-name: argo-rollouts\n                    meta.helm.sh/release-namespace: argo-rollouts\nSelector:           app.kubernetes.io/component=rollouts-controller,app.kubernetes.io/instance=argo-rollouts,app.kubernetes.io/name=argo-rollouts\nReplicas:           2 desired | 2 updated | 2 total | 2 available | 0 unavailable\nStrategyType:       Recreate\nMinReadySeconds:    0\nPod Template:\n  Labels:           app.kubernetes.io/component=rollouts-controller\n                    app.kubernetes.io/instance=argo-rollouts\n                    app.kubernetes.io/name=argo-rollouts\n  Service Account:  argo-rollouts\n  Containers:\n   argo-rollouts:\n    Image:       quay.io/argoproj/argo-rollouts:v1.4.0\n    Ports:       8090/TCP, 8080/TCP\n    Host Ports:  0/TCP, 0/TCP\n    Args:\n      --leader-elect\n    Liveness:     http-get http://:healthz/healthz delay=30s timeout=10s period=20s #success=1 #failure=3\n    Readiness:    http-get http://:metrics/metrics delay=15s timeout=4s period=5s #success=1 #failure=3\n    Environment:  &lt;none&gt;\n    Mounts:       &lt;none&gt;\n  Volumes:        &lt;none&gt;\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   argo-rollouts-648bff954f (2/2 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  118s  deployment-controller  Scaled up replica set argo-rollouts-648bff954f to 2\n</code></pre> </p> <p>pods <pre><code>$ kubectl get pods -n argo-rollouts\nNAME                             READY   STATUS    RESTARTS   AGE\nargo-rollouts-648bff954f-4svmt   1/1     Running   0          4m22s\nargo-rollouts-648bff954f-bm994   1/1     Running   0          4m22s\n</code></pre> </p> </li> <li> <p>argo-rollouts controller\u8d77\u52d5\u6642\u306e\u30ed\u30b0\u3092\u78ba\u8a8d</p> <ul> <li> <p><code>leader election</code> \u304c\u52d5\u4f5c\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d     leader\u3068\u306a\u3063\u305fcontroller log <pre><code>$ kubectl logs -n argo-rollouts argo-rollouts-648bff954f-4svmt\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Argo Rollouts starting\" version=v1.4.0+e40c9fe\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Creating event broadcaster\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Setting up event handlers\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Setting up experiments event handlers\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Setting up analysis event handlers\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Leaderelection get id argo-rollouts-648bff954f-4svmt_01e7fcf9-d81e-4e6b-b311-15706341d8af\"\nI0129 08:46:03.959656       1 leaderelection.go:248] attempting to acquire leader lease argo-rollouts/argo-rollouts-controller-lock...\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Starting Healthz Server at 0.0.0.0:8080\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Starting Metric Server at 0.0.0.0:8090\"\nI0129 08:46:03.999015       1 leaderelection.go:258] successfully acquired lease argo-rollouts/argo-rollouts-controller-lock\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"New leader elected: argo-rollouts-648bff954f-4svmt_01e7fcf9-d81e-4e6b-b311-15706341d8af\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"I am the new leader: argo-rollouts-648bff954f-4svmt_01e7fcf9-d81e-4e6b-b311-15706341d8af\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Starting Controllers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Waiting for controller's informer caches to sync\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started controller\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Starting analysis workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started 30 analysis workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Starting Experiment workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started Experiment workers\"\ntime=\"2023-01-29T08:46:04Z\" level=warning msg=\"Controller is running.\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Starting Ingress workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Starting Service workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started Ingress workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started Service workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Starting Rollout workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started rollout workers\"\n</code></pre> </p> <p>leader\u3067\u306f\u306a\u304f\u5f85\u6a5f\u3068\u306a\u3063\u305fcontroller log <pre><code>$ kubectl logs -n argo-rollouts argo-rollouts-648bff954f-bm994\ntime=\"2023-01-29T08:46:06Z\" level=info msg=\"Argo Rollouts starting\" version=v1.4.0+e40c9fe\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Creating event broadcaster\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Setting up event handlers\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Setting up experiments event handlers\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Setting up analysis event handlers\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Leaderelection get id argo-rollouts-648bff954f-bm994_e3f2195c-58be-4086-8d08-1264405dcd59\"\nI0129 08:46:07.397481       1 leaderelection.go:248] attempting to acquire leader lease argo-rollouts/argo-rollouts-controller-lock...\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Starting Healthz Server at 0.0.0.0:8080\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Starting Metric Server at 0.0.0.0:8090\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"New leader elected: argo-rollouts-648bff954f-4svmt_01e7fcf9-d81e-4e6b-b311-15706341d8af\"\n</code></pre> </p> </li> </ul> </li> </ol>"},{"location":"addons/argo/argo-rollouts/install/#kubectl-plugin","title":"kubectl plugin","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/kubectl-plugin/<ul> <li> <p><code>kubectl argo rollouts</code> \u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u3059\u308b     <pre><code>curl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-linux-arm64\nchmod +x kubectl-argo-rollouts-linux-arm64\nsudo mv kubectl-argo-rollouts-linux-arm64 /usr/local/bin/kubectl-argo-rollouts\n</code></pre></p> <pre><code>$ kubectl argo rollouts version\nkubectl-argo-rollouts: v1.4.0+e40c9fe\n  BuildDate: 2023-01-09T20:20:38Z\n  GitCommit: e40c9fe8a2f7fee9d8ee1c56b4c6c7b983fce135\n  GitTreeState: clean\n  GoVersion: go1.19.4\n  Compiler: gc\n  Platform: linux/arm64\n</code></pre> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/install/#ui-dashboard","title":"UI Dashboard","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/dashboard/</li> <li>https://argoproj.github.io/argo-rollouts/generated/kubectl-argo-rollouts/kubectl-argo-rollouts_dashboard/</li> <li>https://github.com/argoproj/argo-helm/tree/main/charts/argo-rollouts#ui-dashboard</li> <li>install<ol> <li>Values\u3092\u8ffd\u52a0\u3057\u3066helm install\u3059\u308b<ul> <li><code>dashboard.enabled</code></li> <li><code>dashboard.service.type</code> (metallb\u306b\u3088\u308bIP\u30a2\u30c9\u30ec\u30b9\u3092\u6255\u3044\u51fa\u3057\u3001Service\u306eLoadBalancer IP\u30a2\u30c9\u30ec\u30b9\u3068\u3059\u308b)     <pre><code>helm upgrade -i argo-rollouts argo/argo-rollouts --namespace argo-rollouts --create-namespace \\\n  --set dashboard.enabled=true --set dashboard.service.type=LoadBalancer\n</code></pre></li> </ul> </li> <li>Pod     <pre><code>$ kubectl get pods -n argo-rollouts\nNAME                                       READY   STATUS    RESTARTS       AGE\nargo-rollouts-648bff954f-4svmt             1/1     Running   1 (4h5m ago)   4h37m\nargo-rollouts-648bff954f-bm994             1/1     Running   0              4h37m\nargo-rollouts-dashboard-6cc9c45468-2hxlq   1/1     Running   0              12m\n</code></pre></li> <li>Service     <pre><code>$ kubectl get service -n argo-rollouts\nNAME                      TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)          AGE\nargo-rollouts-dashboard   LoadBalancer   10.32.0.153   192.168.3.203   3100:30809/TCP   51s\n\n$ kubectl describe service -n argo-rollouts\nName:                     argo-rollouts-dashboard\nNamespace:                argo-rollouts\nLabels:                   app.kubernetes.io/component=rollouts-dashboard\n                          app.kubernetes.io/instance=argo-rollouts\n                          app.kubernetes.io/managed-by=Helm\n                          app.kubernetes.io/name=argo-rollouts\n                          app.kubernetes.io/part-of=argo-rollouts\n                          app.kubernetes.io/version=v1.4.0\n                          helm.sh/chart=argo-rollouts-2.22.1\nAnnotations:              meta.helm.sh/release-name: argo-rollouts\n                          meta.helm.sh/release-namespace: argo-rollouts\nSelector:                 app.kubernetes.io/component=rollouts-dashboard,app.kubernetes.io/instance=argo-rollouts,app.kubernetes.io/name=argo-rollouts\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.32.0.153\nIPs:                      10.32.0.153\nLoadBalancer Ingress:     192.168.3.203\nPort:                     dashboard  3100/TCP\nTargetPort:               3100/TCP\nNodePort:                 dashboard  30809/TCP\nEndpoints:                &lt;none&gt;\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:\n  Type    Reason       Age   From                Message\n  ----    ------       ----  ----                -------\n  Normal  IPAllocated  90s   metallb-controller  Assigned IP [\"192.168.3.203\"]\n</code></pre></li> </ol> </li> </ul>"},{"location":"addons/argo/argo-rollouts/traffic-management/","title":"Traffic Management","text":""},{"location":"addons/argo/argo-rollouts/traffic-management/#traffic-management","title":"Traffic Management","text":""},{"location":"addons/argo/argo-rollouts/traffic-management/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/traffic-management/</li> <li>https://argoproj.github.io/argo-rollouts/features/traffic-management/alb/</li> <li>https://argoproj.github.io/argo-rollouts/getting-started/alb/</li> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/</li> <li>https://aws.amazon.com/jp/blogs/news/using-aws-load-balancer-controller-for-blue-green-deployment-canary-deployment-and-a-b-testing/</li> </ul>"},{"location":"addons/argo/argo-rollouts/traffic-management/#traffic-management_1","title":"Traffic Management\u3068\u306f","text":"<p>Argo Rollouts\u3067deploy\u4e2d\u306e\u65b0version application\u3078\u6d41\u3059traffic\u306e\u5272\u5408\u3092\u5236\u5fa1\u3067\u304d\u307e\u3059\u3002 getting-started \u306e\u4f8b\u3067\u3082\u3042\u308b\u901a\u308a\u3001default\u3067\u306f\u65b0version application\u3068\u65e7version application\u306eService selecttor\u306b\u7d10\u4ed8\u304fReplicaSet\u306ePod\u6570\u3092\u5909\u52d5\u3055\u305b\u308b\u3053\u3068\u3067traffic\u306e\u5272\u5408\u3092\u5236\u5fa1\u3057\u307e\u3059\u3002 Argo Rollouts\u306e <code>Traffic Management</code> \u3067\u306f\u30b5\u30fc\u30d3\u30b9\u30e1\u30c3\u30b7\u30e5(<code>AWS ALB Ingress Controller</code> \u3084 <code>Nginx Ingress Controller</code> \u542b\u3080) \u3068\u9023\u643a\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002</p> <p>Traffic Management\u306f\u5927\u304d\u304f3\u3064\u306e\u624b\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002</p> <ol> <li>Raw percentages<ul> <li>\u65b0version\u306bN%\u3001\u65e7version\u306bN% \u3068\u3044\u3063\u305f\u5272\u5408\u3067\u6307\u5b9a\u3059\u308b</li> <li>ALB Ingress, Nginx Ingress\u3082\u5bfe\u5fdc<ul> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/nginx/nginx.go</li> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/alb/alb.go</li> </ul> </li> </ul> </li> <li>Header-based routing<ul> <li>\u4efb\u610f\u306eheader\u304c\u30ea\u30af\u30a8\u30b9\u30c8\u306b\u542b\u307e\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u65b0version\u306b\u6d41\u3059</li> <li>ALB Ingress\u5bfe\u5fdc<ul> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/nginx/nginx.go</li> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/alb/alb.go</li> </ul> </li> </ul> </li> <li>Mirrored traffic<ul> <li>\u307e\u3063\u305f\u304f\u540c\u3058traffic\u3092\u65b0version\u306b\u3082\u6d41\u3059\u3002\u305f\u3060\u3057\u3001response\u306f\u7121\u8996\u3055\u308c\u308b</li> <li>ALB Ingress, Nginx Ingress\u3082\u975e\u5bfe\u5fdc<ul> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/nginx/nginx.go</li> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/alb/alb.go</li> </ul> </li> </ul> </li> </ol>"},{"location":"addons/argo/argo-rollouts/traffic-management/#_2","title":"\u8a2d\u5b9a","text":""},{"location":"addons/argo/argo-rollouts/traffic-management/#alb","title":"ALB\u306e\u5834\u5408","text":"<ul> <li> <p>https://argoproj.github.io/argo-rollouts/features/traffic-management/alb/#usage</p> <ul> <li> <p>Rollout</p> <ul> <li><code>spec.strategy.canary.canaryService</code></li> <li><code>spec.strategy.canary.stableService</code></li> <li> <p><code>spec.strategy.canary.trafficRouting</code> (ServiceMesh\u306b\u3088\u3063\u3066parameter\u304c\u7570\u306a\u308b)</p> <pre><code>spec:\n  ...\n  strategy:\n    canary:\n      canaryService: canary-service\n      stableService: stable-service\n      trafficRouting:\n        alb:\n          ingress: ingress\n          servicePort: 443\n</code></pre> </li> </ul> </li> <li> <p>Ingress</p> <ul> <li> <p><code>spec.rules</code> \u4ee5\u4e0b\u306erule\u304c Rollout \u306e <code>spec.strategy.canary.stableService</code> \u3068\u4e00\u81f4\u3059\u308brule\u3067deploy\u3055\u308c\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</p> <pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress\n  annotations:\n    kubernetes.io/ingress.class: alb\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /*\n        backend:\n          # serviceName must match either: canary.trafficRouting.alb.rootService (if specified),\n          # or canary.stableService (if rootService is omitted)\n          serviceName: stable-service\n          # servicePort must be the value: use-annotation\n          # This instructs AWS Load Balancer Controller to look to annotations on how to direct traffic\n          servicePort: use-annotation\n</code></pre> </li> <li> <p>rollout\u304cdeploy\u4e2d</p> <ul> <li>ALB Controller\u304c\u7406\u89e3\u3067\u304d\u308bJSON payload\u3092\u542b\u3080 <code>alb.ingress.kubernetes.io/actions.&lt;SERVICE-NAME&gt;</code> annotation\u3092Rollouts Controller\u304cIngress\u30ea\u30bd\u30fc\u30b9\u306b\u6ce8\u5165\u3057\u307e\u3059     <pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/actions.stable-service: |\n      {\n        \"Type\":\"forward\",\n        \"ForwardConfig\":{\n          \"TargetGroups\":[\n            {\n                \"Weight\":10,\n                \"ServiceName\":\"canary-service\",\n                \"ServicePort\":\"80\"\n            },\n            {\n                \"Weight\":90,\n                \"ServiceName\":\"stable-service\",\n                \"ServicePort\":\"80\"\n            }\n          ]\n        }\n      }\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /*\n        backend:\n          serviceName: stable-service\n          servicePort: use-annotation\n</code></pre></li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/openebs/about/","title":"About OpenEBS","text":""},{"location":"addons/openebs/about/#openebs","title":"OpenEBS","text":"<ul> <li><code>OpenEBS</code> \u306f\u3001Kubernetes\u30ef\u30fc\u30ab\u30fc\u30ce\u30fc\u30c9\u304c\u5229\u7528\u3067\u304d\u308b\u3042\u3089\u3086\u308b\u30b9\u30c8\u30ec\u30fc\u30b8(\u30ed\u30fc\u30ab\u30eb\u3084\u5206\u6563\u306a\u3069) \u3092Kubernetes Persistent Volumes\u306b\u5909\u63db\u3057\u307e\u3059</li> </ul>"},{"location":"addons/openebs/about/#reference","title":"Reference","text":"<ul> <li>https://openebs.io/</li> <li>https://github.com/openebs/openebs</li> <li>https://github.com/openebs/charts</li> <li>https://openebs.github.io/charts/</li> <li>https://blog.openebs.io/arming-kubernetes-with-openebs-1-b450f41e0c1f</li> <li>https://note.com/ryoma_0923/n/n7d2837212028</li> <li>https://qiita.com/ysakashita/items/8ca805cb6ac10df911be</li> <li>https://blog.cybozu.io/entry/2018/03/29/080000</li> <li>https://www.infoq.com/jp/news/2020/08/kubernetes-storage-kubera/</li> </ul>"},{"location":"addons/openebs/about/#openebs_1","title":"OpenEBS \u306b\u3064\u3044\u3066","text":"<ul> <li>MayaData\u304c\u5b9f\u88c5\u3092\u516c\u958b\u3057\u3001\u73fe\u5728\u306fCNCF sandbox project\u306eContainer Storage Interface (CSI) Provider\u3067\u3059</li> <li><code>Container Attached Storage(CAS)</code> \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u3092\u63a1\u7528\u3057\u3066\u3044\u308b</li> <li>Dynamic Provisioner\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u308b<ul> <li>Kubernetes Native\u306e local PersistentVolume volume \u306fStatic Provisioning\u306e\u307f\u3092\u30b5\u30dd\u30fc\u30c8</li> </ul> </li> </ul>"},{"location":"addons/openebs/about/#container-attached-storagecas","title":"<code>Container Attached Storage(CAS)</code>","text":"<ul> <li>https://openebs.io/docs/concepts/cas</li> <li>https://www.cncf.io/blog/2018/04/19/container-attached-storage-a-primer/</li> <li>https://www.cncf.io/blog/2020/09/22/container-attached-storage-is-cloud-native-storage-cas/</li> <li>https://www.cncf.io/online-programs/kubernetes-and-storage-kubernetes-for-storage-an-overview/</li> <li> <p>https://blog.mayadata.io/container-attached-storage-cas-vs.-shared-storage-which-one-to-choose</p> <p>In Kubernetes, shared storage is typically achieved by mounting volumes and connecting to an external filesystem or block storage solution. Container Attached Storage (CAS) is a relatively newer solution that allows Kubernetes administrators to deploy storage as containerized microservices in a cluster.</p> </li> </ul> <p><code>Container Attached Storage(CAS)</code> \u3068\u306fPod\u3067\u5229\u7528\u53ef\u80fd\u306a\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u30b3\u30f3\u30c6\u30ca\u5316\u3057\u305f\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9\u3068\u3057\u3066Kubernetes Cluster\u3078\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u4ed5\u7d44\u307f\u3067\u3059\u3002 <code>CAS</code> \u3067\u306fPersistent Volumes\u3092\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9 \u30d9\u30fc\u30b9\u306e\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3068\u3057\u3066\u69cb\u6210\u3057\u307e\u3059\u3002\u305d\u306e\u969b\u3001\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3092\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30b9\u30c8\u30ec\u30fc\u30b8 \u30b3\u30f3\u30c8\u30ed\u30fc\u30e9\u3092\u3001\u72ec\u7acb\u3057\u3066\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u304a\u3088\u3073\u5b9f\u884c\u3067\u304d\u308b \u69cb\u6210\u30e6\u30cb\u30c3\u30c8\u3068\u3057\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002(\u3064\u307e\u308a\u3001OpenEBS \u306b\u304a\u3051\u308bPV\u306fVolume\u3092\u30db\u30b9\u30c8\u3059\u308breplica pod \u3068 replica pod\u3092\u7ba1\u7406\u3059\u308bcontroller pod\u3067\u69cb\u6210\u3055\u308c\u307e\u3059)</p> <ul> <li>\u4ee5\u4e0b\u306f Deploy Jenkins with OpenEBS \u3067\u4f5c\u6210\u3055\u308c\u308b\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3068\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9     <pre><code>$ kubectl get pods -n openebs | grep pvc-2fee1acb\nopenebs                pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-ctrl-66d449fp8jp8   2/2     Running   0                3h58m   10.200.0.39    k8s-master   &lt;none&gt;           &lt;none&gt;\nopenebs                pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-0               1/1     Running   1 (3h34m ago)    4h16m   10.200.2.194   k8s-node2    &lt;none&gt;           &lt;none&gt;\nopenebs                pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-1               1/1     Running   1 (3h40m ago)    4h16m   10.200.0.38    k8s-master   &lt;none&gt;           &lt;none&gt;\nopenebs                pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-2               0/1     Pending   0                4h16m   &lt;none&gt;         &lt;none&gt;       &lt;none&gt;           &lt;none&gt;\n</code></pre></li> </ul>"},{"location":"addons/openebs/about/#node-disk-managerndm","title":"Node Disk Manager(NDM)","text":"<ul> <li>https://openebs.io/docs/main/concepts/ndm <ul> <li>CPU\u3084Memory\u3001Network\u306a\u3069\u3068\u540c\u3058\u3088\u3046\u306bNode\u4e0a\u306eblock device\u3092kubernetes resources(CustomResource)\u3068\u3057\u3066\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8</li> <li>DeamonSet\u3068\u3057\u3066\u5404Node\u306b\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u308b</li> <li>Node\u4e0a\u306eblock device\u3078\u306e\u30a2\u30af\u30bb\u30b9\u3059\u308b\u306e\u306b <code>/dev</code>, <code>/proc</code>, <code>/sys</code> \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u304c\u5fc5\u8981\u306a\u305f\u3081Privileged mode\u3067\u52d5\u4f5c\u3059\u308b</li> <li><code>Local PV</code> \u3068 <code>cStor PV</code> \u3067\u4f7f\u7528\u3055\u308c\u308b<ul> <li><code>JIVA PV</code> \u306fNDM\u3078\u30a2\u30af\u30bb\u30b9\u3057\u3066\u3044\u306a\u3044?</li> <li>https://openebs.io/docs/main/user-guides/ndm </li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/openebs/about/#_1","title":"\u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3","text":"<ul> <li> <p>https://openebs.io/docs/main/concepts/casengines</p> Storage Engine Status Description link <code>Local PV</code> Beta  \u5358\u4e00\u30ce\u30fc\u30c9\u3067\u5229\u7528\u53ef\u80fd\u306aVolume\u3092\u63d0\u4f9b Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8(Kubernetes Native\u306e local PersistentVolume volume \u306fStatic Provisioning(\u4e8b\u524d\u306ePV\u3092\u624b\u52d5\u4f5c\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b)) https://openebs.io/docs/concepts/localpvhttps://openebs.io/docs/main/user-guides/localpv-device <code>JIVA PV</code> Stable  \u8907\u6570\u30ce\u30fc\u30c9\u3067\u30ec\u30d7\u30ea\u30ab\u3092\u69cb\u6210\u3057\u305fVolume\u3092\u63d0\u4f9b(\u9ad8\u53ef\u7528\u6027) iSCSI Volume\u3092\u30a8\u30df\u30e5\u30ec\u30fc\u30c8 Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8 \u30b7\u30f3 \u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3092\u30b5\u30dd\u30fc\u30c8 https://openebs.io/docs/concepts/jiva <code>cStor PV</code> Beta  \u8907\u6570\u30ce\u30fc\u30c9\u3067\u30ec\u30d7\u30ea\u30ab\u3092\u69cb\u6210\u3057\u305fVolume\u3092\u63d0\u4f9b(\u9ad8\u53ef\u7528\u6027) iSCSI Volume\u3092\u30a8\u30df\u30e5\u30ec\u30fc\u30c8 Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8 \u30b7\u30f3 \u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3092\u30b5\u30dd\u30fc\u30c8 Snapshot\u3092\u30b5\u30dd\u30fc\u30c8 https://openebs.io/docs/concepts/cstor <ul> <li>\u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3\u306e\u9078\u629e\u57fa\u6e96\u306b\u3064\u3044\u3066<ul> <li>https://openebs.io/docs/2.12.x/concepts/casengines#cstor-vs-jiva-vs-localpv-features-comparison</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/openebs/install/","title":"Installing OpenEBS","text":""},{"location":"addons/openebs/install/#installing-openebs","title":"Installing OpenEBS","text":"<ol> <li>helm \u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b<ul> <li>https://openebs.io/docs/user-guides/installation#installation-through-helm</li> <li>https://github.com/openebs/charts <pre><code>helm install openebs --namespace openebs openebs/openebs --create-namespace \\\n  --set cstor.enabled=true \\\n  --set jiva.enabled=true\n</code></pre></li> </ul> </li> <li> <p>StorageClass\u3092\u78ba\u8a8d\u3059\u308b     <pre><code>$ kubectl get sc\nNAME                       PROVISIONER           RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nopenebs-device             openebs.io/local      Delete          WaitForFirstConsumer   false                  28m\nopenebs-hostpath           openebs.io/local      Delete          WaitForFirstConsumer   false                  28m\nopenebs-jiva-csi-default   jiva.csi.openebs.io   Delete          Immediate              true                   28mS\n</code></pre></p> </li> <li> <p>OpenEBS\u306e\u5404Pod\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b     <pre><code>$ kubectl get pods -n openebs\nNAME                                            READY   STATUS    RESTARTS      AGE\nopenebs-cstor-admission-server-b74f5487-djhxj   1/1     Running   0             22m\nopenebs-cstor-csi-controller-0                  6/6     Running   0             21m\nopenebs-cstor-csi-node-54prc                    2/2     Running   0             28m\nopenebs-cstor-csi-node-g844x                    2/2     Running   0             22m\nopenebs-cstor-csi-node-vrm2w                    2/2     Running   0             95s\nopenebs-cstor-cspc-operator-84464fb479-vdh67    1/1     Running   0             22m\nopenebs-cstor-cvc-operator-646f6f676b-5t79q     1/1     Running   0             22m\nopenebs-jiva-csi-controller-0                   5/5     Running   3 (16m ago)   28m\nopenebs-jiva-csi-node-2hz6w                     3/3     Running   0             110s\nopenebs-jiva-csi-node-nt6jz                     3/3     Running   0             28m\nopenebs-jiva-csi-node-zm49x                     3/3     Running   0             22m\nopenebs-jiva-operator-f994f6868-w7xrl           1/1     Running   0             22m\nopenebs-localpv-provisioner-55b65f8b55-2mq9z    1/1     Running   3 (16m ago)   28m\nopenebs-ndm-4smgw                               1/1     Running   0             2m37s\nopenebs-ndm-6fpg7                               1/1     Running   0             9m7s\nopenebs-ndm-operator-6c944d87b6-5dq77           1/1     Running   0             22m\nopenebs-ndm-vm255                               1/1     Running   0             28m\n</code></pre></p> </li> </ol>"},{"location":"addons/openebs/install/#deploy-jenkins-with-openebs","title":"Deploy Jenkins with OpenEBS","text":""},{"location":"addons/openebs/install/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://blog.openebs.io/tagged/jenkins</li> <li>https://github.com/openebs/openebs/tree/main/k8s/demo/jenkins</li> </ul>"},{"location":"addons/openebs/install/#_2","title":"\u624b\u9806","text":"<ol> <li>download manifests     <pre><code>wget https://raw.githubusercontent.com/openebs/openebs/master/k8s/demo/jenkins/jenkins.yml\n</code></pre></li> <li> <p>modify manifests     <pre><code>vim jenkins.yml\n</code></pre></p> <p><pre><code>$ diff -u &lt;(curl -s https://raw.githubusercontent.com/openebs/openebs/master/k8s/demo/jenkins/jenkins.yml) &lt;(cat ./jenkins.yml)\n--- /dev/fd/63  2022-10-01 14:11:35.968241073 +0000\n+++ /dev/fd/62  2022-10-01 14:11:35.980240892 +0000\n@@ -3,7 +3,7 @@\n metadata:\n   name: jenkins-claim\n   annotations:\n-    volume.beta.kubernetes.io/storage-class: openebs-jiva-default\n+    volume.beta.kubernetes.io/storage-class: openebs-jiva-csi-default\n spec:\n   accessModes:\n     - ReadWriteOnce\n</code></pre> 1. apply manifests <pre><code>kubectl apply -f jenkins.yml\n</code></pre></p> </li> <li> <p>\u30ea\u30bd\u30fc\u30b9\u78ba\u8a8d     PersistentVolumeClaim</p> <pre><code>$ kubectl describe PersistentVolumeClaim jenkins-claim\nName:          jenkins-claim\nNamespace:     default\nStorageClass:  openebs-jiva-csi-default\nStatus:        Bound\nVolume:        pvc-e69e2bbb-f945-4ec7-b61a-490a66596441\nLabels:        &lt;none&gt;\nAnnotations:   pv.kubernetes.io/bind-completed: yes\n               pv.kubernetes.io/bound-by-controller: yes\n                              volume.beta.kubernetes.io/storage-class: openebs-jiva-csi-default\n                                             volume.beta.kubernetes.io/storage-provisioner: jiva.csi.openebs.io\n                                             Finalizers:    [kubernetes.io/pvc-protection]\n                                             Capacity:      5G\n                                             Access Modes:  RWO\n                                             VolumeMode:    Filesystem\n                                             Used By:       jenkins-7f87d6d6d8-htjx9\n                                             Events:        &lt;none&gt;\n</code></pre> <p>Service</p> <pre><code>$ kubectl describe service jenkins-svc\nName:                     jenkins-svc\nNamespace:                default\nLabels:                   &lt;none&gt;\nAnnotations:              &lt;none&gt;\nSelector:                 app=jenkins-app\nType:                     NodePort\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.32.0.70\nIPs:                      10.32.0.70\nPort:                     &lt;unset&gt;  80/TCP\nTargetPort:               8080/TCP\nNodePort:                 &lt;unset&gt;  30792/TCP\nEndpoints:                10.200.0.45:8080\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n</code></pre> <p>&gt;Deployment</p> <pre><code>$ kubectl describe deployment jenkins\nName:                   jenkins\nNamespace:              default\nCreationTimestamp:      Fri, 30 Sep 2022 04:56:42 +0000\nLabels:                 &lt;none&gt;\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               app=jenkins-app\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=jenkins-app\n  Containers:\n   jenkins:\n    Image:        jenkins/jenkins:lts\n    Port:         8080/TCP\n    Host Port:    0/TCP\n    Environment:  &lt;none&gt;\n    Mounts:\n      /var/jenkins_home from jenkins-home (rw)\n  Volumes:\n   jenkins-home:\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n    ClaimName:  jenkins-claim\n    ReadOnly:   false\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Progressing    True    NewReplicaSetAvailable\n  Available      True    MinimumReplicasAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   jenkins-7f87d6d6d8 (1/1 replicas created)\nEvents:          &lt;none&gt;\n</code></pre> <p>Pod</p> <pre><code>$ kubectl describe pods jenkins-7f87d6d6d8-htjx9\nName:         jenkins-7f87d6d6d8-htjx9\nNamespace:    default\nPriority:     0\nNode:         k8s-master/192.168.3.50\nStart Time:   Sun, 02 Oct 2022 16:49:07 +0000\nLabels:       app=jenkins-app\n              pod-template-hash=7f87d6d6d8\nAnnotations:  &lt;none&gt;\nStatus:       Running\nIP:           10.200.0.45\nIPs:\n  IP:           10.200.0.45\nControlled By:  ReplicaSet/jenkins-7f87d6d6d8\nContainers:\n  jenkins:\n    Container ID:   containerd://9c039657f1a1dee2b4b480aaff1859c9385b9b9d2d874757ad6e0e6c47278bda\n    Image:          jenkins/jenkins:lts\n    Image ID:       docker.io/jenkins/jenkins@sha256:5508cb1317aa0ede06cb34767fb1ab3860d1307109ade577d5df871f62170214\n    Port:           8080/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 02 Oct 2022 16:49:38 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/jenkins_home from jenkins-home (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gzs4t (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             True\n  ContainersReady   True\n  PodScheduled      True\nVolumes:\n  jenkins-home:\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n    ClaimName:  jenkins-claim\n    ReadOnly:   false\n  kube-api-access-gzs4t:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       &lt;nil&gt;\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              &lt;none&gt;\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:                      &lt;none&gt;\n</code></pre> </li> <li> <p>Web\u30a2\u30af\u30bb\u30b9\u78ba\u8a8d</p> image descrioption \u8868\u793a\u306b\u3042\u308b\u901a\u308a<code>/var/jenkins_home/secrets/initialAdminPassword</code>\u306e\u5185\u5bb9\u3092\u8cbc\u308a\u4ed8\u3051\u308b<code>kubectl exec -it jenkins-7f87d6d6d8-wx8nn -- cat /var/jenkins_home/secrets/initialAdminPassword</code> </li> </ol>"},{"location":"autoscaler/","title":"Index","text":"<ul> <li>https://speakerdeck.com/oracle4engineer/kubernetes-autoscale-deep-dive</li> </ul>"},{"location":"aws-eks/aws-load-balancer-controller/","title":"aws-load-balancer-controller","text":""},{"location":"aws-eks/aws-load-balancer-controller/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/</li> </ul>"},{"location":"failure_test/failure_scenario_of_node/","title":"Node","text":""},{"location":"failure_test/failure_scenario_of_node/#worker-node","title":"Worker Node","text":""},{"location":"failure_test/failure_scenario_of_node/#resources","title":"Resources","text":""},{"location":"failure_test/failure_scenario_of_node/#oom-killer","title":"OOM Killer","text":""},{"location":"failure_test/failure_scenario_of_node/#_1","title":"\u53c2\u8003","text":"<ul> <li>k8s.af \u304b\u3089<ul> <li>https://www.bluematador.com/blog/post-mortem-kubernetes-node-oom</li> </ul> </li> <li>https://www.scsk.jp/sp/sysdig/blog/sysdig_monitor/kubernetes_oomcpu.html</li> <li>Goldstine\u7814\u7a76\u6240</li> <li>https://blog.mosuke.tech/entry/2020/03/31/kubernetes-resource/</li> <li>https://blog.mosuke.tech/entry/2021/03/11/kubernetes-node-down/</li> <li>\u5916\u9053\u7236\u306e\u5320<ul> <li>http://blog.father.gedow.net/2019/11/28/eks-kubernetes-ouf-of-memory/</li> </ul> </li> </ul>"},{"location":"failure_test/failure_scenario_of_node/#impact","title":"Impact","text":"<ul> <li>Memory\u304cNode\u4e0a\u9650\u306b\u9054\u3057\u305f\u5834\u5408OOM Killer\u304cPod\u3092\u5f37\u5236\u505c\u6b62\u3059\u308b<ul> <li>\u5f37\u5236\u505c\u6b62\u3059\u308bPod\u306fQoS Class\u306e\u512a\u5148\u5ea6\u3067\u6c7a\u307e\u308b (\u53c2\u8003)</li> <li><code>Note: The kubelet also sets an oom_score_adj value of -997 for containers in Pods that have system-node-critical Priority</code></li> <li>\u540c\u3058QoS Class(Burstable) \u306e\u5834\u5408\u306fPod\u306erequest\u3057\u305f\u30e1\u30e2\u30ea\u30fc\u91cf\u3068\u30ce\u30fc\u30c9\u306e\u30ad\u30e3\u30d1\u30b7\u30c6\u30a3\u306e\u5272\u5408\u306b\u3088\u3063\u3066\u30b9\u30b3\u30a2\u4ed8\u3051\u3055\u308c\u308b</li> </ul> </li> </ul>"},{"location":"failure_test/failure_scenario_of_node/#recommend","title":"Recommend","text":"<ul> <li><code>Requests Memory = Limits Memory</code> \u3067\u8a2d\u5b9a\u3059\u308b</li> <li>Limits Memory\u304cRequests Memory\u3088\u308a\u591a\u3044\u5834\u5408\u3001Requests Memory\u304cAllocatable\u306b\u53ce\u307e\u308b\u3088\u3046\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u3001Allocatable\u3092\u8d85\u3048\u305f\u5834\u5408\u306bNode\u306eOOM Killer\u304c\u767a\u52d5\u3059\u308b\u306e\u3067\u30b7\u30b9\u30c6\u30e0\u5168\u4f53\u306e\u30d7\u30ed\u30bb\u30b9\u304b\u3089Kill\u5bfe\u8c61\u304b\u3089\u9078\u3070\u308c\u308b</li> </ul>"},{"location":"failure_test/failure_scenario_of_node/#node-","title":"Node - \u505c\u6b62","text":""},{"location":"failure_test/failure_scenario_of_node/#_2","title":"\u53c2\u8003","text":"<ul> <li>Goldstine\u7814\u7a76\u6240: Kubernetes\u306e\u30ce\u30fc\u30c9\u969c\u5bb3\u6642\u306ePod\u306e\u52d5\u304d\u306b\u3064\u3044\u3066\u306e\u691c\u8a3c</li> </ul>"},{"location":"failure_test/failure_scenario_of_node/#impact_1","title":"Impact","text":"<ul> <li> <p>ReplicaSet Pod\u306e\u518d\u914d\u7f6e\u304c\u884c\u308f\u308c\u308b</p> </li> <li> <p><code>node_lifecycle_controller</code></p> <ol> <li>Kubelet\u304cNode\u60c5\u5831\u3092\u66f4\u65b0\u3057\u306a\u304f\u306a\u3063\u305f\u3053\u3068\u3092\u691c\u77e5\u3057\u3066Node\u306eStatus\u3092\u5909\u66f4</li> <li><code>key: node.kubernetes.io/unreachable</code> \u306eTaint\u3092\u4ed8\u4e0e</li> </ol> </li> <li> <p>Pod\u306e\u518d\u914d\u7f6e</p> <ul> <li>Pod\u304c\u4f5c\u6210\u3055\u308c\u308b\u6642\u306bDefaultTolerationSeconds AdmissionController\u306b\u3088\u3063\u3066\u4ee5\u4e0b\u306etolerations\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u3044\u308b<ul> <li><code>node.kubernetes.io/not-ready:NoExecute</code></li> <li><code>node.kubernetes.io/unreachable:NoExecute</code></li> </ul> </li> <li>\u3053\u308c\u3089\u306etolerations\u304c <code>tolerationSeconds: 300</code> \u3092\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001300\u79d2\u7d4c\u904e\u5f8c\u306bNode\u304c\u5fa9\u65e7\u305b\u305a<code>node.kubernetes.io/unreachable</code> Taint\u304c\u5916\u308c\u306a\u3044\u5834\u5408Pod\u304cEviction\uff08\u5f37\u5236\u9000\u53bb\uff09\u3055\u308c\u5225\u306e\u30ce\u30fc\u30c9\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u308b</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaulttolerationseconds</li> <li>\u5225\u30ce\u30fc\u30c9\u3078\u306e\u518d\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u307e\u3067\u6700\u59275\u5206\u306e\u30bf\u30a4\u30e0\u30e9\u30b0\u304c\u767a\u751f\u3059\u308b\u5834\u5408\u304c\u3042\u308b</li> <li>DefaultTolerationSeconds</li> </ul> </li> </ul>"},{"location":"failure_test/failure_story/","title":"Failure Story","text":""},{"location":"failure_test/failure_story/#failure-stories","title":"Failure Stories","text":"<ul> <li>https://k8s.af/<ul> <li>https://codeberg.org/hjacobs/kubernetes-failure-stories</li> </ul> </li> </ul>"},{"location":"failure_test/point_of_failure/","title":"Point of failure","text":""},{"location":"failure_test/point_of_failure/#point-of-failure","title":"Point of failure","text":""},{"location":"failure_test/point_of_failure/#kubernetes-component","title":"Kubernetes Component","text":""},{"location":"failure_test/point_of_failure/#controll-plane","title":"Controll Plane","text":"<ul> <li>etcd</li> <li>kube-apiserver</li> <li>kube-controller-manager</li> <li>kube-scheduler</li> </ul>"},{"location":"failure_test/point_of_failure/#worker-node","title":"Worker Node","text":"<ul> <li>resources(cpu, memory, disk, network...)</li> <li>OOM Killer</li> <li>kubelet</li> <li>kube-proxy</li> <li>cni</li> <li>container-runtime</li> </ul>"},{"location":"failure_test/point_of_failure/#addons","title":"Addons","text":"<ul> <li> <p>\u53c2\u8003</p> <ul> <li>https://github.com/kubernetes/kubernetes/tree/master/cluster/addons</li> <li>https://registry.terraform.io/modules/particuleio/addons/kubernetes/latest</li> <li>https://aws.amazon.com/jp/blogs/news/introducing-amazon-eks-add-ons-jp/</li> <li>https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/eks-add-ons.html</li> </ul> </li> <li> <p>CoreDNS</p> </li> <li>Amazon VPC CNI(EKS)</li> </ul>"},{"location":"failure_test/point_of_failure/#controller","title":"Controller","text":"<ul> <li>Ingress Controller</li> <li>External-dns</li> </ul>"},{"location":"failure_test/point_of_failure/#autoscale","title":"AutoScale","text":"<ul> <li>MetricsServer</li> <li> <p>Pod</p> <ul> <li>HPA</li> <li>VPA</li> </ul> </li> <li> <p>Node(Horizontal Scale)</p> <ul> <li>Cluster Autoscaler</li> </ul> </li> </ul>"},{"location":"kubernetes_dashboard/","title":"kubernetes dashboard","text":""},{"location":"kubernetes_dashboard/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://github.com/kubernetes/dashboard</li> <li>https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/</li> </ul>"},{"location":"kyverno/about_kyverno/","title":"Kyverno","text":""},{"location":"kyverno/about_kyverno/#about-kyverno","title":"About Kyverno","text":"<ul> <li>https://kyverno.io/docs/introduction/</li> <li>Kyverno \u306fKubernetes\u306e\u305f\u3081\u306b\u8a2d\u8a08\u3055\u308c\u305fPolicy Engine\u3067\u3059\u3002<ul> <li>Open Policy Agent(OPA) \u306eRego\u306e\u3088\u3046\u306b\u72ec\u81ea\u8a00\u8a9e\u3092\u899a\u3048\u308b\u5fc5\u8981\u306f\u306a\u304fCustomResourceDefinition(CRD)\u3067\u5b9a\u7fa9\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059</li> </ul> </li> </ul>"},{"location":"kyverno/about_kyverno/#about-kyverno-policy","title":"About Kyverno Policy","text":"<ul> <li> <p>\u4ee5\u4e0b\u56f3\u306f \u3053\u3061\u3089 \u304b\u3089\u629c\u7c8b</p> <ul> <li></li> </ul> </li> <li> <p>Kyverno Policy\u306f1\u3064\u4ee5\u4e0a\u306e\u30eb\u30fc\u30eb\u306e\u30b3\u30ec\u30af\u30b7\u30e7\u30f3\u3067\u3059\u3002</p> </li> <li>1\u3064\u306e\u30eb\u30fc\u30eb\u306f2\u3064\u306e\u5ba3\u8a00\u3092\u6301\u3061\u307e\u3059<ol> <li>Policy\u9069\u7528\u5bfe\u8c61\u30ea\u30bd\u30fc\u30b9\u306e\u9078\u629e<ul> <li>Select Resources</li> </ul> </li> <li>Policy\u9069\u7528<ul> <li>Mutate Resources</li> <li>Validate Resources</li> <li>Generate Resources</li> <li>VerifyImages Resources</li> </ul> </li> </ol> </li> </ul>"},{"location":"kyverno/about_kyverno/#kind-of-policy","title":"Kind of Policy","text":"<ul> <li>Cluster wide\u306b\u9069\u7528\u3055\u308c\u308b <code>ClusterPolicy</code> \u3068 namespace\u5358\u4f4d\u306b\u9069\u7528\u3055\u308c\u308b <code>Policy</code> \u304c\u3042\u308b<ul> <li>kyverno.io/v1.ClusterPolicy</li> <li>kyverno.io/v1.Policy</li> </ul> </li> </ul>"},{"location":"kyverno/about_kyverno/#kind-of-resources","title":"Kind of Resources","text":""},{"location":"kyverno/about_kyverno/#select-resources","title":"Select Resources","text":"<ul> <li>Mutate/VerifyImages/Validate/Generate \u306a\u3069\u306epolicy\u3092\u9069\u7528\u3059\u308b\u5bfe\u8c61\u30ea\u30bd\u30fc\u30b9\u3092\u6307\u5b9a\u3059\u308b<ul> <li>\u5bfe\u8c61\u3068\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9\u3092<code>match</code>\u3067\u3001\u5bfe\u8c61\u5916\u3068\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9\u3092<code>exclude</code> \u3067\u6307\u5b9a\u3059\u308b</li> <li>any(<code>OR</code>) \u3082\u3057\u304f\u306f all(<code>AND</code>)\u306e\u6761\u4ef6\u4e0b\u3067 resource filters \u3092\u6307\u5b9a<ul> <li><code>resources</code>: select resources by names, namespaces, kinds, label selectors, annotations, and namespace selectors.</li> <li><code>subjects</code>: select users, user groups, and service accounts</li> <li><code>roles</code>: select namespaced roles</li> <li><code>clusterRoles</code>: select cluster wide roles</li> </ul> </li> </ul> </li> </ul>"},{"location":"kyverno/about_kyverno/#mutate-resources","title":"Mutate Resources","text":"<ul> <li>rule\u306b\u5fdc\u3058\u3066\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u66f8\u304d\u63db\u3048\u308b<ul> <li>\u66f8\u304d\u63db\u3048\u305f\u3044\u5185\u5bb9\u306f <code>RFC 6902 JSON Patch</code>\u3001<code>a strategic merge patch</code>\u3001<code>an overlay pattern</code> \u306e\u3044\u305a\u308c\u304b\u3067\u5b9a\u7fa9\u53ef\u80fd</li> <li>RFC 6902 JSON Patch</li> <li>Strategic Merge Patch</li> <li>an overlay pattern<ul> <li>Conditional logic using anchors</li> <li>Mutate Rule Ordering</li> </ul> </li> </ul> </li> </ul>"},{"location":"kyverno/about_kyverno/#validate-resources","title":"Validate Resources","text":"<ul> <li>\u3053\u308c\u304b\u3089\u9069\u7528\u3059\u308bmanifests\u3001\u307e\u305f\u306f\u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306b\u95a2\u3057\u3066\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u304c\u306a\u3044\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3059\u308b</li> <li><code>spec.validationFailureAction</code><ul> <li><code>enforce</code><ul> <li>\u65b0\u898f\u4f5c\u6210\u306e\u5834\u5408\u306f\u62d2\u5426\u3059\u308b</li> </ul> </li> <li><code>audit</code><ul> <li>\u4f5c\u6210\u306f\u62d2\u5426\u3057\u306a\u3044</li> <li><code>ClusterPolicyReport</code> \u307e\u305f\u306f <code>PolicyReport</code> \u3092\u4f5c\u6210\u3057\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u8a18\u9332\u3059\u308b</li> </ul> </li> </ul> </li> <li><code>spec.background</code><ul> <li>\u5f53\u8a72 <code>ClusterPolicy</code> \u307e\u305f\u306f <code>Policy</code> \u306b\u95a2\u3057\u3066\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u3057\u3066\u3044\u306a\u3044\u304b\u3069\u3046\u304b\u3092\u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306b\u691c\u8a3c\u3059\u308b</li> <li>(<code>enforce</code>\u306e\u5834\u5408\u3067\u3082) \u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306e\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u306f<code>ClusterPolicyReport</code> \u307e\u305f\u306f <code>PolicyReport</code> \u3092\u4f5c\u6210\u3057\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u8a18\u9332\u3059\u308b</li> </ul> </li> <li>Patterns<ul> <li><code>spec.rules[*].va1lidate.pattern</code></li> <li><code>spec.rules[*].va1lidate.anyPattern</code></li> </ul> </li> <li>\u30eb\u30fc\u30eb\u306e\u62d2\u5426<ul> <li><code>deny</code><ul> <li><code>validationFailureAction: enforce</code> \u3092\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b</li> <li><code>deny</code> rule\u306e <code>exclude</code> \u3067\u62d2\u5426\u3055\u305b\u305f\u304f\u306a\u3044subjects\u3084role\u3092\u6307\u5b9a\u3059\u308b\u306a\u3069\u3082\u53ef\u80fd</li> </ul> </li> </ul> </li> </ul>"},{"location":"kyverno/about_kyverno/#generate-resources","title":"Generate Resources","text":"<ul> <li>\u30ea\u30bd\u30fc\u30b9\u306e\u4f5c\u6210\u30fb\u66f4\u65b0\u306b\u57fa\u3065\u3044\u3066\u8ffd\u52a0\u306e\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b</li> <li>\u30ea\u30bd\u30fc\u30b9\u4f5c\u6210\u65b9\u6cd5<ol> <li><code>spec.rules.generate.clone</code><ul> <li>Secret\u3084ConfigMap\u306a\u3069\u65e2\u5b58\u30ea\u30bd\u30fc\u30b9\u3092\u30b3\u30d4\u30fc\u3057\u305f\u3044\u5834\u5408</li> </ul> </li> <li><code>spec.rules.generate.data</code><ul> <li>rule manifests\u306b\u5b9a\u7fa9\u3055\u308c\u305f\u65b0\u898f\u4f5c\u6210\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9</li> </ul> </li> </ol> </li> </ul> <p>Note</p> <p>kyverno policy rule\u306b\u3088\u3063\u3066\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u306f\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u7ba1\u7406\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u3002 <code>spec.rules.generate.synchronize=true</code> \u306e\u5834\u5408\u3001\u5fc5\u8981\u306a\u6a29\u9650\u3092\u6301\u3064\u30e6\u30fc\u30b6\u3084ServiceAccount\u306a\u3069\u304b\u3089\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u304c\u5909\u66f4\u30fb\u524a\u9664\u3055\u308c\u3066\u3082\u5143\u306e\u30ea\u30bd\u30fc\u30b9\u72b6\u614b\u306b\u623b\u3059\u3088\u3046\u306b\u66f4\u65b0\u30fb\u518d\u4f5c\u6210\u3057\u3066\u304f\u308c\u307e\u3059\u3002</p>"},{"location":"kyverno/about_kyverno/#verifyimages-resources","title":"VerifyImages Resources","text":"<ul> <li>image\u306e\u30b7\u30b0\u30cd\u30c1\u30e3(\u7f72\u540d) \u3092\u30c1\u30a7\u30c3\u30af\u3057\u3001\u30c0\u30a4\u30b8\u30a7\u30b9\u30c8\u3092\u8ffd\u52a0\u3059\u308b</li> </ul>"},{"location":"kyverno/installation_kyverno/","title":"Kyverno","text":""},{"location":"kyverno/installation_kyverno/#installation-kyverno","title":"Installation Kyverno","text":""},{"location":"kyverno/installation_kyverno/#kyverno-policy-engine","title":"Kyverno Policy Engine","text":"<ul> <li>https://kyverno.io/docs/installation/</li> <li>\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f\u5927\u304d\u304f\u4ee5\u4e0b2\u30d1\u30bf\u30fc\u30f3<ol> <li>helm chart</li> <li>manifests(yaml)</li> </ol> </li> </ul>"},{"location":"kyverno/installation_kyverno/#_1","title":"\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8","text":"<ul> <li>\u4ee5\u4e0b\u56f3\u306f \u3053\u3061\u3089 \u304b\u3089\u629c\u7c8b<ul> <li></li> </ul> </li> </ul>"},{"location":"kyverno/installation_kyverno/#tls-configuration","title":"TLS Configuration","text":"<ul> <li>Kyverno policy engine \u306fAdmission Webhook\u3068\u3057\u3066\u52d5\u4f5c\u3057\u3001kube-apiserver\u3068\u306e\u901a\u4fe1\u3067TLS\u901a\u4fe1\u3092\u884c\u3046\u305f\u3081\u306b\u8a8d\u8a3c\u5c40\u306e\u7f72\u540d\u304c\u3055\u308c\u305f\u8a3c\u660e\u66f8\u304c\u5fc5\u8981</li> <li>\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u306e\u4f5c\u6210<ul> <li>(helm charts\u306e) <code>createSelfSignedCert</code> (defailt: false)<ul> <li><code>false</code>: <code>kube-controller-manager</code> \u3067\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u4f5c\u6210</li> <li><code>true</code>: \u4f5c\u6210\u6e08\u307f\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u3092\u4f7f\u7528\u3059\u308b<ul> <li>https://kyverno.io/docs/installation/#option-2-use-your-own-ca-signed-certificate</li> </ul> </li> </ul> </li> </ul> </li> <li>\u53c2\u8003<ul> <li>https://kyverno.io/docs/installation/#customize-the-installation-of-kyverno</li> <li>https://github.com/kyverno/kyverno/blob/v1.5.1/charts/kyverno/README.md#tls-configuration</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/</li> <li>https://github.com/kyverno/kyverno/blob/v1.5.1/charts/kyverno/values.yaml#L228-L238</li> </ul> </li> </ul>"},{"location":"kyverno/installation_kyverno/#webhook-configurations","title":"Webhook Configurations","text":"<ul> <li>ValicatingWebhookConfiguration</li> <li>MutatingWebhookConfiguration</li> <li>\u8d77\u52d5\u6642\u306b\u4f5c\u6210<ul> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/kyverno/main.go#L426</li> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/initContainer/main.go#L165</li> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/webhookconfig/monitor.go#L85</li> </ul> </li> <li>\u505c\u6b62\u6642\u306b\u524a\u9664<ul> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/kyverno/main.go#L533-L536</li> </ul> </li> </ul>"},{"location":"kyverno/installation_kyverno/#mutatingwebhookconfigurations","title":"mutatingwebhookconfigurations","text":"<ul> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/config/config.go#L11-L66</li> <li>generateMutatingWebhook<ul> <li>constructVerifyMutatingWebhookConfig</li> <li>constructPolicyMutatingWebhookConfig</li> <li>constructDefaultMutatingWebhookConfig</li> </ul> </li> </ul>"},{"location":"kyverno/installation_kyverno/#validatingwebhookconfigurations","title":"validatingwebhookconfigurations","text":"<ul> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/config/config.go#L11-L66</li> <li>generateValidatingWebhook<ul> <li>constructPolicyValidatingWebhookConfig</li> <li>constructDefaultValidatingWebhookConfig</li> </ul> </li> </ul>"},{"location":"kyverno/installation_kyverno/#installation-polcies","title":"Installation Polcies","text":"<ul> <li><code>Policy</code> \u3082\u3057\u304f\u306f <code>ClusterPolicy</code> \u306emanifests\u3092\u9069\u7528\u3057\u307e\u3059</li> <li>Policy\u306e\u66f8\u304d\u65b9\u3084\u63a8\u5968Policy\u306b\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u3092\u53c2\u7167<ul> <li>https://kyverno.io/policies/</li> <li>https://github.com/kyverno/policies</li> <li>Privilege mode\u3092\u8a31\u53ef\u3057\u306a\u3044Policy\u306esample</li> <li>https://aws.amazon.com/jp/blogs/news/easy-as-one-two-three-policy-management-with-kyverno-on-amazon-eks/</li> </ul> </li> </ul>"},{"location":"kyverno/installation_kyverno/#uninstallation-kyverno","title":"Uninstallation Kyverno","text":"<ul> <li> <p>https://kyverno.io/docs/installation/#uninstalling-kyverno</p> <p>Warning</p> <p>https://github.com/kyverno/kyverno/issues/2750 https://github.com/kyverno/kyverno/issues/2623</p> <p>kyverno Pod\u304cdelete\u3055\u308c\u308b\u969b\u306b <code>mutatingwebhookconfigurations</code> \u3068 <code>validatingwebhookconfigurations</code> \u304c\u524a\u9664\u3055\u308c\u306a\u3044bug\u304c\u3042\u308a\u307e\u3059\u3002 (1.5.2-rc2 image\u3067fix\u3057\u305f\u3088\u3046\u3067\u3059)</p> <p>kyverno controller\u304c\u524a\u9664\u3055\u308cwebhook\u304c\u6b8b\u3063\u3066\u3044\u308b\u5834\u5408\u3001Pod\u3092\u8d77\u52d5\u3057\u3088\u3046\u3068\u3057\u305f\u969b\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u3002</p> <pre><code>warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\nError from server (InternalError): Internal error occurred: failed calling webhook \"validate.kyverno.svc-fail\": Post \"https://kyverno-svc.kyverno.svc:443/validate?timeout=10s\": service \"kyverno-svc\" not found\n</code></pre> <p>\u624b\u52d5\u3067webhook\u3092\u524a\u9664\u3059\u308b\u5834\u5408\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u3092\u53c2\u7167 https://kyverno.io/docs/installation/#clean-up-webhook-configurations</p> </li> </ul>"},{"location":"kyverno/installation_kyverno/#appendix","title":"Appendix","text":""},{"location":"kyverno/installation_kyverno/#kube-controller-manager","title":"<code>kube-controller-manager</code> \u3067\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u4f5c\u6210","text":"<ul> <li>https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/</li> </ul>"},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/","title":"bootstrapping kubernetes-dashboard","text":""},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/#about-kubernetes-dashboard","title":"about kubernetes-dashboard","text":"<p>https://github.com/kubernetes/dashboard</p> <p>Kubernetes Dashboard\u306f\u3001Kubernetes\u30af\u30e9\u30b9\u30bf\u7528\u306e\u6c4e\u7528\u7684\u306aWeb\u30d9\u30fc\u30b9\u306eUI\u3067\u3059\u3002\u30af\u30e9\u30b9\u30bf\u30fc\u3067\u52d5\u4f5c\u3059\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u7ba1\u7406\u3084\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u306e\u307b\u304b\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u81ea\u4f53\u306e\u7ba1\u7406\u3082\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://github.com/kubernetes/dashboard/blob/master/docs/common/dashboard-arguments.md</li> <li>https://itnext.io/how-to-expose-your-kubernetes-dashboard-with-cert-manager-422ab1e3bf30</li> <li>https://magda.io/docs/how-to-setup-https-to-local-cluster.html</li> <li>https://vmwire.com/2022/02/07/running-kubernetes-dashboard-with-signed-certificates/</li> <li>https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard</li> <li>https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard</li> </ul>"},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/#install","title":"install","text":"<ol> <li> <p>create self signed certificate</p> <pre><code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout kubernetes-dashboard.key -out kubernetes-dashboard.crt -subj \"/CN=k8s-dashboard.local/O=k8s-dashboard.local\"\n</code></pre> </li> <li> <p>create ns and secret</p> <pre><code>kubectl create ns kubernetes-dashboard\nkubectl create secret generic kubernetes-dashboard-certs --from-file=./ -n kubernetes-dashboard\n</code></pre> </li> <li> <p>self-signed certificates regist to key chain(Mac OSX)</p> <ul> <li>Chrome\u3067\u30a2\u30af\u30bb\u30b9\u3057\u305f\u969b\u306b\u4e0d\u6b63\u306a\u8a3c\u660e\u66f8\u3068\u3057\u3066\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u3092\u4fe1\u983c\u6e08\u307f\u8a3c\u660e\u66f8\u3068\u3057\u3066\u767b\u9332\u3059\u308b</li> <li></li> </ul> </li> <li> <p>download kubernetes-dashboard manifests</p> <pre><code>sudo curl -o /etc/kubernetes/manifests/kubernetes-dashboard.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n</code></pre> </li> <li> <p>edit of kubernetes-dashboard manifests</p> <pre><code>sudo vim /etc/kubernetes/manifests/kubernetes-dashboard.yaml\n</code></pre> <ul> <li> <p>Service\u30ea\u30bd\u30fc\u30b9\u306eType\u3092 <code>NodePort</code> \u306b\u5909\u66f4</p> <p>diff <pre><code>@@ -37,6 +37,7 @@\n   name: kubernetes-dashboard\n   namespace: kubernetes-dashboard\n spec:\n+  type: NodePort\n   ports:\n     - port: 443\n       targetPort: 8443\n</code></pre></p> </li> <li> <p>kubernetes-dashboard\u30b3\u30f3\u30c6\u30ca\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306bTLS\u8a3c\u660e\u66f8\u30d5\u30a1\u30a4\u30eb\u3068TLS\u9375\u30d5\u30a1\u30a4\u30eb\u3092\u6307\u5b9a\u3059\u308b</p> <p>diff</p> <pre><code>@@ -198,6 +199,8 @@\n           args:\n             - --auto-generate-certificates\n             - --namespace=kubernetes-dashboard\n+            - --tls-cert-file=/tls.crt\n+            - --tls-key-file=/tls.key\n</code></pre> </li> </ul> </li> <li> <p>apply kubernetes-dashboard manifests</p> <pre><code>kubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard.yaml\n</code></pre> </li> <li> <p>create ingress</p> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/kubernetes/manifests/kubernetes-dashboard-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\nspec:\n  tls:\n    - hosts:\n      - k8s-dashboard.local\n      secretName: dashboard-secret-tls\n  rules:\n  - host: k8s-dashboard.local\n    http:\n      paths:\n        - pathType: Prefix\n          path: \"/\"\n          backend:\n            service:\n              name: kubernetes-dashboard\n              port:\n                number: 443\nEOF\n\nkubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard-ingress.yaml\n</code></pre> </li> <li> <p>adding fqdn and node ip address to <code>/etc/hosts</code></p> <ul> <li> <p>get node ip</p> <pre><code>kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard -o json | jq -r .items[].status.hostIP\n</code></pre> </li> <li> <p>adding entry to <code>/etc/hosts</code></p> <pre><code>&lt;Node IP ADDRESS&gt; `k8s-dashboard.local`\n</code></pre> </li> </ul> </li> <li> <p>get node port</p> <pre><code>kubectl get service -n kubernetes-dashboard kubernetes-dashboard -o json | jq -r .spec.ports[].nodePort\n</code></pre> </li> <li> <p>access kubernetes-dashboard with browser</p> <ul> <li><code>https://k8s-dashboard.local:&lt;node port of previous command result&gt;</code><ul> <li></li> </ul> </li> </ul> </li> <li> <p>create token strings</p> <pre><code>kubectl create serviceaccount dashboard -n default\nkubectl create clusterrolebinding dashboard-admin -n default --clusterrole=cluster-admin --serviceaccount=default:dashboard\nTOKEN=`kubectl get secret $(kubectl get serviceaccount dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode`\necho $TOKEN\n</code></pre> </li> <li> <p>input token and login</p> <ul> <li>input previous command result as token strings</li> <li></li> </ul> </li> <li> <p>login successed</p> <ul> <li></li> </ul> </li> </ol>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/","title":"metrics-server","text":"<p><code>metrics-server</code>\u3068\u306fkubelet\u3084kube-apiserver\u304b\u3089\u5404\u7a2e\u30ea\u30bd\u30fc\u30b9\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002 \u53ce\u96c6\u3057\u305f\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u60c5\u5831\u306fautoscaling(HPA \u3084 VPA)\u3092\u884c\u3046\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_1","title":"\u53c2\u8003","text":"<ul> <li>metrics-server<ul> <li>https://github.com/kubernetes-sigs/metrics-server<ul> <li>docs/command-line-flags.txt</li> </ul> </li> <li>https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/</li> </ul> </li> <li>kube-apiserver Aggregation Layer\u8a2d\u5b9a\u306b\u3064\u3044\u3066<ul> <li>https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/</li> <li>https://kubernetes.io/ja/docs/concepts/cluster-administration/proxies/</li> <li>https://github.com/ansilh/kubernetes-the-hardway-virtualbox/blob/master/15.Deploy-Metric-Server.md</li> </ul> </li> </ul>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_2","title":"\u8981\u4ef6","text":"<ul> <li>https://github.com/kubernetes-sigs/metrics-server#requirements<ul> <li>Metrics Server must be reachable from kube-apiserver by container IP address (or node IP if hostNetwork is enabled).</li> <li>The kube-apiserver must enable an aggregation layer.</li> <li>Nodes must have Webhook authentication and authorization enabled.</li> <li>Kubelet certificate needs to be signed by cluster Certificate Authority (or disable certificate validation by passing --kubelet-insecure-tls to Metrics Server)</li> <li>Container runtime must implement a container metrics RPCs (or have cAdvisor support)</li> </ul> </li> </ul>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_3","title":"\u69cb\u7bc9\u624b\u9806","text":""},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#kube-apiserver-aggregation-layer","title":"kube-apiserver Aggregation Layer\u8a2d\u5b9a","text":"<p>Info</p> <p>https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server</p> <p>Metrics Server collects metrics from the Summary API, exposed by Kubelet on each node, and is registered with the main API server via Kubernetes aggregator.</p> <ul> <li>kube-apiserver\u3067Aggregation Layer\u3092\u6709\u52b9\u306b\u3059\u308b<ul> <li>Aggregation Layer\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408\u306fmetrics-server\u3067\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u30ed\u30b0\u304c\u51fa\u3066\u3044\u308b     <pre><code>E0217 15:13:53.378655       1 webhook.go:224] Failed to make webhook authorizer request: Post \"https://10.32.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s\": cont\next canceled\nE0217 15:13:53.378917       1 errors.go:77] Post \"https://10.32.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s\": context canceled\nE0217 15:13:53.379124       1 timeout.go:137] post-timeout activity - time-elapsed: 121.389\u00b5s, GET \"/apis/metrics.k8s.io/v1beta1\" result: &lt;nil&gt;\n</code></pre></li> <li>kube-apiserver front-proxy(for aggregation layer)\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8<ul> <li>front-proxy\u7528CA\u8a3c\u660e\u66f8\u304a\u3088\u3073\u30b5\u30fc\u30d0\u8a3c\u660e\u66f8\u3068\u79d8\u5bc6\u9375\u3092\u751f\u6210\u3059\u308b<ul> <li><code>/var/lib/kubernetes/front-proxy-ca.pem</code></li> <li><code>/var/lib/kubernetes/front-proxy.pem</code></li> <li><code>/var/lib/kubernetes/front-proxy-key.pem</code></li> </ul> </li> </ul> </li> <li>/setup/06_master/03_bootstrapping_kube-apiserver/<ul> <li>kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u8ffd\u52a0     <pre><code>--enable-aggregator-routing=true\n--requestheader-client-ca-file=/var/lib/kubernetes/front-proxy-ca.pem\n--requestheader-allowed-names=front-proxy-ca\n--requestheader-extra-headers-prefix=X-Remote-Extra\n--requestheader-group-headers=X-Remote-Group\n--requestheader-username-headers=X-Remote-User\n--proxy-client-cert-file=/var/lib/kubernetes/front-proxy.pem\n--proxy-client-key-file=/var/lib/kubernetes/front-proxy-key.pem\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#metics-server","title":"metics-server\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ol> <li> <p>manifests\u3092deploy</p> <pre><code>$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre> </li> <li> <p>metrics-server \u304c\u8d77\u52d5\u306f\u3059\u308b\u304cTLS\u95a2\u9023\u306e\u30a8\u30e9\u30fc\u3067\u6b63\u5e38\u306b\u52d5\u4f5c\u3057\u306a\u3044</p> <ul> <li><code>kubectl logs</code> \u30b3\u30de\u30f3\u30c9     <pre><code>Failed to scrape node\" err=\"Get \\\"https://192.168.10.51:10250/metrics/resource\\\": x509: certificate is valid for 192.168.10.50, not 192.168.10.51\" node=\"k8s-node1\n</code></pre></li> </ul> </li> <li> <p>TLS\u8a8d\u8a3c\u8a2d\u5b9a\u306e\u5909\u66f4</p> <ul> <li>metrics-server\u306e\u8d77\u52d5\u5f15\u6570\u306b <code>--kubelet-insecure-tls</code> \u3092\u8ffd\u52a0<ul> <li>https://github.com/kubernetes-sigs/metrics-server/issues/131</li> <li>https://github.com/kubernetes-sigs/metrics-server/issues/300</li> <li>kubectl logs\u30b3\u30de\u30f3\u30c9\u3067\u4ee5\u4e0b\u30a8\u30e9\u30fc\u304c\u51fa\u7d9a\u3051\u3066\u3044\u308b\u5834\u5408\u306e\u5bfe\u51e6</li> <li>TLS\u8a3c\u660e\u66f8\u306e\u691c\u8a3c\u3092\u884c\u308f\u306a\u3044\u3088\u3046\u306b\u3059\u308b(\u8a3c\u660e\u66f8\u306e\u7f72\u540d\u304cmetrics-serverg\u304c\u60f3\u5b9a\u3059\u308bCA\u3067\u306f\u306a\u3044\u305f\u3081)     <pre><code>kubectl patch deploy metrics-server -n kube-system --patch \"\nspec:\n  template:\n    spec:\n      containers:\n      - args:\n        - --cert-dir=/tmp\n        - --secure-port=4443\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        - --kubelet-insecure-tls\n        name: metrics-server\n\"\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>\u8d77\u52d5\u3057\u305f\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>I0217 14:54:41.737667       1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)\nI0217 14:54:42.981853       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController\nI0217 14:54:42.981913       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController\nI0217 14:54:42.981901       1 configmap_cafile_content.go:201] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::client-ca-file\"\nI0217 14:54:42.981970       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\nI0217 14:54:42.981993       1 configmap_cafile_content.go:201] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\"\nI0217 14:54:42.982036       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\nI0217 14:54:42.985611       1 secure_serving.go:266] Serving securely on [::]:4443\nI0217 14:54:42.985841       1 tlsconfig.go:240] \"Starting DynamicServingCertificateController\"\nW0217 14:54:42.986181       1 shared_informer.go:372] The sharedIndexInformer has started, run more than once is not allowed\nI0217 14:54:42.987234       1 dynamic_serving_content.go:131] \"Starting controller\" name=\"serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key\"\nI0217 14:54:43.082779       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\nI0217 14:54:43.082892       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController\nI0217 14:54:43.082896       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\n</code></pre></p> </li> </ol>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_4","title":"\u52d5\u4f5c\u78ba\u8a8d","text":""},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#api-path","title":"API Path","text":"<ul> <li>pods     <pre><code>$ kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/pods\" | jq '.items[] | select(.metadata.name == \"coredns-675db8b7cc-hbzb2\")'\n{\n  \"metadata\": {\n    \"name\": \"coredns-675db8b7cc-hbzb2\",\n    \"namespace\": \"kube-system\",\n    \"creationTimestamp\": \"2022-02-17T16:00:21Z\",\n    \"labels\": {\n      \"k8s-app\": \"kube-dns\",\n      \"pod-template-hash\": \"675db8b7cc\"\n    }\n  },\n  \"timestamp\": \"2022-02-17T16:00:03Z\",\n  \"window\": \"15.692s\",\n  \"containers\": [\n    {\n      \"name\": \"coredns\",\n      \"usage\": {\n        \"cpu\": \"7990556n\",\n        \"memory\": \"14064Ki\"\n      }\n    }\n  ]\n}\n</code></pre></li> <li>nodes     <pre><code>$ kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | jq .                                                                                              [22/47786]\n{\n  \"kind\": \"NodeMetricsList\",\n  \"apiVersion\": \"metrics.k8s.io/v1beta1\",\n  \"metadata\": {},\n  \"items\": [\n    {\n      \"metadata\": {\n        \"name\": \"k8s-master\",\n        \"creationTimestamp\": \"2022-02-17T14:58:58Z\",\n        \"labels\": {\n          \"beta.kubernetes.io/arch\": \"arm64\",\n          \"beta.kubernetes.io/os\": \"linux\",\n          \"kubernetes.io/arch\": \"arm64\",\n          \"kubernetes.io/hostname\": \"k8s-master\",\n          \"kubernetes.io/os\": \"linux\"\n        }\n      },\n      \"timestamp\": \"2022-02-17T14:58:49Z\",\n      \"window\": \"10.198s\",\n      \"usage\": {\n        \"cpu\": \"273214048n\",\n        \"memory\": \"1024976Ki\"\n      }\n    },\n    {\n      \"metadata\": {\n        \"name\": \"k8s-node1\",\n        \"creationTimestamp\": \"2022-02-17T14:58:58Z\",\n        \"labels\": {\n          \"beta.kubernetes.io/arch\": \"arm64\",\n          \"beta.kubernetes.io/os\": \"linux\",\n          \"kubernetes.io/arch\": \"arm64\",\n          \"kubernetes.io/hostname\": \"k8s-node1\",\n          \"kubernetes.io/os\": \"linux\"\n        }\n      },\n      \"timestamp\": \"2022-02-17T14:58:51Z\",\n      \"window\": \"10.094s\",\n      \"usage\": {\n        \"cpu\": \"141038629n\",\n        \"memory\": \"548580Ki\"\n      }\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#kubectl-top-pod","title":"<code>kubectl top pod</code>","text":"<pre><code>$ kubectl top nodes\nNAME         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nk8s-master   260m         7%     999Mi           80%\nk8s-node1    130m         3%     527Mi           42%\n</code></pre>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#kubectl-top-pod_1","title":"<code>kubectl top pod</code>","text":"<pre><code>$ kubectl top pods -A\nNAMESPACE     NAME                                 CPU(cores)   MEMORY(bytes)\nkube-system   coredns-675db8b7cc-hbzb2             7m           13Mi\nkube-system   etcd-k8s-master                      31m          108Mi\nkube-system   kube-apiserver-k8s-master            62m          220Mi\nkube-system   kube-controller-manager-k8s-master   24m          72Mi\nkube-system   kube-proxy-2kmcf                     1m           23Mi\nkube-system   kube-proxy-fxcgv                     1m           12Mi\nkube-system   kube-scheduler-k8s-master            3m           26Mi\nkube-system   metrics-server-8bb87844c-v67lj       12m          15Mi\n</code></pre>"},{"location":"monitoring/prometheus/install_cadvisor/","title":"Install cadvisor","text":""},{"location":"monitoring/prometheus/install_cadvisor/#about-cadvisor","title":"About cadvisor","text":"<p>https://github.com/google/cadvisor</p> <p>cadvisor(Container Advisor) \u306f\u5b9f\u884c\u4e2d\u306e\u30b3\u30f3\u30c6\u30ca\u306e\u30ea\u30bd\u30fc\u30b9\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53ce\u96c6\u3059\u308b\u305f\u3081\u306e\u30c4\u30fc\u30eb\u3067\u3059\u3002 kube-state-metrics \u306fKubernetes Object\u306b\u5bfe\u3059\u308b\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u53ce\u96c6\u3092\u884c\u3046\u3082\u306e\u306a\u306e\u3067\u3001<code>Pod</code> \u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306f\u53ce\u96c6\u3055\u308c\u307e\u3059\u304c\u30b3\u30f3\u30c6\u30ca\u3054\u3068\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306f\u53ce\u96c6\u3055\u308c\u307e\u305b\u3093\u3002</p> <p><code>cadvisor</code> \u3067\u53ce\u96c6\u3055\u308c\u308b\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306b\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md</p>"},{"location":"monitoring/prometheus/install_cadvisor/#install","title":"Install","text":"<ul> <li> <p>install kustomize    <pre><code>curl -s -o /tmp/install_kustomize.sh \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\"\n\n# Raspberry Pi 4 is aarch64 (ARM 64-bit architecture)\n# refs https://github.com/kubernetes-sigs/kustomize/issues/4696\nsed -i -e 's/arm64/aarch64/g' /tmp/install_kustomize.sh\n\nbash -x ./tmp/install_kustomize.sh\nsudo mv ./kustomize /usr/local/bin/\n</code></pre></p> </li> <li> <p>install cadvisor</p> <ul> <li>https://github.com/google/cadvisor/tree/master/deploy/kubernetes#cadvisor-kubernetes-daemonset <pre><code># https://github.com/google/cadvisor/releases\nVERSION=v0.46.0\n\ngit clone https://github.com/google/cadvisor.git ~/work/cadvisor\ncd deploy/kubernetes/base &amp;&amp; kustomize edit set image gcr.io/cadvisor/cadvisor:${VERSION} &amp;&amp; cd ../../..\nkubectl kustomize deploy/kubernetes/base | kubectl apply -f -\n</code></pre></li> </ul> </li> </ul>"},{"location":"monitoring/prometheus/install_cadvisor/#dashboard","title":"Dashboard","text":"<ul> <li>https://grafana.com/grafana/dashboards/14282-cadvisor-exporter/</li> <li>Install Grafana &gt; Install Node Exporter Full Dashboards \u53c2\u7167</li> </ul>"},{"location":"monitoring/prometheus/install_grafana/","title":"Install Grafana","text":""},{"location":"monitoring/prometheus/install_grafana/#install-grafana","title":"Install Grafana","text":"<ul> <li>https://github.com/grafana/helm-charts/tree/main/charts/grafana</li> </ul>"},{"location":"monitoring/prometheus/install_grafana/#install","title":"Install","text":"<ol> <li> <p>values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63</p> <ul> <li> <p>Ephemeral Storage\u306e <code>storageClass</code> \u3092 OpenEBS\u306ejiva storage\u306e\u3082\u306e\u3068\u3059\u308b( e.g. <code>openebs-jiva-csi-default</code>)</p> <p>Info</p> <ul> <li>Installing OpenEBS \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0</li> </ul> </li> <li> <p>Grafana WebUI\u3078\u306e\u30a2\u30af\u30bb\u30b9\u306b<code>MetalLB</code>\u306eexternal IP\u30a2\u30c9\u30ec\u30b9\u3092\u4f7f\u7528\u3059\u308b</p> <ul> <li><code>service.type: LoadBalancer</code></li> <li><code>annotations</code> \u306b <code>metallb.universe.tf/address-pool: ip-pool</code></li> </ul> </li> </ul> <pre><code>mkdir -p ~/work/prometheus/grafana\ncurl -so ~/work/prometheus/grafana/grafana.yaml https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml\nvim ~/work/prometheus/grafana/grafana.yaml\n</code></pre> <p>grafana.yaml \u4fee\u6b63\u5f8c\u306ediff</p> <pre><code>$ diff -u &lt;(curl -s https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml) &lt;(cat ~/work/prometheus/grafana/grafana.yaml)\n--- /dev/fd/63  2022-10-30 15:05:32.554153834 +0000\n+++ /dev/fd/62  2022-10-30 15:05:32.566153656 +0000\n@@ -157,12 +157,13 @@\n ##\n service:\n   enabled: true\n-  type: ClusterIP\n+  type: LoadBalancer\n   port: 80\n   targetPort: 3000\n     # targetPort: 4181 To be used with a proxy extraContainer\n   ## Service annotations. Can be templated.\n-  annotations: {}\n+  annotations:\n+    metallb.universe.tf/address-pool: ip-pool\n   labels: {}\n   portName: service\n   # Adds the appProtocol field to the service. This allows to work with istio protocol selection. Ex: \"http\" or \"tcp\"\n@@ -297,10 +297,10 @@\n persistence:\n   type: pvc\n   enabled: false\n-  # storageClassName: default\n+  storageClassName: openebs-jiva-csi-default\n   accessModes:\n     - ReadWriteOnce\n-  size: 10Gi\n+  size: 5Gi\n   # annotations: {}\n   finalizers:\n     - kubernetes.io/pvc-protection\n@@ -507,15 +507,14 @@\n ## Configure grafana datasources\n ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n ##\n-datasources: {}\n-#  datasources.yaml:\n-#    apiVersion: 1\n-#    datasources:\n-#    - name: Prometheus\n-#      type: prometheus\n-#      url: http://prometheus-prometheus-server\n-#      access: proxy\n-#      isDefault: true\n+datasources:\n+  datasources.yaml:\n+    apiVersion: 1\n+    datasources:\n+    - name: Prometheus\n+      type: prometheus\n+      url: http://prometheus-server.monitoring.svc.cluster.local\n+      isDefault: true\n #    - name: CloudWatch\n #      type: cloudwatch\n #      access: proxy\n</code></pre> </li> <li> <p>install</p> <pre><code>helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml\n</code></pre> <p>\u5b9f\u884c\u30ed\u30b0</p> <pre><code>$ helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml\nRelease \"grafana\" does not exist. Installing it now.\nW1030 15:08:59.041312  221007 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\nW1030 15:08:59.265293  221007 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\nNAME: grafana\nLAST DEPLOYED: Sun Oct 30 15:08:54 2022\nNAMESPACE: monitoring\nSTATUS: deployed\nREVISION: 1\nNOTES:\n1. Get your 'admin' user password by running:\n\n   kubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\n2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:\n\n   grafana.monitoring.svc.cluster.local\n\n   Get the Grafana URL to visit by running these commands in the same shell:\nNOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        You can watch the status of by running 'kubectl get svc --namespace monitoring -w grafana'\n     export SERVICE_IP=$(kubectl get svc --namespace monitoring grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n     http://$SERVICE_IP:80\n\n3. Login with the password from step 1 and the username: admin\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Grafana pod is terminated.                            #####\n#################################################################################\n</code></pre> </li> <li> <p>grafana Service\u306eMetalLB\u3067\u6255\u3044\u51fa\u3055\u308c\u305fExternal IP\u30a2\u30c9\u30ec\u30b9\u3092\u78ba\u8a8d\u3059\u308b     <pre><code>kubectl get service -n monitoring grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre></p> </li> <li> <p>\u30d6\u30e9\u30a6\u30b6\u304b\u3089\u30a2\u30af\u30bb\u30b9</p> <ol> <li> <p>\u30ed\u30b0\u30a4\u30f3\u753b\u9762</p> username <code>admin</code> <code>password</code> grafana\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6642\u306e\u8868\u793a\u3092\u53c2\u8003\u306b\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067<code>admin</code> \u306e <code>password</code> \u3092\u53d6\u5f97<code>kubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo</code> <p> </p> </li> <li> <p>\u30ed\u30b0\u30a4\u30f3\u6210\u529f</p> <p></p> </li> </ol> </li> </ol>"},{"location":"monitoring/prometheus/install_grafana/#install-node-exporter-full-dashboards","title":"Install <code>Node Exporter Full</code> Dashboards","text":""},{"location":"monitoring/prometheus/install_grafana/#grafana","title":"grafana\u3068\u4e00\u7dd2\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408","text":"<p>Info</p> <ul> <li>grafana helm chart\u3067\u306fgrafana dashboard import\u8a2d\u5b9a\u304c\u53ef\u80fd\u3067\u3059\u3002   <code>.Values.dashboards</code> \u306b\u8a2d\u5b9a\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001configmap.yaml \u306b\u3042\u308b <code>download_dashboards.sh</code> \u3067\u6307\u5b9a\u3057\u305fdashboard import\u8a2d\u5b9a\u304c\u5165\u308a\u307e\u3059\u3002\u305d\u3057\u3066 Pod\u306einitContainers \u3067 <code>download-dashboards.sh</code> \u3092\u5b9f\u884c\u3057\u3066\u3044\u307e\u3059\u3002<ul> <li>https://github.com/grafana/helm-charts/blob/grafana-6.43.5/charts/grafana/README.md</li> <li>https://github.com/grafana/helm-charts/blob/grafana-6.43.5/charts/grafana/values.yaml#L629-L658</li> </ul> </li> </ul> <ol> <li> <p>values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63     <pre><code>vim ~/work/prometheus/grafana/grafana.yaml\n</code></pre></p> <p>grafana.yaml \u4fee\u6b63\u5f8c\u306ediff(<code>cadvisor-exporter</code> Dashboard\u3082\u5165\u308c\u3066\u3044\u308b\u4f8b\u3067\u3059)</p> <pre><code>$ diff -u &lt;(curl -s https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml) &lt;(cat ~/work/prometheus/grafana/grafana.yaml)\n\n~ snip ~\n\n@@ -632,7 +632,17 @@\n ##\n ## dashboards per provider, use provider name as key.\n ##\n-dashboards: {}\n+dashboards:\n+  default:\n+    node-exporter-full:\n+      # https://grafana.com/grafana/dashboards/1860-node-exporter-full/\n+      gnetId: 1860\n+      datasource: Prometheus\n+    cadvisor-exporter:\n+      # https://grafana.com/grafana/dashboards/14282-cadvisor-exporter/\n+      gnetId: 14282\n+      datasource: Prometheus\n+\n   # default:\n   #   some-dashboard:\n   #     json: |\n</code></pre> </li> <li> <p>install     <pre><code>helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml\n</code></pre></p> </li> <li> <p><code>grafana\u30b3\u30f3\u30c6\u30ca:/var/lib/grafana/dashboards/default/</code> \u306b Dashboard\u5b9a\u7fa9\u3067\u3042\u308bjson\u30d5\u30a1\u30a4\u30eb\u304c\u914d\u7f6e\u6e08\u307f\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>$ kubectl exec -it -c grafana -n monitoring grafana-5f4c7d46db-xtktb -- ls -l /var/lib/grafana/dashboards/default/\ntotal 232\n-rw-r--r--    1 grafana  472          18484 Nov  8 16:03 cadvisor-exporter.json\n-rw-r--r--    1 grafana  472         215154 Nov  8 16:03 node-exporter-full.json\n</code></pre></p> </li> </ol>"},{"location":"monitoring/prometheus/install_grafana/#_1","title":"\u624b\u52d5\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408","text":"<ol> <li> <p>Dashboards\u30da\u30fc\u30b8\u306b\u30a2\u30af\u30bb\u30b9</p> <p></p> </li> <li> <p><code>New</code> -&gt; <code>New Dashboard</code> -&gt; <code>Import</code> \u3092\u9078\u629e</p> <p></p> </li> <li> <p><code>Node Exporter Full</code> \u306e Dashboard ID\u3092\u5165\u529b</p> <ul> <li>https://grafana.com/grafana/dashboards/1860-node-exporter-full/</li> </ul> <p></p> </li> <li> <p><code>Import</code> \u3092\u5b9f\u884c\u3059\u308b</p> <p></p> </li> <li> <p><code>Node Exporter Full</code> \u304c\u8868\u793a\u3055\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> <p></p> </li> </ol>"},{"location":"monitoring/prometheus/install_prometheus/","title":"Install Prometheus","text":""},{"location":"monitoring/prometheus/install_prometheus/#prometheus-installing","title":"Prometheus Installing","text":"<ul> <li>https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus</li> </ul>"},{"location":"monitoring/prometheus/install_prometheus/#install","title":"Install","text":"<ol> <li> <p>values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63</p> <ul> <li> <p>Ephemeral Storage\u306e <code>storageClass</code> \u3092 OpenEBS\u306ejiva storage\u306e\u3082\u306e\u3068\u3059\u308b( e.g. <code>openebs-jiva-csi-default</code>)</p> <p>Info</p> <ul> <li>Installing OpenEBS \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0</li> </ul> </li> <li> <p><code>Prometheus</code> \u3084 <code>Alertmanager</code> \u306e WebUI\u3092\u5229\u7528\u3057\u305f\u3044\u5834\u5408</p> <ul> <li>NodeIp:NodePort \u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u5834\u5408<ul> <li><code>service.type: NodePort</code></li> </ul> </li> <li> <p><code>MetalLB</code> \u3067\u6255\u3044\u51fa\u3057\u305fexternal ip\u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u5834\u5408</p> <ul> <li><code>service.type: LoadBalancer</code></li> <li><code>annotations</code> \u306b <code>metallb.universe.tf/address-pool: ip-pool</code></li> </ul> <p>Info</p> <ul> <li>MetalLB \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0</li> </ul> </li> </ul> </li> </ul> <pre><code>mkdir -p ~/work/prometheus\ncurl -so ~/work/prometheus/prometheus.yaml https://raw.githubusercontent.com/prometheus-community/helm-charts/main/charts/prometheus/values.yaml\nvim ~/work/prometheus/prometheus.yaml\n</code></pre> <p>prometheus.yaml \u4fee\u6b63\u5f8c\u306ediff</p> <pre><code>$ diff -u &lt;(curl -s https://raw.githubusercontent.com/prometheus-community/helm-charts/main/charts/prometheus/values.yaml) &lt;(cat ~/work/prometheus/prometheus.yaml)\n--- /dev/fd/63  2022-10-30 14:07:09.532561438 +0000\n+++ /dev/fd/62  2022-10-30 14:07:09.540561310 +0000\n@@ -231,7 +231,7 @@\n     ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n     ##   GKE, AWS &amp; OpenStack)\n     ##\n-    # storageClass: \"-\"\n+    storageClass: \"openebs-jiva-csi-default\"\n\n     ## alertmanager data Persistent Volume Binding Mode\n     ## If defined, volumeBindingMode: &lt;volumeBindingMode&gt;\n@@ -947,7 +947,7 @@\n     ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n     ##   GKE, AWS &amp; OpenStack)\n     ##\n-    # storageClass: \"-\"\n+    storageClass: \"openebs-jiva-csi-default\"\n\n     ## Prometheus server data Persistent Volume Binding Mode\n     ## If defined, volumeBindingMode: &lt;volumeBindingMode&gt;\n@@ -1403,7 +1403,7 @@\n     ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n     ##   GKE, AWS &amp; OpenStack)\n     ##\n-    # storageClass: \"-\"\n+    storageClass: \"openebs-jiva-csi-default\"\n\n     ## pushgateway data Persistent Volume Binding Mode\n     ## If defined, volumeBindingMode: &lt;volumeBindingMode&gt;\n</code></pre> </li> <li> <p>install</p> <pre><code>helm upgrade -i prometheus -n monitoring --create-namespace prometheus-community/prometheus -f ~/work/prometheus/prometheus.yaml\n</code></pre> </li> <li> <p>\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u8a8d\u3059\u308b     Service</p> <pre><code>$ kubectl get services -n monitoring\nNAME                            TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)        AGE\nprometheus-alertmanager         ClusterIP      10.32.0.114   &lt;none&gt;          80/TCP         33h\nprometheus-kube-state-metrics   ClusterIP      10.32.0.246   &lt;none&gt;          8080/TCP       33h\nprometheus-node-exporter        ClusterIP      10.32.0.31    &lt;none&gt;          9100/TCP       33h\nprometheus-pushgateway          ClusterIP      10.32.0.138   &lt;none&gt;          9091/TCP       33h\nprometheus-server               ClusterIP      10.32.0.75    &lt;none&gt;          80/TCP         33h\n</code></pre> <p>Deployment <pre><code>$ kubectl get deployments -n monitoring\nNAME                            READY   UP-TO-DATE   AVAILABLE   AGE\nprometheus-alertmanager         1/1     1            1           33h\nprometheus-kube-state-metrics   1/1     1            1           33h\nprometheus-pushgateway          1/1     1            1           33h\nprometheus-server               1/1     1            1           33h\n</code></pre> </p> <p>DaemonSet <pre><code>$ kubectl get DaemonSet -n monitoring -l app=prometheus\nNAME                       DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\nprometheus-node-exporter   2         2         2       2            2           &lt;none&gt;          35h\n</code></pre> </p> <p>PersistentVolumeClaim <pre><code>$ kubectl get PersistentVolumeClaim -n monitoring\nNAME                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS               AGE\nprometheus-alertmanager   Bound    pvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd   2Gi        RWO            openebs-jiva-csi-default   33h\nprometheus-server         Bound    pvc-836ef65a-da18-4453-96a6-7b909d0c668b   8Gi        RWO            openebs-jiva-csi-default   33h\n</code></pre> </p> <p>PersistentVolume <pre><code>$ kubectl get PersistentVolume -n monitoring pvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd pvc-836ef65a-da18-4453-96a6-7b909d0c668b\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                STORAGECLASS               REASON   AGE\npvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd   2Gi        RWO            Delete           Bound    monitoring/prometheus-alertmanager   openebs-jiva-csi-default            33h\npvc-836ef65a-da18-4453-96a6-7b909d0c668b   8Gi        RWO            Delete           Bound    monitoring/prometheus-server         openebs-jiva-csi-default            33h\n</code></pre> </p> <p>ServiceAccount <pre><code>$ kubectl get ServiceAccount -n monitoring -l app=prometheus\nNAME                                   SECRETS   AGE\ndefault                                1         9d\nprometheus-alertmanager                1         35h\nprometheus-kube-prometheus-admission   1         4d23h\nprometheus-kube-state-metrics          1         35h\nprometheus-node-exporter               1         35h\nprometheus-pushgateway                 1         35h\nprometheus-server                      1         35h\n</code></pre> </p> <p>ClusterRole <pre><code>$ kubectl get ClusterRole -n monitoring -l app=prometheus\nNAME                      CREATED AT\nprometheus-alertmanager   2022-10-30T03:25:38Z\nprometheus-pushgateway    2022-10-30T03:25:38Z\nprometheus-server         2022-10-30T03:25:38Z\n</code></pre> </p> <p>ClusterRoleBinding <pre><code>$ kubectl get ClusterRoleBinding -n monitoring -l app=prometheus\nNAME                      ROLE                                  AGE\nprometheus-alertmanager   ClusterRole/prometheus-alertmanager   35h\nprometheus-pushgateway    ClusterRole/prometheus-pushgateway    35h\nprometheus-server         ClusterRole/prometheus-server         35h\n</code></pre> </p> <p>ConfigMap <pre><code>$ kubectl get ConfigMap -n monitoring -l app=prometheus\nNAME                      DATA   AGE\nprometheus-alertmanager   2      35h\nprometheus-server         6      35h\n</code></pre> </p> <p>Info</p> <ul> <li>ConfigMap\u306b\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b <code>prometheus.yml</code> \u306e\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u305f\u3044\u5834\u5408\u306f\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067\u78ba\u8a8d     <pre><code>kubectl get ConfigMap -n monitoring prometheus-server -o jsonpath=\"{.data.prometheus\\.yml}\"\n</code></pre></li> </ul> </li> <li> <p>Confirm FQDN of prometheus-server service A Record</p> <ul> <li>Grafana\u3092helm install\u3059\u308b\u969b\u306b <code>datasources.datasources[0].url</code> \u306b\u8a2d\u5b9a\u3059\u308bFQDN\u3092\u78ba\u8a8d\u3059\u308b</li> <li> <p><code>prometheus-server.monitoring.svc.cluster.local</code></p> <pre><code>$ nslookup prometheus-server.monitoring.svc.cluster.local `kubectl get ep -n kube-system kube-dns -o jsonpath=\"{.subsets[0].addresses[0].ip}\"`\nServer:         10.200.2.235\nAddress:        10.200.2.235#53\n\nName:   prometheus-server.monitoring.svc.cluster.local\nAddress: 10.32.0.75\n</code></pre> </li> </ul> </li> </ol>"},{"location":"operations/backup_etcd/","title":"Backup etcd","text":""},{"location":"operations/backup_etcd/#etcdctl","title":"<code>etcdctl</code> \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<pre><code>sudo apt install etcd-client\n</code></pre>"},{"location":"operations/backup_etcd/#ercd","title":"ercd \u758e\u901a\u78ba\u8a8d","text":"<ul> <li>https://github.com/etcd-io/etcd/tree/main/etcdctl#endpoint-health <pre><code>ETCD_ENDPOINT=https://192.168.10.50:2379\nETCD_CACERT=/etc/etcd/ca.pem\nETCD_CERT=/etc/etcd/kubernetes.pem\nETCD_KEY=/etc/etcd/kubernetes-key.pem\n\nETCDCTL_API=3 etcdctl endpoint health \\\n  --endpoints=${ETCD_ENDPOINT} \\\n  --cacert=${ETCD_CACERT} \\\n  --cert=${ETCD_CERT} \\\n  --key=${ETCD_KEY}\n</code></pre><ul> <li><code>https://192.168.10.50:2379 is healthy: successfully committed proposal: took = 32.557549ms</code> \u306a\u3069\u306e\u6a19\u6e96\u51fa\u529b\u304c\u78ba\u8a8d\u3067\u304d\u308c\u3070\u758e\u901a\u3067\u304d\u3066\u3044\u308b</li> </ul> </li> </ul>"},{"location":"operations/backup_etcd/#etcd","title":"etcd \u30d0\u30c3\u30af\u30a2\u30c3\u30d7","text":"<ul> <li>https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-save-filename <pre><code>ETCD_ENDPOINT=https://192.168.10.50:2379\nETCD_CACERT=/etc/etcd/ca.pem\nETCD_CERT=/etc/etcd/kubernetes.pem\nETCD_KEY=/etc/etcd/kubernetes-key.pem\n\nETCDCTL_API=3 etcdctl snapshot save snapshot.db \\\n  --endpoints=${ETCD_ENDPOINT} \\\n  --cacert=${ETCD_CACERT} \\\n  --cert=${ETCD_CERT} \\\n  --key=${ETCD_KEY}\n</code></pre></li> </ul>"},{"location":"operations/backup_etcd/#_1","title":"\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30d5\u30a1\u30a4\u30eb\u306e\u60c5\u5831\u3092\u78ba\u8a8d","text":"<ul> <li>https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-status-filename <pre><code>ETCDCTL_API=3 etcdctl snapshot status snapshot.db\n</code></pre><ul> <li>\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u51fa\u529b\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059       <pre><code>+----------+----------+------------+------------+---------+\n|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE | VERSION |\n+----------+----------+------------+------------+---------+\n| c9a815d0 |     3356 |        488 |     1.1 MB |         |\n+----------+----------+------------+------------+---------+\n</code></pre></li> </ul> </li> </ul>"},{"location":"operations/backup_etcd/#etcd_1","title":"etcd \u30ea\u30b9\u30c8\u30a2","text":"<ul> <li>https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-restore-options-filename <pre><code>ETCD_ENDPOINT=https://192.168.10.50:2379\nETCD_CACERT=/etc/etcd/ca.pem\nETCD_CERT=/etc/etcd/kubernetes.pem\nETCD_KEY=/etc/etcd/kubernetes-key.pem\n\nETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\\n  --endpoints=${ETCD_ENDPOINT} \\\n  --cacert=${ETCD_CACERT} \\\n  --cert=${ETCD_CERT} \\\n  --key=${ETCD_KEY}\n</code></pre></li> </ul>"},{"location":"pod_security/","title":"Index","text":""},{"location":"pod_security/#pod","title":"Pod","text":"<p>https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</p>"},{"location":"secrets/about_secrets/","title":"About Secrets","text":""},{"location":"secrets/about_secrets/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://kubernetes.io/ja/docs/concepts/configuration/secret/</li> <li>https://kubernetes.io/ja/docs/tasks/configmap-secret/managing-secret-using-kubectl/</li> </ul>"},{"location":"secrets/about_secrets/#overview","title":"Overview","text":"<ul> <li>API Key\u3001Token\u3001SSH Key\u306a\u3069\u6a5f\u5bc6\u6027\u306e\u9ad8\u3044\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u305f\u3081\u306e\u30ea\u30bd\u30fc\u30b9</li> <li>\u53c2\u7167\u65b9\u6cd5</li> <li>Volume\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u30de\u30a6\u30f3\u30c8</li> <li>\u74b0\u5883\u5909\u6570</li> <li>imagePullSecrets</li> <li>\u5024\u306e\u683c\u7d0d\u30d5\u30a9\u30fc\u30de\u30c3\u30c8</li> <li><code>date</code>: base64\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3055\u308c\u305f\u6587\u5b57\u5217</li> <li><code>stringData</code>: \u5e73\u6587\u306a\u3069\u305d\u306e\u307e\u307e\u683c\u7d0d\u3057\u305f\u3044\u5834\u5408</li> </ul>"},{"location":"secrets/about_secrets/#kind-of-secret","title":"Kind of secret","text":""},{"location":"secrets/about_secrets/#builtin-type","title":"Builtin Type","text":"<ul> <li>Opaque</li> <li>kubernetes.io/service-account-token</li> <li>kubernetes.io/dockercfg</li> <li>kubernetes.io/dockerconfigjson</li> <li>kubernetes.io/basic-auth</li> <li>kubernetes.io/ssh-auth</li> <li>kubernetes.io/tls</li> <li>bootstrap.kubernetes.io/token</li> </ul>"},{"location":"setup/01_setup_RaspberryPi/","title":"01. Raspberry Pi \u69cb\u6210","text":""},{"location":"setup/01_setup_RaspberryPi/#raspberry-pi","title":"Raspberry Pi \u69cb\u6210","text":""},{"location":"setup/01_setup_RaspberryPi/#_1","title":"\u8cfc\u5165\u3057\u305f\u3082\u306e","text":"<ul> <li>Raspberry Pi 4 Model B/2GB x3</li> <li>GeeekPi Raspberry Pi4 \u30af\u30e9\u30b9\u30bf\u30fc\u30b1\u30fc\u30b9(\u51b7\u5374\u30d5\u30a1\u30f3\u4ed8\u304d) x1</li> <li>Anker PowerPort I PD - 1 PD &amp; 4 PowerIQ x1</li> <li>Amazon\u30d9\u30fc\u30b7\u30c3\u30af HDMI\u30b1\u30fc\u30d6\u30eb 0.9m (\u30bf\u30a4\u30d7A\u30aa\u30b9 - \u30de\u30a4\u30af\u30ed\u30bf\u30a4\u30d7D\u30aa\u30b9) x1</li> <li>Samsung EVO Plus 32GB microSDHC x3</li> <li>Anker USB Type C \u30b1\u30fc\u30d6\u30eb PowerLine USB-C &amp; USB-A 3.0 \u30b1\u30fc\u30d6\u30eb x3</li> </ul>"},{"location":"setup/02_setup_ubuntu20-04-LTS/","title":"02. OS Install","text":""},{"location":"setup/02_setup_ubuntu20-04-LTS/#os-setup","title":"OS Setup","text":""},{"location":"setup/02_setup_ubuntu20-04-LTS/#ubuntuserver20042lts-setup","title":"UbuntuServer20.04.2LTS Setup","text":"<ol> <li>GeeekPi\u30b1\u30fc\u30b9\u306e\u7d44\u307f\u7acb\u3066 &amp; \u7d50\u7dda &amp; \u8d77\u52d5\u78ba\u8a8d</li> <li>Raspberry Pi Imager \u3067SD\u30ab\u30fc\u30c9\u306bUbuntuServer20.04.2LTS(64bit) \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb</li> <li>Raspberry Pi 4\u3078SD\u30ab\u30fc\u30c9\u3092\u633f\u5165\u3057\u8d77\u52d5</li> <li> <p>wifi\u8a2d\u5b9a    <pre><code>sudo vim /etc/netplan/50-cloud-init.yaml\n\n# config check\nsudo netplan --debug try\nsudo netplan --debug generate\n\n# \u9069\u7528\nsudo netplan --debug apply\n</code></pre></p> <p>/etc/netplan/50-cloud-init.yaml (master\u306e\u5834\u5408)</p> <pre><code>network:\n  ethernets:\n      eth0:\n          dhcp4: true\n          optional: true\n  version: 2\n  wifis:\n    wlan0:\n      optional: true\n      dhcp4: false\n      addresses:\n      - 192.168.3.50/24\n      gateway4: 192.168.3.1\n      nameservers:\n        addresses:\n        - 8.8.8.8\n        - 8.8.4.4\n        search: []\n      access-points:\n        \"&lt;SSID\u540d&gt;\":\n          password: \"&lt;\u30d1\u30b9\u30ef\u30fc\u30c9&gt;\"\n</code></pre> </li> <li> <p>package\u66f4\u65b0    <pre><code>sudo apt update\nsudo apt upgrade -y\n</code></pre></p> </li> <li>\u65e5\u672c\u8a9e\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u5909\u66f4\u3057\u518d\u8d77\u52d5    <pre><code>sudo dpkg-reconfigure keyboard-configuration\nsudo reboot\n</code></pre><ul> <li><code>Generic 105-key (Intl) PC</code> \u3092\u9078\u629e</li> <li><code>Japanese</code> \u3092\u9078\u629e</li> <li><code>Japanese</code> \u3092\u9078\u629e</li> <li><code>The default for the keyboard layout</code> \u3092\u9078\u629e</li> <li><code>No compose key</code> \u3092\u9078\u629e</li> </ul> </li> <li>LOCALE    <pre><code>sudo apt install -y language-pack-ja\nsudo update-locale LANG=ja_JP.UTF-8\n</code></pre></li> <li>hostname    <pre><code>#name=k8s-node1\n#name=k8s-node2\nname=k8s-master\necho ${name} | sudo tee /etc/hostname\nsudo sed -i -e 's/127.0.1.1.*/127.0.1.1\\t'$name'/' /etc/hosts\n</code></pre></li> <li><code>/etc/hosts</code></li> <li>(\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5909\u308f\u3063\u3066\u3082\u758e\u901a\u3057\u305f\u3044\u305f\u3081) k8s\u5185\u3067\u306f\u30db\u30b9\u30c8\u540d\u901a\u4fe1\u3059\u308b\u305f\u3081\u306bhost\u3092\u66f8\u304d\u8fbc\u3080<ul> <li><code>k8s-master</code></li> <li><code>k8s-node1</code></li> <li><code>k8s-node2</code></li> </ul> </li> </ol>"},{"location":"setup/03_common_settings/","title":"03. master/node\u3067\u5171\u901a\u624b\u9806","text":""},{"location":"setup/03_common_settings/#swap","title":"swap\u3092\u7121\u52b9\u306b\u3059\u308b","text":"<ol> <li> <p>swap\u304c\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d    <pre><code>$ free -h\n              total        used        free      shared  buff/cache   available\nMem:          1.8Gi        54Mi       1.5Gi       8.0Mi       244Mi       1.7Gi\nSwap:          99Mi          0B        99Mi\n</code></pre></p> </li> <li> <p>swap\u3092\u7121\u52b9\u306b\u8a2d\u5b9a\u3059\u308b    <pre><code>sudo swapoff --all\n\n# Ubuntu Desktop 22.04 LTS (for RPi4)\nsudo systemctl stop dphys-swapfile\nsudo systemctl disable dphys-swapfile\n</code></pre></p> </li> <li> <p>swap\u304c\u7121\u52b9\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b    <pre><code>$ free -h\n              total        used        free      shared  buff/cache   available\nMem:          1.8Gi        57Mi       1.5Gi       8.0Mi       251Mi       1.7Gi\nSwap:            0B          0B          0B\n\n$ systemctl status dphys-swapfile\n\u25cf dphys-swapfile.service - dphys-swapfile - set up, mount/unmount, and delete a swap file\n   Loaded: loaded (/lib/systemd/system/dphys-swapfile.service; disabled; vendor preset: enabled)\n   Active: inactive (dead)\n     Docs: man:dphys-swapfile(8)\n\n12\u6708 30 20:48:54 k8s-master1 systemd[1]: Starting dphys-swapfile - set up, mount/unmount, and delete a swap file...\n12\u6708 30 20:48:55 k8s-master1 dphys-swapfile[330]: want /var/swap=100MByte, checking existing: keeping it\n12\u6708 30 20:48:55 k8s-master1 systemd[1]: Started dphys-swapfile - set up, mount/unmount, and delete a swap file.\n12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopping dphys-swapfile - set up, mount/unmount, and delete a swap file...\n12\u6708 31 06:57:57 k8s-master1 systemd[1]: dphys-swapfile.service: Succeeded.\n12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopped dphys-swapfile - set up, mount/unmount, and delete a swap file.\n</code></pre></p> </li> </ol>"},{"location":"setup/03_common_settings/#cgroupfs-memory","title":"cgroupfs \u306ememory\u3092\u6709\u52b9\u306b\u3059\u308b","text":"<ol> <li> <p>kernel\u306eboot option\u306b <code>cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</code> \u3092\u8ffd\u8a18\u3059\u308b    <pre><code>sudo vim /boot/firmware/cmdline.txt\n</code></pre></p> <ul> <li> <p><code>cmdline.txt</code> \u306e\u6709\u52b9\u884c\u306e\u78ba\u8a8d</p> <pre><code>$ sudo sed -e 's/\\s/\\n/g' /boot/firmware/cmdline.txt\nconsole=serial0,115200\nconsole=tty1\nroot=PARTUUID=fb7271c3-02\nrootfstype=ext4\nelevator=deadline\nfsck.repair=yes\nrootwait\nquiet\nsplash\nplymouth.ignore-serial-consoles\ncgroup_enable=cpuset\ncgroup_memory=1\ncgroup_enable=memory\n\n$ sudo reboot\n\n$ cat /proc/cgroups\n#subsys_name    hierarchy       num_cgroups     enabled\ncpuset  9       1       1\ncpu     5       34      1\ncpuacct 5       34      1\nblkio   10      34      1\nmemory  8       80      1\ndevices 4       34      1\nfreezer 7       1       1\nnet_cls 2       1       1\nperf_event      6       1       1\nnet_prio        2       1       1\npids    3       39      1\n</code></pre> </li> </ul> </li> </ol>"},{"location":"setup/03_common_settings/#container-runtime","title":"Container RunTime\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ul> <li><code>containerd</code> \u3092\u63a1\u7528</li> <li>\u521d\u671f\u306f <code>cri-o</code> \u3092\u63a1\u7528\u3057\u3066\u3044\u305f\u304c\u4ee5\u4e0b\u7406\u7531\u3067<code>containerd</code>\u3078\u5909\u66f4<ul> <li>CNCF\u3067Graduated Project(cri-o\u306fincubating)</li> <li>AWS eks-optimized AMI\u3067\u306fcontainerd\u304c\u6a19\u6e96\u3068\u306a\u308a\u305d\u3046(eks 1.22 \u3067\u306fdocker\u304cdefault)</li> <li><code>buildkit</code> \u3068\u7d44\u307f\u5408\u308f\u305b\u3066image build\u304c\u53ef\u80fd (cri-o\u3067\u306e\u53ef\u5426\u306f\u672a\u78ba\u8a8d)<ul> <li>https://speakerdeck.com/ktock/dockerkaracontainerdhefalseyi-xing?slide=7</li> </ul> </li> </ul> </li> </ul>"},{"location":"setup/03_common_settings/#_1","title":"\u524d\u63d0\u4f5c\u696d","text":"<ul> <li> <p>https://kubernetes.io/docs/setup/production-environment/container-runtimes/</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\n# sysctl params required by setup, params persist across reboots\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n\n# Apply sysctl params without reboot\nsudo sysctl --system\n</code></pre> </li> </ul>"},{"location":"setup/03_common_settings/#containerd","title":"Containerd","text":"<p>https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd https://github.com/containerd/containerd/blob/main/docs/getting-started.md</p> <ol> <li>\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb<ul> <li>for apt package<ul> <li>refs https://docs.docker.com/engine/install/ubuntu/ <pre><code>sudo apt update\nsudo apt install -y ca-certificates curl gnupg lsb-release\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt update\nsudo apt install -y containerd.io\n</code></pre></li> </ul> </li> </ul> </li> <li><code>/etc/containerd/config.toml</code> <pre><code>sudo containerd config default | sudo tee /etc/containerd/config.toml\nsudo vim /etc/containerd/config.toml\n</code></pre><ul> <li>refs https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd<ul> <li>Configuring the systemd cgroup driver</li> <li>Overriding the sandbox (pause) image</li> </ul> </li> </ul> </li> <li>restart containerd     <pre><code>sudo systemctl restart containerd\n</code></pre></li> </ol>"},{"location":"setup/03_common_settings/#cri-o","title":"CRI-O","text":"<p>https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o</p> <ol> <li>kernel module\u306eload<ul> <li>overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module <code>overlay</code></li> <li>iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module <code>br_netfilter</code> <pre><code>\n</code></pre></li> </ul> </li> <li> <p>kernel parameter\u306eset</p> <ul> <li>iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a   <pre><code>\n</code></pre></li> </ul> </li> <li> <p>kernel module\u306eload</p> <ul> <li>overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module <code>overlay</code></li> <li>iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module <code>br_netfilter</code> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/crio.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n</code></pre></li> </ul> </li> <li> <p>kernel parameter\u306eset</p> <ul> <li>iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a   <pre><code>cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\n\n# https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n\nsudo sysctl --system\n</code></pre></li> </ul> </li> <li> <p>system\u8d77\u52d5\u6642\u306b kernel parameter \u3092\u518d\u8aad\u307f\u8fbc\u307f\u3055\u305b\u308b</p> <ul> <li>kube-proxy\u306b\u3066\u5fc5\u8981\u306akernel parameter\u8a2d\u5b9a(kubelet\u8a2d\u5b9a\u624b\u9806\u306b\u3066\u5f8c\u8ff0) \u304ciptables\u8d77\u52d5\u6642\u306ekernel module load\u3067\u4e0a\u66f8\u304d\u3055\u308c\u308b\u305f\u3081</li> <li>\u5229\u7528\u74b0\u5883\u304c <code>/etc/sysconfig/iptables-config</code> \u3092\u5229\u7528\u53ef\u80fd\u306a\u3089 <code>IPTABLES_MODULES_UNLOAD=\"no\"</code> \u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u672c\u624b\u9806\u306f\u4e0d\u8981\u3067\u3059     <pre><code>egrep  \"sysctl\\s+--system\" /etc/rc.local &gt; /dev/null || sudo bash -c \"echo \\\"sysctl --system\\\" &gt;&gt; /etc/rc.local\"\negrep  \"sysctl\\s+--system\" /etc/rc.local\n</code></pre></li> </ul> </li> <li> <p>CRI-O\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb</p> <ul> <li>https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.21/xUbuntu_20.04/arm64/ <pre><code>VERSION=1.21\nOS=xUbuntu_20.04\n\nsudo bash -c \"echo \\\"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /\\\" &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\"\nsudo bash -c \"echo \\\"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /\\\" &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list\"\n\ncurl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key add -\ncurl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key add -\n\nsudo apt update\nsudo apt install -y cri-o cri-o-runc\n\nsudo apt install -y conntrack\n</code></pre></li> </ul> </li> <li> <p>storage driver\u3092 <code>overlay2</code> \u3078\u5909\u66f4\u3059\u308b    <pre><code>sudo vim /etc/containers/storage.conf\nsudo vim /etc/crio/crio.conf\n</code></pre></p> <ul> <li><code>/etc/crio/crio.conf</code> \u3078graph driver\u8a2d\u5b9a\u3092\u5165\u308c\u308b<ul> <li>podman\u3084buildah\u3067build\u3057\u305flocal image\u3092\u53c2\u7167\u3059\u308b\u305f\u3081</li> <li><code>[crio]</code> \u30bb\u30af\u30b7\u30e7\u30f3\u306b\u5165\u308c\u308b    <pre><code>graphroot = \"/var/lib/containers/storage\"\n</code></pre> /etc/containers/storage.conf <pre><code>[storage]\ndriver = \"overlay2\"\nrunroot = \"/run/containers/storage\"\ngraphroot = \"/var/lib/containers/storage\"\n\n[storage.options]\nadditionalimagestores = [\n]\n\n[storage.options.overlay]\nmountopt = \"nodev\"\n\n[storage.options.thinpool]\n</code></pre> </li> </ul> </li> </ul> </li> </ol> /etc/crio/crio.conf <pre><code>[crio]\nstorage_driver = \"overlay2\"\ngraphroot = \"/var/lib/containers/storage\"\nlog_dir = \"/var/log/crio/pods\"\nversion_file = \"/var/run/crio/version\"\nversion_file_persist = \"/var/lib/crio/version\"\nclean_shutdown_file = \"/var/lib/crio/clean.shutdown\"\n\n[crio.api]\nlisten = \"/var/run/crio/crio.sock\"\nstream_address = \"127.0.0.1\"\nstream_port = \"0\"\nstream_enable_tls = false\nstream_idle_timeout = \"\"\nstream_tls_cert = \"\"\nstream_tls_key = \"\"\nstream_tls_ca = \"\"\ngrpc_max_send_msg_size = 16777216\ngrpc_max_recv_msg_size = 16777216\n\n[crio.runtime]\nno_pivot = false\ndecryption_keys_path = \"/etc/crio/keys/\"\nconmon = \"\"\nconmon_cgroup = \"system.slice\"\nconmon_env = [\n        \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n]\ndefault_env = [\n]\nseccomp_profile = \"\"\nseccomp_use_default_when_empty = false\napparmor_profile = \"crio-default\"\nirqbalance_config_file = \"/etc/sysconfig/irqbalance\"\ncgroup_manager = \"systemd\"\nseparate_pull_cgroup = \"\"\ndefault_capabilities = [\n        \"CHOWN\",\n        \"DAC_OVERRIDE\",\n        \"FSETID\",\n        \"FOWNER\",\n        \"SETGID\",\n        \"SETUID\",\n        \"SETPCAP\",\n        \"NET_BIND_SERVICE\",\n        \"KILL\",\n]\ndefault_sysctls = [\n]\nadditional_devices = [\n]\nhooks_dir = [\n        \"/usr/share/containers/oci/hooks.d\",\n]pids_limit = 1024\nlog_size_max = -1\nlog_to_journald = false\ncontainer_exits_dir = \"/var/run/crio/exits\"\ncontainer_attach_socket_dir = \"/var/run/crio\"\nbind_mount_prefix = \"\"\nread_only = false\nlog_level = \"info\"\nlog_filter = \"\"\nuid_mappings = \"\"\ngid_mappings = \"\"\nctr_stop_timeout = 30\ndrop_infra_ctr = false\ninfra_ctr_cpuset = \"\"\nnamespaces_dir = \"/var/run\"\npinns_path = \"\"\ndefault_runtime = \"runc\"\n\n[crio.runtime.runtimes.runc]\nruntime_path = \"\"\nruntime_type = \"oci\"\nruntime_root = \"/run/runc\"\nallowed_annotations = [\n        \"io.containers.trace-syscall\",\n]\n\n[crio.image]\ndefault_transport = \"docker://\"\nglobal_auth_file = \"\"\npause_image = \"k8s.gcr.io/pause:3.2\"\npause_image_auth_file = \"\"\npause_command = \"/pause\"\nsignature_policy = \"\"\nimage_volumes = \"mkdir\"\nbig_files_temporary_dir = \"\"\n\n[crio.network]\nnetwork_dir = \"/etc/cni/net.d/\"\nplugin_dirs = [\n        \"/opt/cni/bin/\",\n]\n[crio.metrics]\nenable_metrics = false\nmetrics_port = 9090\nmetrics_socket = \"\"\n</code></pre> <ol> <li>crio\u3092\u518d\u8d77\u52d5\u3059\u308b    <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart crio\n</code></pre></li> </ol>"},{"location":"setup/03_common_settings/#cli-tool","title":"CLI TOOL","text":"<ol> <li> <p>nerdctl</p> <ul> <li>containerd\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u516c\u958b\u3057\u3066\u3044\u308bdocker-cli\u4e92\u63db\u306eCLI</li> <li>https://github.com/containerd/nerdctl</li> <li> <p>https://speakerdeck.com/ktock/dockerkaracontainerdhefalseyi-xing?slide=22</p> <pre><code>NERDCTL_VERSION=`curl -s -L https://api.github.com/repos/containerd/nerdctl/releases/latest | jq -r .tag_name`\ncurl -L -s https://github.com/containerd/nerdctl/releases/download/${NERDCTL_VERSION}/nerdctl-`echo ${NERDCTL_VERSION} | sed -e 's/^v//'`-linux-arm64.tar.gz | sudo tar -zxC /usr/local/bin/\n\nls -l /usr/local/bin\n</code></pre> </li> </ul> </li> <li> <p>buildkit</p> <ul> <li>https://github.com/moby/buildkit</li> <li> <p><code>nerdctl build</code> \u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981</p> <pre><code>BUILDKIT_VERSION=`curl -s -L https://api.github.com/repos/moby/buildkit/releases/latest | jq -r .tag_name`\ncurl -L -s https://github.com/moby/buildkit/releases/download/${BUILDKIT_VERSION}/buildkit-${BUILDKIT_VERSION}.linux-arm64.tar.gz | sudo tar -zxC /tmp/\nsudo mv /tmp/bin/* /usr/local/bin/\nls -l /usr/local/bin\n\nsudo curl -sL https://raw.githubusercontent.com/moby/buildkit/${BUILDKIT_VERSION}/examples/systemd/system/buildkit.socket -o /etc/systemd/system/buildkit.socket\nsudo curl -sL https://raw.githubusercontent.com/moby/buildkit/${BUILDKIT_VERSION}/examples/systemd/system/buildkit.service -o /etc/systemd/system/buildkit.service\nsudo systemctl enable buildkit.socket buildkit.service\nsudo systemctl start buildkit.socket buildkit.service\n</code></pre> </li> </ul> </li> <li> <p>buildah </p> <ul> <li>cri-o \u5c0e\u5165\u6642\u671f\u306bimage build\u3067\u4f7f\u7528(containerd\u3067\u306f\u524d\u8ff0\u306enerdctl\u3078\u79fb\u884c\u6e08\u307f)</li> <li>https://github.com/containers/buildah/blob/master/install.md <pre><code>sudo apt-get -qq -y install buildah\n</code></pre></li> </ul> </li> <li> <p>cri-tools(crictl) \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb</p> <ul> <li>https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md <pre><code>VERSION=\"v1.22.0\"\nARCH=\"arm64\"\nDOWNLOAD_URL=\"https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-${VERSION}-linux-${ARCH}.tar.gz\"\ncurl -L ${DOWNLOAD_URL} | sudo tar -zxC /usr/local/bin\n\nls -l /usr/local/bin\n</code></pre></li> </ul> </li> <li> <p>podman \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(optional)</p> <ul> <li>https://podman.io/getting-started/installation <pre><code>sudo apt-get -y install podman\nsudo rm -f /etc/cni/net.d/87-podman-bridge.conflist\n</code></pre></li> </ul> </li> </ol>"},{"location":"setup/03_common_settings/#cni-plugin","title":"CNI Plugin\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ul> <li>https://github.com/containernetworking/plugins <pre><code>sudo mkdir -p /opt/cni/bin\nCNI_PLUGIN_VERSION=`curl -s -L https://api.github.com/repos/containernetworking/plugins/releases/latest | jq -r .tag_name`\nARCH=\"arm64\"\nDOWNLOAD_URL=\"https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGIN_VERSION}/cni-plugins-linux-${ARCH}-${CNI_PLUGIN_VERSION}.tgz\"\ncurl -L ${DOWNLOAD_URL} | sudo tar -zxC /opt/cni/bin\n\nls -l /opt/cni/bin/\n</code></pre></li> <li>cni config\u3092\u4f5c\u6210\u3059\u308b<ul> <li>https://www.cni.dev/plugins/current/main/bridge/ <pre><code>POD_CIDR=\"10.200.0.0/24\"\ncat &lt;&lt;EOF | sudo tee /etc/cni/net.d/10-bridge.conf\n{\n    \"cniVersion\": \"0.4.0\",\n    \"name\": \"bridge\",\n    \"type\": \"bridge\",\n    \"bridge\": \"cnio0\",\n    \"isGateway\": true,\n    \"ipMasq\": true,\n    \"ipam\": {\n        \"type\": \"host-local\",\n        \"ranges\": [\n          [{\"subnet\": \"${POD_CIDR}\"}]\n        ],\n        \"routes\": [{\"dst\": \"0.0.0.0/0\"}]\n    }\n}\nEOF\n\ncat &lt;&lt;EOF | sudo tee /etc/cni/net.d/20-loopback.conf\n{\n    \"cniVersion\": \"0.4.0\",\n    \"name\": \"lo\",\n    \"type\": \"loopback\"\n}\nEOF\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/03_common_settings/#kubectl","title":"kubectl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ul> <li>https://kubernetes.io/ja/docs/tasks/tools/install-kubectl/</li> </ul>"},{"location":"setup/04_creation_certificate/","title":"\u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210","text":""},{"location":"setup/04_creation_certificate/#_1","title":"\u624b\u9806","text":""},{"location":"setup/04_creation_certificate/#cfssl","title":"cfssl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ul> <li>\u8a3c\u660e\u66f8\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e cfssl \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b<ul> <li>https://qiita.com/iaoiui/items/fc2ea829498402d4a8e3</li> <li>https://coreos.com/os/docs/latest/generate-self-signed-certificates.html <pre><code>sudo apt install -y golang-cfssl\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#ca","title":"CA(\u8a8d\u8a3c\u5c40) \u4f5c\u6210","text":"<pre><code>cat &lt;&lt; EOF &gt; ca-config.json\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"8760h\"\n        },\n        \"profiles\": {\n            \"kubernetes\": {\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ],\n                \"expiry\": \"8760h\"\n            }\n        }\n    }\n}\nEOF\n\ncat &lt;&lt; EOF &gt; ca-csr.json\n{\n    \"CN\": \"Kubernetes\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"Kubernetes\",\n            \"OU\": \"CA\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>ca-key.pem</code></li> <li><code>ca.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#_2","title":"\u8a3c\u660e\u66f8\u306e\u4f5c\u6210","text":""},{"location":"setup/04_creation_certificate/#_3","title":"\u7ba1\u7406\u8005\u30e6\u30fc\u30b6 \u8a3c\u660e\u66f8","text":"<pre><code>cat &lt;&lt; EOF &gt; admin-csr.json\n{\n    \"CN\": \"admin\",\n    \"hosts\": [],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"system:masters\",\n            \"OU\": \"Kubernetes The HardWay\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  admin-csr.json | cfssljson -bare admin\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>admin-key.pem</code></li> <li><code>admin.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kubelet","title":"kubelet\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8","text":"<ul> <li><code>EXTERNAL_IP</code><ul> <li>master\u30b5\u30fc\u30d0\u306ehostname</li> <li>master\u304c\u8907\u6570\u30b5\u30fc\u30d0\u69cb\u6210\u306e\u5834\u5408\u306f\u4e0a\u4f4d\u306eLB IP</li> </ul> </li> </ul> <pre><code>for instance in k8s-master k8s-node1 k8s-node2; do\n\ncat &lt;&lt; EOF &gt; ${instance}-csr.json\n{\n   \"CN\": \"system:node:${instance}\",\n   \"key\": {\n       \"algo\": \"rsa\",\n       \"size\": 2048\n   },\n   \"names\": [\n       {\n           \"C\": \"JP\",\n           \"L\": \"Tokyo\",\n           \"O\": \"system:nodes\",\n           \"OU\": \"Kubernetes The HardWay\",\n           \"ST\": \"Sample\"\n       }\n   ]\n}\nEOF\n\nEXTERNAL_IP=k8s-master\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=${instance},${EXTERNAL_IP} \\\n  -profile=kubernetes \\\n  ${instance}-csr.json | cfssljson -bare ${instance}\n\ndone\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>k8s-node1-key.pem</code></li> <li><code>k8s-node1.pem</code></li> <li><code>k8s-node2-key.pem</code></li> <li><code>k8s-node2.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kube-proxy","title":"kube-proxy\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8","text":"<pre><code>cat &lt;&lt; EOF &gt; kube-proxy-csr.json\n{\n    \"CN\": \"system:kube-proxy\",\n    \"hosts\": [],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"system:node-proxier\",\n            \"OU\": \"Kubernetes The Hard Way\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  kube-proxy-csr.json | cfssljson -bare kube-proxy\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>kube-proxy-key.pem</code></li> <li><code>kube-proxy.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kube-controller-manage","title":"kube-controller-manage\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8","text":"<pre><code>cat &lt;&lt; EOF &gt; kube-controller-manager-csr.json\n{\n  \"CN\": \"system:kube-controller-manager\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"JP\",\n      \"L\": \"Tokyo\",\n      \"O\": \"system:kube-controller-manager\",\n      \"OU\": \"Kubernetes The Hard Way\",\n      \"ST\": \"Sample\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>kube-controller-manager-key.pem</code></li> <li><code>kube-controller-manager.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kube-scheduler","title":"kube-scheduler\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8","text":"<pre><code>cat &lt;&lt; EOF &gt; kube-scheduler-csr.json\n{\n  \"CN\": \"system:kube-scheduler\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"Portland\",\n      \"O\": \"system:kube-scheduler\",\n      \"OU\": \"Kubernetes The Hard Way\",\n      \"ST\": \"Sample\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  kube-scheduler-csr.json | cfssljson -bare kube-scheduler\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>kube-scheduler-key.pem</code></li> <li><code>kube-scheduler.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kube-apiserver","title":"kube-apiserver\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8","text":"<ul> <li><code>10.32.0.1</code><ul> <li>Cluster IP</li> </ul> </li> </ul> <pre><code>KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local\n\ncat &lt;&lt; EOF &gt; kubernetes-csr.json\n{\n    \"CN\": \"Kubernetes\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"Kubernetes\",\n            \"OU\": \"Kubernetes The Hard Way\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=10.32.0.1,k8s-master,k8s-node1,k8s-node2,127.0.0.1,${KUBERNETES_HOSTNAMES} \\\n  -profile=kubernetes \\\n  kubernetes-csr.json | cfssljson -bare kubernetes\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>kubernetes-key.pem</code></li> <li><code>kubernetes.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kube-apiserver-front-proxyfor-aggregation-layer","title":"kube-apiserver front-proxy(for aggregation layer)\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8","text":"<ol> <li> <p>CA(\u8a8d\u8a3c\u5c40)\u4f5c\u6210     <pre><code>cat &lt;&lt; EOF &gt; front-proxy-ca-config.json\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"8760h\"\n        },\n        \"profiles\": {\n            \"kubernetes\": {\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ],\n                \"expiry\": \"8760h\"\n            }\n        }\n    }\n}\nEOF\n\ncat &lt;&lt; EOF &gt; front-proxy-ca-csr.json\n{\n    \"CN\": \"Kubernetes\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"Kubernetes\",\n            \"OU\": \"CA\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca\n</code></pre></p> </li> <li> <p>front-proxy\u7528\u8a3c\u660e\u66f8\u306e\u4f5c\u6210     <pre><code>cat &lt;&lt; EOF &gt; front-proxy-csr.json\n{\n    \"CN\": \"front-proxy-ca\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"Kubernetes\",\n            \"OU\": \"Kubernetes The Hard Way\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=front-proxy-ca.pem \\\n  -ca-key=front-proxy-ca-key.pem \\\n  -config=front-proxy-ca-config.json \\\n  -profile=kubernetes \\\n  front-proxy-csr.json | cfssljson -bare front-proxy\n</code></pre></p> </li> <li> <p>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> <ul> <li><code>front-proxy-key.pem</code></li> <li><code>front-proxy.pem</code></li> </ul> </li> </ol>"},{"location":"setup/04_creation_certificate/#service-account","title":"service-account\u306e\u8a3c\u660e\u66f8","text":"<pre><code>cat &lt;&lt; EOF &gt; service-account-csr.json\n{\n  \"CN\": \"service-accounts\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"JP\",\n      \"L\": \"Tokyo\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes The Hard Way\",\n      \"ST\": \"Sample\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  service-account-csr.json | cfssljson -bare service-account\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>service-account-key.pem</code></li> <li><code>service-account.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#masternode","title":"\u8a3c\u660e\u66f8\u3092master/node\u3078\u30b3\u30d4\u30fc\u3059\u308b","text":"<ul> <li>master</li> <li>node</li> </ul>"},{"location":"setup/04_creation_certificate/#_4","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://kubernetes.io/ja/docs/setup/best-practices/certificates/</li> <li>https://kubernetes.io/ja/docs/concepts/cluster-administration/certificates/</li> <li>https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf</li> </ul>"},{"location":"setup/05_creating_config/","title":"\u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210","text":"<ul> <li>Controll Plane\u3068Node\u306e\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e <code>.kubeconfig</code> \u3092\u4f5c\u6210\u3059\u308b</li> </ul>"},{"location":"setup/05_creating_config/#_1","title":"\u624b\u9806","text":""},{"location":"setup/05_creating_config/#kubelet","title":"kubelet","text":"<pre><code>KUBERNETES_PUBLIC_ADDRESS=k8s-master\n\nfor instance in k8s-master k8s-node1 k8s-node2; do\n    kubectl config set-cluster kubernetes \\\n        --certificate-authority=ca.pem \\\n        --embed-certs=true \\\n        --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\\n        --kubeconfig=${instance}.kubeconfig\n\n    kubectl config set-credentials system:node:${instance} \\\n        --client-certificate=${instance}.pem \\\n        --client-key=${instance}-key.pem \\\n        --embed-certs=true \\\n        --kubeconfig=${instance}.kubeconfig\n\n    kubectl config set-context default \\\n        --cluster=kubernetes \\\n        --user=system:node:${instance} \\\n        --kubeconfig=${instance}.kubeconfig\n\n    kubectl config use-context default --kubeconfig=${instance}.kubeconfig\ndone\n</code></pre>"},{"location":"setup/05_creating_config/#kube-proxy","title":"kube-proxy","text":"<pre><code>KUBERNETES_PUBLIC_ADDRESS=k8s-master\n\nkubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\\n    --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config set-credentials system:kube-proxy \\\n    --client-certificate=kube-proxy.pem \\\n    --client-key=kube-proxy-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config set-context default \\\n    --cluster=kubernetes \\\n    --user=system:kube-proxy \\\n    --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig\n</code></pre>"},{"location":"setup/05_creating_config/#kube-controller-manager","title":"kube-controller-manager","text":"<pre><code>KUBE_API_SERVER_ADDRESS=k8s-master\n\nkubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-credentials system:kube-controller-manager \\\n    --client-certificate=kube-controller-manager.pem \\\n    --client-key=kube-controller-manager-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-context default \\\n    --cluster=kubernetes \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig\n</code></pre>"},{"location":"setup/05_creating_config/#kube-scheduler","title":"kube-scheduler","text":"<pre><code>KUBE_API_SERVER_ADDRESS=k8s-master\n\nkubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\\n    --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config set-credentials system:kube-scheduler \\\n    --client-certificate=kube-scheduler.pem \\\n    --client-key=kube-scheduler-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config set-context default \\\n    --cluster=kubernetes \\\n    --user=system:kube-scheduler \\\n    --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig\n</code></pre>"},{"location":"setup/05_creating_config/#admin","title":"admin","text":"<pre><code>KUBERNETES_PUBLIC_ADDRESS=k8s-master\n\nkubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\\n    --kubeconfig=admin.kubeconfig\n\nkubectl config set-credentials admin \\\n    --client-certificate=admin.pem \\\n    --client-key=admin-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=admin.kubeconfig\n\nkubectl config set-context default \\\n    --cluster=kubernetes \\\n    --user=admin \\\n    --kubeconfig=admin.kubeconfig\n\nkubectl config use-context default --kubeconfig=admin.kubeconfig\n\nsudo mkdir -p /var/lib/kubernetes/\nsudo cp admin.kubeconfig /var/lib/kubernetes/admin.kubeconfig\n</code></pre>"},{"location":"setup/05_creating_config/#_2","title":"\u53c2\u8003\u8cc7\u6599","text":"<ul> <li>https://github.com/kelseyhightower/kubernetes/blob/master/docs/05-kubernetes-configuration-files.md</li> <li>https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf</li> <li>https://h3poteto.hatenablog.com/entry/2020/08/20/180552</li> <li><code>kubectl config set-cluster</code><ul> <li>https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-cluster/</li> <li>https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-cluster-em-</li> </ul> </li> <li><code>kubectl config set-credentials</code><ul> <li>https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-credentials/</li> <li>https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-credentials-em-</li> </ul> </li> <li><code>kubectl config set-context</code><ul> <li>https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-context/</li> <li>https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-context-em-</li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/","title":"bootstrapping kubelet(master/worker \u5171\u901a)","text":"<ul> <li><code>kubelet</code> \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002</li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#worker-node","title":"worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206","text":"<ul> <li>Reserve Compute Resources for System Daemons<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/</li> <li> <p><code>Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9</code> = <code>Node resource - system-reserved - kube-reserved - eviction-threshold</code> \u3089\u3057\u3044</p> name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi </li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_1","title":"\u624b\u9806","text":""},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubelet","title":"<code>kubelet</code> \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9","text":"<pre><code>VERSION=\"v1.22.0\"\nARCH=\"arm64\"\n\nsudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet\nsudo chmod +x /usr/bin/kubelet\n</code></pre>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeconfig","title":"kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e","text":"<pre><code># host=\"k8s-node2\"\n# host=\"k8s-node1\"\nhost=\"k8s-master\"\n\nsudo install -o root -g root -m 755 -d /etc/kubelet.d\nsudo install -o root -g root -m 755 -d /var/lib/kubernetes\nsudo install -o root -g root -m 755 -d /var/lib/kubelet\nsudo cp ca.pem /var/lib/kubernetes/\nsudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/\nsudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig\n</code></pre>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#varlibkubeletkubelet-configyaml","title":"<code>/var/lib/kubelet/kubelet-config.yaml</code> \u3092\u4f5c\u6210\u3059\u308b","text":"<ul> <li><code>clusterDNS</code> \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b</li> <li><code>podCIDR</code> \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b   <pre><code># host=\"k8s-node2\"\n# host=\"k8s-node1\"\nhost=\"k8s-master\"\n\ncat &lt;&lt; EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml\n---\nkind: KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\n\n# https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/\nstaticPodPath: /etc/kubelet.d\n\n# kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f\n#   - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b\n#   - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981\nauthentication:\n  anonymous:\n    enabled: true\n  webhook:\n    enabled: false\n    cacheTTL: \"2m\"\n  x509:\n    clientCAFile: \"/var/lib/kubernetes/ca.pem\"\n\n# kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a\n#   - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow\n#   - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981\nauthorization:\n  mode: AlwaysAllow\n\nclusterDomain: \"cluster.local\"\nclusterDNS:\n  - \"10.32.0.10\"\npodCIDR: \"10.200.0.0/24\"\nruntimeRequestTimeout: \"15m\"\ntlsCertFile: \"/var/lib/kubelet/${host}.pem\"\ntlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\"\n\n# Reserve Compute Resources for System Daemons\n# https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/\n#\n# Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044\n#\n# system-reserved\n#   - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b\n#\n# kube-reserved\n#   - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b\nenforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"]\ncgroupsPerQOS: true\ncgroupDriver: systemd\ncgroupRoot: /\nsystemCgroups: /systemd/system.slice\nsystemReservedCgroup: /system.slice\nsystemReserved:\n  cpu: 256m\n  memory: 256Mi\nruntimeCgroups: /kube.slice/containerd.service\nkubeletCgroups: /kube.slice/kubelet.service\nkubeReservedCgroup: /kube.slice\nkubeReserved:\n  cpu: 1024m\n  memory: 1024Mi\nEOF\n</code></pre></li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#etcsystemdsystemkubeletservice","title":"<code>/etc/systemd/system/kubelet.service</code> \u3092\u914d\u7f6e","text":"<pre><code>cat &lt;&lt; 'EOF' | sudo tee /etc/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=containerd.service\nRequires=containerd.service\n\n[Service]\nRestart=on-failure\nRestartSec=5\n\nExecStartPre=/usr/bin/mkdir -p \\\n  /sys/fs/cgroup/kube.slice \\\n  /sys/fs/cgroup/system.slice \\\n  /sys/fs/cgroup/systemd/kube.slice \\\n  /sys/fs/cgroup/cpuset/kube.slice \\\n  /sys/fs/cgroup/cpuset/system.slice \\\n  /sys/fs/cgroup/pids/kube.slice \\\n  /sys/fs/cgroup/pids/system.slice \\\n  /sys/fs/cgroup/memory/kube.slice \\\n  /sys/fs/cgroup/memory/system.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/system.slice \\\n  /sys/fs/cgroup/hugetlb/system.slice \\\n  /sys/fs/cgroup/hugetlb/kube.slice\n\nExecStart=/usr/bin/kubelet \\\n  --config=/var/lib/kubelet/kubelet-config.yaml \\\n  --kubeconfig=/var/lib/kubelet/kubeconfig \\\n  --network-plugin=cni \\\n  --container-runtime=remote \\\n  --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\\n  --register-node=true \\\n  --v=2\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeletservice","title":"<code>kubelet.service</code> \u3092\u8d77\u52d5","text":"<pre><code>sudo systemctl enable kubelet.service\nsudo systemctl start kubelet.service\n</code></pre>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_2","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":""},{"location":"setup/06_master/01_bootstrapping_kubelet/#cgroup","title":"cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408","text":"<pre><code>kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist\n</code></pre> <ul> <li> <p><code>kubelet</code> \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( <code>--v 10</code> )</p> <pre><code>cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice]\n</code></pre> </li> <li> <p>\u5bfe\u5fdc     <code>kubelet.service</code> \u306e <code>ExecStartPre</code> \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b</p> <pre><code>ExecStartPre=/usr/bin/mkdir -p \\\n  /sys/fs/cgroup/systemd/kube.slice \\\n  /sys/fs/cgroup/cpuset/kube.slice \\\n  /sys/fs/cgroup/cpuset/system.slice \\\n  /sys/fs/cgroup/pids/kube.slice \\\n  /sys/fs/cgroup/pids/system.slice \\\n  /sys/fs/cgroup/memory/kube.slice \\\n  /sys/fs/cgroup/memory/system.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice\n</code></pre> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cgroupsystemreserved-memory-size","title":"cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f","text":"<ul> <li>\u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f    <pre><code>kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy\n</code></pre></li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeconfig-cn-node","title":"kubeconfig \u306e\u8a3c\u660e\u66f8\u306e <code>CN</code> \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b","text":"<pre><code>360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\"\n</code></pre> <ul> <li>kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b       <pre><code>sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject:\n</code></pre><ul> <li><code>k8s-master</code> \u304c\u6b63\u3057\u3044\u306e\u306b <code>CN = system:node:k8s-node1</code> \u3068\u306a\u3063\u3066\u3044\u305f    <pre><code>root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject:\n    Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#node-specpodcidr-cidr","title":"<code>Node</code> \u30ea\u30bd\u30fc\u30b9\u306e <code>spec.podCIDR</code> \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044","text":"<ul> <li> <p>\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044</p> <ul> <li>flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f...  <pre><code>kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'\n</code></pre></li> </ul> </li> <li> <p><code>kube-controller-manager</code> \u306e\u30ed\u30b0</p> <ul> <li><code>Set node k8s-node1 PodCIDR to [10.200.0.0/24]</code> \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8<ul> <li><code>kube-controller-manager</code> \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b <code>--allocate-node-cidrs=true</code> \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... <pre><code>actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist\nrange_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24]\nttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\"\ncontroller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}]\ncontroller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes\nnode_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\"\ncontroller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1\nnode_lifecycle_controller.go:1429] Initializing eviction metric for zone:\nnode_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp.\nevent.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\"\nnode_lifecycle_controller.go:1245] Controller detected that zone  is now in state Normal.\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#webhook-authentication","title":"Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044","text":"<pre><code>I0214 07:03:56.822586       1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\"\nF0214 07:03:56.822637       1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication\ngoroutine 1 [running]:\n</code></pre> <ul> <li>https://kubernetes.io/docs/reference/access-authn-authz/webhook/</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication</li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cni-plugin-etccninetd-cni-plugin","title":"CNI Plugin\u3092 <code>/etc/cni/net.d</code> \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044","text":"<pre><code>kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\ncni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d\nkubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\n</code></pre> <ul> <li>CNI Plugin\u3092 <code>/etc/cni/net.d</code> \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b<ul> <li>https://github.com/containernetworking/plugins/releases</li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubelet-cannot-determine-cpu-online-state","title":"Kubelet cannot determine CPU online state","text":"<pre><code>sysinfo.go:203] Nodes topology is not available, providing CPU topology\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory\ngce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping\nmachine.go:72] Cannot read number of physical cores correctly, number of cores set to 0\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping\nmachine.go:86] Cannot read number of sockets correctly, number of sockets set to 0\ncontainer_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name:\n</code></pre> <ul> <li>\u65e2\u77e5\u3089\u3057\u3044<ul> <li>https://github.com/kubernetes/kubernetes/issues/95039</li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cni-plugin-not-initialized","title":"cni plugin not initialized","text":"<ul> <li><code>/opt/cni/bin</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f <code>/etc/cni/net.d</code> \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b     <pre><code>\"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\n</code></pre></li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cni-config-uninitialized","title":"cni config uninitialized","text":"<ul> <li><code>/opt/cni/bin</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f <code>/etc/cni/net.d</code> \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b     <pre><code>\"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni config uninitialized\"\n</code></pre></li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_3","title":"\u53c2\u8003","text":"<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/</li> <li>https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</li> <li>https://cyberagent.ai/blog/tech/4036/<ul> <li>kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b</li> </ul> </li> <li>https://downloadkubernetes.com/</li> <li> <p>https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/</p> </li> <li> <p>Node Authorization</p> <ul> <li>https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/node/</li> <li>https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/</li> </ul> </li> <li> <p>static pod</p> <ul> <li>https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/</li> <li>https://kubernetes.io/docs/concepts/policy/pod-security-policy/</li> <li>https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver</li> <li><code>PodSecurityPolicy</code> \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf(<code>false</code> \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f <code>true</code> \u306b\u76f4\u3059)</li> <li>https://github.com/kubernetes/kubernetes/issues/70952</li> </ul> </li> </ul>"},{"location":"setup/06_master/02_bootstrapping_etcd/","title":"bootstrapping etcd","text":"<p>coreos\u304cetcd docker image \u3092\u63d0\u4f9b \u3057\u3066\u3044\u307e\u3059\u304c\u3001Raspberry Pi\u306b\u642d\u8f09\u3055\u308c\u3066\u3044\u308bARM CPU\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3 <code>armv8(64bit)</code> \u3067\u5229\u7528\u53ef\u80fd\u306aimage\u306f\u63d0\u4f9b\u3057\u3066\u3044\u306a\u3044\u305f\u3081\u3001image\u3092build\u3057\u307e\u3059\u3002</p>"},{"location":"setup/06_master/02_bootstrapping_etcd/#_1","title":"\u624b\u9806","text":"<ol> <li> <p><code>Dockerfile_etcd.armhf</code> \u3092\u4f5c\u6210\u3059\u308b   Dockerfile_etcd.armhf <pre><code>cat &lt;&lt; 'EOF' &gt; Dockerfile_etcd.armhf\nFROM quay.io/coreos/etcd:v3.4.20\n\nCOPY ca.pem /etc/etcd/\nCOPY kubernetes-key.pem /etc/etcd/\nCOPY kubernetes.pem /etc/etcd/\n\nENV ETCD_UNSUPPORTED_ARCH=arm64\n\nEXPOSE 2379 2380\n\nVOLUME [\"/etcd-data\"]\n\nENTRYPOINT [\"/usr/local/bin/etcd\"]\nEOF\n</code></pre> </p> </li> <li> <p>image build    <pre><code>sudo mkdir -p /etcd-data\nsudo nerdctl build --namespace k8s.io -t k8s-etcd --file=Dockerfile_etcd.armhf ./\n</code></pre></p> </li> <li> <p>pod manifests\u3092 <code>/etc/kubelet.d</code> \u3078\u4f5c\u6210\u3059\u308b   /etc/kubelet.d/etcd.yaml <pre><code>cat &lt;&lt; EOF | sudo tee /etc/kubelet.d/etcd.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://k8s-master:2379\n  name: etcd\n  namespace: kube-system\n  labels:\n    tier: control-plane\n    component: etcd\n\nspec:\n  # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n  priorityClassName: system-node-critical\n  hostNetwork: true\n  volumes:\n  - name: etcd-data-volume\n    hostPath:\n      path: /etcd-data\n      type: Directory\n  containers:\n    - name: etcd\n      image: k8s-etcd:latest\n      imagePullPolicy: IfNotPresent\n      volumeMounts:\n      - mountPath: /etcd-data\n        name: etcd-data-volume\n      env:\n      - name: ETCD_UNSUPPORTED_ARCH\n        value: \"arm64\"\n      resources:\n        requests:\n          cpu: 0.5\n          memory: \"384Mi\"\n        limits:\n          cpu: 1\n          memory: \"384Mi\"\n      command:\n        - /usr/local/bin/etcd\n        - --data-dir=/etcd-data\n        - --advertise-client-urls=https://k8s-master:2379,https://k8s-master:2380\n        - --listen-client-urls=https://0.0.0.0:2379\n        - --initial-advertise-peer-urls=https://k8s-master:2380\n        - --listen-peer-urls=https://0.0.0.0:2380\n        - --name=etcd0\n        - --cert-file=/etc/etcd/kubernetes.pem\n        - --key-file=/etc/etcd/kubernetes-key.pem\n        - --peer-cert-file=/etc/etcd/kubernetes.pem\n        - --peer-key-file=/etc/etcd/kubernetes-key.pem\n        - --trusted-ca-file=/etc/etcd/ca.pem\n        - --peer-trusted-ca-file=/etc/etcd/ca.pem\n        - --peer-client-cert-auth\n        - --client-cert-auth\n        - --initial-cluster-token=etcd-cluster-1\n        - --initial-cluster=etcd0=https://k8s-master:2380\n        - --initial-cluster-state=new\nEOF\n</code></pre> </p> </li> <li> <p><code>crictl</code> \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b    <pre><code>$ sudo crictl ps --name etcd\nCONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID\n72f58248ec087       6e8b8110dc13cfe61d75f867a22c39766a397989413570500f51dedf94be7a12   25 seconds ago       Running             etcd                0                   206c5b952097a\n</code></pre></p> </li> </ol>"},{"location":"setup/06_master/02_bootstrapping_etcd/#_2","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://etcd.io/docs/v2/docker_guide/</li> <li>https://quay.io/repository/coreos/etcd?tag=latest&amp;tab=tags</li> <li>https://github.com/etcd-io/etcd</li> </ul>"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/","title":"bootstrapping kube-apiserver","text":""},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_1","title":"\u624b\u9806","text":"<ol> <li> <p><code>Dockerfile_kube-apiserver.armhf</code> \u3092\u4f5c\u6210\u3059\u308b   Dockerfile_kube-apiserver.armhf <pre><code>cat &lt;&lt; 'EOF' &gt; Dockerfile_kube-apiserver.armhf\nFROM arm64v8/ubuntu:bionic\n\nARG VERSION=\"v1.22.0\"\nARG ARCH=\"arm64\"\n\nRUN set -ex \\\n  &amp;&amp; apt update \\\n  &amp;&amp; apt install -y wget \\\n  &amp;&amp; apt clean \\\n  &amp;&amp; wget --quiet -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-apiserver \\\n  &amp;&amp; chmod +x /usr/bin/kube-apiserver \\\n  &amp;&amp; install -o root -g root -m 755 -d /var/lib/kubernetes \\\n  &amp;&amp; install -o root -g root -m 755 -d /etc/kubernetes/config \\\n  &amp;&amp; install -o root -g root -m 755 -d /etc/kubernetes/webhook\n\nCOPY ca.pem \\\n     ca-key.pem \\\n     kubernetes-key.pem \\\n     kubernetes.pem \\\n     service-account-key.pem \\\n     service-account.pem \\\n     encryption-config.yaml \\\n     front-proxy-ca.pem \\\n     front-proxy.pem \\\n     front-proxy-key.pem \\\n     /var/lib/kubernetes/\n\nCOPY authorization-config.yaml /etc/kubernetes/webhook/\n\nEXPOSE 6443\n\nENTRYPOINT [\"/usr/bin/kube-apiserver\"]\nEOF\n</code></pre> </p> </li> <li> <p>encryption-provider-config \u3092\u4f5c\u6210\u3059\u308b</p> <ul> <li><code>--encryption-provider-config</code> \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u3066secret\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u6697\u53f7\u5316\u3059\u308b\u305f\u3081\u306e\u9375\u3092\u5b9a\u7fa9\u3059\u308b<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#encrypting-your-data</li> <li>https://access.redhat.com/documentation/ja-jp/openshift_container_platform/3.11/html/cluster_administration/admin-guide-encrypting-data-at-datastore encryption-config.yaml <pre><code>ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)\n\ncat &lt;&lt; EOF &gt; encryption-config.yaml\n---\nkind: EncryptionConfig\napiVersion: v1\nresources:\n  - resources:\n      - secrets\n    providers:\n      - aescbc:\n          keys:\n            - name: key1\n              secret: ${ENCRYPTION_KEY}\n      - identity: {}\nEOF\n</code></pre> </li> </ul> </li> </ul> </li> <li> <p>webbhook config\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b</p> <ul> <li><code>--authorization-webhook-config-file</code> \u3067\u6307\u5b9a\u3059\u308b\u30d5\u30a1\u30a4\u30eb    authorization-config.yaml <pre><code>KUBE_API_SERVER_ADDRESS=k8s-master\n\ncat &lt;&lt; EOF &gt; authorization-config.yaml\n---\napiVersion: v1\n# kind of the API object\nkind: Config\n# clusters refers to the remote service.\nclusters:\n  - name: kubernetes\n    cluster:\n      certificate-authority: /var/lib/kubernetes/ca.pem       # CA for verifying the remote service.\n      server: https://${KUBE_API_SERVER_ADDRESS}:6443/authenticate # URL of remote service to query. Must use 'https'.\n\n# users refers to the API server's webhook configuration.\nusers:\n  - name: api-server-webhook\n    user:\n      client-certificate: /var/lib/kubernetes/kubernetes.pem  # cert for the webhook plugin to use\n      client-key: /var/lib/kubernetes/kubernetes-key.pem      # key matching the cert\n\n# kubeconfig files require a context. Provide one for the API server.\ncurrent-context: webhook\ncontexts:\n- context:\n    cluster: kubernetes\n    user: api-server-webhook\n  name: webhook\nEOF\n</code></pre> </li> </ul> </li> <li> <p>image build    <pre><code>sudo nerdctl build --namespace k8s.io -t k8s-kube-apiserver --file=Dockerfile_kube-apiserver.armhf ./\n</code></pre></p> </li> <li> <p>pod manifests\u3092 <code>/etc/kubelet.d</code> \u3078\u4f5c\u6210\u3059\u308b</p> <ul> <li><code>--advertise-address</code> \u30aa\u30d7\u30b7\u30e7\u30f3\u306fIP\u30a2\u30c9\u30ec\u30b9\u3067\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b(hostname\u3067\u306f\u8d77\u52d5\u3057\u306a\u304b\u3063\u305f)   /etc/kubelet.d/kube-api-server.yaml <pre><code>KUBE_API_SERVER_ADDRESS=192.168.3.50\n\ncat &lt;&lt; EOF | sudo tee /etc/kubelet.d/kube-api-server.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: runtime/default\n  labels:\n    tier: control-plane\n    component: kube-apiserver\n\nspec:\n  # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n  priorityClassName: system-node-critical\n  hostNetwork: true\n  containers:\n    - name: kube-apiserver\n      image: k8s-kube-apiserver:latest\n      imagePullPolicy: IfNotPresent\n      resources:\n        requests:\n          memory: \"512Mi\"\n        limits:\n          memory: \"1024Mi\"\n      command:\n        - /usr/bin/kube-apiserver\n        - --advertise-address=k8s-master\n        - --allow-privileged=true\n        - --anonymous-auth=false\n        - --apiserver-count=1\n        - --audit-log-maxage=30\n        - --audit-log-maxbackup=3\n        - --audit-log-maxsize=100\n        - --audit-log-path=/var/log/audit.log\n        - --authorization-mode=Node,RBAC,Webhook\n        - --authorization-webhook-config-file=/etc/kubernetes/webhook/authorization-config.yaml\n        - --authentication-token-webhook-cache-ttl=2m\n        - --authentication-token-webhook-version=v1\n        - --bind-address=0.0.0.0\n        - --client-ca-file=/var/lib/kubernetes/ca.pem\n        - --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,RuntimeClass\n        - --etcd-cafile=/var/lib/kubernetes/ca.pem\n        - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem\n        - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem\n        - --etcd-servers=https://k8s-master:2379\n        - --event-ttl=1h\n        - --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml\n        - --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem\n        - --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem\n        - --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem\n        - --runtime-config=authentication.k8s.io/v1beta1=true\n        - --feature-gates=APIPriorityAndFairness=false\n        - --service-account-key-file=/var/lib/kubernetes/service-account.pem\n        - --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem\n        - --service-account-issuer=api\n        - --service-account-api-audiences=api\n        - --service-cluster-ip-range=10.32.0.0/24\n        - --service-node-port-range=30000-32767\n        - --tls-cert-file=/var/lib/kubernetes/kubernetes.pem\n        - --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem\n        - --http2-max-streams-per-connection=3000\n        - --max-requests-inflight=3000\n        - --max-mutating-requests-inflight=1000\n        - --enable-aggregator-routing=true\n        - --requestheader-client-ca-file=/var/lib/kubernetes/front-proxy-ca.pem\n        - --requestheader-allowed-names=front-proxy-ca\n        - --requestheader-extra-headers-prefix=X-Remote-Extra\n        - --requestheader-group-headers=X-Remote-Group\n        - --requestheader-username-headers=X-Remote-User\n        - --proxy-client-cert-file=/var/lib/kubernetes/front-proxy.pem\n        - --proxy-client-key-file=/var/lib/kubernetes/front-proxy-key.pem\n        - --v=2\nEOF\n</code></pre> </li> </ul> </li> <li> <p><code>crictl</code> \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b    <pre><code>$ sudo crictl ps --name kube-apiserver\nCONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID\n82c371fd9d99e       83e685a0b921ef5dd91eb3cdf208ba70690c1dd7decfc39bb3903be6ede752e6   24 seconds ago      Running             kube-apiserver      0                   6af4d1b99fa37\n</code></pre></p> </li> <li> <p>master node\u306bPod\u304cschedule\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u308b</p> <ul> <li>taint\u3092\u8a2d\u5b9a\u3059\u308b<ul> <li>https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/</li> <li>https://kubernetes.io/ja/docs/concepts/scheduling-eviction/taint-and-toleration/ <pre><code>kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule\n</code></pre> <pre><code>$ kubectl get node k8s-master -o=jsonpath='{.spec.taints}'\n[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\"}]\n</code></pre></li> </ul> </li> </ul> </li> </ol>"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_2","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":"<ol> <li> <p><code>failed creating mandatory flowcontrol settings: failed getting mandatory FlowSchema exempt due to the server was unable to return a response in the time allotted, but may still be processing the request</code></p> <ul> <li>https://github.com/kubernetes/kubernetes/issues/97525#issuecomment-753022219<ul> <li>kube-apiserver\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u4ee5\u4e0b2\u3064\u3092\u4ed8\u52a0\u3059\u308b<ul> <li><code>--feature-gates=APIPriorityAndFairness=false</code></li> <li><code>--runtime-config=flowcontrol.apiserver.k8s.io/v1beta1=false</code></li> </ul> </li> </ul> </li> </ul> </li> <li> <p><code>failed to run Kubelet: no client provided, cannot use webhook authentication</code></p> <ul> <li>kubelet\u304cWebhook\u8a8d\u8a3c\u3092\u671f\u5f85\u3057\u3066\u3044\u308b\u306e\u306bkube-api-server\u3067Webhook\u8a8d\u8a3c\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408<ul> <li>Webhook\u8a8d\u8a3c\u3092\u6709\u52b9\u306b\u3059\u308b</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/webhook/<ul> <li>https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication</li> </ul> </li> </ul> </li> </ul> </li> <li> <p><code>Failed creating a mirror pod for \"kube-scheduler-k8s-master_kube-system(a4a914cd05761a5a4335e2510ca075aa)\": pods \"kube-scheduler-k8s-master\" is forbidden: PodSecurityPolicy: no providers available to validate pod request</code></p> <ul> <li>StaticPod\u3092\u8d77\u52d5\u3057\u305f\u969b\u306bkubelet\u304b\u3089kube-apiserver\u3078mirror pod\u60c5\u5831\u3092\u767b\u9332\u3057\u3088\u3046\u3068\u3057\u3066PodSecurityPolicy\u306b\u3088\u308a\u62d2\u5426\u3055\u308c\u305f</li> </ul> </li> </ol>"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_3","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/webhook/</li> </ul>"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#old","title":"old","text":"<ol> <li>default\u306ePodSecurityPolicy(PSP)\u3092\u4f5c\u6210\u3059\u308b<ul> <li> <p><code>staticPod</code> \u3092\u4f5c\u6210\u3059\u308b\u969b\u306bkubelet\u304b\u3089mirror pod\u4f5c\u6210\u30ea\u30af\u30a8\u30b9\u30c8\u304c\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3059 (\u53c2\u8003)   PSP / ClusterRole / ClusterRoleBinding <pre><code>cat &lt;&lt; EOF | kubectl apply --kubeconfig admin.kubeconfig -f -\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  annotations:\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'\n    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'\n    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'\n  name: default\nspec:\n  # allowedCapabilities: []  # default set of capabilities are implicitly allowed\n  allowedCapabilities:\n    - '*'\n    # - NET_ADMIN\n    # - NET_RAW\n    # - SYS_ADMIN\n  fsGroup:\n    rule: 'MustRunAs'\n    ranges:\n      # Forbid adding the root group.\n      - min: 1\n        max: 65535\n  hostIPC: true\n  hostNetwork: true\n  hostPID: true\n  privileged: true\n  allowPrivilegeEscalation: true\n  readOnlyRootFilesystem: true\n  runAsUser:\n    rule: 'MustRunAsNonRoot'\n  seLinux:\n    rule: 'RunAsNonRoot'\n  supplementalGroups:\n    rule: 'RunAsNonRoot'\n    ranges:\n      # Forbid adding the root group.\n      - min: 1\n        max: 65535\n  volumes:\n  - 'configMap'\n  - 'downwardAPI'\n  - 'emptyDir'\n  - 'persistentVolumeClaim'\n  - 'projected'\n  - 'secret'\n  - 'hostPath'\n  hostNetwork: true\n  runAsUser:\n    rule: 'RunAsAny'\n  seLinux:\n    rule: 'RunAsAny'\n  supplementalGroups:\n    rule: 'RunAsAny'\n  fsGroup:\n    rule: 'RunAsAny'\n\n---\n\n# Cluster role which grants access to the default pod security policy\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: default-psp\nrules:\n- apiGroups:\n  - policy\n  resourceNames:\n  - default\n  resources:\n  - podsecuritypolicies\n  verbs:\n  - use\n\n---\n\n# Cluster role binding for default pod security policy granting all authenticated users access\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: default-psp\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: default-psp\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:authenticated\nEOF\n</code></pre> </p> <pre><code>$ cat &lt;&lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f -\n\n  &lt;\u7701\u7565&gt;\n\npodsecuritypolicy.policy/default created\nclusterrole.rbac.authorization.k8s.io/default-psp created\nclusterrolebinding.rbac.authorization.k8s.io/default-psp created\n</code></pre> </li> </ul> </li> </ol>"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/","title":"bootstrapping kube-controller-manager","text":""},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_1","title":"\u624b\u9806","text":"<ol> <li> <p><code>Dockerfile_kube-controller-manager.armhf</code> \u3092\u4f5c\u6210\u3059\u308b   Dockerfile_kube-controller-manager.armhf <pre><code>cat &lt;&lt; 'EOF' &gt; Dockerfile_kube-controller-manager.armhf\nFROM arm64v8/ubuntu:bionic\n\nARG VERSION=\"v1.22.0\"\nARG ARCH=\"arm64\"\n\nRUN set -ex \\\n  &amp;&amp; apt update \\\n  &amp;&amp; apt install -y wget \\\n  &amp;&amp; apt clean \\\n  &amp;&amp; wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-controller-manager \\\n  &amp;&amp; chmod +x /usr/bin/kube-controller-manager \\\n  &amp;&amp; install -o root -g root -m 755 -d /var/lib/kubernetes \\\n  &amp;&amp; install -o root -g root -m 755 -d /etc/kubernetes/config\n\nCOPY ca.pem \\\n     ca-key.pem \\\n     service-account-key.pem \\\n     kube-controller-manager.kubeconfig \\\n     /var/lib/kubernetes/\n\nENTRYPOINT [\"/usr/bin/kube-controller-manager\"]\nEOF\n</code></pre> </p> </li> <li> <p>image build    <pre><code>sudo nerdctl build --namespace k8s.io -t k8s-kube-controller-manager --file=Dockerfile_kube-controller-manager.armhf ./\n</code></pre></p> </li> <li> <p>pod manifests\u3092 <code>/etc/kubelet.d</code> \u3078\u4f5c\u6210\u3059\u308b</p> </li> <li> <p><code>--allocate-node-cidrs=true</code></p> <ul> <li> <p>Node resource\u306e <code>spec.podCIDR</code> \u3078CIDR\u304c\u8a2d\u5b9a\u3055\u308c\u308b      <pre><code>kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'\n</code></pre></p> <ul> <li><code>spec.podCIDR</code> \u306e\u5024\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044node instance\u3067\u306fCNI Plugin(flannel)\u304c\u6b63\u5e38\u52d5\u4f5c\u3057\u306a\u304b\u3063\u305f</li> </ul> </li> </ul> <p>/etc/kubelet.d/kube-controller-manager.yaml <pre><code>cat &lt;&lt; EOF | sudo tee /etc/kubelet.d/kube-controller-manager.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\n  namespace: kube-system\n  labels:\n    tier: control-plane\n    component: kube-controller-manager\n\nspec:\n  # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n  priorityClassName: system-node-critical\n  hostNetwork: true\n  containers:\n    - name: kube-controller-manager\n      image: k8s-kube-controller-manager:latest\n      imagePullPolicy: IfNotPresent\n      resources:\n        requests:\n          cpu: \"256m\"\n          memory: \"128Mi\"\n        limits:\n          cpu: \"384m\"\n          memory: \"128Mi\"\n      command:\n        - /usr/bin/kube-controller-manager\n        - --bind-address=0.0.0.0\n        - --cluster-cidr=10.200.0.0/16\n        - --allocate-node-cidrs=true\n        - --node-cidr-mask-size=24\n        - --cluster-name=kubernetes\n        - --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem\n        - --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem\n        - --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig\n        - --leader-elect=false\n        - --root-ca-file=/var/lib/kubernetes/ca.pem\n        - --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem\n        - --service-cluster-ip-range=10.32.0.0/24\n        - --use-service-account-credentials=true\n        - --v=2\nEOF\n</code></pre> </p> </li> <li> <p><code>crictl</code> \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b    <pre><code>$ sudo crictl ps --name kube-controller-manager\nCONTAINER           IMAGE                                                              CREATED             STATE               NAME                      ATTEMPT             POD ID\na72cec7323686       4ada5d332b2c795b6333b8b6c538491dec96fb80f81b600359615651725b0ccf   20 seconds ago      Running             kube-controller-manager   0                   526d7f2e9d3cb\n</code></pre></p> </li> </ol>"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_2","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":"<ol> <li>Client.Timeout\u3092\u8d85\u3048\u305f\u305f\u3081\u3001kube-control-manager\u3068kube-scheduler\u304c\u30ed\u30c3\u30af\u3092\u53d6\u5f97\u3067\u304d\u306a\u3044<ul> <li>\u767a\u751f\u3057\u305f\u3089\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u518d\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067\u56de\u5fa9\u3059\u308b</li> <li>kube-apiserver\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u304c\u4e0a\u304c\u308b\u3068\u767a\u751f\u3057\u6613\u304f\u306a\u308b     <pre><code>E0325 11:08:47.205570       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://192.168.10.50:6443/apis/coordination.k8s.io/v1/namespaces/kube-\nsystem/leases/kube-controller-manager?timeout=10s\": context deadline exceeded\nI0325 11:08:47.205695       1 leaderelection.go:278] failed to renew lease kube-system/kube-controller-manager: timed out waiting for the condition\nF0325 11:08:47.205929       1 controllermanager.go:294] leaderelection lost\n</code></pre></li> </ul> </li> </ol>"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_3","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/</li> <li>node(flannel)\u306e <code>Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned</code> \u30a8\u30e9\u30fc\u306b\u95a2\u3057\u3066</li> <li>https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/</li> <li>https://devops.stackexchange.com/questions/5898/how-to-get-kubernetes-pod-network-cidr</li> </ul>"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/","title":"bootstrapping kube-scheduler","text":""},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/#_1","title":"\u624b\u9806","text":"<ol> <li> <p><code>Dockerfile_kube-scheduler.armhf</code> \u3092\u4f5c\u6210\u3059\u308b   Dockerfile_kube-scheduler.armhf <pre><code>cat &lt;&lt; 'EOF' &gt; Dockerfile_kube-scheduler.armhf\nFROM arm64v8/ubuntu:bionic\n\nARG VERSION=\"v1.22.0\"\nARG ARCH=\"arm64\"\n\nRUN set -ex \\\n  &amp;&amp; apt update \\\n  &amp;&amp; apt install -y wget \\\n  &amp;&amp; apt clean \\\n  &amp;&amp; wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-scheduler \\\n  &amp;&amp; chmod +x /usr/bin/kube-scheduler \\\n  &amp;&amp; install -o root -g root -m 755 -d /var/lib/kubernetes \\\n  &amp;&amp; install -o root -g root -m 755 -d /etc/kubernetes/config\n\nCOPY kube-scheduler.yaml /etc/kubernetes/config/\nCOPY kube-scheduler.kubeconfig /var/lib/kubernetes/\n\nENTRYPOINT [\"/usr/bin/kube-scheduler\"]\nEOF\n</code></pre> </p> </li> <li> <p>kube-scheduler\u306econfig\u751f\u6210</p> </li> <li> <p>k8s 1.19.0 \u3067 <code>KubeSchedulerConfiguration</code> \u304c beta\u306bupdate\u3055\u308c\u3066\u3044\u307e\u3059</p> <ul> <li>https://qiita.com/everpeace/items/7dbf14773db82e765370 kube-scheduler.yaml <pre><code>cat &lt;&lt; EOF &gt; kube-scheduler.yaml\n---\napiVersion: kubescheduler.config.k8s.io/v1beta1\nkind: KubeSchedulerConfiguration\nclientConnection:\n  kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\"\nleaderElection:\n  leaderElect: false\nEOF\n</code></pre> </li> </ul> </li> <li> <p>image build    <pre><code>sudo nerdctl build --namespace k8s.io -t k8s-kube-scheduler --file=Dockerfile_kube-scheduler.armhf ./\n</code></pre></p> </li> <li> <p>pod manifests\u3092 <code>/etc/kubelet.d</code> \u3078\u4f5c\u6210\u3059\u308b   /etc/kubelet.d/kube-scheduler.yaml <pre><code>cat &lt;&lt; EOF | sudo tee /etc/kubelet.d/kube-scheduler.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-scheduler\n  namespace: kube-system\n  labels:\n    tier: control-plane\n    component: kube-scheduler\n\nspec:\n  # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n  priorityClassName: system-node-critical\n  hostNetwork: true\n  containers:\n    - name: kube-scheduler\n      image: k8s-kube-scheduler:latest\n      imagePullPolicy: IfNotPresent\n      resources:\n        requests:\n          cpu: \"256m\"\n          memory: \"128Mi\"\n        limits:\n          cpu: \"384m\"\n          memory: \"128Mi\"\n      command:\n        - /usr/bin/kube-scheduler\n        - --config=/etc/kubernetes/config/kube-scheduler.yaml\n        - --v=2\nEOF\n</code></pre> </p> </li> <li> <p><code>crictl</code> \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b    <pre><code>$ sudo crictl ps --name kube-scheduler\nCONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID\na19648dec2d54       70e852515b3c74175bb3ad4855287cb81101921b2b1f5a890fa4ebd0eeeee684   15 seconds ago      Running             kube-scheduler      0                   da1d0572bc2b1\n</code></pre></p> </li> </ol>"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/#_2","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/</li> </ul>"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/","title":"kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b","text":"<p><code>kubectl</code> \u3084\u4ed6Client tool\u3067\u306fkube-apiserver\u3078\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u6295\u3052\u307e\u3059\u3002<code>kube-apiserver</code> \u3067\u306fetcd\u306b\u683c\u7d0d\u3055\u308c\u305f\u60c5\u5831\u3092\u57fa\u306b\u5404worker node(\u306e <code>kubelet</code>) \u3068\u3084\u308a\u3068\u308a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002(\u4f8b\u3048\u3070exec,top,logs\u306a\u3069) <code>kube-apiserver</code> \u304b\u3089 <code>kubelet</code> \u306e\u5fc5\u8981\u306a\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u3092\u4ed8\u4e0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/#_1","title":"\u624b\u9806","text":"<ol> <li> <p>ClusterRole <code>system:kube-apiserver-to-kubelet</code> \u3092\u4f5c\u6210</p> <ul> <li><code>rbac.authorization.kubernetes.io/autoupdate</code> annotations<ul> <li>\u8d77\u52d5\u3059\u308b\u305f\u3073\u306b\u3001API\u30b5\u30fc\u30d0\u30fc\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRole\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a29\u9650\u3067\u66f4\u65b0\u3057\u3001   \u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRoleBinding\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308bsubjects\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002   \u3053\u308c\u306b\u3088\u308a\u3001\u8aa4\u3063\u305f\u5909\u66f4\u3092\u30af\u30e9\u30b9\u30bf\u304c\u4fee\u5fa9\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001   \u65b0\u3057\u3044Kubernetes\u30ea\u30ea\u30fc\u30b9\u3067\u6a29\u9650\u3068subjects\u304c\u5909\u66f4\u3055\u308c\u3066\u3082\u3001   Role\u3068RoleBinding\u3092\u6700\u65b0\u306e\u72b6\u614b\u306b\u4fdd\u3064\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</li> </ul> </li> <li><code>kubernetes.io/bootstrapping: rbac-defaults</code> labels<ul> <li>k8s\u306e\u65e2\u5b9a\u30af\u30e9\u30b9\u30bf\u30ed\u30fc\u30eb\u3068\u65e2\u5b9a\u30ed\u30fc\u30eb\u30d0\u30a4\u30f3\u30c9\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3059</li> </ul> </li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl apply --kubeconfig admin.kubeconfig -f -\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: system:kube-apiserver-to-kubelet\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/proxy\n      - nodes/stats\n      - nodes/log\n      - nodes/spec\n      - nodes/metrics\n    verbs:\n      - \"*\"\nEOF\n</code></pre> </li> <li> <p><code>Kubernetes</code> \u30e6\u30fc\u30b6\u3078<code>system:kube-apiserver-to-kubelet</code> ClusterRole\u3092\u7d10\u4ed8\u3051\u308b</p> <ul> <li><code>roleRef</code> \u3067\u7d10\u4ed8\u3051\u305fRole\u3092\u6307\u5b9a\u3059\u308b</li> <li> <p><code>subjects</code> \u3067Role\u3092\u7d10\u4ed8\u3051\u308bAccount\u3092\u6307\u5b9a\u3059\u308b   <pre><code>cat &lt;&lt; EOF | kubectl apply --kubeconfig admin.kubeconfig -f -\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:kube-apiserver\n  namespace: \"\"\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:kube-apiserver-to-kubelet\nsubjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: User\n    name: Kubernetes\nEOF\n</code></pre></p> <ul> <li>\u3053\u306e\u7d10\u4ed8\u3051\u304c\u6b63\u3057\u304f\u306a\u3044\u3001\u3082\u3057\u304f\u306f\u672a\u8a2d\u5b9a\u306e\u5834\u5408\u3001token\u4ed8\u304d\u3067kubectl\u3092\u5229\u7528\u3057\u305f\u5834\u5408\u306b\u4ee5\u4e0b\u30a8\u30e9\u30fc\u3068\u306a\u308b   <pre><code>Error from server (Forbidden): Forbidden (user=Kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-proxy)\n</code></pre></li> </ul> </li> </ul> </li> </ol>"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/#_2","title":"\u53c2\u8003\u8cc7\u6599","text":"<ul> <li>https://kubernetes.io/ja/docs/reference/access-authn-authz/rbac/</li> <li>https://qiita.com/sheepland/items/67a5bb9b19d8686f389d</li> </ul>"},{"location":"setup/06_master/07_controller_health_check/","title":"Kubernetes API \u306e\u30d8\u30eb\u30b9\u30c1\u30a7\u30c3\u30af","text":""},{"location":"setup/06_master/07_controller_health_check/#_1","title":"\u624b\u9806","text":""},{"location":"setup/06_master/07_controller_health_check/#_2","title":"\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u8d77\u52d5\u78ba\u8a8d","text":"<p><pre><code>kubectl get pods -n kube-system\n</code></pre> \u5b9f\u884c\u4f8b <pre><code>$ kubectl get pods -n kube-system\nNAME                                 READY   STATUS    RESTARTS   AGE\netcd-k8s-master                      1/1     Running   0          5m56s\nkube-apiserver-k8s-master            1/1     Running   0          6m7s\nkube-controller-manager-k8s-master   1/1     Running   0          4m2s\nkube-scheduler-k8s-master            1/1     Running   0          2m48s\n</code></pre> </p>"},{"location":"setup/06_master/07_controller_health_check/#master-noderesource","title":"master node\u4e0a\u306eresource\u78ba\u8a8d","text":"<p><pre><code>kubectl get nodes\nkubectl describe node &lt;pod_name&gt;\n</code></pre> \u5b9f\u884c\u4f8b <pre><code>$ kubectl get nodes\nNAME         STATUS   ROLES    AGE     VERSION\nk8s-master   Ready    &lt;none&gt;   7m57s   v1.20.1\n\n$ kubectl describe node k8s-master\nName:               k8s-master\nRoles:              &lt;none&gt;\nLabels:             beta.kubernetes.io/arch=arm64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=arm64\n                    kubernetes.io/hostname=k8s-master\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 17 Apr 2021 15:13:42 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-master\n  AcquireTime:     &lt;unset&gt;\n  RenewTime:       Sat, 17 Apr 2021 16:34:29 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 17 Apr 2021 16:34:09 +0000   Sat, 17 Apr 2021 15:13:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 17 Apr 2021 16:34:09 +0000   Sat, 17 Apr 2021 15:13:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 17 Apr 2021 16:34:09 +0000   Sat, 17 Apr 2021 15:13:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 17 Apr 2021 16:34:09 +0000   Sat, 17 Apr 2021 15:13:52 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.10.50\n  Hostname:    k8s-master\nCapacity:\n  cpu:                4\n  ephemeral-storage:  30459624Ki\n  memory:             1892528Ki\n  pods:               110\nAllocatable:\n  cpu:                3400m\n  ephemeral-storage:  28071589432\n  memory:             1380528Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 58f6de70444c4198b56b30122b6c77dc\n  System UUID:                58f6de70444c4198b56b30122b6c77dc\n  Boot ID:                    79af3428-cf70-4189-a447-0b917a035a42\n  Kernel Version:             5.4.0-1032-raspi\n  OS Image:                   Ubuntu 20.04.2 LTS\n  Operating System:           linux\n  Architecture:               arm64\n  Container Runtime Version:  cri-o://1.20.2\n  Kubelet Version:            v1.20.1\n  Kube-Proxy Version:         v1.20.1\nPodCIDR:                      10.200.0.0/24\nPodCIDRs:                     10.200.0.0/24\nNon-terminated Pods:          (4 in total)\n  Namespace                   Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                  ------------  ----------  ---------------  -------------  ---\n  kube-system                 etcd-k8s-master                       500m (14%)    1 (29%)     256Mi (18%)      384Mi (28%)    78m\n  kube-system                 kube-apiserver-k8s-master             500m (14%)    1 (29%)     256Mi (18%)      384Mi (28%)    78m\n  kube-system                 kube-controller-manager-k8s-master    100m (2%)     300m (8%)   128Mi (9%)       256Mi (18%)    76m\n  kube-system                 kube-scheduler-k8s-master             100m (2%)     300m (8%)   128Mi (9%)       256Mi (18%)    75m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1200m (35%)  2600m (76%)\n  memory             768Mi (56%)  1280Mi (94%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:              &lt;none&gt;\n</code></pre> </p>"},{"location":"setup/06_master/07_controller_health_check/#health-checks","title":"health checks","text":"<p>kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067 <code>--anonymous-auth=false</code> \u3092\u4ed8\u52a0\u3057\u3066\u3044\u308b\u305f\u3081 <code>https://localhost:6443</code> \u3078\u306eanonymous\u30a2\u30ab\u30a6\u30f3\u30c8\u3067\u306e\u78ba\u8a8d\u306f\u884c\u308f\u305a\u306b <code>kubectl</code> \u3067\u78ba\u8a8d\u3059\u308b</p> <ol> <li> <p>API endpoints for health     <pre><code>kubectl get --raw='/readyz?verbose'\n</code></pre> \u5b9f\u884c\u4f8b <pre><code>$ kubectl get --raw='/readyz?verbose'\n[+]ping ok\n[+]log ok\n[+]etcd ok\n[+]informer-sync ok\n[+]poststarthook/start-kube-apiserver-admission-initializer ok\n[+]poststarthook/generic-apiserver-start-informers ok\n[+]poststarthook/max-in-flight-filter ok\n[+]poststarthook/start-apiextensions-informers ok\n[+]poststarthook/start-apiextensions-controllers ok\n[+]poststarthook/crd-informer-synced ok\n[+]poststarthook/bootstrap-controller ok\n[+]poststarthook/rbac/bootstrap-roles ok\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\n[+]poststarthook/priority-and-fairness-config-producer ok\n[+]poststarthook/start-cluster-authentication-info-controller ok\n[+]poststarthook/start-kube-aggregator-informers ok\n[+]poststarthook/apiservice-registration-controller ok\n[+]poststarthook/apiservice-status-available-controller ok\n[+]poststarthook/kube-apiserver-autoregistration ok\n[+]autoregister-completion ok\n[+]poststarthook/apiservice-openapi-controller ok\n[+]shutdown ok\nreadyz check passed\n</code></pre> </p> </li> <li> <p>Individual health checks     <pre><code>kubectl get --raw='/livez/etcd'\n</code></pre> \u5b9f\u884c\u4f8b <pre><code>$ kubectl get --raw='/livez/etcd'\nok\n\n$ kubectl get --raw='/livez/poststarthook/start-apiextensions-controllers'\nok\n</code></pre> </p> </li> </ol>"},{"location":"setup/06_master/07_controller_health_check/#_3","title":"\u53c2\u8003\u8cc7\u6599","text":"<ul> <li>https://kubernetes.io/docs/reference/using-api/health-checks/</li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/","title":"bootstrapping kubelet(master/worker \u5171\u901a)","text":"<ul> <li><code>kubelet</code> \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002</li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#worker-node","title":"worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206","text":"<ul> <li>Reserve Compute Resources for System Daemons<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/</li> <li> <p><code>Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9</code> = <code>Node resource - system-reserved - kube-reserved - eviction-threshold</code> \u3089\u3057\u3044</p> name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi </li> </ul> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_1","title":"\u624b\u9806","text":""},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubelet","title":"<code>kubelet</code> \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9","text":"<pre><code>VERSION=\"v1.22.0\"\nARCH=\"arm64\"\n\nsudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet\nsudo chmod +x /usr/bin/kubelet\n</code></pre>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeconfig","title":"kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e","text":"<pre><code># host=\"k8s-node2\"\n# host=\"k8s-node1\"\nhost=\"k8s-master\"\n\nsudo install -o root -g root -m 755 -d /etc/kubelet.d\nsudo install -o root -g root -m 755 -d /var/lib/kubernetes\nsudo install -o root -g root -m 755 -d /var/lib/kubelet\nsudo cp ca.pem /var/lib/kubernetes/\nsudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/\nsudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig\n</code></pre>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#varlibkubeletkubelet-configyaml","title":"<code>/var/lib/kubelet/kubelet-config.yaml</code> \u3092\u4f5c\u6210\u3059\u308b","text":"<ul> <li><code>clusterDNS</code> \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b</li> <li><code>podCIDR</code> \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b   <pre><code># host=\"k8s-node2\"\n# host=\"k8s-node1\"\nhost=\"k8s-master\"\n\ncat &lt;&lt; EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml\n---\nkind: KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\n\n# https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/\nstaticPodPath: /etc/kubelet.d\n\n# kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f\n#   - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b\n#   - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981\nauthentication:\n  anonymous:\n    enabled: true\n  webhook:\n    enabled: false\n    cacheTTL: \"2m\"\n  x509:\n    clientCAFile: \"/var/lib/kubernetes/ca.pem\"\n\n# kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a\n#   - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow\n#   - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981\nauthorization:\n  mode: AlwaysAllow\n\nclusterDomain: \"cluster.local\"\nclusterDNS:\n  - \"10.32.0.10\"\npodCIDR: \"10.200.0.0/24\"\nruntimeRequestTimeout: \"15m\"\ntlsCertFile: \"/var/lib/kubelet/${host}.pem\"\ntlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\"\n\n# Reserve Compute Resources for System Daemons\n# https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/\n#\n# Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044\n#\n# system-reserved\n#   - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b\n#\n# kube-reserved\n#   - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b\nenforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"]\ncgroupsPerQOS: true\ncgroupDriver: systemd\ncgroupRoot: /\nsystemCgroups: /systemd/system.slice\nsystemReservedCgroup: /system.slice\nsystemReserved:\n  cpu: 256m\n  memory: 256Mi\nruntimeCgroups: /kube.slice/containerd.service\nkubeletCgroups: /kube.slice/kubelet.service\nkubeReservedCgroup: /kube.slice\nkubeReserved:\n  cpu: 1024m\n  memory: 1024Mi\nEOF\n</code></pre></li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#etcsystemdsystemkubeletservice","title":"<code>/etc/systemd/system/kubelet.service</code> \u3092\u914d\u7f6e","text":"<pre><code>cat &lt;&lt; 'EOF' | sudo tee /etc/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=containerd.service\nRequires=containerd.service\n\n[Service]\nRestart=on-failure\nRestartSec=5\n\nExecStartPre=/usr/bin/mkdir -p \\\n  /sys/fs/cgroup/kube.slice \\\n  /sys/fs/cgroup/system.slice \\\n  /sys/fs/cgroup/systemd/kube.slice \\\n  /sys/fs/cgroup/cpuset/kube.slice \\\n  /sys/fs/cgroup/cpuset/system.slice \\\n  /sys/fs/cgroup/pids/kube.slice \\\n  /sys/fs/cgroup/pids/system.slice \\\n  /sys/fs/cgroup/memory/kube.slice \\\n  /sys/fs/cgroup/memory/system.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/system.slice \\\n  /sys/fs/cgroup/hugetlb/system.slice \\\n  /sys/fs/cgroup/hugetlb/kube.slice\n\nExecStart=/usr/bin/kubelet \\\n  --config=/var/lib/kubelet/kubelet-config.yaml \\\n  --kubeconfig=/var/lib/kubelet/kubeconfig \\\n  --network-plugin=cni \\\n  --container-runtime=remote \\\n  --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\\n  --register-node=true \\\n  --v=2\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeletservice","title":"<code>kubelet.service</code> \u3092\u8d77\u52d5","text":"<pre><code>sudo systemctl enable kubelet.service\nsudo systemctl start kubelet.service\n</code></pre>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_2","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":""},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cgroup","title":"cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408","text":"<pre><code>kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist\n</code></pre> <ul> <li> <p><code>kubelet</code> \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( <code>--v 10</code> )</p> <pre><code>cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice]\n</code></pre> </li> <li> <p>\u5bfe\u5fdc     <code>kubelet.service</code> \u306e <code>ExecStartPre</code> \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b</p> <pre><code>ExecStartPre=/usr/bin/mkdir -p \\\n  /sys/fs/cgroup/systemd/kube.slice \\\n  /sys/fs/cgroup/cpuset/kube.slice \\\n  /sys/fs/cgroup/cpuset/system.slice \\\n  /sys/fs/cgroup/pids/kube.slice \\\n  /sys/fs/cgroup/pids/system.slice \\\n  /sys/fs/cgroup/memory/kube.slice \\\n  /sys/fs/cgroup/memory/system.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice\n</code></pre> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cgroupsystemreserved-memory-size","title":"cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f","text":"<ul> <li>\u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f    <pre><code>kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy\n</code></pre></li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeconfig-cn-node","title":"kubeconfig \u306e\u8a3c\u660e\u66f8\u306e <code>CN</code> \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b","text":"<pre><code>360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\"\n</code></pre> <ul> <li>kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b       <pre><code>sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject:\n</code></pre><ul> <li><code>k8s-master</code> \u304c\u6b63\u3057\u3044\u306e\u306b <code>CN = system:node:k8s-node1</code> \u3068\u306a\u3063\u3066\u3044\u305f    <pre><code>root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject:\n    Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#node-specpodcidr-cidr","title":"<code>Node</code> \u30ea\u30bd\u30fc\u30b9\u306e <code>spec.podCIDR</code> \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044","text":"<ul> <li> <p>\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044</p> <ul> <li>flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f...  <pre><code>kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'\n</code></pre></li> </ul> </li> <li> <p><code>kube-controller-manager</code> \u306e\u30ed\u30b0</p> <ul> <li><code>Set node k8s-node1 PodCIDR to [10.200.0.0/24]</code> \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8<ul> <li><code>kube-controller-manager</code> \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b <code>--allocate-node-cidrs=true</code> \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... <pre><code>actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist\nrange_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24]\nttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\"\ncontroller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}]\ncontroller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes\nnode_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\"\ncontroller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1\nnode_lifecycle_controller.go:1429] Initializing eviction metric for zone:\nnode_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp.\nevent.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\"\nnode_lifecycle_controller.go:1245] Controller detected that zone  is now in state Normal.\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#webhook-authentication","title":"Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044","text":"<pre><code>I0214 07:03:56.822586       1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\"\nF0214 07:03:56.822637       1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication\ngoroutine 1 [running]:\n</code></pre> <ul> <li>https://kubernetes.io/docs/reference/access-authn-authz/webhook/</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication</li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cni-plugin-etccninetd-cni-plugin","title":"CNI Plugin\u3092 <code>/etc/cni/net.d</code> \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044","text":"<pre><code>kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\ncni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d\nkubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\n</code></pre> <ul> <li>CNI Plugin\u3092 <code>/etc/cni/net.d</code> \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b<ul> <li>https://github.com/containernetworking/plugins/releases</li> </ul> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubelet-cannot-determine-cpu-online-state","title":"Kubelet cannot determine CPU online state","text":"<pre><code>sysinfo.go:203] Nodes topology is not available, providing CPU topology\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory\ngce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping\nmachine.go:72] Cannot read number of physical cores correctly, number of cores set to 0\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping\nmachine.go:86] Cannot read number of sockets correctly, number of sockets set to 0\ncontainer_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name:\n</code></pre> <ul> <li>\u65e2\u77e5\u3089\u3057\u3044<ul> <li>https://github.com/kubernetes/kubernetes/issues/95039</li> </ul> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cni-plugin-not-initialized","title":"cni plugin not initialized","text":"<ul> <li><code>/opt/cni/bin</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f <code>/etc/cni/net.d</code> \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b     <pre><code>\"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\n</code></pre></li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cni-config-uninitialized","title":"cni config uninitialized","text":"<ul> <li><code>/opt/cni/bin</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f <code>/etc/cni/net.d</code> \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b     <pre><code>\"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni config uninitialized\"\n</code></pre></li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_3","title":"\u53c2\u8003","text":"<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/</li> <li>https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</li> <li>https://cyberagent.ai/blog/tech/4036/<ul> <li>kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b</li> </ul> </li> <li>https://downloadkubernetes.com/</li> <li> <p>https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/</p> </li> <li> <p>Node Authorization</p> <ul> <li>https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/node/</li> <li>https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/</li> </ul> </li> <li> <p>static pod</p> <ul> <li>https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/</li> <li>https://kubernetes.io/docs/concepts/policy/pod-security-policy/</li> <li>https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver</li> <li><code>PodSecurityPolicy</code> \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf(<code>false</code> \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f <code>true</code> \u306b\u76f4\u3059)</li> <li>https://github.com/kubernetes/kubernetes/issues/70952</li> </ul> </li> </ul>"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/","title":"bootstrapping kube-proxy","text":""},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/#kube-proxy","title":"kube-proxy \u3068\u306f","text":"<p>kube-proxy \u3068\u306f\u5404worker node\u3067\u52d5\u4f5c\u3059\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30d7\u30ed\u30ad\u30b7\u3092\u5b9f\u73fe\u3059\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3067\u3059\u3002\u5177\u4f53\u7684\u306b\u306fSertvice\u30ea\u30bd\u30fc\u30b9\u3067\u4f5c\u6210\u3055\u308c\u308bCluster IP\u3084Node Port\u306e\u7ba1\u7406\u3068\u305d\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30c6\u30fc\u30d6\u30eb\u306e\u7ba1\u7406\u3001\u307e\u305fnginx ingress controller\u3092\u5229\u7528\u3057\u305fIngress\u30ea\u30bd\u30fc\u30b9\u3067\u306fPod\u3078\u306e\u8ca0\u8377\u5206\u6563\u306bkube-proxy\u3092\u6d3b\u7528\u6307\u5b9a\u305f\u308a\u3059\u308b\u305d\u3046\u3067\u3059(<code>With NGINX, we\u2019ll use the DNS name or virtual IP address to identify the service, and rely on kube-proxy to perform the internal load-balancing across the pool of pods.</code>)</p>"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/#_1","title":"\u624b\u9806","text":"<ol> <li> <p><code>Dockerfile_kube-proxy.armhf</code> \u3092\u4f5c\u6210\u3059\u308b  Dockerfile_kube-proxy.armhf <pre><code>cat &lt;&lt; 'EOF' &gt; Dockerfile_kube-proxy.armhf\nFROM arm64v8/ubuntu:bionic\n\nARG VERSION=\"v1.22.0\"\nARG ARCH=\"arm64\"\n\nRUN set -ex \\\n  &amp;&amp; apt update \\\n  &amp;&amp; apt install -y wget \\\n  &amp;&amp; apt clean \\\n  &amp;&amp; wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-proxy \\\n  &amp;&amp; chmod +x /usr/bin/kube-proxy \\\n  &amp;&amp; install -o root -g root -m 755 -d /var/lib/kube-proxy \\\n  &amp;&amp; install -o root -g root -m 755 -d /etc/kubernetes/config\n\nCOPY kube-proxy.kubeconfig /var/lib//kube-proxy/kubeconfig\n\nENTRYPOINT [\"/usr/bin/kube-proxy\"]\nEOF\n</code></pre> </p> </li> <li> <p>image build    <pre><code>sudo nerdctl build --namespace k8s.io -f Dockerfile_kube-proxy.armhf -t k8s-kube-proxy ./\n</code></pre></p> </li> <li> <p>kernel parameter    <pre><code>cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/kubelet.conf\n# kube-proxy\nnet.ipv4.conf.all.route_localnet = 1\nnet.netfilter.nf_conntrack_max = 131072\nnet.netfilter.nf_conntrack_tcp_timeout_established = 86400\nnet.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600\nEOF\n\nsudo sysctl --system\n\ncat &lt;&lt;EOF | sudo tee /etc/modprobe.d/kube-proxy.conf\noptions nf_conntrack hashsize=32768\nEOF\n\nsudo /sbin/modprobe nf_conntrack hashsize=32768\n</code></pre></p> </li> <li> <p>pod manifests\u3092 <code>/etc/kubernetes/manifests/</code> \u3078\u4f5c\u6210\u3059\u308b  /etc/kubernetes/manifests/kube-proxy.yaml <pre><code>cluster_cidr=\"10.200.0.0/16\"\nsudo mkdir -p /etc/kubernetes/manifests\n\ncat &lt;&lt; EOF | sudo tee /etc/kubernetes/manifests/kube-proxy.yaml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app: kube-proxy\n  name: kube-proxy-configuration\n  namespace: kube-system\ndata:\n  config.conf: |-\n    ---\n    apiVersion: kubeproxy.config.k8s.io/v1alpha1\n    kind: KubeProxyConfiguration\n    clientConnection:\n      kubeconfig: \"/var/lib/kube-proxy/kubeconfig\"\n    mode: \"iptables\"\n    clusterCIDR: \"${cluster_cidr}\"\n\n    # https://kubernetes.io/docs/reference/config-api/kube-proxy-config.v1alpha1/\n    # metricsBindAddress: 127.0.0.1:10249\n    metricsBindAddress: 0.0.0.0:10249\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\n  labels:\n    component: kube-proxy\n    # TODO\n    # master node\u306baddon-manager\u3092\u5c0e\u5165\u3057\u305f\u3089\u30b3\u30e1\u30f3\u30c8\u5916\u3059\n    # addonmanager.kubernetes.io/mode=Reconcile\nspec:\n  selector:\n    matchLabels:\n      name: kube-proxy\n  # https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#performing-a-rolling-update\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    # template \u4ee5\u4e0b\u306fpod templates\n    #   (apiVersion\u3084kind\u3092\u3082\u305f\u306a\u3044\u3053\u3068\u3092\u9664\u3044\u3066\u306f\u3001Pod\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3068\u540c\u3058\u30b9\u30ad\u30fc\u30de)\n    #   https://kubernetes.io/ja/docs/concepts/workloads/controllers/daemonset/\n    metadata:\n      labels:\n        name: kube-proxy\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n      priorityClassName: system-node-critical\n      hostNetwork: true\n      containers:\n        - name: kube-proxy\n          image: localhost/k8s-kube-proxy:latest\n          securityContext:\n            capabilities:\n              add:\n                - SYS_ADMIN\n                - NET_ADMIN\n                - NET_RAW\n          command:\n            - /usr/bin/kube-proxy\n            - --config=/var/lib/kube-proxy/kube-proxy-config.yaml\n          imagePullPolicy: IfNotPresent\n          resources:\n            requests:\n              cpu: \"256m\"\n          volumeMounts:\n          - name: kube-proxy-configuration-volume\n            mountPath: /var/lib/kube-proxy/kube-proxy-config.yaml\n          - name: conntrack-command\n            mountPath: /usr/sbin/conntrack\n          - name: iptables-command\n            mountPath: /usr/sbin/iptables\n          - name: iptables-restore-command\n            mountPath: /usr/sbin/iptables-restore\n          - name: iptables-save-command\n            mountPath: /usr/sbin/iptables-save\n          - name: xtables-lock-file\n            mountPath: /run/xtables.lock\n          - name: usr-lib-dir\n            mountPath: /usr/lib\n          - name: lib-dir\n            mountPath: /lib\n          - name: sys-dir\n            mountPath: /sys\n      volumes:\n      - name: kube-proxy-configuration\n        configMap:\n          name: kube-proxy-configuration\n      - name: conntrack-command\n        hostPath:\n          path: /usr/sbin/conntrack\n      - name: iptables-command\n        hostPath:\n          path: /usr/sbin/iptables\n      - name: iptables-restore-command\n        hostPath:\n          path: /usr/sbin/iptables-restore\n      - name: iptables-save-command\n        hostPath:\n          path: /usr/sbin/iptables-save\n      - name: xtables-lock-file\n        hostPath:\n          path: /run/xtables.lock\n      - name: usr-lib-dir\n        hostPath:\n          path: /usr/lib\n      - name: lib-dir\n        hostPath:\n          path: /lib\n      - name: sys-dir\n        hostPath:\n          path: /sys\nEOF\n</code></pre> </p> </li> <li> <p>pod\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b    <pre><code>kubectl apply -f /etc/kubernetes/manifests/kube-proxy.yaml\n</code></pre></p> </li> </ol>"},{"location":"setup/08_flannel/bootstrapping_flannel/","title":"bootstrapping flannel","text":""},{"location":"setup/08_flannel/bootstrapping_flannel/#_1","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://github.com/flannel-io/flannel/</li> <li>https://github.com/flannel-io/flannel/blob/master/Documentation/troubleshooting.md</li> <li><code>/opt/bin/flanneld</code> \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3</li> <li>https://github.com/flannel-io/flannel/blob/master/main.go#L110-L132</li> </ul>"},{"location":"setup/08_flannel/bootstrapping_flannel/#_2","title":"\u624b\u9806","text":"<ol> <li> <p>vxlan module\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b     <pre><code>sudo apt install -y linux-modules-extra-raspi locate\nsudo updatedb\nsudo modprobe vxlan\nsudo lsmod | grep vxlan\n</code></pre></p> </li> <li> <p>flannel k8s manifests\u3092\u516c\u5f0f\u304b\u3089\u53d6\u5f97\u3059\u308b</p> </li> <li> <p>master\u30d6\u30e9\u30f3\u30c1\u304b\u3089\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u304c2021/03/06 \u6642\u70b9\u3067\u306f release tag <code>v0.13.1-rc2</code> \u306e\u5185\u5bb9</p> <pre><code>sudo mkdir -p /etc/kubernetes/manifests\nsudo curl -o /etc/kubernetes/manifests/kube-flannel.yml -sSL https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml\n</code></pre> </li> <li> <p>manifests\u3092\u4fee\u6b63\u3059\u308b</p> <ul> <li><code>net-conf.json</code> \u306e <code>Network</code> \u3092controller-manager\u306e<code>--cluster-cidr</code>\u3067\u6307\u5b9a\u3057\u305f\u5024\u306b\u5909\u66f4\u3059\u308b</li> <li>etcd\u306b\u7528\u3044\u305fcertificate\u3084admin\u306ekubeconfig\u3092kube-flannel\u30b3\u30f3\u30c6\u30ca\u3078bind mount\u3059\u308b</li> <li><code>/opt/bin/flanneld</code> \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3<ul> <li><code>--kubeconfig-file</code></li> <li><code>--etcd-endpoints</code></li> <li><code>--etcd-prefix</code></li> <li><code>--etcd-keyfile</code></li> <li><code>--etcd-certfile</code></li> <li><code>--etcd-cafile</code></li> <li> <p><code>--v</code></p> <p><pre><code>sudo vim /etc/kubernetes/manifests/kube-flannel.yml\n</code></pre> \u4fee\u6b63\u5f8c\u306e<code>/etc/kubernetes/manifests/kube-flannel.yml</code> <pre><code>---\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: psp.flannel.unprivileged\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default\n    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default\n    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default\nspec:\n  privileged: false\n  volumes:\n  - configMap\n  - secret\n  - emptyDir\n  - hostPath\n  allowedHostPaths:\n  - pathPrefix: \"/etc/cni/net.d\"\n  - pathPrefix: \"/etc/kube-flannel\"\n  - pathPrefix: \"/run/flannel\"\n  readOnlyRootFilesystem: false\n  # Users and groups\n  runAsUser:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n  # Privilege Escalation\n  allowPrivilegeEscalation: false\n  defaultAllowPrivilegeEscalation: false\n  # Capabilities\n  allowedCapabilities: ['NET_ADMIN', 'NET_RAW']\n  defaultAddCapabilities: []\n  required\n  seLinux:\n    # SELinux is unused in CaaSP\n    rule: 'RunAsAny'\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n- apiGroups: ['extensions']\n  resources: ['podsecuritypolicies']\n  verbs: ['use']\n  resourceNames: ['psp.flannel.unprivileged']\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  verbs:\n  - get\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes/status\n  verbs:\n  - patch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      \"name\": \"cbr0\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"flannel\",\n          \"delegate\": {\n            \"hairpinMode\": true,\n            \"isDefaultGateway\": true\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"capabilities\": {\n            \"portMappings\": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"10.200.0.0/16\",\n      \"Backend\": {\n        \"Type\": \"vxlan\"\n      }\n    }\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/os\n                operator: In\n                values:\n                - linux\n      hostNetwork: true\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni-plugin\n       #image: flannelcni/flannel-cni-plugin:v1.0.1 for ppc64le and mips64le (dockerhub limitations may apply)\n        image: rancher/mirrored-flannelcni-flannel-cni-plugin:v1.0.1\n        command:\n        - cp\n        args:\n        - -f\n        - /flannel\n        - /opt/cni/bin/flannel\n        volumeMounts:\n        - name: cni-plugin\n          mountPath: /opt/cni/bin\n      - name: install-cni\n       #image: flannelcni/flannel:v0.16.3 for ppc64le and mips64le (dockerhub limitations may apply)\n        image: rancher/mirrored-flannelcni-flannel:v0.16.3\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n       #image: flannelcni/flannel:v0.16.3 for ppc64le and mips64le (dockerhub limitations may apply)\n        image: rancher/mirrored-flannelcni-flannel:v0.16.3\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        - --kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig\n        - --etcd-endpoints=https://k8s-master:4001\n        - --etcd-prefix=/coreos.com/network\n        - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem\n        - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem\n        - --etcd-cafile=/var/lib/kubernetes/ca.pem\n        - --v=10\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: false\n          capabilities:\n            add: [\"NET_ADMIN\", \"NET_RAW\"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: var-lib-kubernetes-dir\n          mountPath: /var/lib/kubernetes/\n      volumes:\n      - name: run\n        hostPath:\n          path: /run/flannel\n      - name: cni-plugin\n        hostPath:\n          path: /opt/cni/bin\n      - name: cni\n        hostPath:\n          path: /etc/cni/net.d\n      - name: flannel-cfg\n        configMap:\n          name: kube-flannel-cfg\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: var-lib-kubernetes-dir\n        hostPath:\n          path: /var/lib/kubernetes\n</code></pre></p> </li> </ul> </li> </ul> </li> <li> <p>flannel Pod\u3092\u30c7\u30d7\u30ed\u30a4</p> </li> </ol> <pre><code>kubectl apply -f /etc/kubernetes/manifests/kube-flannel.yml\n</code></pre>"},{"location":"setup/08_flannel/bootstrapping_flannel/#_3","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":"<ol> <li> <p>flannel\u304c\u53c2\u7167\u3059\u308bkubeconfig\u304c\u6b63\u3057\u304f\u306a\u3044</p> <ul> <li>flannel-io/flannel Documentation/kube-flannel.yml \u305d\u306e\u307e\u307e\u3060\u3068\u767a\u751f\u3057\u305f     <pre><code>Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\nerror creating inClusterConfig, falling back to default config: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory\nFailed to create SubnetManager: fail to create kubernetes config: invalid configuration: no configuration has been provided, try setting KUBERNETES_MASTER environment variable\n</code></pre></li> </ul> </li> <li> <p><code>Failed to create SubnetManager: fail to create kubernetes config: stat \"/var/lib/kubernetes/admin.kubeconfig\": no such file or directory</code></p> <ul> <li><code>--kubeconfig-file</code> \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u305fkubeconfig path\u304c\u6b63\u3057\u304f\u306a\u3044<ul> <li>\u79c1\u306e\u30b1\u30fc\u30b9\u3067\u306f <code>--kubeconfig-file=\"/var/lib/kubernetes/admin.kubeconfig\"</code> \u3068\u3057\u3066\u3044\u308b\u3068\u30c0\u30d6\u30eb\u30af\u30a9\u30fc\u30c8(\") \u304c\u30d1\u30b9\u306b\u542b\u307e\u308c\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6c17\u4ed8\u304f\u306e\u306b\u6642\u9593\u304b\u304b\u308a\u307e\u3057\u305f(\u6b63\u3057\u304f\u306f <code>--kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig</code>)</li> </ul> </li> </ul> </li> <li> <p><code>Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned</code></p> <ul> <li>Node\u30ea\u30bd\u30fc\u30b9\u306e <code>.spec.podCIDR</code> \u304c\u767b\u9332\u3055\u308c\u3066\u3044\u306a\u3044</li> <li>\u78ba\u8a8d\u65b9\u6cd5     <pre><code>kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'\n</code></pre></li> <li> <p><code>kube-controller-manager</code> \u306e\u8a2d\u5b9a\u4e0d\u5099\u306e\u53ef\u80fd\u6027</p> <ul> <li>\u4ee5\u4e0b\u8a2d\u5b9a\u5909\u66f4\u5f8c\u3001kubelet\u306e\u8d77\u52d5\u524d\u306b <code>kubectl delete node &lt;NODE&gt;</code> \u3092\u5b9f\u884c\u3059\u308b<ul> <li>https://github.com/flannel-io/flannel/issues/728#issuecomment-325347810<ul> <li>podCIDR\u5272\u308a\u5f53\u3066\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044<ul> <li><code>--cluster-cidr=&lt;CIDR&gt;</code></li> <li><code>--allocate-node-cidrs=true</code></li> </ul> </li> </ul> </li> <li>https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/<ul> <li><code>--service-cluster-ip-range=&lt;CIDR&gt;</code> \u3068 (flannnel\u306e)<code>--pod-cidr=&lt;CIDR&gt;</code> \u304c\u88ab\u3063\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>(kube-controller-manager \u304c\u6b63\u3057\u3044\u5834\u5408) Node\u30ea\u30bd\u30fc\u30b9\u306e <code>.spec.podCIDR</code> \u3092\u624b\u52d5\u8a2d\u5b9a\u3057\u3066\u56de\u907f\u3059\u308b     <pre><code>kubectl patch node &lt;NODE_NAME&gt; -p '{\"spec\":{\"podCIDR\":\"&lt;SUBNET&gt;\"}}'\n</code></pre></p> </li> </ul> </li> <li> <p><code>Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again.</code></p> <ul> <li><code>kubectl delete pod kube-flannel-...</code> \u3067 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9 <code>flannel.1</code> \u304c\u958b\u653e\u3055\u308c\u306a\u3044<ul> <li>https://github.com/flannel-io/flannel/issues/1060</li> <li>\u53e4\u3044 <code>flannel.1</code> \u3092\u524a\u9664     <pre><code>ip a\nsudo ip link delete flannel.1\n</code></pre></li> </ul> </li> </ul> </li> </ol>"},{"location":"setup/09_coredns/bootstrapping_coredns/","title":"bootstrapping CoreDNS","text":""},{"location":"setup/09_coredns/bootstrapping_coredns/#coredns","title":"CoreDNS \u3068\u306f","text":"<p>CoreDNS \u306fCNCF\u306egraduated project \u3068\u3057\u3066\u30db\u30b9\u30c8\u3055\u308c\u3066\u3044\u308bDNS\u30b5\u30fc\u30d0\u3067\u3001Kubernetes 1.13 \u4ee5\u964d\u306b\u3066\u30c7\u30d5\u30a9\u30eb\u30c8DNS\u30b5\u30fc\u30d0\u3068\u3057\u3066\u63a1\u7528 \u3055\u308c\u3066\u304a\u308a\u3001Cluster\u5185\u3067\u306eService\u30ea\u30bd\u30fc\u30b9\u306e\u540d\u524d\u89e3\u6c7a\u306b\u5229\u7528\u3057\u3066\u3044\u307e\u3059 (Kubernetes DNS-Based Service Discovery)\u3002AWS Load Balancer Controller\u306a\u3069Cluster\u5916\u3078\u30a8\u30f3\u30c9\u30dd\u30a4\u30f3\u30c8\u3092\u516c\u958b\u3059\u308b\u5834\u5408\u306f\u5225\u9014\u5916\u90e8DNS\u30b5\u30fc\u30d0(Route53\u306a\u3069)\u3092\u5229\u7528\u3057\u307e\u3059(CoreDNS\u3092Cluster\u5916\u90e8\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd)\u3002</p> <p>CoreDNS\u306f\u3042\u3089\u3086\u308b\u51e6\u7406\u3092Plugin\u3068\u3057\u3066\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002CoreDNS\u5358\u4f53\u306fDNS\u30af\u30a8\u30ea\u30fc\u3092\u89e3\u91c8\u3057\u3066\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb(<code>./Corefile</code>)\u306b\u8a18\u8ff0\u3055\u308c\u305fPlugin\u306b\u51e6\u7406\u3092\u53d7\u3051\u6e21\u3057\u307e\u3059\u3002(\u53c2\u8003)</p> <p>kubernetes plugin \u306fKubernetes DNS-Based Service Discovery Specification.\u306e\u5b9f\u88c5\u3067\u3059\u3002 <code>Corefile</code> \u3067kubernetes plugin\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u5229\u7528\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u8a2d\u5b9a\u306f kubernetes/coredns.yaml.sed \u306e\u5185\u5bb9\u3067\u3059\u3002<code>fallthrough</code> \u3068\u306fNXDOMAIN(\u4e0d\u5728\u5fdc\u7b54)\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408\u306b\u51e6\u7406\u3092\u4e0b\u6d41\u306ePlugin\u306b\u6e21\u3057\u3066\u304f\u308c\u307e\u3059(\u3053\u306e\u8a2d\u5b9a\u3067\u306f\u9006\u5f15\u304d\u3067NXDOMAIN\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408)\u3002</p> <pre><code>.:53 {\n\n  kubernetes cluster.local in-addr.arpa ip6.arpa {\n    fallthrough in-addr.arpa ip6.arpa\n  }\n\n}\n</code></pre>"},{"location":"setup/09_coredns/bootstrapping_coredns/#_1","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://coredns.io/</li> <li>https://github.com/coredns/coredns</li> <li>https://github.com/coredns/deployment</li> <li>https://github.com/coredns/helm</li> <li>https://engineer.retty.me/entry/2020/12/15/161544</li> <li>https://www.netone.co.jp/knowledge-center/netone-blog/20191226-1/</li> <li>https://www.scsk.jp/sp/sysdig/blog/container_monitoring/coredns.html</li> </ul>"},{"location":"setup/09_coredns/bootstrapping_coredns/#_2","title":"\u624b\u9806","text":"<ol> <li>coredns k8s manifests\u3092\u516c\u5f0f\u304b\u3089\u53d6\u5f97\u3059\u308b     <pre><code>git clone https://github.com/coredns/deployment.git coredns_deployment\ncd coredns_deployment/kubernetes/\n\nbash deploy.sh -i 10.32.0.10  -s -t coredns.yaml.sed | kubectl apply -f -\n</code></pre></li> </ol>"},{"location":"setup/09_coredns/bootstrapping_coredns/#coredns_1","title":"CoreDNS\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u53d6\u5f97\u306b\u3064\u3044\u3066","text":"<ul> <li><code>Corefile</code> \u3067\u4ee5\u4e0b\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u304a\u304f\u3053\u3068\u3067Prometheus\u7528\u306b <code>9153/TCP</code> \u3067\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u516c\u958b\u3067\u304d\u307e\u3059    <pre><code>.:53 {\n\n  prometheus :9153\n\n}\n</code></pre></li> </ul>"},{"location":"setup/09_coredns/bootstrapping_coredns/#_3","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":"<ol> <li> <p><code>kubectl get pods -n kube-system</code> \u3067\u3044\u3064\u307e\u3067\u7d4c\u3063\u3066\u3082<code>ContainerCreating</code> \u306e\u307e\u307e</p> <ul> <li><code>kubectl describe pod -n kube-system &lt;POD_ID&gt;</code> <pre><code>Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"&lt;CONTAINER_ID&gt;\" network for pod \"&lt;POD_ID&gt;\": networkPlugin cni failed to set up pod \"&lt;&lt;POD_NAME&gt;\" network: failed to Statfs \"/proc/15875/ns/net\": no such file or directory, failed to clean up sandbox container \"&lt;CONTAINER_ID&gt;\" network for pod \"&lt;POD_ID&gt;\": networkPlugin cni failed to teardown pod \"&lt;POD_NAME&gt;\" network: neither iptables nor ip6tables usable]\n</code></pre></li> <li>controller-manager<ul> <li>https://github.com/kubernetes/kubernetes/blob/v1.20.2/pkg/controller/endpointslice/utils.go#L407-L415 <pre><code>couldn't find ipfamilies for headless service: kube-system/kube-dns. This could happen if controller manager is connected to an old apiserver that does not support ip families yet. EndpointSlices for this Service will use IPv4 as the IP Family based on familyOf(ClusterIP:10.32.0.10).\n</code></pre> <pre><code>$ kubectl get service -n kube-system -o jsonpath='{.items[*].spec.clusterIP}'\n10.32.0.10\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>cni0(flannel)\u306e\u8d77\u52d5\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b     <pre><code>failed to set bridge addr: \"cni0\" already has an IP address different from 10.200.1.1/24\n</code></pre></p> </li> <li> <p>\u540d\u524d\u89e3\u6c7a\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b</p> <ul> <li>https://coredns.io/plugins/loop/#troubleshooting <pre><code>[FATAL] plugin/loop: Loop (127.0.0.1:36286 -&gt; :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 1048258276942848743.906062863108256161.\"\n</code></pre></li> </ul> </li> <li> <p><code>i/o timeout</code></p> <pre><code>[ERROR] plugin/errors: 2 1233258971421873826.4416823678189275919. HINFO: read udp 10.200.0.5:35249-&gt;8.8.4.4:53: i/o timeout\n</code></pre> <ul> <li> <p>Pod\u306e\u30b3\u30f3\u30c6\u30ca\u304b\u3089\u540d\u524d\u89e3\u6c7a\u304c\u3067\u304d\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b</p> <ul> <li><code>/etc/resolv.conf</code> \u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u3044\u304b\u78ba\u8a8d\u3059\u308b</li> </ul> <pre><code>kubectl run nginx --image=nginx\nPOD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl exec -it $POD_NAME -- bash\n\ncat /etc/resolv.conf\n\napt-get update              # \u5916\u306b\u3059\u3089\u51fa\u308c\u306a\u3044\u5834\u5408\u306f\u5931\u6557\u3059\u308b\napt-get install dnsutils\nnslookup kubernetes\n</code></pre> </li> <li> <p>kube-apiserver \u3078\u306e\u758e\u901a\u304c\u3067\u304d\u3066\u3044\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b</p> <ul> <li>kube-proxy -&gt; flannel \u306e\u9806\u3067pod\u306e\u518d\u8d77\u52d5\u3092\u884c\u3063\u3066\u307f\u308b</li> </ul> </li> </ul> </li> </ol>"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/","title":"bootstrapping nginx ingress controller","text":""},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/</li> <li>https://kubernetes.github.io/ingress-nginx/</li> <li>https://github.com/nginxinc/kubernetes-ingress</li> </ul>"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_2","title":"\u624b\u9806","text":""},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_3","title":"\u69cb\u7bc9","text":"<ul> <li>https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters</li> </ul>"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_4","title":"\u52d5\u4f5c\u78ba\u8a8d","text":""},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#nginx-ingressclass","title":"nginx IngressClass \u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d","text":"<pre><code>$ kubectl describe ingressclasses nginx\nName:         nginx\nLabels:       app.kubernetes.io/component=controller\n              app.kubernetes.io/instance=ingress-nginx\n              app.kubernetes.io/name=ingress-nginx\n              app.kubernetes.io/part-of=ingress-nginx\n              app.kubernetes.io/version=1.3.0\nAnnotations:  &lt;none&gt;\nController:   k8s.io/ingress-nginx\nEvents:       &lt;none&gt;\n</code></pre>"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#ingressworker-nodeip","title":"Ingress\u3092\u4f5c\u6210\u3057\u3066worker node\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d","text":"<ol> <li> <p>manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210</p> <p>/etc/kubernetes/manifests/04_nginx_ingress_controller.yaml <pre><code>sudo tee /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml &lt;&lt; EOF &gt; /dev/null\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  type: NodePort\n  ports:\n    - name: node-port\n      protocol: TCP\n      port: 8080\n      targetPort: 80\n      nodePort: 30011\n  selector:\n    app: nginx\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx-test-ingress\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: &lt;Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN&gt;\nspec:\n  ingressClassName: nginx\n  defaultBackend:\n    service:\n      name: nginx-service\n      port:\n        number: 8080\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-service\n            port:\n              number: 8080\n  # external-dns.alpha.kubernetes.io/hostname annotations\u3092\u6307\u5b9a\u305b\u305arule\u6bce\u306b\u8a2d\u5b9a\u3059\u308b\u4f8b\n  # - host: &lt;Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN&gt;\n  #   http:\n  #     paths:\n  #       - path: /\n  #         pathType: Prefix\n  #         backend:\n  #           service:\n  #             name: nginx-service\n  #             port:\n  #               number: 8080\nEOF\n</code></pre> </p> </li> <li> <p><code>&lt;Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN&gt;</code> \u306e\u7b87\u6240\u3092\u4fee\u6b63\u3059\u308b     <pre><code>sudo vim /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml\n</code></pre></p> </li> <li> <p>\u30ea\u30bd\u30fc\u30b9\u4f5c\u6210     <pre><code>kubectl apply -f /etc.kubernetes/manifests/04_nginx_ingress_controller.yaml\n</code></pre></p> </li> <li> <p>service\u3067node port(30011/TCP) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>$ kubectl describe services nginx-service\nName:                     nginx-service\nNamespace:                default\nLabels:                   &lt;none&gt;\nAnnotations:              &lt;none&gt;\nSelector:                 app=nginx\nType:                     NodePort\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.32.0.145\nIPs:                      10.32.0.145\nPort:                     node-port  8080/TCP\nTargetPort:               80/TCP\nNodePort:                 node-port  30011/TCP\nEndpoints:                10.200.1.184:80\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n</code></pre></p> </li> <li> <p>ingress\u3067worker node\u306eIP\u30a2\u30c9\u30ec\u30b9(wlan0: <code>192.168.10.51</code>) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>$ kubectl describe ingresses nginx-test-ingress\nName:             nginx-test-ingress\nNamespace:        default\nAddress:          192.168.10.51\nDefault backend:  nginx-service:8080 (10.200.1.184:80)\nRules:\n  Host        Path  Backends\n  ----        ----  --------\n  *\n              /   nginx-service:8080 (10.200.1.184:80)\nAnnotations:  &lt;none&gt;\nEvents:\n  Type    Reason  Age                  From                      Message\n  ----    ------  ----                 ----                      -------\n  Normal  Sync    5m39s (x7 over 25h)  nginx-ingress-controller  Scheduled for sync\n</code></pre></p> </li> <li> <p>MacBookPro\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u3067curl\u306b\u3066\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>$ curl --include http://192.168.10.51:30011/\nHTTP/1.1 200 OK\nDate: Tue, 21 Sep 2021 16:35:07 GMT\nContent-Type: text/html\nContent-Length: 612\nConnection: keep-alive\nLast-Modified: Tue, 04 Dec 2018 14:44:49 GMT\nETag: \"5c0692e1-264\"\nAccept-Ranges: bytes\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> </li> </ol>"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_5","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":"<ul> <li> <p><code>Error when getting IngressClass nginx: the server could not find the requested resource</code></p> <ul> <li>https://github.com/nginxinc/kubernetes-ingress/issues/1906</li> <li>https://qiita.com/smallpalace/items/7a6844651d1d7b43b411<ul> <li>https://github.com/kubernetes/ingress-nginx/issues/7448</li> <li>https://github.com/kubernetes/ingress-nginx/blob/3c0bfc1ca3eb48246b12e77d40bde1162633efae/deploy/static/provider/baremetal/deploy.yaml</li> </ul> </li> </ul> </li> <li> <p><code>error validating data: ValidationError</code></p> <ul> <li><code>--validate=false</code> \u3092\u4ed8\u52a0\u3059\u308b</li> <li>ValidatingWebhookConfiguration \u3092\u524a\u9664\u3059\u308b (issue comment)</li> </ul> </li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/","title":"bootstrapping external-dns","text":""},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_1","title":"\u53c2\u8003\u60c5\u5831","text":"<ul> <li>https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/integrations/external_dns/</li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#kubernetes-122","title":"kubernetes 1.22 \u5bfe\u5fdc\u72b6\u6cc1\u306b\u3064\u3044\u3066","text":"<ul> <li>v0.10.0 \u3067\u5bfe\u5fdc\u6e08\u307f</li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#aws-route53external-dns-controller","title":"\u30aa\u30f3\u30d7\u30ec\u3067AWS Route53\u5411\u3051external-dns controller\u306e\u8d77\u52d5\u65b9\u6cd5\u306b\u3064\u3044\u3066","text":"<ul> <li>https://stackoverflow.com/questions/60267737/is-it-possible-to-use-aws-route-53-as-a-dns-provider-for-a-bare-metal-k8s-cluste</li> <li>https://github.com/kubernetes-sigs/external-dns/issues/539</li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#wan-ipdns-provider","title":"\u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306eWAN IP\u30a2\u30c9\u30ec\u30b9\u3092DNS Provider\u306b\u901a\u77e5\u3057\u305f\u3044","text":"<ul> <li>https://github.com/kubernetes-sigs/external-dns/issues/1394</li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#public-iproute53","title":"\u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306b\u304a\u3051\u308bPublic IP\u3092\u6255\u3044\u51fa\u3057\u3064\u3064route53\u3078\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3059\u308b","text":"<ul> <li>k8s cluster\u5185\u90e8\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u306f\u306a\u304f\u3001worker node\u306einterface(wlan0)\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u308bIP\u30a2\u30c9\u30ec\u30b9(\u4fbf\u5b9c\u7684\u306bpublic ip\u3068\u4eee\u5b9a)\u3068\u540c\u3058\u30ec\u30f3\u30b8\u3067IP\u30a2\u30c9\u30ec\u30b9\u3092\u6255\u3044\u51fa\u3059\u3088\u3046\u306a\u69cb\u6210<ul> <li>\u3053\u308c\u306f\u5c06\u6765\u7528</li> <li>metallb \u3067k8s cluster\u306e\u5916\u5074\u306bLoadBalancer\u3092\u4f5c\u6210\u3057Public IP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b</li> <li>https://blog.web-apps.tech/type-loadbalancer_by_metallb/</li> </ul> </li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_2","title":"\u69cb\u7bc9\u624b\u9806","text":""},{"location":"setup/11_external_dns/bootstrapping_external_dns/#1-aws-iam-policy","title":"1. AWS IAM Policy\u4f5c\u6210","text":"<ol> <li> <p><code>external-dns-controller-policy-document.json</code> \u3092\u4f5c\u6210</p> <p>external-dns-controller-policy-document.json</p> <pre><code>```\nsudo tee external-dns-controller-policy-document.json &lt;&lt; EOF &gt; /dev/null\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\nEOF\n```\n</code></pre> </li> <li> <p>policy\u3092\u4f5c\u6210     <pre><code>aws iam create-policy --policy-name k8s-external-dns-policy  --policy-document file://external-dns-controller-policy-document.json\n</code></pre></p> </li> </ol>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#2-aws-iam-user","title":"2. AWS IAM User\u4f5c\u6210","text":"<ol> <li> <p>user\u3092\u4f5c\u6210     <pre><code>aws iam create-user --user-name k8s-external-dns\n</code></pre></p> </li> <li> <p>\u4f5c\u6210\u3057\u305fIAM Policy\u3092\u30a2\u30bf\u30c3\u30c1\u3059\u308b     <pre><code>aws iam attach-user-policy --user-name k8s-external-dns --policy-arn arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:policy/k8s-external-dns-policy\n</code></pre></p> </li> <li> <p>\u4f5c\u6210\u3057\u305fIAM User\u306ecredential\u3092\u78ba\u8a8d\u3059\u308b(Deployments\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u74b0\u5883\u5909\u6570\u3068\u3057\u3066\u30bb\u30c3\u30c8\u3059\u308b)</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> </ul> </li> </ol>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#3-external-dns-controller","title":"3. external-dns controller\u3092\u30c7\u30d7\u30ed\u30a4","text":"<ul> <li> <p>point</p> <ul> <li>https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md \u3092\u30d9\u30fc\u30b9\u306bbare-metal\u5411\u3051\u306b\u4fee\u6b63</li> <li>namespace\u306f <code>kube-system</code></li> <li>aws credential\u306f <code>k8s-external-dns</code> iam user\u306e\u3082\u306e</li> <li>hosted_zone_id \u306fexternal-dns\u3067\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3055\u305b\u305f\u3044Route53 zone</li> </ul> </li> <li> <p>manifests\u306b\u4ee3\u5165\u3059\u308b\u5909\u6570\u3092\u5b9a\u7fa9</p> variable name description <code>DOMAIN</code> external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306e\u30c9\u30e1\u30a4\u30f3 <code>HOSTED_ZONE_ID</code> external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306eHosted Zone ID <code>AWS_ACCESS_KEY_ID</code> external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 <code>AWS_SECRET_ACCESS_KEY</code> external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 <code>AWS_DEFAULT_REGION</code> external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 <pre><code>DOMAIN=\"XXXXXXX.com\"\nHOSTED_ZONE_ID=\"XXXXXXX\"\nAWS_ACCESS_KEY_ID=\"XXXXXXX\"\nAWS_SECRET_ACCESS_KEY=\"XXXXXXX\"\nAWS_DEFAULT_REGION=\"XXXXXXX\"\n</code></pre> </li> <li> <p>manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210     /etc/kubernetes/manifests/external-dns.yaml</p> <pre><code>```\nAWS_DEFAULT_REGION=\"ap-north-east-1\"\n\nsudo tee /etc/kubernetes/manifests/external-dns.yaml &lt;&lt;  EOF &gt; /dev/null\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: kube-system\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: kube-system\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: k8s.gcr.io/external-dns/external-dns:v0.10.0\n        env:\n        - name: AWS_ACCESS_KEY_ID\n          value: &lt;k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_ACCESS_KEY_ID&gt;\n        - name: AWS_SECRET_ACCESS_KEY\n          value:  &lt;k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_SECRET_ACCESS_KEY&gt;\n        - name: AWS_DEFAULT_REGION\n          value: \"${AWS_DEFAULT_REGION}\"\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=${DOMAIN} # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=aws\n        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=&lt;HOSTED_ZONE_ID&gt;\n        - --txt-prefix=prefix_\n        - --log-level=debug\n      securityContext:\n        fsGroup: 65534 # For ExternalDNS to be able to read Kubernetes and AWS token files\nEOF\n```\n</code></pre> </li> <li> <p>\u30c7\u30d7\u30ed\u30a4     <pre><code>kubectl apply -f /etc/kubernetes/manifests/external-dns.yaml\n</code></pre></p> </li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_3","title":"\u52d5\u4f5c\u78ba\u8a8d","text":"<ul> <li> <p>external-dns \u30b3\u30f3\u30c6\u30ca\u30ed\u30b0</p> <ul> <li>\u30c7\u30d7\u30ed\u30a4\u6e08\u307fingress\u306ehostname\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3092\u78ba\u8a8d     <pre><code>time=\"2021-09-26T04:59:24Z\" level=info msg=\"Instantiating new Kubernetes client\"\ntime=\"2021-09-26T04:59:24Z\" level=debug msg=\"apiServerURL: \"\ntime=\"2021-09-26T04:59:24Z\" level=debug msg=\"kubeConfig: \"\ntime=\"2021-09-26T04:59:24Z\" level=info msg=\"Using inCluster-config based on serviceaccount-token\"\ntime=\"2021-09-26T04:59:24Z\" level=info msg=\"Created Kubernetes client https://10.32.0.1:443\"\ntime=\"2021-09-26T04:59:30Z\" level=debug msg=\"Refreshing zones list cache\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/&lt;HOSTED_ZONE_ID&gt; (domain: example.com.)\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/kube-dns\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/metrics-server\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/kubernetes\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/nginx-service\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller-admission\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN A  192.168.10.51 []]\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/&lt;HOSTED_ZONE_ID&gt; (domain: example.com.)\"\ntime=\"2021-09-26T04:59:31Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/&lt;HOSTED_ZONE_ID&gt; (domain: example.com.)\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com A [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com TXT [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-09-26T04:59:32Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;] were successfully updated\"\n</code></pre></li> </ul> </li> <li> <p>route53 record set</p> <ul> <li>\u5bfe\u8c61\u306ehosted zone\u306bingress\u306ehost\u3067\u6307\u5b9a\u3057\u305fhostname\u3067\u30ec\u30b3\u30fc\u30c9\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>$ aws route53 list-resource-record-sets --output json --hosted-zone-id &lt;HOSTED_ZONE_ID&gt; | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\"))'\n[\n  {\n    \"Name\": \"dev1.example.com.\",\n    \"Type\": \"A\",\n    \"TTL\": 300,\n    \"ResourceRecords\": [\n      {\n        \"Value\": \"192.168.10.51\"\n      }\n    ]\n  },\n  {\n    \"Name\": \"dev1.example.com.\",\n    \"Type\": \"TXT\",\n    \"TTL\": 300,\n    \"ResourceRecords\": [\n      {\n        \"Value\": \"\\\"heritage=external-dns,external-dns/owner=&lt;HOSTED_ZONE_ID&gt;,external-dns/resource=ingress/default/nginx-test-ingress\\\"\"\n      }\n    ]\n  }\n]\n</code></pre></li> </ul> </li> <li> <p>\u767b\u9332\u3055\u308c\u305fA\u30ec\u30b3\u30fc\u30c9\u306ehostname\u304c\u540d\u524d\u89e3\u6c7a\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> <ul> <li>k8s service\u30ea\u30bd\u30fc\u30b9\u304b\u3089\u898b\u308b\u3068node port\u306b\u5bfe\u3059\u308bnode address\u306f\u81ea\u5b85\u74b0\u5883\u306eWifi\u30eb\u30fc\u30bf\u3067\u6255\u3044\u51fa\u3059\u30ec\u30f3\u30b8(192.168.10.0/24)\u306a\u306e\u3067\u60f3\u5b9a\u901a\u308a     <pre><code>$ dig +noall +answer dev1.example.com\ndev1.example.com.      283     IN      A       192.168.10.51\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#appendix","title":"Appendix","text":""},{"location":"setup/11_external_dns/bootstrapping_external_dns/#ingressannotations","title":"Ingress\u30ea\u30bd\u30fc\u30b9\u3067\u4f7f\u7528\u53ef\u80fd\u306aannotations","text":"<ul> <li> <p>https://github.com/kubernetes-sigs/external-dns/blob/v0.10.0/source/source.go#L40-L68</p> annotations describe <code>external-dns.alpha.kubernetes.io/controller</code> \u8907\u6570\u306eDNS Controller\u304c\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u3069\u306eDNS Controller\u304c\u8cac\u4efb\u3092\u8ca0\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u628a\u63e1\u3059\u308b\u305f\u3081\u306b\u6307\u5b9a\u3059\u308b <code>external-dns.alpha.kubernetes.io/hostname</code> \u4f7f\u7528\u3059\u308b\u30db\u30b9\u30c8\u540d\u3092\u6307\u5b9a\u3059\u308bService\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u3067\u6307\u5b9a\u3059\u308bIngress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u304brule\u306ehost attr\u3067\u6307\u5b9a\u3059\u308b <code>external-dns.alpha.kubernetes.io/access</code> \u30d1\u30d6\u30ea\u30c3\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u3068\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u306e\u3069\u3061\u3089\u3092\u4f7f\u7528\u3059\u308b\u304b\u3092\u6307\u5b9a\u3059\u308b <code>external-dns.alpha.kubernetes.io/target</code> CNAME record\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\u306bCNAME record\u306evalue\u3068\u306a\u308b\u5024\u3092\u6307\u5b9a\u3059\u308b <code>external-dns.alpha.kubernetes.io/ttl</code> DNS record\u306eTTL\u3092\u6307\u5b9a\u3059\u308b(default: 300) <code>external-dns.alpha.kubernetes.io/alias</code> <code>true</code> \u3067ALIAS record\u3092\u4f5c\u6210\u3059\u308b <code>external-dns.alpha.kubernetes.io/ingress-hostname-source</code> Ingress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306bhostname\u306e\u6307\u5b9a\u65b9\u6cd5\u3092annotations\u304brule\u306ehost attr\u304b\u3092\u9650\u5b9a\u3067\u304d\u308b <code>external-dns.alpha.kubernetes.io/internal-hostname</code> Target IP\u30a2\u30c9\u30ec\u30b9\u3092Cluster IP\u30a2\u30c9\u30ec\u30b9\u3068\u3059\u308b\u5834\u5408\u306b\u6307\u5b9a\u3059\u308b <code>external-dns.alpha.kubernetes.io/set-identifier</code> AWS Route53\u306b\u304a\u3044\u3066DNS Name\u3068Type\u304c\u540c\u3058\u5834\u5408\u306b\u91cd\u307f\u4ed8\u3051\u306a\u3069\u306b\u3088\u308b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30dd\u30ea\u30b7\u30fc\u3092\u5b9a\u7fa9\u3059\u308b\u969b\u306e\u8b58\u5225\u5b50Record ID\u3068\u306a\u308b\u5024 </li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#external-dnsacname","title":"external-dns\u3067A\u30ec\u30b3\u30fc\u30c9\u3067\u306f\u306a\u304fCNAME\u30ec\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3059\u308b","text":"<ol> <li> <p>controller\u306e\u8d77\u52d5\u5f15\u6570\u306b <code>--txt-prefix=&lt;prefix\u6587\u5b57\u5217&gt;</code> \u3092\u8ffd\u52a0</p> <ul> <li>\u6307\u5b9a\u3057\u305f\u6587\u5b57\u5217\u304cTXT record\u306e\u30ec\u30b3\u30fc\u30c9\u540d(A\u30ec\u30b3\u30fc\u30c9\u306e\u5834\u5408\u306fA\u30ec\u30b3\u30fc\u30c9\u3068\u540c\u540d)\u306eprefix\u3068\u3057\u3066\u8ffd\u52a0\u3055\u308c\u307e\u3059</li> <li>CNAME\u30ec\u30b3\u30fc\u30c9\u306f(TXT\u30ec\u30b3\u30fc\u30c9\u3067\u3042\u3063\u3066\u3082)\u4ed6\u306e\u30ec\u30b3\u30fc\u30c9\u3068\u5171\u5b58\u3067\u304d\u306a\u3044\u4ed5\u69d8\u3067\u3059(RFC 1034\u30bb\u30af\u30b7\u30e7\u30f33.6.2\u3001RFC 1912\u30bb\u30af\u30b7\u30e7\u30f32.4</li> <li>https://github.com/kubernetes-sigs/external-dns/blob/master/docs/faq.md#im-using-an-elb-with-txt-registry-but-the-cname-record-clashes-with-the-txt-record-how-to-avoid-this</li> </ul> </li> <li> <p>Ingress\u30ea\u30bd\u30fc\u30b9\u306eannotations\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b</p> <ul> <li><code>external-dns.alpha.kubernetes.io/hostname</code> \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eFQDN\u3092\u6307\u5b9a</li> <li> <p><code>external-dns.alpha.kubernetes.io/target</code> \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eValue\u3068\u306a\u308b\u53c2\u7167\u5148\u306eFQDN\u307e\u305f\u306fIP\u30a2\u30c9\u30ec\u30b9\u306a\u3069\u6307\u5b9a</p> <pre><code>external-dns.alpha.kubernetes.io/hostname: dev1.example.com\nexternal-dns.alpha.kubernetes.io/target: alias1.example.com\n</code></pre> </li> </ul> </li> <li> <p><code>--txt-prefix=prefix_</code> \u3067\u52d5\u4f5c\u78ba\u8a8d</p> <ol> <li> <p>Ingress\u30ea\u30bd\u30fc\u30b9     </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx-test-ingress\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: dev1.example.com\n    external-dns.alpha.kubernetes.io/target: alias1.example.com\nspec:\n  ingressClassName: nginx\n  defaultBackend:\n    service:\n      name: nginx-service\n      port:\n        number: 8080\n  rules:\n    - host: dev1.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: nginx-service\n                port:\n                  number: 8080\n</code></pre> </li> <li> <p>external-dns log     </p> <pre><code>time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN CNAME  alias1.example.com [] dev1.example.com 0 IN CNAME  alias1.example.com []]\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Removing duplicate endpoint dev1.example.com 0 IN CNAME  alias1.example.com []\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/&lt;HOSTED_ZONE_ID&gt; (domain: example.com.)\"\ntime=\"2021-10-01T09:05:18Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/&lt;HOSTED_ZONE_ID&gt; (domain: example.com.)\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding prefix_dev1.example.com. to zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE dev1.example.com CNAME [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE prefix_dev1.example.com TXT [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-10-01T09:05:19Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;] were successfully updated\"\n</code></pre> </li> <li> <p>route53 \u30ec\u30b3\u30fc\u30c9\u78ba\u8a8d     </p> <pre><code>$ aws route53 list-resource-record-sets --output json --hosted-zone-id &lt;HOSTED_ZONE_ID&gt; | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\" or .Name == \"prefix_dev1.example.com.\"))'\n[\n  {\n    \"Name\": \"prefix_dev1.example.com.\",\n    \"Type\": \"TXT\",\n    \"TTL\": 300,\n    \"ResourceRecords\": [\n      {\n        \"Value\": \"\\\"heritage=external-dns,external-dns/owner=&lt;HOSTED_ZONE_ID&gt;,external-dns/resource=ingress/default/nginx-test-ingress\\\"\"\n      }\n    ]\n  },\n  {\n    \"Name\": \"dev1.example.com.\",\n    \"Type\": \"CNAME\",\n    \"TTL\": 300,\n    \"ResourceRecords\": [\n      {\n        \"Value\": \"alias1.example.com\"\n      }\n    ]\n  }\n]\n\n$ dig +noall +answer dev1.example.com\ndev1.example.com.      300     IN      CNAME   alias1.example.com.\n</code></pre> </li> </ol> </li> </ol>"},{"location":"static_pod/","title":"Index","text":"<p>8\u6708 31 12:57:50 k8s-master kubelet[1912]: E0831 12:57:50.411795    1912 kubelet.go:1635] Failed creating a mirror pod for \"etcd-k8s-master_kube-system(8fff3e1f31b52ebeb520767ae50cc739)\": pods \"etcd-k8s-master\" is forbidden: PodSecurityPolicy: no providers available to validate pod request</p> <p>8\u6708 31 12:58:55 k8s-master kubelet[1912]: I0831 12:58:55.446865    1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-apiserver-k8s-master_kube-system(b2f9759c-9ffe-436c-9ebb-0b5f9c68a452)\" 8\u6708 31 12:58:58 k8s-master kubelet[1912]: I0831 12:58:58.433779    1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-scheduler-k8s-master_kube-system(b502a669-2571-41f7-a705-3aa194ade526)\" 8\u6708 31 12:59:00 k8s-master kubelet[1912]: I0831 12:59:00.432199    1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-controller-manager-k8s-master_kube-system(f834a62b-d802-40c5-90df-24a49a38efb6)\" 8\u6708 31 12:59:11 k8s-master kubelet[1912]: I0831 12:59:11.421459    1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"etcd-k8s-master_kube-system(e612e1ab-a907-4955-8b4c-97d5a37b11fa)\" 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849447    1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/etcd-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849581    1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-apiserver-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849654    1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-controller-manager-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849719    1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-scheduler-k8s-master\" status=Running</p>"}]}