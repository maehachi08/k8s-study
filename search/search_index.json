{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"addons/metallb/","text":"About MetalLB https://metallb.universe.tf/ https://github.com/metallb/metallb Bare Metal\u306a\u74b0\u5883\u3067LoadBlancer\u30b5\u30fc\u30d3\u30b9\u3092\u63d0\u4f9b\u3059\u308baddon\u3067\u3059\u3002 CloudProvider\u304c\u63d0\u4f9b\u3059\u308bKubernetes\u30b5\u30fc\u30d3\u30b9\u3067\u306f\u5f53\u8a72CloudProvider\u306eLoadBlancer\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3067\u304d\u307e\u3059\u3002 AWS\u3067\u306f AWS Load Balancer Controller \u3092\u5c0e\u5165\u3059\u308b\u3053\u3068\u3067Ingress\u30ea\u30bd\u30fc\u30b9\u3067Elastic Load Balancer\u306e\u4f5c\u6210\u3092\u884c\u3046\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 Bare Metal\u306a\u74b0\u5883\u3067 Nginx Ingress Controller \u3092\u5c0e\u5165\u3057\u305f\u5834\u5408\u3001Cluster\u5185\u306ePod Network\u306eIP\u30a2\u30c9\u30ec\u30b9\u304c\u30a2\u30b5\u30a4\u30f3\u3055\u308c\u307e\u3059\u3002 \u79c1\u306fRaspberry pi(ubuntu server)\u3067Kubernetes Cluster\u3092\u69cb\u7bc9\u3057\u3066\u304a\u308a\u3001Node IP\u30a2\u30c9\u30ec\u30b9\u306f\u81ea\u5b85\u306eWiFi\u30eb\u30fc\u30bf( 192.168.3.0/24 )\u304b\u3089\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u3002 Cluster Cidr( 10.200.0.0/16 )\u3067\u306fMacBook\u306a\u3069Cluster\u5916\u306e\u30d6\u30e9\u30a6\u30b6\u30a2\u30af\u30bb\u30b9\u304c\u3067\u304d\u305a\u3001(NAT\u5909\u63db\u306a\u3069\u3092\u99c6\u4f7f\u3059\u308c\u3070\u53ef\u80fd\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c) \u5c11\u3005\u4e0d\u4fbf\u3067\u3059\u3002 MetalLB\u3067\u306f Service \u30ea\u30bd\u30fc\u30b9\u3067 type: LoadBalancer \u3092\u6307\u5b9a\u53ef\u80fd\u3068\u3057\u3001\u304b\u3064 192.168.3.200/27 \u306a\u3069Kubernetes Cluster\u306e\u5916\u90e8\u306b\u516c\u958b\u53ef\u80fd\u306aIP\u30a2\u30c9\u30ec\u30b9\u306e\u5272\u308a\u5f53\u3066\u304c\u53ef\u80fd\u3067\u3059\u3002 \u53c2\u8003 https://blog.framinal.life/entry/2020/04/16/022042 https://garafu.blogspot.com/2019/06/install-metallb.html Installation https://metallb.universe.tf/installation/ Configuration https://metallb.universe.tf/configuration/ IPAddressPool type: LoadBalancer \u3092\u6307\u5b9a\u3057\u305fService\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306e\u30d7\u30fc\u30eb\u3092\u5b9a\u7fa9 L2Advertisement IP\u30a2\u30c9\u30ec\u30b9\u306eAdvertisement\u3092\u884c\u3046k8s node\u3092\u6307\u5b9a\u3059\u308b https://metallb.universe.tf/configuration/_advanced_l2_configuration/ manifests --- apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: ip-pool namespace: metallb-system spec: addresses: - 192.168.3.200-192.168.3.210 --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: ip-pool-advertisement namespace: metallb-system spec: ipAddressPools: - ip-pool nodeSelectors: - matchLabels: kubernetes.io/hostname: k8s-master - matchLabels: kubernetes.io/hostname: k8s-node1 - matchLabels: kubernetes.io/hostname: k8s-node2 Kubernetes Dashboard \u3092MetalLB\u3067\u6255\u3044\u51fa\u3057\u305fIP\u30a2\u30c9\u30ec\u30b9\u3067\u30a2\u30af\u30bb\u30b9\u3057\u3066\u307f\u308b edit of kubernetes-dashboard manifests Service\u30ea\u30bd\u30fc\u30b9\u306eType\u3092 LoadBalancer \u306b\u5909\u66f4 metallb.universe.tf/address-pool annotations\u3092\u8ffd\u8a18 @@ -36,7 +36,10 @@ k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard + annotations: + metallb.universe.tf/address-pool: ip-pool spec: + type: LoadBalancer ports: - port: 443 targetPort: 8443 apply kubernetes-dashboard manifests kubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard.yaml check service and ingress service $ kubectl describe svc -n kubernetes-dashboard kubernetes-dashboard Name: kubernetes-dashboard Namespace: kubernetes-dashboard Labels: k8s-app=kubernetes-dashboard Annotations: metallb.universe.tf/address-pool: ip-pool Selector: k8s-app=kubernetes-dashboard Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.32.0.177 IPs: 10.32.0.177 LoadBalancer Ingress: 192.168.3.201 Port: <unset> 443/TCP TargetPort: 8443/TCP NodePort: <unset> 30522/TCP Endpoints: 10.200.2.78:8443 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 53m metallb-controller Assigned IP [\"192.168.3.201\"] Normal nodeAssigned 50s (x34 over 53m) metallb-speaker announcing from node \"k8s-master\" with protocol \"layer2\" ingress $ kubectl describe ingress -n kubernetes-dashboard dashboard-ingress Name: dashboard-ingress Labels: <none> Namespace: kubernetes-dashboard Address: 192.168.3.200 Ingress Class: <none> Default backend: <default> TLS: dashboard-secret-tls terminates k8s-dashboard.local Rules: Host Path Backends ---- ---- -------- k8s-dashboard.local / kubernetes-dashboard:443 (10.200.2.78:8443) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/backend-protocol: HTTPS nginx.ingress.kubernetes.io/ssl-passthrough: true Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 53m (x2 over 54m) nginx-ingress-controller Scheduled for sync Service\u306b\u5272\u308a\u5f53\u3066\u3089\u308c\u3066\u3044\u308b 192.168.3.201 \u3067\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d NodePort\u306a\u3069\u306e\u6307\u5b9a\u306f\u4e0d\u8981\u306a\u306e\u3067\u30a2\u30af\u30bb\u30b9\u304c\u624b\u8efd","title":"MetalLB"},{"location":"addons/metallb/#about-metallb","text":"https://metallb.universe.tf/ https://github.com/metallb/metallb Bare Metal\u306a\u74b0\u5883\u3067LoadBlancer\u30b5\u30fc\u30d3\u30b9\u3092\u63d0\u4f9b\u3059\u308baddon\u3067\u3059\u3002 CloudProvider\u304c\u63d0\u4f9b\u3059\u308bKubernetes\u30b5\u30fc\u30d3\u30b9\u3067\u306f\u5f53\u8a72CloudProvider\u306eLoadBlancer\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3067\u304d\u307e\u3059\u3002 AWS\u3067\u306f AWS Load Balancer Controller \u3092\u5c0e\u5165\u3059\u308b\u3053\u3068\u3067Ingress\u30ea\u30bd\u30fc\u30b9\u3067Elastic Load Balancer\u306e\u4f5c\u6210\u3092\u884c\u3046\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 Bare Metal\u306a\u74b0\u5883\u3067 Nginx Ingress Controller \u3092\u5c0e\u5165\u3057\u305f\u5834\u5408\u3001Cluster\u5185\u306ePod Network\u306eIP\u30a2\u30c9\u30ec\u30b9\u304c\u30a2\u30b5\u30a4\u30f3\u3055\u308c\u307e\u3059\u3002 \u79c1\u306fRaspberry pi(ubuntu server)\u3067Kubernetes Cluster\u3092\u69cb\u7bc9\u3057\u3066\u304a\u308a\u3001Node IP\u30a2\u30c9\u30ec\u30b9\u306f\u81ea\u5b85\u306eWiFi\u30eb\u30fc\u30bf( 192.168.3.0/24 )\u304b\u3089\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u3002 Cluster Cidr( 10.200.0.0/16 )\u3067\u306fMacBook\u306a\u3069Cluster\u5916\u306e\u30d6\u30e9\u30a6\u30b6\u30a2\u30af\u30bb\u30b9\u304c\u3067\u304d\u305a\u3001(NAT\u5909\u63db\u306a\u3069\u3092\u99c6\u4f7f\u3059\u308c\u3070\u53ef\u80fd\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c) \u5c11\u3005\u4e0d\u4fbf\u3067\u3059\u3002 MetalLB\u3067\u306f Service \u30ea\u30bd\u30fc\u30b9\u3067 type: LoadBalancer \u3092\u6307\u5b9a\u53ef\u80fd\u3068\u3057\u3001\u304b\u3064 192.168.3.200/27 \u306a\u3069Kubernetes Cluster\u306e\u5916\u90e8\u306b\u516c\u958b\u53ef\u80fd\u306aIP\u30a2\u30c9\u30ec\u30b9\u306e\u5272\u308a\u5f53\u3066\u304c\u53ef\u80fd\u3067\u3059\u3002","title":"About MetalLB"},{"location":"addons/metallb/#_1","text":"https://blog.framinal.life/entry/2020/04/16/022042 https://garafu.blogspot.com/2019/06/install-metallb.html","title":"\u53c2\u8003"},{"location":"addons/metallb/#installation","text":"https://metallb.universe.tf/installation/","title":"Installation"},{"location":"addons/metallb/#configuration","text":"https://metallb.universe.tf/configuration/ IPAddressPool type: LoadBalancer \u3092\u6307\u5b9a\u3057\u305fService\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306e\u30d7\u30fc\u30eb\u3092\u5b9a\u7fa9 L2Advertisement IP\u30a2\u30c9\u30ec\u30b9\u306eAdvertisement\u3092\u884c\u3046k8s node\u3092\u6307\u5b9a\u3059\u308b https://metallb.universe.tf/configuration/_advanced_l2_configuration/ manifests --- apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: ip-pool namespace: metallb-system spec: addresses: - 192.168.3.200-192.168.3.210 --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: ip-pool-advertisement namespace: metallb-system spec: ipAddressPools: - ip-pool nodeSelectors: - matchLabels: kubernetes.io/hostname: k8s-master - matchLabels: kubernetes.io/hostname: k8s-node1 - matchLabels: kubernetes.io/hostname: k8s-node2","title":"Configuration"},{"location":"addons/metallb/#kubernetes-dashboard-metallbip","text":"edit of kubernetes-dashboard manifests Service\u30ea\u30bd\u30fc\u30b9\u306eType\u3092 LoadBalancer \u306b\u5909\u66f4 metallb.universe.tf/address-pool annotations\u3092\u8ffd\u8a18 @@ -36,7 +36,10 @@ k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard + annotations: + metallb.universe.tf/address-pool: ip-pool spec: + type: LoadBalancer ports: - port: 443 targetPort: 8443 apply kubernetes-dashboard manifests kubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard.yaml check service and ingress service $ kubectl describe svc -n kubernetes-dashboard kubernetes-dashboard Name: kubernetes-dashboard Namespace: kubernetes-dashboard Labels: k8s-app=kubernetes-dashboard Annotations: metallb.universe.tf/address-pool: ip-pool Selector: k8s-app=kubernetes-dashboard Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.32.0.177 IPs: 10.32.0.177 LoadBalancer Ingress: 192.168.3.201 Port: <unset> 443/TCP TargetPort: 8443/TCP NodePort: <unset> 30522/TCP Endpoints: 10.200.2.78:8443 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 53m metallb-controller Assigned IP [\"192.168.3.201\"] Normal nodeAssigned 50s (x34 over 53m) metallb-speaker announcing from node \"k8s-master\" with protocol \"layer2\" ingress $ kubectl describe ingress -n kubernetes-dashboard dashboard-ingress Name: dashboard-ingress Labels: <none> Namespace: kubernetes-dashboard Address: 192.168.3.200 Ingress Class: <none> Default backend: <default> TLS: dashboard-secret-tls terminates k8s-dashboard.local Rules: Host Path Backends ---- ---- -------- k8s-dashboard.local / kubernetes-dashboard:443 (10.200.2.78:8443) Annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/backend-protocol: HTTPS nginx.ingress.kubernetes.io/ssl-passthrough: true Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 53m (x2 over 54m) nginx-ingress-controller Scheduled for sync Service\u306b\u5272\u308a\u5f53\u3066\u3089\u308c\u3066\u3044\u308b 192.168.3.201 \u3067\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d NodePort\u306a\u3069\u306e\u6307\u5b9a\u306f\u4e0d\u8981\u306a\u306e\u3067\u30a2\u30af\u30bb\u30b9\u304c\u624b\u8efd","title":"Kubernetes Dashboard \u3092MetalLB\u3067\u6255\u3044\u51fa\u3057\u305fIP\u30a2\u30c9\u30ec\u30b9\u3067\u30a2\u30af\u30bb\u30b9\u3057\u3066\u307f\u308b"},{"location":"addons/openebs/about/","text":"OpenEBS OpenEBS \u306f\u3001Kubernetes\u30ef\u30fc\u30ab\u30fc\u30ce\u30fc\u30c9\u304c\u5229\u7528\u3067\u304d\u308b\u3042\u3089\u3086\u308b\u30b9\u30c8\u30ec\u30fc\u30b8(\u30ed\u30fc\u30ab\u30eb\u3084\u5206\u6563\u306a\u3069) \u3092Kubernetes Persistent Volumes\u306b\u5909\u63db\u3057\u307e\u3059 Reference https://openebs.io/ https://github.com/openebs/openebs https://github.com/openebs/charts https://openebs.github.io/charts/ https://blog.openebs.io/arming-kubernetes-with-openebs-1-b450f41e0c1f https://note.com/ryoma_0923/n/n7d2837212028 https://qiita.com/ysakashita/items/8ca805cb6ac10df911be https://blog.cybozu.io/entry/2018/03/29/080000 https://www.infoq.com/jp/news/2020/08/kubernetes-storage-kubera/ OpenEBS \u306b\u3064\u3044\u3066 MayaData\u304c\u5b9f\u88c5\u3092\u516c\u958b\u3057\u3001\u73fe\u5728\u306fCNCF sandbox project\u306e Container Storage Interface (CSI) Provider\u3067\u3059 Container Attached Storage(CAS) \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u3092\u63a1\u7528\u3057\u3066\u3044\u308b Dynamic Provisioner\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u308b Kubernetes Native\u306e local PersistentVolume volume \u306fStatic Provisioning\u306e\u307f\u3092\u30b5\u30dd\u30fc\u30c8 Container Attached Storage(CAS) https://openebs.io/docs/concepts/cas https://www.cncf.io/blog/2018/04/19/container-attached-storage-a-primer/ https://www.cncf.io/blog/2020/09/22/container-attached-storage-is-cloud-native-storage-cas/ https://www.cncf.io/online-programs/kubernetes-and-storage-kubernetes-for-storage-an-overview/ https://blog.mayadata.io/container-attached-storage-cas-vs.-shared-storage-which-one-to-choose In Kubernetes, shared storage is typically achieved by mounting volumes and connecting to an external filesystem or block storage solution. Container Attached Storage (CAS) is a relatively newer solution that allows Kubernetes administrators to deploy storage as containerized microservices in a cluster. Container Attached Storage(CAS) \u3068\u306fPod\u3067\u5229\u7528\u53ef\u80fd\u306a\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u30b3\u30f3\u30c6\u30ca\u5316\u3057\u305f\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9\u3068\u3057\u3066Kubernetes Cluster\u3078\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u4ed5\u7d44\u307f\u3067\u3059\u3002 CAS \u3067\u306fPersistent Volumes\u3092\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9 \u30d9\u30fc\u30b9\u306e\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3068\u3057\u3066\u69cb\u6210\u3057\u307e\u3059\u3002\u305d\u306e\u969b\u3001\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3092\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30b9\u30c8\u30ec\u30fc\u30b8 \u30b3\u30f3\u30c8\u30ed\u30fc\u30e9\u3092\u3001\u72ec\u7acb\u3057\u3066\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u304a\u3088\u3073\u5b9f\u884c\u3067\u304d\u308b \u69cb\u6210\u30e6\u30cb\u30c3\u30c8\u3068\u3057\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002(\u3064\u307e\u308a\u3001OpenEBS \u306b\u304a\u3051\u308bPV\u306fVolume\u3092\u30db\u30b9\u30c8\u3059\u308breplica pod \u3068 replica pod\u3092\u7ba1\u7406\u3059\u308bcontroller pod\u3067\u69cb\u6210\u3055\u308c\u307e\u3059) \u4ee5\u4e0b\u306f Deploy Jenkins with OpenEBS \u3067\u4f5c\u6210\u3055\u308c\u308b\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3068\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9 $ kubectl get pods -n openebs | grep pvc-2fee1acb openebs pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-ctrl-66d449fp8jp8 2/2 Running 0 3h58m 10.200.0.39 k8s-master <none> <none> openebs pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-0 1/1 Running 1 (3h34m ago) 4h16m 10.200.2.194 k8s-node2 <none> <none> openebs pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-1 1/1 Running 1 (3h40m ago) 4h16m 10.200.0.38 k8s-master <none> <none> openebs pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-2 0/1 Pending 0 4h16m <none> <none> <none> <none> Node Disk Manager(NDM) https://openebs.io/docs/main/concepts/ndm CPU\u3084Memory\u3001Network\u306a\u3069\u3068\u540c\u3058\u3088\u3046\u306bNode\u4e0a\u306eblock device\u3092kubernetes resources(CustomResource)\u3068\u3057\u3066\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8 DeamonSet\u3068\u3057\u3066\u5404Node\u306b\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u308b Node\u4e0a\u306eblock device\u3078\u306e\u30a2\u30af\u30bb\u30b9\u3059\u308b\u306e\u306b /dev , /proc , /sys \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u304c\u5fc5\u8981\u306a\u305f\u3081Privileged mode\u3067\u52d5\u4f5c\u3059\u308b Local PV \u3068 cStor PV \u3067\u4f7f\u7528\u3055\u308c\u308b JIVA PV \u306fNDM\u3078\u30a2\u30af\u30bb\u30b9\u3057\u3066\u3044\u306a\u3044? https://openebs.io/docs/main/user-guides/ndm \u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3 https://openebs.io/docs/main/concepts/casengines Storage Engine Status Description link Local PV Beta \u5358\u4e00\u30ce\u30fc\u30c9\u3067\u5229\u7528\u53ef\u80fd\u306aVolume\u3092\u63d0\u4f9b Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8(Kubernetes Native\u306e local PersistentVolume volume \u306fStatic Provisioning(\u4e8b\u524d\u306ePV\u3092\u624b\u52d5\u4f5c\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b)) https://openebs.io/docs/concepts/localpv https://openebs.io/docs/main/user-guides/localpv-device JIVA PV Stable \u8907\u6570\u30ce\u30fc\u30c9\u3067\u30ec\u30d7\u30ea\u30ab\u3092\u69cb\u6210\u3057\u305fVolume\u3092\u63d0\u4f9b(\u9ad8\u53ef\u7528\u6027) iSCSI Volume\u3092\u30a8\u30df\u30e5\u30ec\u30fc\u30c8 Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8 \u30b7\u30f3 \u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3092\u30b5\u30dd\u30fc\u30c8 https://openebs.io/docs/concepts/jiva cStor PV Beta \u8907\u6570\u30ce\u30fc\u30c9\u3067\u30ec\u30d7\u30ea\u30ab\u3092\u69cb\u6210\u3057\u305fVolume\u3092\u63d0\u4f9b(\u9ad8\u53ef\u7528\u6027) iSCSI Volume\u3092\u30a8\u30df\u30e5\u30ec\u30fc\u30c8 Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8 \u30b7\u30f3 \u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3092\u30b5\u30dd\u30fc\u30c8 Snapshot\u3092\u30b5\u30dd\u30fc\u30c8 https://openebs.io/docs/concepts/cstor \u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3\u306e\u9078\u629e\u57fa\u6e96\u306b\u3064\u3044\u3066 https://openebs.io/docs/2.12.x/concepts/casengines#cstor-vs-jiva-vs-localpv-features-comparison","title":"About OpenEBS"},{"location":"addons/openebs/about/#openebs","text":"OpenEBS \u306f\u3001Kubernetes\u30ef\u30fc\u30ab\u30fc\u30ce\u30fc\u30c9\u304c\u5229\u7528\u3067\u304d\u308b\u3042\u3089\u3086\u308b\u30b9\u30c8\u30ec\u30fc\u30b8(\u30ed\u30fc\u30ab\u30eb\u3084\u5206\u6563\u306a\u3069) \u3092Kubernetes Persistent Volumes\u306b\u5909\u63db\u3057\u307e\u3059","title":"OpenEBS"},{"location":"addons/openebs/about/#reference","text":"https://openebs.io/ https://github.com/openebs/openebs https://github.com/openebs/charts https://openebs.github.io/charts/ https://blog.openebs.io/arming-kubernetes-with-openebs-1-b450f41e0c1f https://note.com/ryoma_0923/n/n7d2837212028 https://qiita.com/ysakashita/items/8ca805cb6ac10df911be https://blog.cybozu.io/entry/2018/03/29/080000 https://www.infoq.com/jp/news/2020/08/kubernetes-storage-kubera/","title":"Reference"},{"location":"addons/openebs/about/#openebs_1","text":"MayaData\u304c\u5b9f\u88c5\u3092\u516c\u958b\u3057\u3001\u73fe\u5728\u306fCNCF sandbox project\u306e Container Storage Interface (CSI) Provider\u3067\u3059 Container Attached Storage(CAS) \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u3092\u63a1\u7528\u3057\u3066\u3044\u308b Dynamic Provisioner\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u308b Kubernetes Native\u306e local PersistentVolume volume \u306fStatic Provisioning\u306e\u307f\u3092\u30b5\u30dd\u30fc\u30c8","title":"OpenEBS \u306b\u3064\u3044\u3066"},{"location":"addons/openebs/about/#container-attached-storagecas","text":"https://openebs.io/docs/concepts/cas https://www.cncf.io/blog/2018/04/19/container-attached-storage-a-primer/ https://www.cncf.io/blog/2020/09/22/container-attached-storage-is-cloud-native-storage-cas/ https://www.cncf.io/online-programs/kubernetes-and-storage-kubernetes-for-storage-an-overview/ https://blog.mayadata.io/container-attached-storage-cas-vs.-shared-storage-which-one-to-choose In Kubernetes, shared storage is typically achieved by mounting volumes and connecting to an external filesystem or block storage solution. Container Attached Storage (CAS) is a relatively newer solution that allows Kubernetes administrators to deploy storage as containerized microservices in a cluster. Container Attached Storage(CAS) \u3068\u306fPod\u3067\u5229\u7528\u53ef\u80fd\u306a\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u30b3\u30f3\u30c6\u30ca\u5316\u3057\u305f\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9\u3068\u3057\u3066Kubernetes Cluster\u3078\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u4ed5\u7d44\u307f\u3067\u3059\u3002 CAS \u3067\u306fPersistent Volumes\u3092\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9 \u30d9\u30fc\u30b9\u306e\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3068\u3057\u3066\u69cb\u6210\u3057\u307e\u3059\u3002\u305d\u306e\u969b\u3001\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3092\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30b9\u30c8\u30ec\u30fc\u30b8 \u30b3\u30f3\u30c8\u30ed\u30fc\u30e9\u3092\u3001\u72ec\u7acb\u3057\u3066\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u304a\u3088\u3073\u5b9f\u884c\u3067\u304d\u308b \u69cb\u6210\u30e6\u30cb\u30c3\u30c8\u3068\u3057\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002(\u3064\u307e\u308a\u3001OpenEBS \u306b\u304a\u3051\u308bPV\u306fVolume\u3092\u30db\u30b9\u30c8\u3059\u308breplica pod \u3068 replica pod\u3092\u7ba1\u7406\u3059\u308bcontroller pod\u3067\u69cb\u6210\u3055\u308c\u307e\u3059) \u4ee5\u4e0b\u306f Deploy Jenkins with OpenEBS \u3067\u4f5c\u6210\u3055\u308c\u308b\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3068\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9 $ kubectl get pods -n openebs | grep pvc-2fee1acb openebs pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-ctrl-66d449fp8jp8 2/2 Running 0 3h58m 10.200.0.39 k8s-master <none> <none> openebs pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-0 1/1 Running 1 (3h34m ago) 4h16m 10.200.2.194 k8s-node2 <none> <none> openebs pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-1 1/1 Running 1 (3h40m ago) 4h16m 10.200.0.38 k8s-master <none> <none> openebs pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-2 0/1 Pending 0 4h16m <none> <none> <none> <none>","title":"Container Attached Storage(CAS)"},{"location":"addons/openebs/about/#node-disk-managerndm","text":"https://openebs.io/docs/main/concepts/ndm CPU\u3084Memory\u3001Network\u306a\u3069\u3068\u540c\u3058\u3088\u3046\u306bNode\u4e0a\u306eblock device\u3092kubernetes resources(CustomResource)\u3068\u3057\u3066\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8 DeamonSet\u3068\u3057\u3066\u5404Node\u306b\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u308b Node\u4e0a\u306eblock device\u3078\u306e\u30a2\u30af\u30bb\u30b9\u3059\u308b\u306e\u306b /dev , /proc , /sys \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u304c\u5fc5\u8981\u306a\u305f\u3081Privileged mode\u3067\u52d5\u4f5c\u3059\u308b Local PV \u3068 cStor PV \u3067\u4f7f\u7528\u3055\u308c\u308b JIVA PV \u306fNDM\u3078\u30a2\u30af\u30bb\u30b9\u3057\u3066\u3044\u306a\u3044? https://openebs.io/docs/main/user-guides/ndm","title":"Node Disk Manager(NDM)"},{"location":"addons/openebs/about/#_1","text":"https://openebs.io/docs/main/concepts/casengines Storage Engine Status Description link Local PV Beta \u5358\u4e00\u30ce\u30fc\u30c9\u3067\u5229\u7528\u53ef\u80fd\u306aVolume\u3092\u63d0\u4f9b Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8(Kubernetes Native\u306e local PersistentVolume volume \u306fStatic Provisioning(\u4e8b\u524d\u306ePV\u3092\u624b\u52d5\u4f5c\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b)) https://openebs.io/docs/concepts/localpv https://openebs.io/docs/main/user-guides/localpv-device JIVA PV Stable \u8907\u6570\u30ce\u30fc\u30c9\u3067\u30ec\u30d7\u30ea\u30ab\u3092\u69cb\u6210\u3057\u305fVolume\u3092\u63d0\u4f9b(\u9ad8\u53ef\u7528\u6027) iSCSI Volume\u3092\u30a8\u30df\u30e5\u30ec\u30fc\u30c8 Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8 \u30b7\u30f3 \u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3092\u30b5\u30dd\u30fc\u30c8 https://openebs.io/docs/concepts/jiva cStor PV Beta \u8907\u6570\u30ce\u30fc\u30c9\u3067\u30ec\u30d7\u30ea\u30ab\u3092\u69cb\u6210\u3057\u305fVolume\u3092\u63d0\u4f9b(\u9ad8\u53ef\u7528\u6027) iSCSI Volume\u3092\u30a8\u30df\u30e5\u30ec\u30fc\u30c8 Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8 \u30b7\u30f3 \u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3092\u30b5\u30dd\u30fc\u30c8 Snapshot\u3092\u30b5\u30dd\u30fc\u30c8 https://openebs.io/docs/concepts/cstor \u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3\u306e\u9078\u629e\u57fa\u6e96\u306b\u3064\u3044\u3066 https://openebs.io/docs/2.12.x/concepts/casengines#cstor-vs-jiva-vs-localpv-features-comparison","title":"\u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3"},{"location":"addons/openebs/install/","text":"Installing OpenEBS helm \u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b https://openebs.io/docs/user-guides/installation#installation-through-helm https://github.com/openebs/charts helm install openebs --namespace openebs openebs/openebs --create-namespace \\ --set cstor.enabled=true \\ --set jiva.enabled=true StorageClass\u3092\u78ba\u8a8d\u3059\u308b $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE openebs-device openebs.io/local Delete WaitForFirstConsumer false 28m openebs-hostpath openebs.io/local Delete WaitForFirstConsumer false 28m openebs-jiva-csi-default jiva.csi.openebs.io Delete Immediate true 28mS OpenEBS\u306e\u5404Pod\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ kubectl get pods -n openebs NAME READY STATUS RESTARTS AGE openebs-cstor-admission-server-b74f5487-djhxj 1/1 Running 0 22m openebs-cstor-csi-controller-0 6/6 Running 0 21m openebs-cstor-csi-node-54prc 2/2 Running 0 28m openebs-cstor-csi-node-g844x 2/2 Running 0 22m openebs-cstor-csi-node-vrm2w 2/2 Running 0 95s openebs-cstor-cspc-operator-84464fb479-vdh67 1/1 Running 0 22m openebs-cstor-cvc-operator-646f6f676b-5t79q 1/1 Running 0 22m openebs-jiva-csi-controller-0 5/5 Running 3 (16m ago) 28m openebs-jiva-csi-node-2hz6w 3/3 Running 0 110s openebs-jiva-csi-node-nt6jz 3/3 Running 0 28m openebs-jiva-csi-node-zm49x 3/3 Running 0 22m openebs-jiva-operator-f994f6868-w7xrl 1/1 Running 0 22m openebs-localpv-provisioner-55b65f8b55-2mq9z 1/1 Running 3 (16m ago) 28m openebs-ndm-4smgw 1/1 Running 0 2m37s openebs-ndm-6fpg7 1/1 Running 0 9m7s openebs-ndm-operator-6c944d87b6-5dq77 1/1 Running 0 22m openebs-ndm-vm255 1/1 Running 0 28m Deploy Jenkins with OpenEBS \u53c2\u8003 https://blog.openebs.io/tagged/jenkins https://github.com/openebs/openebs/tree/main/k8s/demo/jenkins \u624b\u9806 download manifests wget https://raw.githubusercontent.com/openebs/openebs/master/k8s/demo/jenkins/jenkins.yml modify manifests vim jenkins.yml $ diff -u <(curl -s https://raw.githubusercontent.com/openebs/openebs/master/k8s/demo/jenkins/jenkins.yml) <(cat ./jenkins.yml) --- /dev/fd/63 2022-10-01 14:11:35.968241073 +0000 +++ /dev/fd/62 2022-10-01 14:11:35.980240892 +0000 @@ -3,7 +3,7 @@ metadata: name: jenkins-claim annotations: - volume.beta.kubernetes.io/storage-class: openebs-jiva-default + volume.beta.kubernetes.io/storage-class: openebs-jiva-csi-default spec: accessModes: - ReadWriteOnce 1. apply manifests kubectl apply -f jenkins.yml \u30ea\u30bd\u30fc\u30b9\u78ba\u8a8d PersistentVolumeClaim $ kubectl describe PersistentVolumeClaim jenkins-claim Name: jenkins-claim Namespace: default StorageClass: openebs-jiva-csi-default Status: Bound Volume: pvc-e69e2bbb-f945-4ec7-b61a-490a66596441 Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-class: openebs-jiva-csi-default volume.beta.kubernetes.io/storage-provisioner: jiva.csi.openebs.io Finalizers: [kubernetes.io/pvc-protection] Capacity: 5G Access Modes: RWO VolumeMode: Filesystem Used By: jenkins-7f87d6d6d8-htjx9 Events: <none> Service $ kubectl describe service jenkins-svc Name: jenkins-svc Namespace: default Labels: <none> Annotations: <none> Selector: app=jenkins-app Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.32.0.70 IPs: 10.32.0.70 Port: <unset> 80/TCP TargetPort: 8080/TCP NodePort: <unset> 30792/TCP Endpoints: 10.200.0.45:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> > Deployment $ kubectl describe deployment jenkins Name: jenkins Namespace: default CreationTimestamp: Fri, 30 Sep 2022 04:56:42 +0000 Labels: <none> Annotations: deployment.kubernetes.io/revision: 1 Selector: app=jenkins-app Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=jenkins-app Containers: jenkins: Image: jenkins/jenkins:lts Port: 8080/TCP Host Port: 0/TCP Environment: <none> Mounts: /var/jenkins_home from jenkins-home (rw) Volumes: jenkins-home: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: jenkins-claim ReadOnly: false Conditions: Type Status Reason ---- ------ ------ Progressing True NewReplicaSetAvailable Available True MinimumReplicasAvailable OldReplicaSets: <none> NewReplicaSet: jenkins-7f87d6d6d8 (1/1 replicas created) Events: <none> Pod $ kubectl describe pods jenkins-7f87d6d6d8-htjx9 Name: jenkins-7f87d6d6d8-htjx9 Namespace: default Priority: 0 Node: k8s-master/192.168.3.50 Start Time: Sun, 02 Oct 2022 16:49:07 +0000 Labels: app=jenkins-app pod-template-hash=7f87d6d6d8 Annotations: <none> Status: Running IP: 10.200.0.45 IPs: IP: 10.200.0.45 Controlled By: ReplicaSet/jenkins-7f87d6d6d8 Containers: jenkins: Container ID: containerd://9c039657f1a1dee2b4b480aaff1859c9385b9b9d2d874757ad6e0e6c47278bda Image: jenkins/jenkins:lts Image ID: docker.io/jenkins/jenkins@sha256:5508cb1317aa0ede06cb34767fb1ab3860d1307109ade577d5df871f62170214 Port: 8080/TCP Host Port: 0/TCP State: Running Started: Sun, 02 Oct 2022 16:49:38 +0000 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/jenkins_home from jenkins-home (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gzs4t (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: jenkins-home: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: jenkins-claim ReadOnly: false kube-api-access-gzs4t: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: <none> Web\u30a2\u30af\u30bb\u30b9\u78ba\u8a8d image descrioption \u8868\u793a\u306b\u3042\u308b\u901a\u308a /var/jenkins_home/secrets/initialAdminPassword \u306e\u5185\u5bb9\u3092\u8cbc\u308a\u4ed8\u3051\u308b kubectl exec -it jenkins-7f87d6d6d8-wx8nn -- cat /var/jenkins_home/secrets/initialAdminPassword","title":"Installing OpenEBS"},{"location":"addons/openebs/install/#installing-openebs","text":"helm \u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b https://openebs.io/docs/user-guides/installation#installation-through-helm https://github.com/openebs/charts helm install openebs --namespace openebs openebs/openebs --create-namespace \\ --set cstor.enabled=true \\ --set jiva.enabled=true StorageClass\u3092\u78ba\u8a8d\u3059\u308b $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE openebs-device openebs.io/local Delete WaitForFirstConsumer false 28m openebs-hostpath openebs.io/local Delete WaitForFirstConsumer false 28m openebs-jiva-csi-default jiva.csi.openebs.io Delete Immediate true 28mS OpenEBS\u306e\u5404Pod\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ kubectl get pods -n openebs NAME READY STATUS RESTARTS AGE openebs-cstor-admission-server-b74f5487-djhxj 1/1 Running 0 22m openebs-cstor-csi-controller-0 6/6 Running 0 21m openebs-cstor-csi-node-54prc 2/2 Running 0 28m openebs-cstor-csi-node-g844x 2/2 Running 0 22m openebs-cstor-csi-node-vrm2w 2/2 Running 0 95s openebs-cstor-cspc-operator-84464fb479-vdh67 1/1 Running 0 22m openebs-cstor-cvc-operator-646f6f676b-5t79q 1/1 Running 0 22m openebs-jiva-csi-controller-0 5/5 Running 3 (16m ago) 28m openebs-jiva-csi-node-2hz6w 3/3 Running 0 110s openebs-jiva-csi-node-nt6jz 3/3 Running 0 28m openebs-jiva-csi-node-zm49x 3/3 Running 0 22m openebs-jiva-operator-f994f6868-w7xrl 1/1 Running 0 22m openebs-localpv-provisioner-55b65f8b55-2mq9z 1/1 Running 3 (16m ago) 28m openebs-ndm-4smgw 1/1 Running 0 2m37s openebs-ndm-6fpg7 1/1 Running 0 9m7s openebs-ndm-operator-6c944d87b6-5dq77 1/1 Running 0 22m openebs-ndm-vm255 1/1 Running 0 28m","title":"Installing OpenEBS"},{"location":"addons/openebs/install/#deploy-jenkins-with-openebs","text":"","title":"Deploy Jenkins with OpenEBS"},{"location":"addons/openebs/install/#_1","text":"https://blog.openebs.io/tagged/jenkins https://github.com/openebs/openebs/tree/main/k8s/demo/jenkins","title":"\u53c2\u8003"},{"location":"addons/openebs/install/#_2","text":"download manifests wget https://raw.githubusercontent.com/openebs/openebs/master/k8s/demo/jenkins/jenkins.yml modify manifests vim jenkins.yml $ diff -u <(curl -s https://raw.githubusercontent.com/openebs/openebs/master/k8s/demo/jenkins/jenkins.yml) <(cat ./jenkins.yml) --- /dev/fd/63 2022-10-01 14:11:35.968241073 +0000 +++ /dev/fd/62 2022-10-01 14:11:35.980240892 +0000 @@ -3,7 +3,7 @@ metadata: name: jenkins-claim annotations: - volume.beta.kubernetes.io/storage-class: openebs-jiva-default + volume.beta.kubernetes.io/storage-class: openebs-jiva-csi-default spec: accessModes: - ReadWriteOnce 1. apply manifests kubectl apply -f jenkins.yml \u30ea\u30bd\u30fc\u30b9\u78ba\u8a8d PersistentVolumeClaim $ kubectl describe PersistentVolumeClaim jenkins-claim Name: jenkins-claim Namespace: default StorageClass: openebs-jiva-csi-default Status: Bound Volume: pvc-e69e2bbb-f945-4ec7-b61a-490a66596441 Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-class: openebs-jiva-csi-default volume.beta.kubernetes.io/storage-provisioner: jiva.csi.openebs.io Finalizers: [kubernetes.io/pvc-protection] Capacity: 5G Access Modes: RWO VolumeMode: Filesystem Used By: jenkins-7f87d6d6d8-htjx9 Events: <none> Service $ kubectl describe service jenkins-svc Name: jenkins-svc Namespace: default Labels: <none> Annotations: <none> Selector: app=jenkins-app Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.32.0.70 IPs: 10.32.0.70 Port: <unset> 80/TCP TargetPort: 8080/TCP NodePort: <unset> 30792/TCP Endpoints: 10.200.0.45:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> > Deployment $ kubectl describe deployment jenkins Name: jenkins Namespace: default CreationTimestamp: Fri, 30 Sep 2022 04:56:42 +0000 Labels: <none> Annotations: deployment.kubernetes.io/revision: 1 Selector: app=jenkins-app Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=jenkins-app Containers: jenkins: Image: jenkins/jenkins:lts Port: 8080/TCP Host Port: 0/TCP Environment: <none> Mounts: /var/jenkins_home from jenkins-home (rw) Volumes: jenkins-home: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: jenkins-claim ReadOnly: false Conditions: Type Status Reason ---- ------ ------ Progressing True NewReplicaSetAvailable Available True MinimumReplicasAvailable OldReplicaSets: <none> NewReplicaSet: jenkins-7f87d6d6d8 (1/1 replicas created) Events: <none> Pod $ kubectl describe pods jenkins-7f87d6d6d8-htjx9 Name: jenkins-7f87d6d6d8-htjx9 Namespace: default Priority: 0 Node: k8s-master/192.168.3.50 Start Time: Sun, 02 Oct 2022 16:49:07 +0000 Labels: app=jenkins-app pod-template-hash=7f87d6d6d8 Annotations: <none> Status: Running IP: 10.200.0.45 IPs: IP: 10.200.0.45 Controlled By: ReplicaSet/jenkins-7f87d6d6d8 Containers: jenkins: Container ID: containerd://9c039657f1a1dee2b4b480aaff1859c9385b9b9d2d874757ad6e0e6c47278bda Image: jenkins/jenkins:lts Image ID: docker.io/jenkins/jenkins@sha256:5508cb1317aa0ede06cb34767fb1ab3860d1307109ade577d5df871f62170214 Port: 8080/TCP Host Port: 0/TCP State: Running Started: Sun, 02 Oct 2022 16:49:38 +0000 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/jenkins_home from jenkins-home (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gzs4t (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: jenkins-home: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: jenkins-claim ReadOnly: false kube-api-access-gzs4t: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: <none> Web\u30a2\u30af\u30bb\u30b9\u78ba\u8a8d image descrioption \u8868\u793a\u306b\u3042\u308b\u901a\u308a /var/jenkins_home/secrets/initialAdminPassword \u306e\u5185\u5bb9\u3092\u8cbc\u308a\u4ed8\u3051\u308b kubectl exec -it jenkins-7f87d6d6d8-wx8nn -- cat /var/jenkins_home/secrets/initialAdminPassword","title":"\u624b\u9806"},{"location":"autoscaler/","text":"https://speakerdeck.com/oracle4engineer/kubernetes-autoscale-deep-dive","title":"Index"},{"location":"aws-eks/aws-load-balancer-controller/","text":"aws-load-balancer-controller \u53c2\u8003 https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/","title":"aws-load-balancer-controller"},{"location":"aws-eks/aws-load-balancer-controller/#aws-load-balancer-controller","text":"","title":"aws-load-balancer-controller"},{"location":"aws-eks/aws-load-balancer-controller/#_1","text":"https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/","title":"\u53c2\u8003"},{"location":"failure_test/failure_scenario_of_node/","text":"Worker Node Resources OOM Killer \u53c2\u8003 k8s.af \u304b\u3089 https://www.bluematador.com/blog/post-mortem-kubernetes-node-oom https://www.scsk.jp/sp/sysdig/blog/sysdig_monitor/kubernetes_oomcpu.html Goldstine\u7814\u7a76\u6240 https://blog.mosuke.tech/entry/2020/03/31/kubernetes-resource/ https://blog.mosuke.tech/entry/2021/03/11/kubernetes-node-down/ \u5916\u9053\u7236\u306e\u5320 http://blog.father.gedow.net/2019/11/28/eks-kubernetes-ouf-of-memory/ Impact Memory\u304cNode\u4e0a\u9650\u306b\u9054\u3057\u305f\u5834\u5408OOM Killer\u304cPod\u3092\u5f37\u5236\u505c\u6b62\u3059\u308b \u5f37\u5236\u505c\u6b62\u3059\u308bPod\u306fQoS Class\u306e\u512a\u5148\u5ea6\u3067\u6c7a\u307e\u308b ( \u53c2\u8003 ) Note: The kubelet also sets an oom_score_adj value of -997 for containers in Pods that have system-node-critical Priority \u540c\u3058QoS Class(Burstable) \u306e\u5834\u5408\u306fPod\u306erequest\u3057\u305f\u30e1\u30e2\u30ea\u30fc\u91cf\u3068\u30ce\u30fc\u30c9\u306e\u30ad\u30e3\u30d1\u30b7\u30c6\u30a3\u306e\u5272\u5408\u306b\u3088\u3063\u3066\u30b9\u30b3\u30a2\u4ed8\u3051\u3055\u308c\u308b Recommend Requests Memory = Limits Memory \u3067\u8a2d\u5b9a\u3059\u308b Limits Memory\u304cRequests Memory\u3088\u308a\u591a\u3044\u5834\u5408\u3001Requests Memory\u304cAllocatable\u306b\u53ce\u307e\u308b\u3088\u3046\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u3001Allocatable\u3092\u8d85\u3048\u305f\u5834\u5408\u306bNode\u306eOOM Killer\u304c\u767a\u52d5\u3059\u308b\u306e\u3067\u30b7\u30b9\u30c6\u30e0\u5168\u4f53\u306e\u30d7\u30ed\u30bb\u30b9\u304b\u3089Kill\u5bfe\u8c61\u304b\u3089\u9078\u3070\u308c\u308b Node - \u505c\u6b62 \u53c2\u8003 Goldstine\u7814\u7a76\u6240: Kubernetes\u306e\u30ce\u30fc\u30c9\u969c\u5bb3\u6642\u306ePod\u306e\u52d5\u304d\u306b\u3064\u3044\u3066\u306e\u691c\u8a3c Impact ReplicaSet Pod\u306e\u518d\u914d\u7f6e\u304c\u884c\u308f\u308c\u308b node_lifecycle_controller Kubelet\u304cNode\u60c5\u5831\u3092\u66f4\u65b0\u3057\u306a\u304f\u306a\u3063\u305f\u3053\u3068\u3092\u691c\u77e5\u3057\u3066Node\u306eStatus\u3092\u5909\u66f4 key: node.kubernetes.io/unreachable \u306eTaint\u3092\u4ed8\u4e0e Pod\u306e\u518d\u914d\u7f6e Pod\u304c\u4f5c\u6210\u3055\u308c\u308b\u6642\u306bDefaultTolerationSeconds AdmissionController\u306b\u3088\u3063\u3066\u4ee5\u4e0b\u306etolerations\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u3044\u308b node.kubernetes.io/not-ready:NoExecute node.kubernetes.io/unreachable:NoExecute \u3053\u308c\u3089\u306etolerations\u304c tolerationSeconds: 300 \u3092\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001300\u79d2\u7d4c\u904e\u5f8c\u306bNode\u304c\u5fa9\u65e7\u305b\u305a node.kubernetes.io/unreachable Taint\u304c\u5916\u308c\u306a\u3044\u5834\u5408Pod\u304cEviction\uff08\u5f37\u5236\u9000\u53bb\uff09\u3055\u308c\u5225\u306e\u30ce\u30fc\u30c9\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u308b https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaulttolerationseconds \u5225\u30ce\u30fc\u30c9\u3078\u306e\u518d\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u307e\u3067\u6700\u59275\u5206\u306e\u30bf\u30a4\u30e0\u30e9\u30b0\u304c\u767a\u751f\u3059\u308b\u5834\u5408\u304c\u3042\u308b DefaultTolerationSeconds","title":"Node"},{"location":"failure_test/failure_scenario_of_node/#worker-node","text":"","title":"Worker Node"},{"location":"failure_test/failure_scenario_of_node/#resources","text":"","title":"Resources"},{"location":"failure_test/failure_scenario_of_node/#oom-killer","text":"","title":"OOM Killer"},{"location":"failure_test/failure_scenario_of_node/#_1","text":"k8s.af \u304b\u3089 https://www.bluematador.com/blog/post-mortem-kubernetes-node-oom https://www.scsk.jp/sp/sysdig/blog/sysdig_monitor/kubernetes_oomcpu.html Goldstine\u7814\u7a76\u6240 https://blog.mosuke.tech/entry/2020/03/31/kubernetes-resource/ https://blog.mosuke.tech/entry/2021/03/11/kubernetes-node-down/ \u5916\u9053\u7236\u306e\u5320 http://blog.father.gedow.net/2019/11/28/eks-kubernetes-ouf-of-memory/","title":"\u53c2\u8003"},{"location":"failure_test/failure_scenario_of_node/#impact","text":"Memory\u304cNode\u4e0a\u9650\u306b\u9054\u3057\u305f\u5834\u5408OOM Killer\u304cPod\u3092\u5f37\u5236\u505c\u6b62\u3059\u308b \u5f37\u5236\u505c\u6b62\u3059\u308bPod\u306fQoS Class\u306e\u512a\u5148\u5ea6\u3067\u6c7a\u307e\u308b ( \u53c2\u8003 ) Note: The kubelet also sets an oom_score_adj value of -997 for containers in Pods that have system-node-critical Priority \u540c\u3058QoS Class(Burstable) \u306e\u5834\u5408\u306fPod\u306erequest\u3057\u305f\u30e1\u30e2\u30ea\u30fc\u91cf\u3068\u30ce\u30fc\u30c9\u306e\u30ad\u30e3\u30d1\u30b7\u30c6\u30a3\u306e\u5272\u5408\u306b\u3088\u3063\u3066\u30b9\u30b3\u30a2\u4ed8\u3051\u3055\u308c\u308b","title":"Impact"},{"location":"failure_test/failure_scenario_of_node/#recommend","text":"Requests Memory = Limits Memory \u3067\u8a2d\u5b9a\u3059\u308b Limits Memory\u304cRequests Memory\u3088\u308a\u591a\u3044\u5834\u5408\u3001Requests Memory\u304cAllocatable\u306b\u53ce\u307e\u308b\u3088\u3046\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u3001Allocatable\u3092\u8d85\u3048\u305f\u5834\u5408\u306bNode\u306eOOM Killer\u304c\u767a\u52d5\u3059\u308b\u306e\u3067\u30b7\u30b9\u30c6\u30e0\u5168\u4f53\u306e\u30d7\u30ed\u30bb\u30b9\u304b\u3089Kill\u5bfe\u8c61\u304b\u3089\u9078\u3070\u308c\u308b","title":"Recommend"},{"location":"failure_test/failure_scenario_of_node/#node-","text":"","title":"Node - \u505c\u6b62"},{"location":"failure_test/failure_scenario_of_node/#_2","text":"Goldstine\u7814\u7a76\u6240: Kubernetes\u306e\u30ce\u30fc\u30c9\u969c\u5bb3\u6642\u306ePod\u306e\u52d5\u304d\u306b\u3064\u3044\u3066\u306e\u691c\u8a3c","title":"\u53c2\u8003"},{"location":"failure_test/failure_scenario_of_node/#impact_1","text":"ReplicaSet Pod\u306e\u518d\u914d\u7f6e\u304c\u884c\u308f\u308c\u308b node_lifecycle_controller Kubelet\u304cNode\u60c5\u5831\u3092\u66f4\u65b0\u3057\u306a\u304f\u306a\u3063\u305f\u3053\u3068\u3092\u691c\u77e5\u3057\u3066Node\u306eStatus\u3092\u5909\u66f4 key: node.kubernetes.io/unreachable \u306eTaint\u3092\u4ed8\u4e0e Pod\u306e\u518d\u914d\u7f6e Pod\u304c\u4f5c\u6210\u3055\u308c\u308b\u6642\u306bDefaultTolerationSeconds AdmissionController\u306b\u3088\u3063\u3066\u4ee5\u4e0b\u306etolerations\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u3044\u308b node.kubernetes.io/not-ready:NoExecute node.kubernetes.io/unreachable:NoExecute \u3053\u308c\u3089\u306etolerations\u304c tolerationSeconds: 300 \u3092\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001300\u79d2\u7d4c\u904e\u5f8c\u306bNode\u304c\u5fa9\u65e7\u305b\u305a node.kubernetes.io/unreachable Taint\u304c\u5916\u308c\u306a\u3044\u5834\u5408Pod\u304cEviction\uff08\u5f37\u5236\u9000\u53bb\uff09\u3055\u308c\u5225\u306e\u30ce\u30fc\u30c9\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u308b https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaulttolerationseconds \u5225\u30ce\u30fc\u30c9\u3078\u306e\u518d\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u307e\u3067\u6700\u59275\u5206\u306e\u30bf\u30a4\u30e0\u30e9\u30b0\u304c\u767a\u751f\u3059\u308b\u5834\u5408\u304c\u3042\u308b DefaultTolerationSeconds","title":"Impact"},{"location":"failure_test/failure_story/","text":"Failure Stories https://k8s.af/ https://codeberg.org/hjacobs/kubernetes-failure-stories","title":"Failure Story"},{"location":"failure_test/failure_story/#failure-stories","text":"https://k8s.af/ https://codeberg.org/hjacobs/kubernetes-failure-stories","title":"Failure Stories"},{"location":"failure_test/point_of_failure/","text":"Point of failure Kubernetes Component Controll Plane etcd kube-apiserver kube-controller-manager kube-scheduler Worker Node resources(cpu, memory, disk, network...) OOM Killer kubelet kube-proxy cni container-runtime Addons \u53c2\u8003 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons https://registry.terraform.io/modules/particuleio/addons/kubernetes/latest https://aws.amazon.com/jp/blogs/news/introducing-amazon-eks-add-ons-jp/ https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/eks-add-ons.html CoreDNS Amazon VPC CNI(EKS) Controller Ingress Controller External-dns AutoScale MetricsServer Pod HPA VPA Node(Horizontal Scale) Cluster Autoscaler","title":"Point of failure"},{"location":"failure_test/point_of_failure/#point-of-failure","text":"","title":"Point of failure"},{"location":"failure_test/point_of_failure/#kubernetes-component","text":"","title":"Kubernetes Component"},{"location":"failure_test/point_of_failure/#controll-plane","text":"etcd kube-apiserver kube-controller-manager kube-scheduler","title":"Controll Plane"},{"location":"failure_test/point_of_failure/#worker-node","text":"resources(cpu, memory, disk, network...) OOM Killer kubelet kube-proxy cni container-runtime","title":"Worker Node"},{"location":"failure_test/point_of_failure/#addons","text":"\u53c2\u8003 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons https://registry.terraform.io/modules/particuleio/addons/kubernetes/latest https://aws.amazon.com/jp/blogs/news/introducing-amazon-eks-add-ons-jp/ https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/eks-add-ons.html CoreDNS Amazon VPC CNI(EKS)","title":"Addons"},{"location":"failure_test/point_of_failure/#controller","text":"Ingress Controller External-dns","title":"Controller"},{"location":"failure_test/point_of_failure/#autoscale","text":"MetricsServer Pod HPA VPA Node(Horizontal Scale) Cluster Autoscaler","title":"AutoScale"},{"location":"kubernetes_dashboard/","text":"kubernetes dashboard \u53c2\u8003 https://github.com/kubernetes/dashboard https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/","title":"kubernetes dashboard"},{"location":"kubernetes_dashboard/#kubernetes-dashboard","text":"","title":"kubernetes dashboard"},{"location":"kubernetes_dashboard/#_1","text":"https://github.com/kubernetes/dashboard https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/","title":"\u53c2\u8003"},{"location":"kyverno/about_kyverno/","text":"Kyverno About Kyverno https://kyverno.io/docs/introduction/ Kyverno \u306fKubernetes\u306e\u305f\u3081\u306b\u8a2d\u8a08\u3055\u308c\u305fPolicy Engine\u3067\u3059\u3002 Open Policy Agent(OPA) \u306eRego\u306e\u3088\u3046\u306b\u72ec\u81ea\u8a00\u8a9e\u3092\u899a\u3048\u308b\u5fc5\u8981\u306f\u306a\u304fCustomResourceDefinition(CRD)\u3067\u5b9a\u7fa9\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059 About Kyverno Policy \u4ee5\u4e0b\u56f3\u306f \u3053\u3061\u3089 \u304b\u3089\u629c\u7c8b Kyverno Policy\u306f1\u3064\u4ee5\u4e0a\u306e\u30eb\u30fc\u30eb\u306e\u30b3\u30ec\u30af\u30b7\u30e7\u30f3\u3067\u3059\u3002 1\u3064\u306e\u30eb\u30fc\u30eb\u306f2\u3064\u306e\u5ba3\u8a00\u3092\u6301\u3061\u307e\u3059 Policy\u9069\u7528\u5bfe\u8c61\u30ea\u30bd\u30fc\u30b9\u306e\u9078\u629e Select Resources Policy\u9069\u7528 Mutate Resources Validate Resources Generate Resources VerifyImages Resources Kind of Policy Cluster wide\u306b\u9069\u7528\u3055\u308c\u308b ClusterPolicy \u3068 namespace\u5358\u4f4d\u306b\u9069\u7528\u3055\u308c\u308b Policy \u304c\u3042\u308b kyverno.io/v1.ClusterPolicy kyverno.io/v1.Policy Kind of Resources Select Resources Mutate/VerifyImages/Validate/Generate \u306a\u3069\u306epolicy\u3092\u9069\u7528\u3059\u308b\u5bfe\u8c61\u30ea\u30bd\u30fc\u30b9\u3092\u6307\u5b9a\u3059\u308b \u5bfe\u8c61\u3068\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9\u3092 match \u3067\u3001\u5bfe\u8c61\u5916\u3068\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9\u3092 exclude \u3067\u6307\u5b9a\u3059\u308b any( OR ) \u3082\u3057\u304f\u306f all( AND )\u306e\u6761\u4ef6\u4e0b\u3067 resource filters \u3092\u6307\u5b9a resources : select resources by names, namespaces, kinds, label selectors, annotations, and namespace selectors. subjects : select users, user groups, and service accounts roles : select namespaced roles clusterRoles : select cluster wide roles Mutate Resources rule\u306b\u5fdc\u3058\u3066\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u66f8\u304d\u63db\u3048\u308b \u66f8\u304d\u63db\u3048\u305f\u3044\u5185\u5bb9\u306f RFC 6902 JSON Patch \u3001 a strategic merge patch \u3001 an overlay pattern \u306e\u3044\u305a\u308c\u304b\u3067\u5b9a\u7fa9\u53ef\u80fd RFC 6902 JSON Patch Strategic Merge Patch an overlay pattern Conditional logic using anchors Mutate Rule Ordering Validate Resources \u3053\u308c\u304b\u3089\u9069\u7528\u3059\u308bmanifests\u3001\u307e\u305f\u306f\u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306b\u95a2\u3057\u3066\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u304c\u306a\u3044\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3059\u308b spec.validationFailureAction enforce \u65b0\u898f\u4f5c\u6210\u306e\u5834\u5408\u306f\u62d2\u5426\u3059\u308b audit \u4f5c\u6210\u306f\u62d2\u5426\u3057\u306a\u3044 ClusterPolicyReport \u307e\u305f\u306f PolicyReport \u3092\u4f5c\u6210\u3057\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u8a18\u9332\u3059\u308b spec.background \u5f53\u8a72 ClusterPolicy \u307e\u305f\u306f Policy \u306b\u95a2\u3057\u3066\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u3057\u3066\u3044\u306a\u3044\u304b\u3069\u3046\u304b\u3092\u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306b\u691c\u8a3c\u3059\u308b ( enforce \u306e\u5834\u5408\u3067\u3082) \u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306e\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u306f ClusterPolicyReport \u307e\u305f\u306f PolicyReport \u3092\u4f5c\u6210\u3057\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u8a18\u9332\u3059\u308b Patterns spec.rules[*].va1lidate.pattern spec.rules[*].va1lidate.anyPattern \u30eb\u30fc\u30eb\u306e\u62d2\u5426 deny validationFailureAction: enforce \u3092\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b deny rule\u306e exclude \u3067\u62d2\u5426\u3055\u305b\u305f\u304f\u306a\u3044subjects\u3084role\u3092\u6307\u5b9a\u3059\u308b\u306a\u3069\u3082\u53ef\u80fd Generate Resources \u30ea\u30bd\u30fc\u30b9\u306e\u4f5c\u6210\u30fb\u66f4\u65b0\u306b\u57fa\u3065\u3044\u3066\u8ffd\u52a0\u306e\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b \u30ea\u30bd\u30fc\u30b9\u4f5c\u6210\u65b9\u6cd5 spec.rules.generate.clone Secret\u3084ConfigMap\u306a\u3069\u65e2\u5b58\u30ea\u30bd\u30fc\u30b9\u3092\u30b3\u30d4\u30fc\u3057\u305f\u3044\u5834\u5408 spec.rules.generate.data rule manifests\u306b\u5b9a\u7fa9\u3055\u308c\u305f\u65b0\u898f\u4f5c\u6210\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9 Note kyverno policy rule\u306b\u3088\u3063\u3066\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u306f\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u7ba1\u7406\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u3002 spec.rules.generate.synchronize=true \u306e\u5834\u5408\u3001\u5fc5\u8981\u306a\u6a29\u9650\u3092\u6301\u3064\u30e6\u30fc\u30b6\u3084ServiceAccount\u306a\u3069\u304b\u3089\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u304c\u5909\u66f4\u30fb\u524a\u9664\u3055\u308c\u3066\u3082\u5143\u306e\u30ea\u30bd\u30fc\u30b9\u72b6\u614b\u306b\u623b\u3059\u3088\u3046\u306b\u66f4\u65b0\u30fb\u518d\u4f5c\u6210\u3057\u3066\u304f\u308c\u307e\u3059\u3002 VerifyImages Resources image\u306e\u30b7\u30b0\u30cd\u30c1\u30e3(\u7f72\u540d) \u3092\u30c1\u30a7\u30c3\u30af\u3057\u3001\u30c0\u30a4\u30b8\u30a7\u30b9\u30c8\u3092\u8ffd\u52a0\u3059\u308b","title":"01. About Kyverno"},{"location":"kyverno/about_kyverno/#kyverno","text":"","title":"Kyverno"},{"location":"kyverno/about_kyverno/#about-kyverno","text":"https://kyverno.io/docs/introduction/ Kyverno \u306fKubernetes\u306e\u305f\u3081\u306b\u8a2d\u8a08\u3055\u308c\u305fPolicy Engine\u3067\u3059\u3002 Open Policy Agent(OPA) \u306eRego\u306e\u3088\u3046\u306b\u72ec\u81ea\u8a00\u8a9e\u3092\u899a\u3048\u308b\u5fc5\u8981\u306f\u306a\u304fCustomResourceDefinition(CRD)\u3067\u5b9a\u7fa9\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059","title":"About Kyverno"},{"location":"kyverno/about_kyverno/#about-kyverno-policy","text":"\u4ee5\u4e0b\u56f3\u306f \u3053\u3061\u3089 \u304b\u3089\u629c\u7c8b Kyverno Policy\u306f1\u3064\u4ee5\u4e0a\u306e\u30eb\u30fc\u30eb\u306e\u30b3\u30ec\u30af\u30b7\u30e7\u30f3\u3067\u3059\u3002 1\u3064\u306e\u30eb\u30fc\u30eb\u306f2\u3064\u306e\u5ba3\u8a00\u3092\u6301\u3061\u307e\u3059 Policy\u9069\u7528\u5bfe\u8c61\u30ea\u30bd\u30fc\u30b9\u306e\u9078\u629e Select Resources Policy\u9069\u7528 Mutate Resources Validate Resources Generate Resources VerifyImages Resources","title":"About Kyverno Policy"},{"location":"kyverno/about_kyverno/#kind-of-policy","text":"Cluster wide\u306b\u9069\u7528\u3055\u308c\u308b ClusterPolicy \u3068 namespace\u5358\u4f4d\u306b\u9069\u7528\u3055\u308c\u308b Policy \u304c\u3042\u308b kyverno.io/v1.ClusterPolicy kyverno.io/v1.Policy","title":"Kind of Policy"},{"location":"kyverno/about_kyverno/#kind-of-resources","text":"","title":"Kind of Resources"},{"location":"kyverno/about_kyverno/#select-resources","text":"Mutate/VerifyImages/Validate/Generate \u306a\u3069\u306epolicy\u3092\u9069\u7528\u3059\u308b\u5bfe\u8c61\u30ea\u30bd\u30fc\u30b9\u3092\u6307\u5b9a\u3059\u308b \u5bfe\u8c61\u3068\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9\u3092 match \u3067\u3001\u5bfe\u8c61\u5916\u3068\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9\u3092 exclude \u3067\u6307\u5b9a\u3059\u308b any( OR ) \u3082\u3057\u304f\u306f all( AND )\u306e\u6761\u4ef6\u4e0b\u3067 resource filters \u3092\u6307\u5b9a resources : select resources by names, namespaces, kinds, label selectors, annotations, and namespace selectors. subjects : select users, user groups, and service accounts roles : select namespaced roles clusterRoles : select cluster wide roles","title":"Select Resources"},{"location":"kyverno/about_kyverno/#mutate-resources","text":"rule\u306b\u5fdc\u3058\u3066\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u66f8\u304d\u63db\u3048\u308b \u66f8\u304d\u63db\u3048\u305f\u3044\u5185\u5bb9\u306f RFC 6902 JSON Patch \u3001 a strategic merge patch \u3001 an overlay pattern \u306e\u3044\u305a\u308c\u304b\u3067\u5b9a\u7fa9\u53ef\u80fd RFC 6902 JSON Patch Strategic Merge Patch an overlay pattern Conditional logic using anchors Mutate Rule Ordering","title":"Mutate Resources"},{"location":"kyverno/about_kyverno/#validate-resources","text":"\u3053\u308c\u304b\u3089\u9069\u7528\u3059\u308bmanifests\u3001\u307e\u305f\u306f\u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306b\u95a2\u3057\u3066\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u304c\u306a\u3044\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3059\u308b spec.validationFailureAction enforce \u65b0\u898f\u4f5c\u6210\u306e\u5834\u5408\u306f\u62d2\u5426\u3059\u308b audit \u4f5c\u6210\u306f\u62d2\u5426\u3057\u306a\u3044 ClusterPolicyReport \u307e\u305f\u306f PolicyReport \u3092\u4f5c\u6210\u3057\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u8a18\u9332\u3059\u308b spec.background \u5f53\u8a72 ClusterPolicy \u307e\u305f\u306f Policy \u306b\u95a2\u3057\u3066\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u3057\u3066\u3044\u306a\u3044\u304b\u3069\u3046\u304b\u3092\u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306b\u691c\u8a3c\u3059\u308b ( enforce \u306e\u5834\u5408\u3067\u3082) \u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306e\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u306f ClusterPolicyReport \u307e\u305f\u306f PolicyReport \u3092\u4f5c\u6210\u3057\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u8a18\u9332\u3059\u308b Patterns spec.rules[*].va1lidate.pattern spec.rules[*].va1lidate.anyPattern \u30eb\u30fc\u30eb\u306e\u62d2\u5426 deny validationFailureAction: enforce \u3092\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b deny rule\u306e exclude \u3067\u62d2\u5426\u3055\u305b\u305f\u304f\u306a\u3044subjects\u3084role\u3092\u6307\u5b9a\u3059\u308b\u306a\u3069\u3082\u53ef\u80fd","title":"Validate Resources"},{"location":"kyverno/about_kyverno/#generate-resources","text":"\u30ea\u30bd\u30fc\u30b9\u306e\u4f5c\u6210\u30fb\u66f4\u65b0\u306b\u57fa\u3065\u3044\u3066\u8ffd\u52a0\u306e\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b \u30ea\u30bd\u30fc\u30b9\u4f5c\u6210\u65b9\u6cd5 spec.rules.generate.clone Secret\u3084ConfigMap\u306a\u3069\u65e2\u5b58\u30ea\u30bd\u30fc\u30b9\u3092\u30b3\u30d4\u30fc\u3057\u305f\u3044\u5834\u5408 spec.rules.generate.data rule manifests\u306b\u5b9a\u7fa9\u3055\u308c\u305f\u65b0\u898f\u4f5c\u6210\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9 Note kyverno policy rule\u306b\u3088\u3063\u3066\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u306f\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u7ba1\u7406\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u3002 spec.rules.generate.synchronize=true \u306e\u5834\u5408\u3001\u5fc5\u8981\u306a\u6a29\u9650\u3092\u6301\u3064\u30e6\u30fc\u30b6\u3084ServiceAccount\u306a\u3069\u304b\u3089\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u304c\u5909\u66f4\u30fb\u524a\u9664\u3055\u308c\u3066\u3082\u5143\u306e\u30ea\u30bd\u30fc\u30b9\u72b6\u614b\u306b\u623b\u3059\u3088\u3046\u306b\u66f4\u65b0\u30fb\u518d\u4f5c\u6210\u3057\u3066\u304f\u308c\u307e\u3059\u3002","title":"Generate Resources"},{"location":"kyverno/about_kyverno/#verifyimages-resources","text":"image\u306e\u30b7\u30b0\u30cd\u30c1\u30e3(\u7f72\u540d) \u3092\u30c1\u30a7\u30c3\u30af\u3057\u3001\u30c0\u30a4\u30b8\u30a7\u30b9\u30c8\u3092\u8ffd\u52a0\u3059\u308b","title":"VerifyImages Resources"},{"location":"kyverno/installation_kyverno/","text":"Kyverno Installation Kyverno Kyverno Policy Engine https://kyverno.io/docs/installation/ \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f\u5927\u304d\u304f\u4ee5\u4e0b2\u30d1\u30bf\u30fc\u30f3 helm chart manifests(yaml) \u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8 \u4ee5\u4e0b\u56f3\u306f \u3053\u3061\u3089 \u304b\u3089\u629c\u7c8b TLS Configuration Kyverno policy engine \u306fAdmission Webhook\u3068\u3057\u3066\u52d5\u4f5c\u3057\u3001kube-apiserver\u3068\u306e\u901a\u4fe1\u3067TLS\u901a\u4fe1\u3092\u884c\u3046\u305f\u3081\u306b\u8a8d\u8a3c\u5c40\u306e\u7f72\u540d\u304c\u3055\u308c\u305f\u8a3c\u660e\u66f8\u304c\u5fc5\u8981 \u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u306e\u4f5c\u6210 (helm charts\u306e) createSelfSignedCert (defailt: false) false : kube-controller-manager \u3067\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u4f5c\u6210 true : \u4f5c\u6210\u6e08\u307f\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u3092\u4f7f\u7528\u3059\u308b https://kyverno.io/docs/installation/#option-2-use-your-own-ca-signed-certificate \u53c2\u8003 https://kyverno.io/docs/installation/#customize-the-installation-of-kyverno https://github.com/kyverno/kyverno/blob/v1.5.1/charts/kyverno/README.md#tls-configuration https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/ https://github.com/kyverno/kyverno/blob/v1.5.1/charts/kyverno/values.yaml#L228-L238 Webhook Configurations ValicatingWebhookConfiguration MutatingWebhookConfiguration \u8d77\u52d5\u6642\u306b\u4f5c\u6210 https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/kyverno/main.go#L426 https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/initContainer/main.go#L165 https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/webhookconfig/monitor.go#L85 \u505c\u6b62\u6642\u306b\u524a\u9664 https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/kyverno/main.go#L533-L536 mutatingwebhookconfigurations https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/config/config.go#L11-L66 generateMutatingWebhook constructVerifyMutatingWebhookConfig constructPolicyMutatingWebhookConfig constructDefaultMutatingWebhookConfig validatingwebhookconfigurations https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/config/config.go#L11-L66 generateValidatingWebhook constructPolicyValidatingWebhookConfig constructDefaultValidatingWebhookConfig Installation Polcies Policy \u3082\u3057\u304f\u306f ClusterPolicy \u306emanifests\u3092\u9069\u7528\u3057\u307e\u3059 Policy\u306e\u66f8\u304d\u65b9\u3084\u63a8\u5968Policy\u306b\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u3092\u53c2\u7167 https://kyverno.io/policies/ https://github.com/kyverno/policies Privilege mode\u3092\u8a31\u53ef\u3057\u306a\u3044Policy\u306esample https://aws.amazon.com/jp/blogs/news/easy-as-one-two-three-policy-management-with-kyverno-on-amazon-eks/ Uninstallation Kyverno https://kyverno.io/docs/installation/#uninstalling-kyverno Warning https://github.com/kyverno/kyverno/issues/2750 https://github.com/kyverno/kyverno/issues/2623 kyverno Pod\u304cdelete\u3055\u308c\u308b\u969b\u306b mutatingwebhookconfigurations \u3068 validatingwebhookconfigurations \u304c\u524a\u9664\u3055\u308c\u306a\u3044bug\u304c\u3042\u308a\u307e\u3059\u3002 (1.5.2-rc2 image\u3067fix\u3057\u305f\u3088\u3046\u3067\u3059) kyverno controller\u304c\u524a\u9664\u3055\u308cwebhook\u304c\u6b8b\u3063\u3066\u3044\u308b\u5834\u5408\u3001Pod\u3092\u8d77\u52d5\u3057\u3088\u3046\u3068\u3057\u305f\u969b\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u3002 warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. Error from server (InternalError): Internal error occurred: failed calling webhook \"validate.kyverno.svc-fail\": Post \"https://kyverno-svc.kyverno.svc:443/validate?timeout=10s\": service \"kyverno-svc\" not found \u624b\u52d5\u3067webhook\u3092\u524a\u9664\u3059\u308b\u5834\u5408\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u3092\u53c2\u7167 https://kyverno.io/docs/installation/#clean-up-webhook-configurations Appendix kube-controller-manager \u3067\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u4f5c\u6210 https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/","title":"02. Installation Kyverno"},{"location":"kyverno/installation_kyverno/#kyverno","text":"","title":"Kyverno"},{"location":"kyverno/installation_kyverno/#installation-kyverno","text":"","title":"Installation Kyverno"},{"location":"kyverno/installation_kyverno/#kyverno-policy-engine","text":"https://kyverno.io/docs/installation/ \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f\u5927\u304d\u304f\u4ee5\u4e0b2\u30d1\u30bf\u30fc\u30f3 helm chart manifests(yaml)","title":"Kyverno Policy Engine"},{"location":"kyverno/installation_kyverno/#_1","text":"\u4ee5\u4e0b\u56f3\u306f \u3053\u3061\u3089 \u304b\u3089\u629c\u7c8b","title":"\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8"},{"location":"kyverno/installation_kyverno/#tls-configuration","text":"Kyverno policy engine \u306fAdmission Webhook\u3068\u3057\u3066\u52d5\u4f5c\u3057\u3001kube-apiserver\u3068\u306e\u901a\u4fe1\u3067TLS\u901a\u4fe1\u3092\u884c\u3046\u305f\u3081\u306b\u8a8d\u8a3c\u5c40\u306e\u7f72\u540d\u304c\u3055\u308c\u305f\u8a3c\u660e\u66f8\u304c\u5fc5\u8981 \u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u306e\u4f5c\u6210 (helm charts\u306e) createSelfSignedCert (defailt: false) false : kube-controller-manager \u3067\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u4f5c\u6210 true : \u4f5c\u6210\u6e08\u307f\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u3092\u4f7f\u7528\u3059\u308b https://kyverno.io/docs/installation/#option-2-use-your-own-ca-signed-certificate \u53c2\u8003 https://kyverno.io/docs/installation/#customize-the-installation-of-kyverno https://github.com/kyverno/kyverno/blob/v1.5.1/charts/kyverno/README.md#tls-configuration https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/ https://github.com/kyverno/kyverno/blob/v1.5.1/charts/kyverno/values.yaml#L228-L238","title":"TLS Configuration"},{"location":"kyverno/installation_kyverno/#webhook-configurations","text":"ValicatingWebhookConfiguration MutatingWebhookConfiguration \u8d77\u52d5\u6642\u306b\u4f5c\u6210 https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/kyverno/main.go#L426 https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/initContainer/main.go#L165 https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/webhookconfig/monitor.go#L85 \u505c\u6b62\u6642\u306b\u524a\u9664 https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/kyverno/main.go#L533-L536","title":"Webhook Configurations"},{"location":"kyverno/installation_kyverno/#mutatingwebhookconfigurations","text":"https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/config/config.go#L11-L66 generateMutatingWebhook constructVerifyMutatingWebhookConfig constructPolicyMutatingWebhookConfig constructDefaultMutatingWebhookConfig","title":"mutatingwebhookconfigurations"},{"location":"kyverno/installation_kyverno/#validatingwebhookconfigurations","text":"https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/config/config.go#L11-L66 generateValidatingWebhook constructPolicyValidatingWebhookConfig constructDefaultValidatingWebhookConfig","title":"validatingwebhookconfigurations"},{"location":"kyverno/installation_kyverno/#installation-polcies","text":"Policy \u3082\u3057\u304f\u306f ClusterPolicy \u306emanifests\u3092\u9069\u7528\u3057\u307e\u3059 Policy\u306e\u66f8\u304d\u65b9\u3084\u63a8\u5968Policy\u306b\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u3092\u53c2\u7167 https://kyverno.io/policies/ https://github.com/kyverno/policies Privilege mode\u3092\u8a31\u53ef\u3057\u306a\u3044Policy\u306esample https://aws.amazon.com/jp/blogs/news/easy-as-one-two-three-policy-management-with-kyverno-on-amazon-eks/","title":"Installation Polcies"},{"location":"kyverno/installation_kyverno/#uninstallation-kyverno","text":"https://kyverno.io/docs/installation/#uninstalling-kyverno Warning https://github.com/kyverno/kyverno/issues/2750 https://github.com/kyverno/kyverno/issues/2623 kyverno Pod\u304cdelete\u3055\u308c\u308b\u969b\u306b mutatingwebhookconfigurations \u3068 validatingwebhookconfigurations \u304c\u524a\u9664\u3055\u308c\u306a\u3044bug\u304c\u3042\u308a\u307e\u3059\u3002 (1.5.2-rc2 image\u3067fix\u3057\u305f\u3088\u3046\u3067\u3059) kyverno controller\u304c\u524a\u9664\u3055\u308cwebhook\u304c\u6b8b\u3063\u3066\u3044\u308b\u5834\u5408\u3001Pod\u3092\u8d77\u52d5\u3057\u3088\u3046\u3068\u3057\u305f\u969b\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u3002 warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. Error from server (InternalError): Internal error occurred: failed calling webhook \"validate.kyverno.svc-fail\": Post \"https://kyverno-svc.kyverno.svc:443/validate?timeout=10s\": service \"kyverno-svc\" not found \u624b\u52d5\u3067webhook\u3092\u524a\u9664\u3059\u308b\u5834\u5408\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u3092\u53c2\u7167 https://kyverno.io/docs/installation/#clean-up-webhook-configurations","title":"Uninstallation Kyverno"},{"location":"kyverno/installation_kyverno/#appendix","text":"","title":"Appendix"},{"location":"kyverno/installation_kyverno/#kube-controller-manager","text":"https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/","title":"kube-controller-manager \u3067\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u4f5c\u6210"},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/","text":"about kubernetes-dashboard https://github.com/kubernetes/dashboard Kubernetes Dashboard\u306f\u3001Kubernetes\u30af\u30e9\u30b9\u30bf\u7528\u306e\u6c4e\u7528\u7684\u306aWeb\u30d9\u30fc\u30b9\u306eUI\u3067\u3059\u3002\u30af\u30e9\u30b9\u30bf\u30fc\u3067\u52d5\u4f5c\u3059\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u7ba1\u7406\u3084\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u306e\u307b\u304b\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u81ea\u4f53\u306e\u7ba1\u7406\u3082\u53ef\u80fd\u3067\u3059\u3002 \u53c2\u8003 https://github.com/kubernetes/dashboard/blob/master/docs/common/dashboard-arguments.md https://itnext.io/how-to-expose-your-kubernetes-dashboard-with-cert-manager-422ab1e3bf30 https://magda.io/docs/how-to-setup-https-to-local-cluster.html https://vmwire.com/2022/02/07/running-kubernetes-dashboard-with-signed-certificates/ https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard install create self signed certificate openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout kubernetes-dashboard.key -out kubernetes-dashboard.crt -subj \"/CN=k8s-dashboard.local/O=k8s-dashboard.local\" create ns and secret kubectl create ns kubernetes-dashboard kubectl create secret generic kubernetes-dashboard-certs --from-file=./ -n kubernetes-dashboard self-signed certificates regist to key chain(Mac OSX) Chrome\u3067\u30a2\u30af\u30bb\u30b9\u3057\u305f\u969b\u306b\u4e0d\u6b63\u306a\u8a3c\u660e\u66f8\u3068\u3057\u3066\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u3092\u4fe1\u983c\u6e08\u307f\u8a3c\u660e\u66f8\u3068\u3057\u3066\u767b\u9332\u3059\u308b download kubernetes-dashboard manifests sudo curl -o /etc/kubernetes/manifests/kubernetes-dashboard.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml edit of kubernetes-dashboard manifests sudo vim /etc/kubernetes/manifests/kubernetes-dashboard.yaml Service\u30ea\u30bd\u30fc\u30b9\u306eType\u3092 NodePort \u306b\u5909\u66f4 diff @@ -37,6 +37,7 @@ name: kubernetes-dashboard namespace: kubernetes-dashboard spec: + type: NodePort ports: - port: 443 targetPort: 8443 kubernetes-dashboard\u30b3\u30f3\u30c6\u30ca\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306bTLS\u8a3c\u660e\u66f8\u30d5\u30a1\u30a4\u30eb\u3068TLS\u9375\u30d5\u30a1\u30a4\u30eb\u3092\u6307\u5b9a\u3059\u308b diff @@ -198,6 +199,8 @@ args: - --auto-generate-certificates - --namespace=kubernetes-dashboard + - --tls-cert-file=/tls.crt + - --tls-key-file=/tls.key apply kubernetes-dashboard manifests kubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard.yaml create ingress cat << EOF | sudo tee /etc/kubernetes/manifests/kubernetes-dashboard-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" spec: tls: - hosts: - k8s-dashboard.local secretName: dashboard-secret-tls rules: - host: k8s-dashboard.local http: paths: - pathType: Prefix path: \"/\" backend: service: name: kubernetes-dashboard port: number: 443 EOF kubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard-ingress.yaml adding fqdn and node ip address to /etc/hosts get node ip kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard -o json | jq -r .items[].status.hostIP adding entry to /etc/hosts <Node IP ADDRESS> `k8s-dashboard.local` get node port kubectl get service -n kubernetes-dashboard kubernetes-dashboard -o json | jq -r .spec.ports[].nodePort access kubernetes-dashboard with browser https://k8s-dashboard.local:<node port of previous command result> create token strings kubectl create serviceaccount dashboard -n default kubectl create clusterrolebinding dashboard-admin -n default --clusterrole=cluster-admin --serviceaccount=default:dashboard TOKEN=`kubectl get secret $(kubectl get serviceaccount dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode` echo $TOKEN input token and login input previous command result as token strings login successed","title":"bootstrapping kubernetes-dashboard"},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/#about-kubernetes-dashboard","text":"https://github.com/kubernetes/dashboard Kubernetes Dashboard\u306f\u3001Kubernetes\u30af\u30e9\u30b9\u30bf\u7528\u306e\u6c4e\u7528\u7684\u306aWeb\u30d9\u30fc\u30b9\u306eUI\u3067\u3059\u3002\u30af\u30e9\u30b9\u30bf\u30fc\u3067\u52d5\u4f5c\u3059\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u7ba1\u7406\u3084\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u306e\u307b\u304b\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u81ea\u4f53\u306e\u7ba1\u7406\u3082\u53ef\u80fd\u3067\u3059\u3002","title":"about kubernetes-dashboard"},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/#_1","text":"https://github.com/kubernetes/dashboard/blob/master/docs/common/dashboard-arguments.md https://itnext.io/how-to-expose-your-kubernetes-dashboard-with-cert-manager-422ab1e3bf30 https://magda.io/docs/how-to-setup-https-to-local-cluster.html https://vmwire.com/2022/02/07/running-kubernetes-dashboard-with-signed-certificates/ https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard","title":"\u53c2\u8003"},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/#install","text":"create self signed certificate openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout kubernetes-dashboard.key -out kubernetes-dashboard.crt -subj \"/CN=k8s-dashboard.local/O=k8s-dashboard.local\" create ns and secret kubectl create ns kubernetes-dashboard kubectl create secret generic kubernetes-dashboard-certs --from-file=./ -n kubernetes-dashboard self-signed certificates regist to key chain(Mac OSX) Chrome\u3067\u30a2\u30af\u30bb\u30b9\u3057\u305f\u969b\u306b\u4e0d\u6b63\u306a\u8a3c\u660e\u66f8\u3068\u3057\u3066\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u3092\u4fe1\u983c\u6e08\u307f\u8a3c\u660e\u66f8\u3068\u3057\u3066\u767b\u9332\u3059\u308b download kubernetes-dashboard manifests sudo curl -o /etc/kubernetes/manifests/kubernetes-dashboard.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml edit of kubernetes-dashboard manifests sudo vim /etc/kubernetes/manifests/kubernetes-dashboard.yaml Service\u30ea\u30bd\u30fc\u30b9\u306eType\u3092 NodePort \u306b\u5909\u66f4 diff @@ -37,6 +37,7 @@ name: kubernetes-dashboard namespace: kubernetes-dashboard spec: + type: NodePort ports: - port: 443 targetPort: 8443 kubernetes-dashboard\u30b3\u30f3\u30c6\u30ca\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306bTLS\u8a3c\u660e\u66f8\u30d5\u30a1\u30a4\u30eb\u3068TLS\u9375\u30d5\u30a1\u30a4\u30eb\u3092\u6307\u5b9a\u3059\u308b diff @@ -198,6 +199,8 @@ args: - --auto-generate-certificates - --namespace=kubernetes-dashboard + - --tls-cert-file=/tls.crt + - --tls-key-file=/tls.key apply kubernetes-dashboard manifests kubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard.yaml create ingress cat << EOF | sudo tee /etc/kubernetes/manifests/kubernetes-dashboard-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" spec: tls: - hosts: - k8s-dashboard.local secretName: dashboard-secret-tls rules: - host: k8s-dashboard.local http: paths: - pathType: Prefix path: \"/\" backend: service: name: kubernetes-dashboard port: number: 443 EOF kubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard-ingress.yaml adding fqdn and node ip address to /etc/hosts get node ip kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard -o json | jq -r .items[].status.hostIP adding entry to /etc/hosts <Node IP ADDRESS> `k8s-dashboard.local` get node port kubectl get service -n kubernetes-dashboard kubernetes-dashboard -o json | jq -r .spec.ports[].nodePort access kubernetes-dashboard with browser https://k8s-dashboard.local:<node port of previous command result> create token strings kubectl create serviceaccount dashboard -n default kubectl create clusterrolebinding dashboard-admin -n default --clusterrole=cluster-admin --serviceaccount=default:dashboard TOKEN=`kubectl get secret $(kubectl get serviceaccount dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode` echo $TOKEN input token and login input previous command result as token strings login successed","title":"install"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/","text":"metrics-server metrics-server \u3068\u306fkubelet\u3084kube-apiserver\u304b\u3089\u5404\u7a2e\u30ea\u30bd\u30fc\u30b9\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002 \u53ce\u96c6\u3057\u305f\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u60c5\u5831\u306fautoscaling( HPA \u3084 VPA )\u3092\u884c\u3046\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u53c2\u8003 metrics-server https://github.com/kubernetes-sigs/metrics-server docs/command-line-flags.txt https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/ kube-apiserver Aggregation Layer\u8a2d\u5b9a\u306b\u3064\u3044\u3066 https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/ https://kubernetes.io/ja/docs/concepts/cluster-administration/proxies/ https://github.com/ansilh/kubernetes-the-hardway-virtualbox/blob/master/15.Deploy-Metric-Server.md \u8981\u4ef6 https://github.com/kubernetes-sigs/metrics-server#requirements Metrics Server must be reachable from kube-apiserver by container IP address (or node IP if hostNetwork is enabled). The kube-apiserver must enable an aggregation layer. Nodes must have Webhook authentication and authorization enabled. Kubelet certificate needs to be signed by cluster Certificate Authority (or disable certificate validation by passing --kubelet-insecure-tls to Metrics Server) Container runtime must implement a container metrics RPCs (or have cAdvisor support) \u69cb\u7bc9\u624b\u9806 kube-apiserver Aggregation Layer\u8a2d\u5b9a Info https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server Metrics Server collects metrics from the Summary API, exposed by Kubelet on each node, and is registered with the main API server via Kubernetes aggregator. kube-apiserver\u3067Aggregation Layer\u3092\u6709\u52b9\u306b\u3059\u308b Aggregation Layer\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408\u306fmetrics-server\u3067\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u30ed\u30b0\u304c\u51fa\u3066\u3044\u308b E0217 15:13:53.378655 1 webhook.go:224] Failed to make webhook authorizer request: Post \"https://10.32.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s\": cont ext canceled E0217 15:13:53.378917 1 errors.go:77] Post \"https://10.32.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s\": context canceled E0217 15:13:53.379124 1 timeout.go:137] post-timeout activity - time-elapsed: 121.389\u00b5s, GET \"/apis/metrics.k8s.io/v1beta1\" result: <nil> kube-apiserver front-proxy(for aggregation layer)\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8 front-proxy\u7528CA\u8a3c\u660e\u66f8\u304a\u3088\u3073\u30b5\u30fc\u30d0\u8a3c\u660e\u66f8\u3068\u79d8\u5bc6\u9375\u3092\u751f\u6210\u3059\u308b /var/lib/kubernetes/front-proxy-ca.pem /var/lib/kubernetes/front-proxy.pem /var/lib/kubernetes/front-proxy-key.pem /setup/06_master/03_bootstrapping_kube-apiserver/ kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u8ffd\u52a0 --enable-aggregator-routing=true --requestheader-client-ca-file=/var/lib/kubernetes/front-proxy-ca.pem --requestheader-allowed-names=front-proxy-ca --requestheader-extra-headers-prefix=X-Remote-Extra --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --proxy-client-cert-file=/var/lib/kubernetes/front-proxy.pem --proxy-client-key-file=/var/lib/kubernetes/front-proxy-key.pem metics-server\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb manifests\u3092deploy $ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml serviceaccount/metrics-server created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrole.rbac.authorization.k8s.io/system:metrics-server created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created service/metrics-server created deployment.apps/metrics-server created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created metrics-server \u304c\u8d77\u52d5\u306f\u3059\u308b\u304cTLS\u95a2\u9023\u306e\u30a8\u30e9\u30fc\u3067\u6b63\u5e38\u306b\u52d5\u4f5c\u3057\u306a\u3044 kubectl logs \u30b3\u30de\u30f3\u30c9 Failed to scrape node\" err=\"Get \\\"https://192.168.10.51:10250/metrics/resource\\\": x509: certificate is valid for 192.168.10.50, not 192.168.10.51\" node=\"k8s-node1 TLS\u8a8d\u8a3c\u8a2d\u5b9a\u306e\u5909\u66f4 metrics-server\u306e\u8d77\u52d5\u5f15\u6570\u306b --kubelet-insecure-tls \u3092\u8ffd\u52a0 https://github.com/kubernetes-sigs/metrics-server/issues/131 https://github.com/kubernetes-sigs/metrics-server/issues/300 kubectl logs\u30b3\u30de\u30f3\u30c9\u3067\u4ee5\u4e0b\u30a8\u30e9\u30fc\u304c\u51fa\u7d9a\u3051\u3066\u3044\u308b\u5834\u5408\u306e\u5bfe\u51e6 TLS\u8a3c\u660e\u66f8\u306e\u691c\u8a3c\u3092\u884c\u308f\u306a\u3044\u3088\u3046\u306b\u3059\u308b(\u8a3c\u660e\u66f8\u306e\u7f72\u540d\u304cmetrics-serverg\u304c\u60f3\u5b9a\u3059\u308bCA\u3067\u306f\u306a\u3044\u305f\u3081) kubectl patch deploy metrics-server -n kube-system --patch \" spec: template: spec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls name: metrics-server \" \u8d77\u52d5\u3057\u305f\u3053\u3068\u3092\u78ba\u8a8d I0217 14:54:41.737667 1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key) I0217 14:54:42.981853 1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController I0217 14:54:42.981913 1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController I0217 14:54:42.981901 1 configmap_cafile_content.go:201] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::client-ca-file\" I0217 14:54:42.981970 1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file I0217 14:54:42.981993 1 configmap_cafile_content.go:201] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\" I0217 14:54:42.982036 1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file I0217 14:54:42.985611 1 secure_serving.go:266] Serving securely on [::]:4443 I0217 14:54:42.985841 1 tlsconfig.go:240] \"Starting DynamicServingCertificateController\" W0217 14:54:42.986181 1 shared_informer.go:372] The sharedIndexInformer has started, run more than once is not allowed I0217 14:54:42.987234 1 dynamic_serving_content.go:131] \"Starting controller\" name=\"serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key\" I0217 14:54:43.082779 1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file I0217 14:54:43.082892 1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController I0217 14:54:43.082896 1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file \u52d5\u4f5c\u78ba\u8a8d API Path pods $ kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/pods\" | jq '.items[] | select(.metadata.name == \"coredns-675db8b7cc-hbzb2\")' { \"metadata\": { \"name\": \"coredns-675db8b7cc-hbzb2\", \"namespace\": \"kube-system\", \"creationTimestamp\": \"2022-02-17T16:00:21Z\", \"labels\": { \"k8s-app\": \"kube-dns\", \"pod-template-hash\": \"675db8b7cc\" } }, \"timestamp\": \"2022-02-17T16:00:03Z\", \"window\": \"15.692s\", \"containers\": [ { \"name\": \"coredns\", \"usage\": { \"cpu\": \"7990556n\", \"memory\": \"14064Ki\" } } ] } nodes $ kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | jq . [22/47786] { \"kind\": \"NodeMetricsList\", \"apiVersion\": \"metrics.k8s.io/v1beta1\", \"metadata\": {}, \"items\": [ { \"metadata\": { \"name\": \"k8s-master\", \"creationTimestamp\": \"2022-02-17T14:58:58Z\", \"labels\": { \"beta.kubernetes.io/arch\": \"arm64\", \"beta.kubernetes.io/os\": \"linux\", \"kubernetes.io/arch\": \"arm64\", \"kubernetes.io/hostname\": \"k8s-master\", \"kubernetes.io/os\": \"linux\" } }, \"timestamp\": \"2022-02-17T14:58:49Z\", \"window\": \"10.198s\", \"usage\": { \"cpu\": \"273214048n\", \"memory\": \"1024976Ki\" } }, { \"metadata\": { \"name\": \"k8s-node1\", \"creationTimestamp\": \"2022-02-17T14:58:58Z\", \"labels\": { \"beta.kubernetes.io/arch\": \"arm64\", \"beta.kubernetes.io/os\": \"linux\", \"kubernetes.io/arch\": \"arm64\", \"kubernetes.io/hostname\": \"k8s-node1\", \"kubernetes.io/os\": \"linux\" } }, \"timestamp\": \"2022-02-17T14:58:51Z\", \"window\": \"10.094s\", \"usage\": { \"cpu\": \"141038629n\", \"memory\": \"548580Ki\" } } ] } kubectl top pod $ kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master 260m 7% 999Mi 80% k8s-node1 130m 3% 527Mi 42% kubectl top pod $ kubectl top pods -A NAMESPACE NAME CPU(cores) MEMORY(bytes) kube-system coredns-675db8b7cc-hbzb2 7m 13Mi kube-system etcd-k8s-master 31m 108Mi kube-system kube-apiserver-k8s-master 62m 220Mi kube-system kube-controller-manager-k8s-master 24m 72Mi kube-system kube-proxy-2kmcf 1m 23Mi kube-system kube-proxy-fxcgv 1m 12Mi kube-system kube-scheduler-k8s-master 3m 26Mi kube-system metrics-server-8bb87844c-v67lj 12m 15Mi","title":"bootstrapping metrics_server"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#metrics-server","text":"metrics-server \u3068\u306fkubelet\u3084kube-apiserver\u304b\u3089\u5404\u7a2e\u30ea\u30bd\u30fc\u30b9\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002 \u53ce\u96c6\u3057\u305f\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u60c5\u5831\u306fautoscaling( HPA \u3084 VPA )\u3092\u884c\u3046\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002","title":"metrics-server"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_1","text":"metrics-server https://github.com/kubernetes-sigs/metrics-server docs/command-line-flags.txt https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/ kube-apiserver Aggregation Layer\u8a2d\u5b9a\u306b\u3064\u3044\u3066 https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/ https://kubernetes.io/ja/docs/concepts/cluster-administration/proxies/ https://github.com/ansilh/kubernetes-the-hardway-virtualbox/blob/master/15.Deploy-Metric-Server.md","title":"\u53c2\u8003"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_2","text":"https://github.com/kubernetes-sigs/metrics-server#requirements Metrics Server must be reachable from kube-apiserver by container IP address (or node IP if hostNetwork is enabled). The kube-apiserver must enable an aggregation layer. Nodes must have Webhook authentication and authorization enabled. Kubelet certificate needs to be signed by cluster Certificate Authority (or disable certificate validation by passing --kubelet-insecure-tls to Metrics Server) Container runtime must implement a container metrics RPCs (or have cAdvisor support)","title":"\u8981\u4ef6"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_3","text":"","title":"\u69cb\u7bc9\u624b\u9806"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#kube-apiserver-aggregation-layer","text":"Info https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server Metrics Server collects metrics from the Summary API, exposed by Kubelet on each node, and is registered with the main API server via Kubernetes aggregator. kube-apiserver\u3067Aggregation Layer\u3092\u6709\u52b9\u306b\u3059\u308b Aggregation Layer\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408\u306fmetrics-server\u3067\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u30ed\u30b0\u304c\u51fa\u3066\u3044\u308b E0217 15:13:53.378655 1 webhook.go:224] Failed to make webhook authorizer request: Post \"https://10.32.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s\": cont ext canceled E0217 15:13:53.378917 1 errors.go:77] Post \"https://10.32.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s\": context canceled E0217 15:13:53.379124 1 timeout.go:137] post-timeout activity - time-elapsed: 121.389\u00b5s, GET \"/apis/metrics.k8s.io/v1beta1\" result: <nil> kube-apiserver front-proxy(for aggregation layer)\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8 front-proxy\u7528CA\u8a3c\u660e\u66f8\u304a\u3088\u3073\u30b5\u30fc\u30d0\u8a3c\u660e\u66f8\u3068\u79d8\u5bc6\u9375\u3092\u751f\u6210\u3059\u308b /var/lib/kubernetes/front-proxy-ca.pem /var/lib/kubernetes/front-proxy.pem /var/lib/kubernetes/front-proxy-key.pem /setup/06_master/03_bootstrapping_kube-apiserver/ kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u8ffd\u52a0 --enable-aggregator-routing=true --requestheader-client-ca-file=/var/lib/kubernetes/front-proxy-ca.pem --requestheader-allowed-names=front-proxy-ca --requestheader-extra-headers-prefix=X-Remote-Extra --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --proxy-client-cert-file=/var/lib/kubernetes/front-proxy.pem --proxy-client-key-file=/var/lib/kubernetes/front-proxy-key.pem","title":"kube-apiserver Aggregation Layer\u8a2d\u5b9a"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#metics-server","text":"manifests\u3092deploy $ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml serviceaccount/metrics-server created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrole.rbac.authorization.k8s.io/system:metrics-server created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created service/metrics-server created deployment.apps/metrics-server created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created metrics-server \u304c\u8d77\u52d5\u306f\u3059\u308b\u304cTLS\u95a2\u9023\u306e\u30a8\u30e9\u30fc\u3067\u6b63\u5e38\u306b\u52d5\u4f5c\u3057\u306a\u3044 kubectl logs \u30b3\u30de\u30f3\u30c9 Failed to scrape node\" err=\"Get \\\"https://192.168.10.51:10250/metrics/resource\\\": x509: certificate is valid for 192.168.10.50, not 192.168.10.51\" node=\"k8s-node1 TLS\u8a8d\u8a3c\u8a2d\u5b9a\u306e\u5909\u66f4 metrics-server\u306e\u8d77\u52d5\u5f15\u6570\u306b --kubelet-insecure-tls \u3092\u8ffd\u52a0 https://github.com/kubernetes-sigs/metrics-server/issues/131 https://github.com/kubernetes-sigs/metrics-server/issues/300 kubectl logs\u30b3\u30de\u30f3\u30c9\u3067\u4ee5\u4e0b\u30a8\u30e9\u30fc\u304c\u51fa\u7d9a\u3051\u3066\u3044\u308b\u5834\u5408\u306e\u5bfe\u51e6 TLS\u8a3c\u660e\u66f8\u306e\u691c\u8a3c\u3092\u884c\u308f\u306a\u3044\u3088\u3046\u306b\u3059\u308b(\u8a3c\u660e\u66f8\u306e\u7f72\u540d\u304cmetrics-serverg\u304c\u60f3\u5b9a\u3059\u308bCA\u3067\u306f\u306a\u3044\u305f\u3081) kubectl patch deploy metrics-server -n kube-system --patch \" spec: template: spec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls name: metrics-server \" \u8d77\u52d5\u3057\u305f\u3053\u3068\u3092\u78ba\u8a8d I0217 14:54:41.737667 1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key) I0217 14:54:42.981853 1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController I0217 14:54:42.981913 1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController I0217 14:54:42.981901 1 configmap_cafile_content.go:201] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::client-ca-file\" I0217 14:54:42.981970 1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file I0217 14:54:42.981993 1 configmap_cafile_content.go:201] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\" I0217 14:54:42.982036 1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file I0217 14:54:42.985611 1 secure_serving.go:266] Serving securely on [::]:4443 I0217 14:54:42.985841 1 tlsconfig.go:240] \"Starting DynamicServingCertificateController\" W0217 14:54:42.986181 1 shared_informer.go:372] The sharedIndexInformer has started, run more than once is not allowed I0217 14:54:42.987234 1 dynamic_serving_content.go:131] \"Starting controller\" name=\"serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key\" I0217 14:54:43.082779 1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file I0217 14:54:43.082892 1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController I0217 14:54:43.082896 1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file","title":"metics-server\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_4","text":"","title":"\u52d5\u4f5c\u78ba\u8a8d"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#api-path","text":"pods $ kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/pods\" | jq '.items[] | select(.metadata.name == \"coredns-675db8b7cc-hbzb2\")' { \"metadata\": { \"name\": \"coredns-675db8b7cc-hbzb2\", \"namespace\": \"kube-system\", \"creationTimestamp\": \"2022-02-17T16:00:21Z\", \"labels\": { \"k8s-app\": \"kube-dns\", \"pod-template-hash\": \"675db8b7cc\" } }, \"timestamp\": \"2022-02-17T16:00:03Z\", \"window\": \"15.692s\", \"containers\": [ { \"name\": \"coredns\", \"usage\": { \"cpu\": \"7990556n\", \"memory\": \"14064Ki\" } } ] } nodes $ kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | jq . [22/47786] { \"kind\": \"NodeMetricsList\", \"apiVersion\": \"metrics.k8s.io/v1beta1\", \"metadata\": {}, \"items\": [ { \"metadata\": { \"name\": \"k8s-master\", \"creationTimestamp\": \"2022-02-17T14:58:58Z\", \"labels\": { \"beta.kubernetes.io/arch\": \"arm64\", \"beta.kubernetes.io/os\": \"linux\", \"kubernetes.io/arch\": \"arm64\", \"kubernetes.io/hostname\": \"k8s-master\", \"kubernetes.io/os\": \"linux\" } }, \"timestamp\": \"2022-02-17T14:58:49Z\", \"window\": \"10.198s\", \"usage\": { \"cpu\": \"273214048n\", \"memory\": \"1024976Ki\" } }, { \"metadata\": { \"name\": \"k8s-node1\", \"creationTimestamp\": \"2022-02-17T14:58:58Z\", \"labels\": { \"beta.kubernetes.io/arch\": \"arm64\", \"beta.kubernetes.io/os\": \"linux\", \"kubernetes.io/arch\": \"arm64\", \"kubernetes.io/hostname\": \"k8s-node1\", \"kubernetes.io/os\": \"linux\" } }, \"timestamp\": \"2022-02-17T14:58:51Z\", \"window\": \"10.094s\", \"usage\": { \"cpu\": \"141038629n\", \"memory\": \"548580Ki\" } } ] }","title":"API Path"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#kubectl-top-pod","text":"$ kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master 260m 7% 999Mi 80% k8s-node1 130m 3% 527Mi 42%","title":"kubectl top pod"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#kubectl-top-pod_1","text":"$ kubectl top pods -A NAMESPACE NAME CPU(cores) MEMORY(bytes) kube-system coredns-675db8b7cc-hbzb2 7m 13Mi kube-system etcd-k8s-master 31m 108Mi kube-system kube-apiserver-k8s-master 62m 220Mi kube-system kube-controller-manager-k8s-master 24m 72Mi kube-system kube-proxy-2kmcf 1m 23Mi kube-system kube-proxy-fxcgv 1m 12Mi kube-system kube-scheduler-k8s-master 3m 26Mi kube-system metrics-server-8bb87844c-v67lj 12m 15Mi","title":"kubectl top pod"},{"location":"monitoring/prometheus/install_cadvisor/","text":"About cadvisor https://github.com/google/cadvisor cadvisor(Container Advisor) \u306f\u5b9f\u884c\u4e2d\u306e\u30b3\u30f3\u30c6\u30ca\u306e\u30ea\u30bd\u30fc\u30b9\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53ce\u96c6\u3059\u308b\u305f\u3081\u306e\u30c4\u30fc\u30eb\u3067\u3059\u3002 kube-state-metrics \u306fKubernetes Object\u306b\u5bfe\u3059\u308b\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u53ce\u96c6\u3092\u884c\u3046\u3082\u306e\u306a\u306e\u3067\u3001 Pod \u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306f\u53ce\u96c6\u3055\u308c\u307e\u3059\u304c\u30b3\u30f3\u30c6\u30ca\u3054\u3068\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306f\u53ce\u96c6\u3055\u308c\u307e\u305b\u3093\u3002 cadvisor \u3067\u53ce\u96c6\u3055\u308c\u308b\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306b\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u307e\u3059\u3002 https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md Install install kustomize curl -s -o /tmp/install_kustomize.sh \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" # Raspberry Pi 4 is aarch64 (ARM 64-bit architecture) # refs https://github.com/kubernetes-sigs/kustomize/issues/4696 sed -i -e 's/arm64/aarch64/g' /tmp/install_kustomize.sh bash -x ./tmp/install_kustomize.sh sudo mv ./kustomize /usr/local/bin/ install cadvisor https://github.com/google/cadvisor/tree/master/deploy/kubernetes#cadvisor-kubernetes-daemonset # https://github.com/google/cadvisor/releases VERSION=v0.46.0 git clone https://github.com/google/cadvisor.git ~/work/cadvisor cd deploy/kubernetes/base && kustomize edit set image gcr.io/cadvisor/cadvisor:${VERSION} && cd ../../.. kubectl kustomize deploy/kubernetes/base | kubectl apply -f - Dashboard https://grafana.com/grafana/dashboards/14282-cadvisor-exporter/ Install Grafana > Install Node Exporter Full Dashboards \u53c2\u7167","title":"Install cadvisor"},{"location":"monitoring/prometheus/install_cadvisor/#about-cadvisor","text":"https://github.com/google/cadvisor cadvisor(Container Advisor) \u306f\u5b9f\u884c\u4e2d\u306e\u30b3\u30f3\u30c6\u30ca\u306e\u30ea\u30bd\u30fc\u30b9\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53ce\u96c6\u3059\u308b\u305f\u3081\u306e\u30c4\u30fc\u30eb\u3067\u3059\u3002 kube-state-metrics \u306fKubernetes Object\u306b\u5bfe\u3059\u308b\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u53ce\u96c6\u3092\u884c\u3046\u3082\u306e\u306a\u306e\u3067\u3001 Pod \u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306f\u53ce\u96c6\u3055\u308c\u307e\u3059\u304c\u30b3\u30f3\u30c6\u30ca\u3054\u3068\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306f\u53ce\u96c6\u3055\u308c\u307e\u305b\u3093\u3002 cadvisor \u3067\u53ce\u96c6\u3055\u308c\u308b\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306b\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u307e\u3059\u3002 https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md","title":"About cadvisor"},{"location":"monitoring/prometheus/install_cadvisor/#install","text":"install kustomize curl -s -o /tmp/install_kustomize.sh \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" # Raspberry Pi 4 is aarch64 (ARM 64-bit architecture) # refs https://github.com/kubernetes-sigs/kustomize/issues/4696 sed -i -e 's/arm64/aarch64/g' /tmp/install_kustomize.sh bash -x ./tmp/install_kustomize.sh sudo mv ./kustomize /usr/local/bin/ install cadvisor https://github.com/google/cadvisor/tree/master/deploy/kubernetes#cadvisor-kubernetes-daemonset # https://github.com/google/cadvisor/releases VERSION=v0.46.0 git clone https://github.com/google/cadvisor.git ~/work/cadvisor cd deploy/kubernetes/base && kustomize edit set image gcr.io/cadvisor/cadvisor:${VERSION} && cd ../../.. kubectl kustomize deploy/kubernetes/base | kubectl apply -f -","title":"Install"},{"location":"monitoring/prometheus/install_cadvisor/#dashboard","text":"https://grafana.com/grafana/dashboards/14282-cadvisor-exporter/ Install Grafana > Install Node Exporter Full Dashboards \u53c2\u7167","title":"Dashboard"},{"location":"monitoring/prometheus/install_grafana/","text":"Install Grafana https://github.com/grafana/helm-charts/tree/main/charts/grafana Install values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63 Ephemeral Storage\u306e storageClass \u3092 OpenEBS\u306ejiva storage\u306e\u3082\u306e\u3068\u3059\u308b( e.g. openebs-jiva-csi-default ) Info Installing OpenEBS \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0 Grafana WebUI\u3078\u306e\u30a2\u30af\u30bb\u30b9\u306b MetalLB \u306eexternal IP\u30a2\u30c9\u30ec\u30b9\u3092\u4f7f\u7528\u3059\u308b service.type: LoadBalancer annotations \u306b metallb.universe.tf/address-pool: ip-pool mkdir -p ~/work/prometheus/grafana curl -so ~/work/prometheus/grafana/grafana.yaml https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml vim ~/work/prometheus/grafana/grafana.yaml grafana.yaml \u4fee\u6b63\u5f8c\u306ediff $ diff -u <(curl -s https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml) <(cat ~/work/prometheus/grafana/grafana.yaml) --- /dev/fd/63 2022-10-30 15:05:32.554153834 +0000 +++ /dev/fd/62 2022-10-30 15:05:32.566153656 +0000 @@ -157,12 +157,13 @@ ## service: enabled: true - type: ClusterIP + type: LoadBalancer port: 80 targetPort: 3000 # targetPort: 4181 To be used with a proxy extraContainer ## Service annotations. Can be templated. - annotations: {} + annotations: + metallb.universe.tf/address-pool: ip-pool labels: {} portName: service # Adds the appProtocol field to the service. This allows to work with istio protocol selection. Ex: \"http\" or \"tcp\" @@ -297,10 +297,10 @@ persistence: type: pvc enabled: false - # storageClassName: default + storageClassName: openebs-jiva-csi-default accessModes: - ReadWriteOnce - size: 10Gi + size: 5Gi # annotations: {} finalizers: - kubernetes.io/pvc-protection @@ -507,15 +507,14 @@ ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## -datasources: {} -# datasources.yaml: -# apiVersion: 1 -# datasources: -# - name: Prometheus -# type: prometheus -# url: http://prometheus-prometheus-server -# access: proxy -# isDefault: true +datasources: + datasources.yaml: + apiVersion: 1 + datasources: + - name: Prometheus + type: prometheus + url: http://prometheus-server.monitoring.svc.cluster.local + isDefault: true # - name: CloudWatch # type: cloudwatch # access: proxy install helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml \u5b9f\u884c\u30ed\u30b0 $ helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml Release \"grafana\" does not exist. Installing it now. W1030 15:08:59.041312 221007 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W1030 15:08:59.265293 221007 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: grafana LAST DEPLOYED: Sun Oct 30 15:08:54 2022 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: 1. Get your 'admin' user password by running: kubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo 2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster: grafana.monitoring.svc.cluster.local Get the Grafana URL to visit by running these commands in the same shell: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc --namespace monitoring -w grafana' export SERVICE_IP=$(kubectl get svc --namespace monitoring grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}') http://$SERVICE_IP:80 3. Login with the password from step 1 and the username: admin ################################################################################# ###### WARNING: Persistence is disabled!!! You will lose your data when ##### ###### the Grafana pod is terminated. ##### ################################################################################# grafana Service\u306eMetalLB\u3067\u6255\u3044\u51fa\u3055\u308c\u305fExternal IP\u30a2\u30c9\u30ec\u30b9\u3092\u78ba\u8a8d\u3059\u308b kubectl get service -n monitoring grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}' \u30d6\u30e9\u30a6\u30b6\u304b\u3089\u30a2\u30af\u30bb\u30b9 \u30ed\u30b0\u30a4\u30f3\u753b\u9762 username admin password grafana\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6642\u306e\u8868\u793a\u3092\u53c2\u8003\u306b\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067 admin \u306e password \u3092\u53d6\u5f97 kubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo \u30ed\u30b0\u30a4\u30f3\u6210\u529f Install Node Exporter Full Dashboards grafana\u3068\u4e00\u7dd2\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408 Info grafana helm chart\u3067\u306fgrafana dashboard import\u8a2d\u5b9a\u304c\u53ef\u80fd\u3067\u3059\u3002 .Values.dashboards \u306b\u8a2d\u5b9a\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001 configmap.yaml \u306b\u3042\u308b download_dashboards.sh \u3067\u6307\u5b9a\u3057\u305fdashboard import\u8a2d\u5b9a\u304c\u5165\u308a\u307e\u3059\u3002\u305d\u3057\u3066 Pod\u306einitContainers \u3067 download-dashboards.sh \u3092\u5b9f\u884c\u3057\u3066\u3044\u307e\u3059\u3002 https://github.com/grafana/helm-charts/blob/grafana-6.43.5/charts/grafana/README.md https://github.com/grafana/helm-charts/blob/grafana-6.43.5/charts/grafana/values.yaml#L629-L658 values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63 vim ~/work/prometheus/grafana/grafana.yaml grafana.yaml \u4fee\u6b63\u5f8c\u306ediff( cadvisor-exporter Dashboard\u3082\u5165\u308c\u3066\u3044\u308b\u4f8b\u3067\u3059) $ diff -u <(curl -s https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml) <(cat ~/work/prometheus/grafana/grafana.yaml) ~ snip ~ @@ -632,7 +632,17 @@ ## ## dashboards per provider, use provider name as key. ## -dashboards: {} +dashboards: + default: + node-exporter-full: + # https://grafana.com/grafana/dashboards/1860-node-exporter-full/ + gnetId: 1860 + datasource: Prometheus + cadvisor-exporter: + # https://grafana.com/grafana/dashboards/14282-cadvisor-exporter/ + gnetId: 14282 + datasource: Prometheus + # default: # some-dashboard: # json: | install helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml grafana\u30b3\u30f3\u30c6\u30ca:/var/lib/grafana/dashboards/default/ \u306b Dashboard\u5b9a\u7fa9\u3067\u3042\u308bjson\u30d5\u30a1\u30a4\u30eb\u304c\u914d\u7f6e\u6e08\u307f\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl exec -it -c grafana -n monitoring grafana-5f4c7d46db-xtktb -- ls -l /var/lib/grafana/dashboards/default/ total 232 -rw-r--r-- 1 grafana 472 18484 Nov 8 16:03 cadvisor-exporter.json -rw-r--r-- 1 grafana 472 215154 Nov 8 16:03 node-exporter-full.json \u624b\u52d5\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408 Dashboards\u30da\u30fc\u30b8\u306b\u30a2\u30af\u30bb\u30b9 New -> New Dashboard -> Import \u3092\u9078\u629e Node Exporter Full \u306e Dashboard ID\u3092\u5165\u529b https://grafana.com/grafana/dashboards/1860-node-exporter-full/ Import \u3092\u5b9f\u884c\u3059\u308b Node Exporter Full \u304c\u8868\u793a\u3055\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d","title":"Install Grafana"},{"location":"monitoring/prometheus/install_grafana/#install-grafana","text":"https://github.com/grafana/helm-charts/tree/main/charts/grafana","title":"Install Grafana"},{"location":"monitoring/prometheus/install_grafana/#install","text":"values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63 Ephemeral Storage\u306e storageClass \u3092 OpenEBS\u306ejiva storage\u306e\u3082\u306e\u3068\u3059\u308b( e.g. openebs-jiva-csi-default ) Info Installing OpenEBS \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0 Grafana WebUI\u3078\u306e\u30a2\u30af\u30bb\u30b9\u306b MetalLB \u306eexternal IP\u30a2\u30c9\u30ec\u30b9\u3092\u4f7f\u7528\u3059\u308b service.type: LoadBalancer annotations \u306b metallb.universe.tf/address-pool: ip-pool mkdir -p ~/work/prometheus/grafana curl -so ~/work/prometheus/grafana/grafana.yaml https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml vim ~/work/prometheus/grafana/grafana.yaml grafana.yaml \u4fee\u6b63\u5f8c\u306ediff $ diff -u <(curl -s https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml) <(cat ~/work/prometheus/grafana/grafana.yaml) --- /dev/fd/63 2022-10-30 15:05:32.554153834 +0000 +++ /dev/fd/62 2022-10-30 15:05:32.566153656 +0000 @@ -157,12 +157,13 @@ ## service: enabled: true - type: ClusterIP + type: LoadBalancer port: 80 targetPort: 3000 # targetPort: 4181 To be used with a proxy extraContainer ## Service annotations. Can be templated. - annotations: {} + annotations: + metallb.universe.tf/address-pool: ip-pool labels: {} portName: service # Adds the appProtocol field to the service. This allows to work with istio protocol selection. Ex: \"http\" or \"tcp\" @@ -297,10 +297,10 @@ persistence: type: pvc enabled: false - # storageClassName: default + storageClassName: openebs-jiva-csi-default accessModes: - ReadWriteOnce - size: 10Gi + size: 5Gi # annotations: {} finalizers: - kubernetes.io/pvc-protection @@ -507,15 +507,14 @@ ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## -datasources: {} -# datasources.yaml: -# apiVersion: 1 -# datasources: -# - name: Prometheus -# type: prometheus -# url: http://prometheus-prometheus-server -# access: proxy -# isDefault: true +datasources: + datasources.yaml: + apiVersion: 1 + datasources: + - name: Prometheus + type: prometheus + url: http://prometheus-server.monitoring.svc.cluster.local + isDefault: true # - name: CloudWatch # type: cloudwatch # access: proxy install helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml \u5b9f\u884c\u30ed\u30b0 $ helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml Release \"grafana\" does not exist. Installing it now. W1030 15:08:59.041312 221007 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W1030 15:08:59.265293 221007 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: grafana LAST DEPLOYED: Sun Oct 30 15:08:54 2022 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: 1. Get your 'admin' user password by running: kubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo 2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster: grafana.monitoring.svc.cluster.local Get the Grafana URL to visit by running these commands in the same shell: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc --namespace monitoring -w grafana' export SERVICE_IP=$(kubectl get svc --namespace monitoring grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}') http://$SERVICE_IP:80 3. Login with the password from step 1 and the username: admin ################################################################################# ###### WARNING: Persistence is disabled!!! You will lose your data when ##### ###### the Grafana pod is terminated. ##### ################################################################################# grafana Service\u306eMetalLB\u3067\u6255\u3044\u51fa\u3055\u308c\u305fExternal IP\u30a2\u30c9\u30ec\u30b9\u3092\u78ba\u8a8d\u3059\u308b kubectl get service -n monitoring grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}' \u30d6\u30e9\u30a6\u30b6\u304b\u3089\u30a2\u30af\u30bb\u30b9 \u30ed\u30b0\u30a4\u30f3\u753b\u9762 username admin password grafana\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6642\u306e\u8868\u793a\u3092\u53c2\u8003\u306b\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067 admin \u306e password \u3092\u53d6\u5f97 kubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo \u30ed\u30b0\u30a4\u30f3\u6210\u529f","title":"Install"},{"location":"monitoring/prometheus/install_grafana/#install-node-exporter-full-dashboards","text":"","title":"Install Node Exporter Full Dashboards"},{"location":"monitoring/prometheus/install_grafana/#grafana","text":"Info grafana helm chart\u3067\u306fgrafana dashboard import\u8a2d\u5b9a\u304c\u53ef\u80fd\u3067\u3059\u3002 .Values.dashboards \u306b\u8a2d\u5b9a\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001 configmap.yaml \u306b\u3042\u308b download_dashboards.sh \u3067\u6307\u5b9a\u3057\u305fdashboard import\u8a2d\u5b9a\u304c\u5165\u308a\u307e\u3059\u3002\u305d\u3057\u3066 Pod\u306einitContainers \u3067 download-dashboards.sh \u3092\u5b9f\u884c\u3057\u3066\u3044\u307e\u3059\u3002 https://github.com/grafana/helm-charts/blob/grafana-6.43.5/charts/grafana/README.md https://github.com/grafana/helm-charts/blob/grafana-6.43.5/charts/grafana/values.yaml#L629-L658 values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63 vim ~/work/prometheus/grafana/grafana.yaml grafana.yaml \u4fee\u6b63\u5f8c\u306ediff( cadvisor-exporter Dashboard\u3082\u5165\u308c\u3066\u3044\u308b\u4f8b\u3067\u3059) $ diff -u <(curl -s https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml) <(cat ~/work/prometheus/grafana/grafana.yaml) ~ snip ~ @@ -632,7 +632,17 @@ ## ## dashboards per provider, use provider name as key. ## -dashboards: {} +dashboards: + default: + node-exporter-full: + # https://grafana.com/grafana/dashboards/1860-node-exporter-full/ + gnetId: 1860 + datasource: Prometheus + cadvisor-exporter: + # https://grafana.com/grafana/dashboards/14282-cadvisor-exporter/ + gnetId: 14282 + datasource: Prometheus + # default: # some-dashboard: # json: | install helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml grafana\u30b3\u30f3\u30c6\u30ca:/var/lib/grafana/dashboards/default/ \u306b Dashboard\u5b9a\u7fa9\u3067\u3042\u308bjson\u30d5\u30a1\u30a4\u30eb\u304c\u914d\u7f6e\u6e08\u307f\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl exec -it -c grafana -n monitoring grafana-5f4c7d46db-xtktb -- ls -l /var/lib/grafana/dashboards/default/ total 232 -rw-r--r-- 1 grafana 472 18484 Nov 8 16:03 cadvisor-exporter.json -rw-r--r-- 1 grafana 472 215154 Nov 8 16:03 node-exporter-full.json","title":"grafana\u3068\u4e00\u7dd2\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408"},{"location":"monitoring/prometheus/install_grafana/#_1","text":"Dashboards\u30da\u30fc\u30b8\u306b\u30a2\u30af\u30bb\u30b9 New -> New Dashboard -> Import \u3092\u9078\u629e Node Exporter Full \u306e Dashboard ID\u3092\u5165\u529b https://grafana.com/grafana/dashboards/1860-node-exporter-full/ Import \u3092\u5b9f\u884c\u3059\u308b Node Exporter Full \u304c\u8868\u793a\u3055\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d","title":"\u624b\u52d5\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408"},{"location":"monitoring/prometheus/install_prometheus/","text":"Prometheus Installing https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus Install values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63 Ephemeral Storage\u306e storageClass \u3092 OpenEBS\u306ejiva storage\u306e\u3082\u306e\u3068\u3059\u308b( e.g. openebs-jiva-csi-default ) Info Installing OpenEBS \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0 Prometheus \u3084 Alertmanager \u306e WebUI\u3092\u5229\u7528\u3057\u305f\u3044\u5834\u5408 NodeIp:NodePort \u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u5834\u5408 service.type: NodePort MetalLB \u3067\u6255\u3044\u51fa\u3057\u305fexternal ip\u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u5834\u5408 service.type: LoadBalancer annotations \u306b metallb.universe.tf/address-pool: ip-pool Info MetalLB \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0 mkdir -p ~/work/prometheus curl -so ~/work/prometheus/prometheus.yaml https://raw.githubusercontent.com/prometheus-community/helm-charts/main/charts/prometheus/values.yaml vim ~/work/prometheus/prometheus.yaml prometheus.yaml \u4fee\u6b63\u5f8c\u306ediff $ diff -u <(curl -s https://raw.githubusercontent.com/prometheus-community/helm-charts/main/charts/prometheus/values.yaml) <(cat ~/work/prometheus/prometheus.yaml) --- /dev/fd/63 2022-10-30 14:07:09.532561438 +0000 +++ /dev/fd/62 2022-10-30 14:07:09.540561310 +0000 @@ -231,7 +231,7 @@ ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## - # storageClass: \"-\" + storageClass: \"openebs-jiva-csi-default\" ## alertmanager data Persistent Volume Binding Mode ## If defined, volumeBindingMode: <volumeBindingMode> @@ -947,7 +947,7 @@ ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## - # storageClass: \"-\" + storageClass: \"openebs-jiva-csi-default\" ## Prometheus server data Persistent Volume Binding Mode ## If defined, volumeBindingMode: <volumeBindingMode> @@ -1403,7 +1403,7 @@ ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## - # storageClass: \"-\" + storageClass: \"openebs-jiva-csi-default\" ## pushgateway data Persistent Volume Binding Mode ## If defined, volumeBindingMode: <volumeBindingMode> install helm upgrade -i prometheus -n monitoring --create-namespace prometheus-community/prometheus -f ~/work/prometheus/prometheus.yaml \u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u8a8d\u3059\u308b Service $ kubectl get services -n monitoring NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus-alertmanager ClusterIP 10.32.0.114 <none> 80/TCP 33h prometheus-kube-state-metrics ClusterIP 10.32.0.246 <none> 8080/TCP 33h prometheus-node-exporter ClusterIP 10.32.0.31 <none> 9100/TCP 33h prometheus-pushgateway ClusterIP 10.32.0.138 <none> 9091/TCP 33h prometheus-server ClusterIP 10.32.0.75 <none> 80/TCP 33h Deployment $ kubectl get deployments -n monitoring NAME READY UP-TO-DATE AVAILABLE AGE prometheus-alertmanager 1/1 1 1 33h prometheus-kube-state-metrics 1/1 1 1 33h prometheus-pushgateway 1/1 1 1 33h prometheus-server 1/1 1 1 33h DaemonSet $ kubectl get DaemonSet -n monitoring -l app=prometheus NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE prometheus-node-exporter 2 2 2 2 2 <none> 35h PersistentVolumeClaim $ kubectl get PersistentVolumeClaim -n monitoring NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE prometheus-alertmanager Bound pvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd 2Gi RWO openebs-jiva-csi-default 33h prometheus-server Bound pvc-836ef65a-da18-4453-96a6-7b909d0c668b 8Gi RWO openebs-jiva-csi-default 33h PersistentVolume $ kubectl get PersistentVolume -n monitoring pvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd pvc-836ef65a-da18-4453-96a6-7b909d0c668b NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd 2Gi RWO Delete Bound monitoring/prometheus-alertmanager openebs-jiva-csi-default 33h pvc-836ef65a-da18-4453-96a6-7b909d0c668b 8Gi RWO Delete Bound monitoring/prometheus-server openebs-jiva-csi-default 33h ServiceAccount $ kubectl get ServiceAccount -n monitoring -l app=prometheus NAME SECRETS AGE default 1 9d prometheus-alertmanager 1 35h prometheus-kube-prometheus-admission 1 4d23h prometheus-kube-state-metrics 1 35h prometheus-node-exporter 1 35h prometheus-pushgateway 1 35h prometheus-server 1 35h ClusterRole $ kubectl get ClusterRole -n monitoring -l app=prometheus NAME CREATED AT prometheus-alertmanager 2022-10-30T03:25:38Z prometheus-pushgateway 2022-10-30T03:25:38Z prometheus-server 2022-10-30T03:25:38Z ClusterRoleBinding $ kubectl get ClusterRoleBinding -n monitoring -l app=prometheus NAME ROLE AGE prometheus-alertmanager ClusterRole/prometheus-alertmanager 35h prometheus-pushgateway ClusterRole/prometheus-pushgateway 35h prometheus-server ClusterRole/prometheus-server 35h ConfigMap $ kubectl get ConfigMap -n monitoring -l app=prometheus NAME DATA AGE prometheus-alertmanager 2 35h prometheus-server 6 35h Info ConfigMap\u306b\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b prometheus.yml \u306e\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u305f\u3044\u5834\u5408\u306f\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067\u78ba\u8a8d kubectl get ConfigMap -n monitoring prometheus-server -o jsonpath=\"{.data.prometheus\\.yml}\" Confirm FQDN of prometheus-server service A Record Grafana\u3092helm install\u3059\u308b\u969b\u306b datasources.datasources[0].url \u306b\u8a2d\u5b9a\u3059\u308bFQDN\u3092\u78ba\u8a8d\u3059\u308b prometheus-server.monitoring.svc.cluster.local $ nslookup prometheus-server.monitoring.svc.cluster.local `kubectl get ep -n kube-system kube-dns -o jsonpath=\"{.subsets[0].addresses[0].ip}\"` Server: 10.200.2.235 Address: 10.200.2.235#53 Name: prometheus-server.monitoring.svc.cluster.local Address: 10.32.0.75","title":"Install Prometheus"},{"location":"monitoring/prometheus/install_prometheus/#prometheus-installing","text":"https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus","title":"Prometheus Installing"},{"location":"monitoring/prometheus/install_prometheus/#install","text":"values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63 Ephemeral Storage\u306e storageClass \u3092 OpenEBS\u306ejiva storage\u306e\u3082\u306e\u3068\u3059\u308b( e.g. openebs-jiva-csi-default ) Info Installing OpenEBS \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0 Prometheus \u3084 Alertmanager \u306e WebUI\u3092\u5229\u7528\u3057\u305f\u3044\u5834\u5408 NodeIp:NodePort \u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u5834\u5408 service.type: NodePort MetalLB \u3067\u6255\u3044\u51fa\u3057\u305fexternal ip\u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u5834\u5408 service.type: LoadBalancer annotations \u306b metallb.universe.tf/address-pool: ip-pool Info MetalLB \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0 mkdir -p ~/work/prometheus curl -so ~/work/prometheus/prometheus.yaml https://raw.githubusercontent.com/prometheus-community/helm-charts/main/charts/prometheus/values.yaml vim ~/work/prometheus/prometheus.yaml prometheus.yaml \u4fee\u6b63\u5f8c\u306ediff $ diff -u <(curl -s https://raw.githubusercontent.com/prometheus-community/helm-charts/main/charts/prometheus/values.yaml) <(cat ~/work/prometheus/prometheus.yaml) --- /dev/fd/63 2022-10-30 14:07:09.532561438 +0000 +++ /dev/fd/62 2022-10-30 14:07:09.540561310 +0000 @@ -231,7 +231,7 @@ ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## - # storageClass: \"-\" + storageClass: \"openebs-jiva-csi-default\" ## alertmanager data Persistent Volume Binding Mode ## If defined, volumeBindingMode: <volumeBindingMode> @@ -947,7 +947,7 @@ ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## - # storageClass: \"-\" + storageClass: \"openebs-jiva-csi-default\" ## Prometheus server data Persistent Volume Binding Mode ## If defined, volumeBindingMode: <volumeBindingMode> @@ -1403,7 +1403,7 @@ ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## - # storageClass: \"-\" + storageClass: \"openebs-jiva-csi-default\" ## pushgateway data Persistent Volume Binding Mode ## If defined, volumeBindingMode: <volumeBindingMode> install helm upgrade -i prometheus -n monitoring --create-namespace prometheus-community/prometheus -f ~/work/prometheus/prometheus.yaml \u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u8a8d\u3059\u308b Service $ kubectl get services -n monitoring NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus-alertmanager ClusterIP 10.32.0.114 <none> 80/TCP 33h prometheus-kube-state-metrics ClusterIP 10.32.0.246 <none> 8080/TCP 33h prometheus-node-exporter ClusterIP 10.32.0.31 <none> 9100/TCP 33h prometheus-pushgateway ClusterIP 10.32.0.138 <none> 9091/TCP 33h prometheus-server ClusterIP 10.32.0.75 <none> 80/TCP 33h Deployment $ kubectl get deployments -n monitoring NAME READY UP-TO-DATE AVAILABLE AGE prometheus-alertmanager 1/1 1 1 33h prometheus-kube-state-metrics 1/1 1 1 33h prometheus-pushgateway 1/1 1 1 33h prometheus-server 1/1 1 1 33h DaemonSet $ kubectl get DaemonSet -n monitoring -l app=prometheus NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE prometheus-node-exporter 2 2 2 2 2 <none> 35h PersistentVolumeClaim $ kubectl get PersistentVolumeClaim -n monitoring NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE prometheus-alertmanager Bound pvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd 2Gi RWO openebs-jiva-csi-default 33h prometheus-server Bound pvc-836ef65a-da18-4453-96a6-7b909d0c668b 8Gi RWO openebs-jiva-csi-default 33h PersistentVolume $ kubectl get PersistentVolume -n monitoring pvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd pvc-836ef65a-da18-4453-96a6-7b909d0c668b NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd 2Gi RWO Delete Bound monitoring/prometheus-alertmanager openebs-jiva-csi-default 33h pvc-836ef65a-da18-4453-96a6-7b909d0c668b 8Gi RWO Delete Bound monitoring/prometheus-server openebs-jiva-csi-default 33h ServiceAccount $ kubectl get ServiceAccount -n monitoring -l app=prometheus NAME SECRETS AGE default 1 9d prometheus-alertmanager 1 35h prometheus-kube-prometheus-admission 1 4d23h prometheus-kube-state-metrics 1 35h prometheus-node-exporter 1 35h prometheus-pushgateway 1 35h prometheus-server 1 35h ClusterRole $ kubectl get ClusterRole -n monitoring -l app=prometheus NAME CREATED AT prometheus-alertmanager 2022-10-30T03:25:38Z prometheus-pushgateway 2022-10-30T03:25:38Z prometheus-server 2022-10-30T03:25:38Z ClusterRoleBinding $ kubectl get ClusterRoleBinding -n monitoring -l app=prometheus NAME ROLE AGE prometheus-alertmanager ClusterRole/prometheus-alertmanager 35h prometheus-pushgateway ClusterRole/prometheus-pushgateway 35h prometheus-server ClusterRole/prometheus-server 35h ConfigMap $ kubectl get ConfigMap -n monitoring -l app=prometheus NAME DATA AGE prometheus-alertmanager 2 35h prometheus-server 6 35h Info ConfigMap\u306b\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b prometheus.yml \u306e\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u305f\u3044\u5834\u5408\u306f\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067\u78ba\u8a8d kubectl get ConfigMap -n monitoring prometheus-server -o jsonpath=\"{.data.prometheus\\.yml}\" Confirm FQDN of prometheus-server service A Record Grafana\u3092helm install\u3059\u308b\u969b\u306b datasources.datasources[0].url \u306b\u8a2d\u5b9a\u3059\u308bFQDN\u3092\u78ba\u8a8d\u3059\u308b prometheus-server.monitoring.svc.cluster.local $ nslookup prometheus-server.monitoring.svc.cluster.local `kubectl get ep -n kube-system kube-dns -o jsonpath=\"{.subsets[0].addresses[0].ip}\"` Server: 10.200.2.235 Address: 10.200.2.235#53 Name: prometheus-server.monitoring.svc.cluster.local Address: 10.32.0.75","title":"Install"},{"location":"operations/backup_etcd/","text":"Backup etcd etcdctl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb sudo apt install etcd-client ercd \u758e\u901a\u78ba\u8a8d https://github.com/etcd-io/etcd/tree/main/etcdctl#endpoint-health ETCD_ENDPOINT=https://192.168.10.50:2379 ETCD_CACERT=/etc/etcd/ca.pem ETCD_CERT=/etc/etcd/kubernetes.pem ETCD_KEY=/etc/etcd/kubernetes-key.pem ETCDCTL_API=3 etcdctl endpoint health \\ --endpoints=${ETCD_ENDPOINT} \\ --cacert=${ETCD_CACERT} \\ --cert=${ETCD_CERT} \\ --key=${ETCD_KEY} https://192.168.10.50:2379 is healthy: successfully committed proposal: took = 32.557549ms \u306a\u3069\u306e\u6a19\u6e96\u51fa\u529b\u304c\u78ba\u8a8d\u3067\u304d\u308c\u3070\u758e\u901a\u3067\u304d\u3066\u3044\u308b etcd \u30d0\u30c3\u30af\u30a2\u30c3\u30d7 https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-save-filename ETCD_ENDPOINT=https://192.168.10.50:2379 ETCD_CACERT=/etc/etcd/ca.pem ETCD_CERT=/etc/etcd/kubernetes.pem ETCD_KEY=/etc/etcd/kubernetes-key.pem ETCDCTL_API=3 etcdctl snapshot save snapshot.db \\ --endpoints=${ETCD_ENDPOINT} \\ --cacert=${ETCD_CACERT} \\ --cert=${ETCD_CERT} \\ --key=${ETCD_KEY} \u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30d5\u30a1\u30a4\u30eb\u306e\u60c5\u5831\u3092\u78ba\u8a8d https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-status-filename ETCDCTL_API=3 etcdctl snapshot status snapshot.db \u4ee5\u4e0b\u306e\u3088\u3046\u306a\u51fa\u529b\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059 +----------+----------+------------+------------+---------+ | HASH | REVISION | TOTAL KEYS | TOTAL SIZE | VERSION | +----------+----------+------------+------------+---------+ | c9a815d0 | 3356 | 488 | 1.1 MB | | +----------+----------+------------+------------+---------+ etcd \u30ea\u30b9\u30c8\u30a2 https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-restore-options-filename ETCD_ENDPOINT=https://192.168.10.50:2379 ETCD_CACERT=/etc/etcd/ca.pem ETCD_CERT=/etc/etcd/kubernetes.pem ETCD_KEY=/etc/etcd/kubernetes-key.pem ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --endpoints=${ETCD_ENDPOINT} \\ --cacert=${ETCD_CACERT} \\ --cert=${ETCD_CERT} \\ --key=${ETCD_KEY}","title":"etcd"},{"location":"operations/backup_etcd/#backup-etcd","text":"","title":"Backup etcd"},{"location":"operations/backup_etcd/#etcdctl","text":"sudo apt install etcd-client","title":"etcdctl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"operations/backup_etcd/#ercd","text":"https://github.com/etcd-io/etcd/tree/main/etcdctl#endpoint-health ETCD_ENDPOINT=https://192.168.10.50:2379 ETCD_CACERT=/etc/etcd/ca.pem ETCD_CERT=/etc/etcd/kubernetes.pem ETCD_KEY=/etc/etcd/kubernetes-key.pem ETCDCTL_API=3 etcdctl endpoint health \\ --endpoints=${ETCD_ENDPOINT} \\ --cacert=${ETCD_CACERT} \\ --cert=${ETCD_CERT} \\ --key=${ETCD_KEY} https://192.168.10.50:2379 is healthy: successfully committed proposal: took = 32.557549ms \u306a\u3069\u306e\u6a19\u6e96\u51fa\u529b\u304c\u78ba\u8a8d\u3067\u304d\u308c\u3070\u758e\u901a\u3067\u304d\u3066\u3044\u308b","title":"ercd \u758e\u901a\u78ba\u8a8d"},{"location":"operations/backup_etcd/#etcd","text":"https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-save-filename ETCD_ENDPOINT=https://192.168.10.50:2379 ETCD_CACERT=/etc/etcd/ca.pem ETCD_CERT=/etc/etcd/kubernetes.pem ETCD_KEY=/etc/etcd/kubernetes-key.pem ETCDCTL_API=3 etcdctl snapshot save snapshot.db \\ --endpoints=${ETCD_ENDPOINT} \\ --cacert=${ETCD_CACERT} \\ --cert=${ETCD_CERT} \\ --key=${ETCD_KEY}","title":"etcd \u30d0\u30c3\u30af\u30a2\u30c3\u30d7"},{"location":"operations/backup_etcd/#_1","text":"https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-status-filename ETCDCTL_API=3 etcdctl snapshot status snapshot.db \u4ee5\u4e0b\u306e\u3088\u3046\u306a\u51fa\u529b\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059 +----------+----------+------------+------------+---------+ | HASH | REVISION | TOTAL KEYS | TOTAL SIZE | VERSION | +----------+----------+------------+------------+---------+ | c9a815d0 | 3356 | 488 | 1.1 MB | | +----------+----------+------------+------------+---------+","title":"\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30d5\u30a1\u30a4\u30eb\u306e\u60c5\u5831\u3092\u78ba\u8a8d"},{"location":"operations/backup_etcd/#etcd_1","text":"https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-restore-options-filename ETCD_ENDPOINT=https://192.168.10.50:2379 ETCD_CACERT=/etc/etcd/ca.pem ETCD_CERT=/etc/etcd/kubernetes.pem ETCD_KEY=/etc/etcd/kubernetes-key.pem ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --endpoints=${ETCD_ENDPOINT} \\ --cacert=${ETCD_CACERT} \\ --cert=${ETCD_CERT} \\ --key=${ETCD_KEY}","title":"etcd \u30ea\u30b9\u30c8\u30a2"},{"location":"pod_security/","text":"Pod https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/","title":"Index"},{"location":"pod_security/#pod","text":"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/","title":"Pod"},{"location":"secrets/about_secrets/","text":"About Secrets \u53c2\u8003 https://kubernetes.io/ja/docs/concepts/configuration/secret/ https://kubernetes.io/ja/docs/tasks/configmap-secret/managing-secret-using-kubectl/ Overview API Key\u3001Token\u3001SSH Key\u306a\u3069\u6a5f\u5bc6\u6027\u306e\u9ad8\u3044\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u305f\u3081\u306e\u30ea\u30bd\u30fc\u30b9 \u53c2\u7167\u65b9\u6cd5 Volume\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u30de\u30a6\u30f3\u30c8 \u74b0\u5883\u5909\u6570 imagePullSecrets \u5024\u306e\u683c\u7d0d\u30d5\u30a9\u30fc\u30de\u30c3\u30c8 date : base64\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3055\u308c\u305f\u6587\u5b57\u5217 stringData : \u5e73\u6587\u306a\u3069\u305d\u306e\u307e\u307e\u683c\u7d0d\u3057\u305f\u3044\u5834\u5408 Kind of secret Builtin Type Opaque kubernetes.io/service-account-token kubernetes.io/dockercfg kubernetes.io/dockerconfigjson kubernetes.io/basic-auth kubernetes.io/ssh-auth kubernetes.io/tls bootstrap.kubernetes.io/token","title":"About Secrets"},{"location":"secrets/about_secrets/#about-secrets","text":"","title":"About Secrets"},{"location":"secrets/about_secrets/#_1","text":"https://kubernetes.io/ja/docs/concepts/configuration/secret/ https://kubernetes.io/ja/docs/tasks/configmap-secret/managing-secret-using-kubectl/","title":"\u53c2\u8003"},{"location":"secrets/about_secrets/#overview","text":"API Key\u3001Token\u3001SSH Key\u306a\u3069\u6a5f\u5bc6\u6027\u306e\u9ad8\u3044\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u305f\u3081\u306e\u30ea\u30bd\u30fc\u30b9 \u53c2\u7167\u65b9\u6cd5 Volume\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u30de\u30a6\u30f3\u30c8 \u74b0\u5883\u5909\u6570 imagePullSecrets \u5024\u306e\u683c\u7d0d\u30d5\u30a9\u30fc\u30de\u30c3\u30c8 date : base64\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3055\u308c\u305f\u6587\u5b57\u5217 stringData : \u5e73\u6587\u306a\u3069\u305d\u306e\u307e\u307e\u683c\u7d0d\u3057\u305f\u3044\u5834\u5408","title":"Overview"},{"location":"secrets/about_secrets/#kind-of-secret","text":"","title":"Kind of secret"},{"location":"secrets/about_secrets/#builtin-type","text":"Opaque kubernetes.io/service-account-token kubernetes.io/dockercfg kubernetes.io/dockerconfigjson kubernetes.io/basic-auth kubernetes.io/ssh-auth kubernetes.io/tls bootstrap.kubernetes.io/token","title":"Builtin Type"},{"location":"setup/01_setup_RaspberryPi/","text":"Raspberry Pi \u69cb\u6210 \u8cfc\u5165\u3057\u305f\u3082\u306e Raspberry Pi 4 Model B/2GB x3 GeeekPi Raspberry Pi4 \u30af\u30e9\u30b9\u30bf\u30fc\u30b1\u30fc\u30b9(\u51b7\u5374\u30d5\u30a1\u30f3\u4ed8\u304d) x1 Anker PowerPort I PD - 1 PD & 4 PowerIQ x1 Amazon\u30d9\u30fc\u30b7\u30c3\u30af HDMI\u30b1\u30fc\u30d6\u30eb 0.9m (\u30bf\u30a4\u30d7A\u30aa\u30b9 - \u30de\u30a4\u30af\u30ed\u30bf\u30a4\u30d7D\u30aa\u30b9) x1 Samsung EVO Plus 32GB microSDHC x3 Anker USB Type C \u30b1\u30fc\u30d6\u30eb PowerLine USB-C & USB-A 3.0 \u30b1\u30fc\u30d6\u30eb x3","title":"01. Raspberry Pi \u69cb\u6210"},{"location":"setup/01_setup_RaspberryPi/#raspberry-pi","text":"","title":"Raspberry Pi \u69cb\u6210"},{"location":"setup/01_setup_RaspberryPi/#_1","text":"Raspberry Pi 4 Model B/2GB x3 GeeekPi Raspberry Pi4 \u30af\u30e9\u30b9\u30bf\u30fc\u30b1\u30fc\u30b9(\u51b7\u5374\u30d5\u30a1\u30f3\u4ed8\u304d) x1 Anker PowerPort I PD - 1 PD & 4 PowerIQ x1 Amazon\u30d9\u30fc\u30b7\u30c3\u30af HDMI\u30b1\u30fc\u30d6\u30eb 0.9m (\u30bf\u30a4\u30d7A\u30aa\u30b9 - \u30de\u30a4\u30af\u30ed\u30bf\u30a4\u30d7D\u30aa\u30b9) x1 Samsung EVO Plus 32GB microSDHC x3 Anker USB Type C \u30b1\u30fc\u30d6\u30eb PowerLine USB-C & USB-A 3.0 \u30b1\u30fc\u30d6\u30eb x3","title":"\u8cfc\u5165\u3057\u305f\u3082\u306e"},{"location":"setup/02_setup_ubuntu20-04-LTS/","text":"OS Setup UbuntuServer20.04.2LTS Setup GeeekPi\u30b1\u30fc\u30b9\u306e\u7d44\u307f\u7acb\u3066 & \u7d50\u7dda & \u8d77\u52d5\u78ba\u8a8d Raspberry Pi Imager \u3067SD\u30ab\u30fc\u30c9\u306bUbuntuServer20.04.2LTS(64bit) \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Raspberry Pi 4\u3078SD\u30ab\u30fc\u30c9\u3092\u633f\u5165\u3057\u8d77\u52d5 wifi\u8a2d\u5b9a sudo vim /etc/netplan/50-cloud-init.yaml # config check sudo netplan --debug try sudo netplan --debug generate # \u9069\u7528 sudo netplan --debug apply /etc/netplan/50-cloud-init.yaml (master\u306e\u5834\u5408) network: ethernets: eth0: dhcp4: true optional: true version: 2 wifis: wlan0: optional: true dhcp4: false addresses: - 192.168.3.50/24 gateway4: 192.168.3.1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 search: [] access-points: \"<SSID\u540d>\": password: \"<\u30d1\u30b9\u30ef\u30fc\u30c9>\" package\u66f4\u65b0 sudo apt update sudo apt upgrade -y \u65e5\u672c\u8a9e\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u5909\u66f4\u3057\u518d\u8d77\u52d5 sudo dpkg-reconfigure keyboard-configuration sudo reboot Generic 105-key (Intl) PC \u3092\u9078\u629e Japanese \u3092\u9078\u629e Japanese \u3092\u9078\u629e The default for the keyboard layout \u3092\u9078\u629e No compose key \u3092\u9078\u629e LOCALE sudo apt install -y language-pack-ja sudo update-locale LANG=ja_JP.UTF-8 hostname #name=k8s-node1 #name=k8s-node2 name=k8s-master echo ${name} | sudo tee /etc/hostname sudo sed -i -e 's/127.0.1.1.*/127.0.1.1\\t'$name'/' /etc/hosts /etc/hosts (\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5909\u308f\u3063\u3066\u3082\u758e\u901a\u3057\u305f\u3044\u305f\u3081) k8s\u5185\u3067\u306f\u30db\u30b9\u30c8\u540d\u901a\u4fe1\u3059\u308b\u305f\u3081\u306bhost\u3092\u66f8\u304d\u8fbc\u3080 k8s-master k8s-node1 k8s-node2","title":"02. OS Install"},{"location":"setup/02_setup_ubuntu20-04-LTS/#os-setup","text":"","title":"OS Setup"},{"location":"setup/02_setup_ubuntu20-04-LTS/#ubuntuserver20042lts-setup","text":"GeeekPi\u30b1\u30fc\u30b9\u306e\u7d44\u307f\u7acb\u3066 & \u7d50\u7dda & \u8d77\u52d5\u78ba\u8a8d Raspberry Pi Imager \u3067SD\u30ab\u30fc\u30c9\u306bUbuntuServer20.04.2LTS(64bit) \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Raspberry Pi 4\u3078SD\u30ab\u30fc\u30c9\u3092\u633f\u5165\u3057\u8d77\u52d5 wifi\u8a2d\u5b9a sudo vim /etc/netplan/50-cloud-init.yaml # config check sudo netplan --debug try sudo netplan --debug generate # \u9069\u7528 sudo netplan --debug apply /etc/netplan/50-cloud-init.yaml (master\u306e\u5834\u5408) network: ethernets: eth0: dhcp4: true optional: true version: 2 wifis: wlan0: optional: true dhcp4: false addresses: - 192.168.3.50/24 gateway4: 192.168.3.1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 search: [] access-points: \"<SSID\u540d>\": password: \"<\u30d1\u30b9\u30ef\u30fc\u30c9>\" package\u66f4\u65b0 sudo apt update sudo apt upgrade -y \u65e5\u672c\u8a9e\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u5909\u66f4\u3057\u518d\u8d77\u52d5 sudo dpkg-reconfigure keyboard-configuration sudo reboot Generic 105-key (Intl) PC \u3092\u9078\u629e Japanese \u3092\u9078\u629e Japanese \u3092\u9078\u629e The default for the keyboard layout \u3092\u9078\u629e No compose key \u3092\u9078\u629e LOCALE sudo apt install -y language-pack-ja sudo update-locale LANG=ja_JP.UTF-8 hostname #name=k8s-node1 #name=k8s-node2 name=k8s-master echo ${name} | sudo tee /etc/hostname sudo sed -i -e 's/127.0.1.1.*/127.0.1.1\\t'$name'/' /etc/hosts /etc/hosts (\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5909\u308f\u3063\u3066\u3082\u758e\u901a\u3057\u305f\u3044\u305f\u3081) k8s\u5185\u3067\u306f\u30db\u30b9\u30c8\u540d\u901a\u4fe1\u3059\u308b\u305f\u3081\u306bhost\u3092\u66f8\u304d\u8fbc\u3080 k8s-master k8s-node1 k8s-node2","title":"UbuntuServer20.04.2LTS Setup"},{"location":"setup/03_common_settings/","text":"swap\u3092\u7121\u52b9\u306b\u3059\u308b swap\u304c\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ free -h total used free shared buff/cache available Mem: 1.8Gi 54Mi 1.5Gi 8.0Mi 244Mi 1.7Gi Swap: 99Mi 0B 99Mi swap\u3092\u7121\u52b9\u306b\u8a2d\u5b9a\u3059\u308b sudo swapoff --all # Ubuntu Desktop 22.04 LTS (for RPi4) sudo systemctl stop dphys-swapfile sudo systemctl disable dphys-swapfile swap\u304c\u7121\u52b9\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b $ free -h total used free shared buff/cache available Mem: 1.8Gi 57Mi 1.5Gi 8.0Mi 251Mi 1.7Gi Swap: 0B 0B 0B $ systemctl status dphys-swapfile \u25cf dphys-swapfile.service - dphys-swapfile - set up, mount/unmount, and delete a swap file Loaded: loaded (/lib/systemd/system/dphys-swapfile.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:dphys-swapfile(8) 12\u6708 30 20:48:54 k8s-master1 systemd[1]: Starting dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 30 20:48:55 k8s-master1 dphys-swapfile[330]: want /var/swap=100MByte, checking existing: keeping it 12\u6708 30 20:48:55 k8s-master1 systemd[1]: Started dphys-swapfile - set up, mount/unmount, and delete a swap file. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopping dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 31 06:57:57 k8s-master1 systemd[1]: dphys-swapfile.service: Succeeded. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopped dphys-swapfile - set up, mount/unmount, and delete a swap file. cgroupfs \u306ememory\u3092\u6709\u52b9\u306b\u3059\u308b kernel\u306eboot option\u306b cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory \u3092\u8ffd\u8a18\u3059\u308b sudo vim /boot/firmware/cmdline.txt cmdline.txt \u306e\u6709\u52b9\u884c\u306e\u78ba\u8a8d $ sudo sed -e 's/\\s/\\n/g' /boot/firmware/cmdline.txt console=serial0,115200 console=tty1 root=PARTUUID=fb7271c3-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait quiet splash plymouth.ignore-serial-consoles cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory $ sudo reboot $ cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 9 1 1 cpu 5 34 1 cpuacct 5 34 1 blkio 10 34 1 memory 8 80 1 devices 4 34 1 freezer 7 1 1 net_cls 2 1 1 perf_event 6 1 1 net_prio 2 1 1 pids 3 39 1 Container RunTime\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb containerd \u3092\u63a1\u7528 \u521d\u671f\u306f cri-o \u3092\u63a1\u7528\u3057\u3066\u3044\u305f\u304c\u4ee5\u4e0b\u7406\u7531\u3067 containerd \u3078\u5909\u66f4 CNCF\u3067Graduated Project ( cri-o\u306fincubating ) AWS eks-optimized AMI\u3067\u306fcontainerd\u304c\u6a19\u6e96\u3068\u306a\u308a\u305d\u3046(eks 1.22 \u3067\u306fdocker\u304cdefault) buildkit \u3068\u7d44\u307f\u5408\u308f\u305b\u3066image build\u304c\u53ef\u80fd (cri-o\u3067\u306e\u53ef\u5426\u306f\u672a\u78ba\u8a8d) https://speakerdeck.com/ktock/dockerkaracontainerdhefalseyi-xing?slide=7 \u524d\u63d0\u4f5c\u696d https://kubernetes.io/docs/setup/production-environment/container-runtimes/ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # sysctl params required by setup, params persist across reboots cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # Apply sysctl params without reboot sudo sysctl --system Containerd https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd https://github.com/containerd/containerd/blob/main/docs/getting-started.md \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb for apt package refs https://docs.docker.com/engine/install/ubuntu/ sudo apt update sudo apt install -y ca-certificates curl gnupg lsb-release sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt update sudo apt install -y containerd.io /etc/containerd/config.toml sudo containerd config default | sudo tee /etc/containerd/config.toml sudo vim /etc/containerd/config.toml refs https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd Configuring the systemd cgroup driver Overriding the sandbox (pause) image restart containerd sudo systemctl restart containerd CRI-O https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o kernel module\u306eload overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module overlay iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module br_netfilter kernel parameter\u306eset iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a kernel module\u306eload overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module overlay iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module br_netfilter cat <<EOF | sudo tee /etc/modules-load.d/crio.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter kernel parameter\u306eset iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf # https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF sudo sysctl --system system\u8d77\u52d5\u6642\u306b kernel parameter \u3092\u518d\u8aad\u307f\u8fbc\u307f\u3055\u305b\u308b kube-proxy\u306b\u3066\u5fc5\u8981\u306akernel parameter\u8a2d\u5b9a(kubelet\u8a2d\u5b9a\u624b\u9806\u306b\u3066\u5f8c\u8ff0) \u304ciptables\u8d77\u52d5\u6642\u306ekernel module load\u3067\u4e0a\u66f8\u304d\u3055\u308c\u308b\u305f\u3081 \u5229\u7528\u74b0\u5883\u304c /etc/sysconfig/iptables-config \u3092\u5229\u7528\u53ef\u80fd\u306a\u3089 IPTABLES_MODULES_UNLOAD=\"no\" \u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u672c\u624b\u9806\u306f\u4e0d\u8981\u3067\u3059 egrep \"sysctl\\s+--system\" /etc/rc.local > /dev/null || sudo bash -c \"echo \\\"sysctl --system\\\" >> /etc/rc.local\" egrep \"sysctl\\s+--system\" /etc/rc.local CRI-O\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.21/xUbuntu_20.04/arm64/ VERSION=1.21 OS=xUbuntu_20.04 sudo bash -c \"echo \\\"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\" sudo bash -c \"echo \\\"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list\" curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key add - curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key add - sudo apt update sudo apt install -y cri-o cri-o-runc sudo apt install -y conntrack storage driver\u3092 overlay2 \u3078\u5909\u66f4\u3059\u308b sudo vim /etc/containers/storage.conf sudo vim /etc/crio/crio.conf /etc/crio/crio.conf \u3078graph driver\u8a2d\u5b9a\u3092\u5165\u308c\u308b podman\u3084buildah\u3067build\u3057\u305flocal image\u3092\u53c2\u7167\u3059\u308b\u305f\u3081 [crio] \u30bb\u30af\u30b7\u30e7\u30f3\u306b\u5165\u308c\u308b graphroot = \"/var/lib/containers/storage\" /etc/containers/storage.conf [storage] driver = \"overlay2\" runroot = \"/run/containers/storage\" graphroot = \"/var/lib/containers/storage\" [storage.options] additionalimagestores = [ ] [storage.options.overlay] mountopt = \"nodev\" [storage.options.thinpool] /etc/crio/crio.conf [crio] storage_driver = \"overlay2\" graphroot = \"/var/lib/containers/storage\" log_dir = \"/var/log/crio/pods\" version_file = \"/var/run/crio/version\" version_file_persist = \"/var/lib/crio/version\" clean_shutdown_file = \"/var/lib/crio/clean.shutdown\" [crio.api] listen = \"/var/run/crio/crio.sock\" stream_address = \"127.0.0.1\" stream_port = \"0\" stream_enable_tls = false stream_idle_timeout = \"\" stream_tls_cert = \"\" stream_tls_key = \"\" stream_tls_ca = \"\" grpc_max_send_msg_size = 16777216 grpc_max_recv_msg_size = 16777216 [crio.runtime] no_pivot = false decryption_keys_path = \"/etc/crio/keys/\" conmon = \"\" conmon_cgroup = \"system.slice\" conmon_env = [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", ] default_env = [ ] seccomp_profile = \"\" seccomp_use_default_when_empty = false apparmor_profile = \"crio-default\" irqbalance_config_file = \"/etc/sysconfig/irqbalance\" cgroup_manager = \"systemd\" separate_pull_cgroup = \"\" default_capabilities = [ \"CHOWN\", \"DAC_OVERRIDE\", \"FSETID\", \"FOWNER\", \"SETGID\", \"SETUID\", \"SETPCAP\", \"NET_BIND_SERVICE\", \"KILL\", ] default_sysctls = [ ] additional_devices = [ ] hooks_dir = [ \"/usr/share/containers/oci/hooks.d\", ]pids_limit = 1024 log_size_max = -1 log_to_journald = false container_exits_dir = \"/var/run/crio/exits\" container_attach_socket_dir = \"/var/run/crio\" bind_mount_prefix = \"\" read_only = false log_level = \"info\" log_filter = \"\" uid_mappings = \"\" gid_mappings = \"\" ctr_stop_timeout = 30 drop_infra_ctr = false infra_ctr_cpuset = \"\" namespaces_dir = \"/var/run\" pinns_path = \"\" default_runtime = \"runc\" [crio.runtime.runtimes.runc] runtime_path = \"\" runtime_type = \"oci\" runtime_root = \"/run/runc\" allowed_annotations = [ \"io.containers.trace-syscall\", ] [crio.image] default_transport = \"docker://\" global_auth_file = \"\" pause_image = \"k8s.gcr.io/pause:3.2\" pause_image_auth_file = \"\" pause_command = \"/pause\" signature_policy = \"\" image_volumes = \"mkdir\" big_files_temporary_dir = \"\" [crio.network] network_dir = \"/etc/cni/net.d/\" plugin_dirs = [ \"/opt/cni/bin/\", ] [crio.metrics] enable_metrics = false metrics_port = 9090 metrics_socket = \"\" crio\u3092\u518d\u8d77\u52d5\u3059\u308b sudo systemctl daemon-reload sudo systemctl restart crio CLI TOOL nerdctl containerd\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u516c\u958b\u3057\u3066\u3044\u308bdocker-cli\u4e92\u63db\u306eCLI https://github.com/containerd/nerdctl https://speakerdeck.com/ktock/dockerkaracontainerdhefalseyi-xing?slide=22 NERDCTL_VERSION=`curl -s -L https://api.github.com/repos/containerd/nerdctl/releases/latest | jq -r .tag_name` curl -L -s https://github.com/containerd/nerdctl/releases/download/${NERDCTL_VERSION}/nerdctl-`echo ${NERDCTL_VERSION} | sed -e 's/^v//'`-linux-arm64.tar.gz | sudo tar -zxC /usr/local/bin/ ls -l /usr/local/bin buildkit https://github.com/moby/buildkit nerdctl build \u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981 BUILDKIT_VERSION=`curl -s -L https://api.github.com/repos/moby/buildkit/releases/latest | jq -r .tag_name` curl -L -s https://github.com/moby/buildkit/releases/download/${BUILDKIT_VERSION}/buildkit-${BUILDKIT_VERSION}.linux-arm64.tar.gz | sudo tar -zxC /tmp/ sudo mv /tmp/bin/* /usr/local/bin/ ls -l /usr/local/bin sudo curl -sL https://raw.githubusercontent.com/moby/buildkit/${BUILDKIT_VERSION}/examples/systemd/system/buildkit.socket -o /etc/systemd/system/buildkit.socket sudo curl -sL https://raw.githubusercontent.com/moby/buildkit/${BUILDKIT_VERSION}/examples/systemd/system/buildkit.service -o /etc/systemd/system/buildkit.service sudo systemctl enable buildkit.socket buildkit.service sudo systemctl start buildkit.socket buildkit.service buildah cri-o \u5c0e\u5165\u6642\u671f\u306bimage build\u3067\u4f7f\u7528(containerd\u3067\u306f\u524d\u8ff0\u306enerdctl\u3078\u79fb\u884c\u6e08\u307f) https://github.com/containers/buildah/blob/master/install.md sudo apt-get -qq -y install buildah cri-tools(crictl) \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md VERSION=\"v1.22.0\" ARCH=\"arm64\" DOWNLOAD_URL=\"https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-${VERSION}-linux-${ARCH}.tar.gz\" curl -L ${DOWNLOAD_URL} | sudo tar -zxC /usr/local/bin ls -l /usr/local/bin podman \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(optional) https://podman.io/getting-started/installation sudo apt-get -y install podman sudo rm -f /etc/cni/net.d/87-podman-bridge.conflist CNI Plugin\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://github.com/containernetworking/plugins sudo mkdir -p /opt/cni/bin CNI_PLUGIN_VERSION=`curl -s -L https://api.github.com/repos/containernetworking/plugins/releases/latest | jq -r .tag_name` ARCH=\"arm64\" DOWNLOAD_URL=\"https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGIN_VERSION}/cni-plugins-linux-${ARCH}-${CNI_PLUGIN_VERSION}.tgz\" curl -L ${DOWNLOAD_URL} | sudo tar -zxC /opt/cni/bin ls -l /opt/cni/bin/ cni config\u3092\u4f5c\u6210\u3059\u308b https://www.cni.dev/plugins/current/main/bridge/ POD_CIDR=\"10.200.0.0/24\" cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf { \"cniVersion\": \"0.4.0\", \"name\": \"bridge\", \"type\": \"bridge\", \"bridge\": \"cnio0\", \"isGateway\": true, \"ipMasq\": true, \"ipam\": { \"type\": \"host-local\", \"ranges\": [ [{\"subnet\": \"${POD_CIDR}\"}] ], \"routes\": [{\"dst\": \"0.0.0.0/0\"}] } } EOF cat <<EOF | sudo tee /etc/cni/net.d/20-loopback.conf { \"cniVersion\": \"0.4.0\", \"name\": \"lo\", \"type\": \"loopback\" } EOF kubectl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://kubernetes.io/ja/docs/tasks/tools/install-kubectl/","title":"03. master/node\u3067\u5171\u901a\u624b\u9806"},{"location":"setup/03_common_settings/#swap","text":"swap\u304c\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ free -h total used free shared buff/cache available Mem: 1.8Gi 54Mi 1.5Gi 8.0Mi 244Mi 1.7Gi Swap: 99Mi 0B 99Mi swap\u3092\u7121\u52b9\u306b\u8a2d\u5b9a\u3059\u308b sudo swapoff --all # Ubuntu Desktop 22.04 LTS (for RPi4) sudo systemctl stop dphys-swapfile sudo systemctl disable dphys-swapfile swap\u304c\u7121\u52b9\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b $ free -h total used free shared buff/cache available Mem: 1.8Gi 57Mi 1.5Gi 8.0Mi 251Mi 1.7Gi Swap: 0B 0B 0B $ systemctl status dphys-swapfile \u25cf dphys-swapfile.service - dphys-swapfile - set up, mount/unmount, and delete a swap file Loaded: loaded (/lib/systemd/system/dphys-swapfile.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:dphys-swapfile(8) 12\u6708 30 20:48:54 k8s-master1 systemd[1]: Starting dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 30 20:48:55 k8s-master1 dphys-swapfile[330]: want /var/swap=100MByte, checking existing: keeping it 12\u6708 30 20:48:55 k8s-master1 systemd[1]: Started dphys-swapfile - set up, mount/unmount, and delete a swap file. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopping dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 31 06:57:57 k8s-master1 systemd[1]: dphys-swapfile.service: Succeeded. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopped dphys-swapfile - set up, mount/unmount, and delete a swap file.","title":"swap\u3092\u7121\u52b9\u306b\u3059\u308b"},{"location":"setup/03_common_settings/#cgroupfs-memory","text":"kernel\u306eboot option\u306b cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory \u3092\u8ffd\u8a18\u3059\u308b sudo vim /boot/firmware/cmdline.txt cmdline.txt \u306e\u6709\u52b9\u884c\u306e\u78ba\u8a8d $ sudo sed -e 's/\\s/\\n/g' /boot/firmware/cmdline.txt console=serial0,115200 console=tty1 root=PARTUUID=fb7271c3-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait quiet splash plymouth.ignore-serial-consoles cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory $ sudo reboot $ cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 9 1 1 cpu 5 34 1 cpuacct 5 34 1 blkio 10 34 1 memory 8 80 1 devices 4 34 1 freezer 7 1 1 net_cls 2 1 1 perf_event 6 1 1 net_prio 2 1 1 pids 3 39 1","title":"cgroupfs \u306ememory\u3092\u6709\u52b9\u306b\u3059\u308b"},{"location":"setup/03_common_settings/#container-runtime","text":"containerd \u3092\u63a1\u7528 \u521d\u671f\u306f cri-o \u3092\u63a1\u7528\u3057\u3066\u3044\u305f\u304c\u4ee5\u4e0b\u7406\u7531\u3067 containerd \u3078\u5909\u66f4 CNCF\u3067Graduated Project ( cri-o\u306fincubating ) AWS eks-optimized AMI\u3067\u306fcontainerd\u304c\u6a19\u6e96\u3068\u306a\u308a\u305d\u3046(eks 1.22 \u3067\u306fdocker\u304cdefault) buildkit \u3068\u7d44\u307f\u5408\u308f\u305b\u3066image build\u304c\u53ef\u80fd (cri-o\u3067\u306e\u53ef\u5426\u306f\u672a\u78ba\u8a8d) https://speakerdeck.com/ktock/dockerkaracontainerdhefalseyi-xing?slide=7","title":"Container RunTime\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/03_common_settings/#_1","text":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # sysctl params required by setup, params persist across reboots cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # Apply sysctl params without reboot sudo sysctl --system","title":"\u524d\u63d0\u4f5c\u696d"},{"location":"setup/03_common_settings/#containerd","text":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd https://github.com/containerd/containerd/blob/main/docs/getting-started.md \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb for apt package refs https://docs.docker.com/engine/install/ubuntu/ sudo apt update sudo apt install -y ca-certificates curl gnupg lsb-release sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt update sudo apt install -y containerd.io /etc/containerd/config.toml sudo containerd config default | sudo tee /etc/containerd/config.toml sudo vim /etc/containerd/config.toml refs https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd Configuring the systemd cgroup driver Overriding the sandbox (pause) image restart containerd sudo systemctl restart containerd","title":"Containerd"},{"location":"setup/03_common_settings/#cri-o","text":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o kernel module\u306eload overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module overlay iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module br_netfilter kernel parameter\u306eset iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a kernel module\u306eload overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module overlay iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module br_netfilter cat <<EOF | sudo tee /etc/modules-load.d/crio.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter kernel parameter\u306eset iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf # https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF sudo sysctl --system system\u8d77\u52d5\u6642\u306b kernel parameter \u3092\u518d\u8aad\u307f\u8fbc\u307f\u3055\u305b\u308b kube-proxy\u306b\u3066\u5fc5\u8981\u306akernel parameter\u8a2d\u5b9a(kubelet\u8a2d\u5b9a\u624b\u9806\u306b\u3066\u5f8c\u8ff0) \u304ciptables\u8d77\u52d5\u6642\u306ekernel module load\u3067\u4e0a\u66f8\u304d\u3055\u308c\u308b\u305f\u3081 \u5229\u7528\u74b0\u5883\u304c /etc/sysconfig/iptables-config \u3092\u5229\u7528\u53ef\u80fd\u306a\u3089 IPTABLES_MODULES_UNLOAD=\"no\" \u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u672c\u624b\u9806\u306f\u4e0d\u8981\u3067\u3059 egrep \"sysctl\\s+--system\" /etc/rc.local > /dev/null || sudo bash -c \"echo \\\"sysctl --system\\\" >> /etc/rc.local\" egrep \"sysctl\\s+--system\" /etc/rc.local CRI-O\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.21/xUbuntu_20.04/arm64/ VERSION=1.21 OS=xUbuntu_20.04 sudo bash -c \"echo \\\"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\" sudo bash -c \"echo \\\"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list\" curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key add - curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key add - sudo apt update sudo apt install -y cri-o cri-o-runc sudo apt install -y conntrack storage driver\u3092 overlay2 \u3078\u5909\u66f4\u3059\u308b sudo vim /etc/containers/storage.conf sudo vim /etc/crio/crio.conf /etc/crio/crio.conf \u3078graph driver\u8a2d\u5b9a\u3092\u5165\u308c\u308b podman\u3084buildah\u3067build\u3057\u305flocal image\u3092\u53c2\u7167\u3059\u308b\u305f\u3081 [crio] \u30bb\u30af\u30b7\u30e7\u30f3\u306b\u5165\u308c\u308b graphroot = \"/var/lib/containers/storage\" /etc/containers/storage.conf [storage] driver = \"overlay2\" runroot = \"/run/containers/storage\" graphroot = \"/var/lib/containers/storage\" [storage.options] additionalimagestores = [ ] [storage.options.overlay] mountopt = \"nodev\" [storage.options.thinpool] /etc/crio/crio.conf [crio] storage_driver = \"overlay2\" graphroot = \"/var/lib/containers/storage\" log_dir = \"/var/log/crio/pods\" version_file = \"/var/run/crio/version\" version_file_persist = \"/var/lib/crio/version\" clean_shutdown_file = \"/var/lib/crio/clean.shutdown\" [crio.api] listen = \"/var/run/crio/crio.sock\" stream_address = \"127.0.0.1\" stream_port = \"0\" stream_enable_tls = false stream_idle_timeout = \"\" stream_tls_cert = \"\" stream_tls_key = \"\" stream_tls_ca = \"\" grpc_max_send_msg_size = 16777216 grpc_max_recv_msg_size = 16777216 [crio.runtime] no_pivot = false decryption_keys_path = \"/etc/crio/keys/\" conmon = \"\" conmon_cgroup = \"system.slice\" conmon_env = [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", ] default_env = [ ] seccomp_profile = \"\" seccomp_use_default_when_empty = false apparmor_profile = \"crio-default\" irqbalance_config_file = \"/etc/sysconfig/irqbalance\" cgroup_manager = \"systemd\" separate_pull_cgroup = \"\" default_capabilities = [ \"CHOWN\", \"DAC_OVERRIDE\", \"FSETID\", \"FOWNER\", \"SETGID\", \"SETUID\", \"SETPCAP\", \"NET_BIND_SERVICE\", \"KILL\", ] default_sysctls = [ ] additional_devices = [ ] hooks_dir = [ \"/usr/share/containers/oci/hooks.d\", ]pids_limit = 1024 log_size_max = -1 log_to_journald = false container_exits_dir = \"/var/run/crio/exits\" container_attach_socket_dir = \"/var/run/crio\" bind_mount_prefix = \"\" read_only = false log_level = \"info\" log_filter = \"\" uid_mappings = \"\" gid_mappings = \"\" ctr_stop_timeout = 30 drop_infra_ctr = false infra_ctr_cpuset = \"\" namespaces_dir = \"/var/run\" pinns_path = \"\" default_runtime = \"runc\" [crio.runtime.runtimes.runc] runtime_path = \"\" runtime_type = \"oci\" runtime_root = \"/run/runc\" allowed_annotations = [ \"io.containers.trace-syscall\", ] [crio.image] default_transport = \"docker://\" global_auth_file = \"\" pause_image = \"k8s.gcr.io/pause:3.2\" pause_image_auth_file = \"\" pause_command = \"/pause\" signature_policy = \"\" image_volumes = \"mkdir\" big_files_temporary_dir = \"\" [crio.network] network_dir = \"/etc/cni/net.d/\" plugin_dirs = [ \"/opt/cni/bin/\", ] [crio.metrics] enable_metrics = false metrics_port = 9090 metrics_socket = \"\" crio\u3092\u518d\u8d77\u52d5\u3059\u308b sudo systemctl daemon-reload sudo systemctl restart crio","title":"CRI-O"},{"location":"setup/03_common_settings/#cli-tool","text":"nerdctl containerd\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u516c\u958b\u3057\u3066\u3044\u308bdocker-cli\u4e92\u63db\u306eCLI https://github.com/containerd/nerdctl https://speakerdeck.com/ktock/dockerkaracontainerdhefalseyi-xing?slide=22 NERDCTL_VERSION=`curl -s -L https://api.github.com/repos/containerd/nerdctl/releases/latest | jq -r .tag_name` curl -L -s https://github.com/containerd/nerdctl/releases/download/${NERDCTL_VERSION}/nerdctl-`echo ${NERDCTL_VERSION} | sed -e 's/^v//'`-linux-arm64.tar.gz | sudo tar -zxC /usr/local/bin/ ls -l /usr/local/bin buildkit https://github.com/moby/buildkit nerdctl build \u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981 BUILDKIT_VERSION=`curl -s -L https://api.github.com/repos/moby/buildkit/releases/latest | jq -r .tag_name` curl -L -s https://github.com/moby/buildkit/releases/download/${BUILDKIT_VERSION}/buildkit-${BUILDKIT_VERSION}.linux-arm64.tar.gz | sudo tar -zxC /tmp/ sudo mv /tmp/bin/* /usr/local/bin/ ls -l /usr/local/bin sudo curl -sL https://raw.githubusercontent.com/moby/buildkit/${BUILDKIT_VERSION}/examples/systemd/system/buildkit.socket -o /etc/systemd/system/buildkit.socket sudo curl -sL https://raw.githubusercontent.com/moby/buildkit/${BUILDKIT_VERSION}/examples/systemd/system/buildkit.service -o /etc/systemd/system/buildkit.service sudo systemctl enable buildkit.socket buildkit.service sudo systemctl start buildkit.socket buildkit.service buildah cri-o \u5c0e\u5165\u6642\u671f\u306bimage build\u3067\u4f7f\u7528(containerd\u3067\u306f\u524d\u8ff0\u306enerdctl\u3078\u79fb\u884c\u6e08\u307f) https://github.com/containers/buildah/blob/master/install.md sudo apt-get -qq -y install buildah cri-tools(crictl) \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md VERSION=\"v1.22.0\" ARCH=\"arm64\" DOWNLOAD_URL=\"https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-${VERSION}-linux-${ARCH}.tar.gz\" curl -L ${DOWNLOAD_URL} | sudo tar -zxC /usr/local/bin ls -l /usr/local/bin podman \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(optional) https://podman.io/getting-started/installation sudo apt-get -y install podman sudo rm -f /etc/cni/net.d/87-podman-bridge.conflist","title":"CLI TOOL"},{"location":"setup/03_common_settings/#cni-plugin","text":"https://github.com/containernetworking/plugins sudo mkdir -p /opt/cni/bin CNI_PLUGIN_VERSION=`curl -s -L https://api.github.com/repos/containernetworking/plugins/releases/latest | jq -r .tag_name` ARCH=\"arm64\" DOWNLOAD_URL=\"https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGIN_VERSION}/cni-plugins-linux-${ARCH}-${CNI_PLUGIN_VERSION}.tgz\" curl -L ${DOWNLOAD_URL} | sudo tar -zxC /opt/cni/bin ls -l /opt/cni/bin/ cni config\u3092\u4f5c\u6210\u3059\u308b https://www.cni.dev/plugins/current/main/bridge/ POD_CIDR=\"10.200.0.0/24\" cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf { \"cniVersion\": \"0.4.0\", \"name\": \"bridge\", \"type\": \"bridge\", \"bridge\": \"cnio0\", \"isGateway\": true, \"ipMasq\": true, \"ipam\": { \"type\": \"host-local\", \"ranges\": [ [{\"subnet\": \"${POD_CIDR}\"}] ], \"routes\": [{\"dst\": \"0.0.0.0/0\"}] } } EOF cat <<EOF | sudo tee /etc/cni/net.d/20-loopback.conf { \"cniVersion\": \"0.4.0\", \"name\": \"lo\", \"type\": \"loopback\" } EOF","title":"CNI Plugin\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/03_common_settings/#kubectl","text":"https://kubernetes.io/ja/docs/tasks/tools/install-kubectl/","title":"kubectl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/04_creation_certificate/","text":"\u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210 \u624b\u9806 cfssl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u8a3c\u660e\u66f8\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e cfssl \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b https://qiita.com/iaoiui/items/fc2ea829498402d4a8e3 https://coreos.com/os/docs/latest/generate-self-signed-certificates.html sudo apt install -y golang-cfssl CA(\u8a8d\u8a3c\u5c40) \u4f5c\u6210 cat << EOF > ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF cat << EOF > ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Sample\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d ca-key.pem ca.pem \u8a3c\u660e\u66f8\u306e\u4f5c\u6210 \u7ba1\u7406\u8005\u30e6\u30fc\u30b6 \u8a3c\u660e\u66f8 cat << EOF > admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d admin-key.pem admin.pem kubelet\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 EXTERNAL_IP master\u30b5\u30fc\u30d0\u306ehostname master\u304c\u8907\u6570\u30b5\u30fc\u30d0\u69cb\u6210\u306e\u5834\u5408\u306f\u4e0a\u4f4d\u306eLB IP for instance in k8s-master k8s-node1 k8s-node2; do cat << EOF > ${instance}-csr.json { \"CN\": \"system:node:${instance}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF EXTERNAL_IP=k8s-master cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${instance},${EXTERNAL_IP} \\ -profile=kubernetes \\ ${instance}-csr.json | cfssljson -bare ${instance} done \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d k8s-node1-key.pem k8s-node1.pem k8s-node2-key.pem k8s-node2.pem kube-proxy\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 cat << EOF > kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-proxy-key.pem kube-proxy.pem kube-controller-manage\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 cat << EOF > kube-controller-manager-csr.json { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-controller-manager-key.pem kube-controller-manager.pem kube-scheduler\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 cat << EOF > kube-scheduler-csr.json { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-scheduler-key.pem kube-scheduler.pem kube-apiserver\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8 10.32.0.1 Cluster IP KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local cat << EOF > kubernetes-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=10.32.0.1,k8s-master,k8s-node1,k8s-node2,127.0.0.1,${KUBERNETES_HOSTNAMES} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kubernetes-key.pem kubernetes.pem kube-apiserver front-proxy(for aggregation layer)\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8 CA(\u8a8d\u8a3c\u5c40)\u4f5c\u6210 cat << EOF > front-proxy-ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF cat << EOF > front-proxy-ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Sample\" } ] } EOF cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca front-proxy\u7528\u8a3c\u660e\u66f8\u306e\u4f5c\u6210 cat << EOF > front-proxy-csr.json { \"CN\": \"front-proxy-ca\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=front-proxy-ca.pem \\ -ca-key=front-proxy-ca-key.pem \\ -config=front-proxy-ca-config.json \\ -profile=kubernetes \\ front-proxy-csr.json | cfssljson -bare front-proxy \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d front-proxy-key.pem front-proxy.pem service-account\u306e\u8a3c\u660e\u66f8 cat << EOF > service-account-csr.json { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d service-account-key.pem service-account.pem \u8a3c\u660e\u66f8\u3092master/node\u3078\u30b3\u30d4\u30fc\u3059\u308b master node \u53c2\u8003\u6587\u732e https://kubernetes.io/ja/docs/setup/best-practices/certificates/ https://kubernetes.io/ja/docs/concepts/cluster-administration/certificates/ https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf","title":"04. \u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210"},{"location":"setup/04_creation_certificate/#tls","text":"","title":"\u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210"},{"location":"setup/04_creation_certificate/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/04_creation_certificate/#cfssl","text":"\u8a3c\u660e\u66f8\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e cfssl \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b https://qiita.com/iaoiui/items/fc2ea829498402d4a8e3 https://coreos.com/os/docs/latest/generate-self-signed-certificates.html sudo apt install -y golang-cfssl","title":"cfssl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/04_creation_certificate/#ca","text":"cat << EOF > ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF cat << EOF > ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Sample\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d ca-key.pem ca.pem","title":"CA(\u8a8d\u8a3c\u5c40) \u4f5c\u6210"},{"location":"setup/04_creation_certificate/#_2","text":"","title":"\u8a3c\u660e\u66f8\u306e\u4f5c\u6210"},{"location":"setup/04_creation_certificate/#_3","text":"cat << EOF > admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d admin-key.pem admin.pem","title":"\u7ba1\u7406\u8005\u30e6\u30fc\u30b6 \u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kubelet","text":"EXTERNAL_IP master\u30b5\u30fc\u30d0\u306ehostname master\u304c\u8907\u6570\u30b5\u30fc\u30d0\u69cb\u6210\u306e\u5834\u5408\u306f\u4e0a\u4f4d\u306eLB IP for instance in k8s-master k8s-node1 k8s-node2; do cat << EOF > ${instance}-csr.json { \"CN\": \"system:node:${instance}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF EXTERNAL_IP=k8s-master cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${instance},${EXTERNAL_IP} \\ -profile=kubernetes \\ ${instance}-csr.json | cfssljson -bare ${instance} done \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d k8s-node1-key.pem k8s-node1.pem k8s-node2-key.pem k8s-node2.pem","title":"kubelet\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-proxy","text":"cat << EOF > kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-proxy-key.pem kube-proxy.pem","title":"kube-proxy\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-controller-manage","text":"cat << EOF > kube-controller-manager-csr.json { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-controller-manager-key.pem kube-controller-manager.pem","title":"kube-controller-manage\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-scheduler","text":"cat << EOF > kube-scheduler-csr.json { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-scheduler-key.pem kube-scheduler.pem","title":"kube-scheduler\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-apiserver","text":"10.32.0.1 Cluster IP KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local cat << EOF > kubernetes-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=10.32.0.1,k8s-master,k8s-node1,k8s-node2,127.0.0.1,${KUBERNETES_HOSTNAMES} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kubernetes-key.pem kubernetes.pem","title":"kube-apiserver\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-apiserver-front-proxyfor-aggregation-layer","text":"CA(\u8a8d\u8a3c\u5c40)\u4f5c\u6210 cat << EOF > front-proxy-ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF cat << EOF > front-proxy-ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Sample\" } ] } EOF cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca front-proxy\u7528\u8a3c\u660e\u66f8\u306e\u4f5c\u6210 cat << EOF > front-proxy-csr.json { \"CN\": \"front-proxy-ca\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=front-proxy-ca.pem \\ -ca-key=front-proxy-ca-key.pem \\ -config=front-proxy-ca-config.json \\ -profile=kubernetes \\ front-proxy-csr.json | cfssljson -bare front-proxy \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d front-proxy-key.pem front-proxy.pem","title":"kube-apiserver front-proxy(for aggregation layer)\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#service-account","text":"cat << EOF > service-account-csr.json { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d service-account-key.pem service-account.pem","title":"service-account\u306e\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#masternode","text":"master node","title":"\u8a3c\u660e\u66f8\u3092master/node\u3078\u30b3\u30d4\u30fc\u3059\u308b"},{"location":"setup/04_creation_certificate/#_4","text":"https://kubernetes.io/ja/docs/setup/best-practices/certificates/ https://kubernetes.io/ja/docs/concepts/cluster-administration/certificates/ https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/05_creating_config/","text":"\u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210 Controll Plane\u3068Node\u306e\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e .kubeconfig \u3092\u4f5c\u6210\u3059\u308b \u624b\u9806 kubelet KUBERNETES_PUBLIC_ADDRESS=k8s-master for instance in k8s-master k8s-node1 k8s-node2; do kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\ --client-certificate=${instance}.pem \\ --client-key=${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done kube-proxy KUBERNETES_PUBLIC_ADDRESS=k8s-master kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig kube-controller-manager KUBE_API_SERVER_ADDRESS=k8s-master kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig kube-scheduler KUBE_API_SERVER_ADDRESS=k8s-master kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig admin KUBERNETES_PUBLIC_ADDRESS=k8s-master kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig sudo mkdir -p /var/lib/kubernetes/ sudo cp admin.kubeconfig /var/lib/kubernetes/admin.kubeconfig \u53c2\u8003\u8cc7\u6599 https://github.com/kelseyhightower/kubernetes/blob/master/docs/05-kubernetes-configuration-files.md https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf https://h3poteto.hatenablog.com/entry/2020/08/20/180552 kubectl config set-cluster https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-cluster/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-cluster-em- kubectl config set-credentials https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-credentials/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-credentials-em- kubectl config set-context https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-context/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-context-em-","title":"05. \u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210"},{"location":"setup/05_creating_config/#kubeconfig","text":"Controll Plane\u3068Node\u306e\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e .kubeconfig \u3092\u4f5c\u6210\u3059\u308b","title":"\u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210"},{"location":"setup/05_creating_config/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/05_creating_config/#kubelet","text":"KUBERNETES_PUBLIC_ADDRESS=k8s-master for instance in k8s-master k8s-node1 k8s-node2; do kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\ --client-certificate=${instance}.pem \\ --client-key=${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done","title":"kubelet"},{"location":"setup/05_creating_config/#kube-proxy","text":"KUBERNETES_PUBLIC_ADDRESS=k8s-master kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig","title":"kube-proxy"},{"location":"setup/05_creating_config/#kube-controller-manager","text":"KUBE_API_SERVER_ADDRESS=k8s-master kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig","title":"kube-controller-manager"},{"location":"setup/05_creating_config/#kube-scheduler","text":"KUBE_API_SERVER_ADDRESS=k8s-master kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig","title":"kube-scheduler"},{"location":"setup/05_creating_config/#admin","text":"KUBERNETES_PUBLIC_ADDRESS=k8s-master kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig sudo mkdir -p /var/lib/kubernetes/ sudo cp admin.kubeconfig /var/lib/kubernetes/admin.kubeconfig","title":"admin"},{"location":"setup/05_creating_config/#_2","text":"https://github.com/kelseyhightower/kubernetes/blob/master/docs/05-kubernetes-configuration-files.md https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf https://h3poteto.hatenablog.com/entry/2020/08/20/180552 kubectl config set-cluster https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-cluster/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-cluster-em- kubectl config set-credentials https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-credentials/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-credentials-em- kubectl config set-context https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-context/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-context-em-","title":"\u53c2\u8003\u8cc7\u6599"},{"location":"setup/06_master/01_bootstrapping_kubelet/","text":"bootstrapping kubelet(master/worker \u5171\u901a) kubelet \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002 worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206 Reserve Compute Resources for System Daemons https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9 = Node resource - system-reserved - kube-reserved - eviction-threshold \u3089\u3057\u3044 name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi \u624b\u9806 kubelet \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 VERSION=\"v1.22.0\" ARCH=\"arm64\" sudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet sudo chmod +x /usr/bin/kubelet kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" sudo install -o root -g root -m 755 -d /etc/kubelet.d sudo install -o root -g root -m 755 -d /var/lib/kubernetes sudo install -o root -g root -m 755 -d /var/lib/kubelet sudo cp ca.pem /var/lib/kubernetes/ sudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/ sudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig /var/lib/kubelet/kubelet-config.yaml \u3092\u4f5c\u6210\u3059\u308b clusterDNS \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b podCIDR \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 # https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ staticPodPath: /etc/kubelet.d # kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f # - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b # - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981 authentication: anonymous: enabled: true webhook: enabled: false cacheTTL: \"2m\" x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" # kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a # - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow # - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981 authorization: mode: AlwaysAllow clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" podCIDR: \"10.200.0.0/24\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${host}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\" # Reserve Compute Resources for System Daemons # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # # Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044 # # system-reserved # - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b # # kube-reserved # - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b enforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"] cgroupsPerQOS: true cgroupDriver: systemd cgroupRoot: / systemCgroups: /systemd/system.slice systemReservedCgroup: /system.slice systemReserved: cpu: 256m memory: 256Mi runtimeCgroups: /kube.slice/containerd.service kubeletCgroups: /kube.slice/kubelet.service kubeReservedCgroup: /kube.slice kubeReserved: cpu: 1024m memory: 1024Mi EOF /etc/systemd/system/kubelet.service \u3092\u914d\u7f6e cat << 'EOF' | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] Restart=on-failure RestartSec=5 ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/kube.slice \\ /sys/fs/cgroup/system.slice \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/system.slice \\ /sys/fs/cgroup/hugetlb/system.slice \\ /sys/fs/cgroup/hugetlb/kube.slice ExecStart=/usr/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --container-runtime=remote \\ --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\ --register-node=true \\ --v=2 [Install] WantedBy=multi-user.target EOF kubelet.service \u3092\u8d77\u52d5 sudo systemctl enable kubelet.service sudo systemctl start kubelet.service \u30a8\u30e9\u30fc\u4e8b\u4f8b cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408 kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist kubelet \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( --v 10 ) cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice] \u5bfe\u5fdc kubelet.service \u306e ExecStartPre \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f \u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy kubeconfig \u306e\u8a3c\u660e\u66f8\u306e CN \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b 360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\" kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: k8s-master \u304c\u6b63\u3057\u3044\u306e\u306b CN = system:node:k8s-node1 \u3068\u306a\u3063\u3066\u3044\u305f root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master Node \u30ea\u30bd\u30fc\u30b9\u306e spec.podCIDR \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044 \u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044 flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f... kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u30ed\u30b0 Set node k8s-node1 PodCIDR to [10.200.0.0/24] \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8 kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b --allocate-node-cidrs=true \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist range_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24] ttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\" controller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}] controller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes node_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\" controller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1 node_lifecycle_controller.go:1429] Initializing eviction metric for zone: node_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp. event.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\" node_lifecycle_controller.go:1245] Controller detected that zone is now in state Normal. Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044 I0214 07:03:56.822586 1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\" F0214 07:03:56.822637 1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication goroutine 1 [running]: https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication CNI Plugin\u3092 /etc/cni/net.d \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044 kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized CNI Plugin\u3092 /etc/cni/net.d \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b https://github.com/containernetworking/plugins/releases Kubelet cannot determine CPU online state sysinfo.go:203] Nodes topology is not available, providing CPU topology sysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory gce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:72] Cannot read number of physical cores correctly, number of cores set to 0 machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:86] Cannot read number of sockets correctly, number of sockets set to 0 container_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name: \u65e2\u77e5\u3089\u3057\u3044 https://github.com/kubernetes/kubernetes/issues/95039 cni plugin not initialized /opt/cni/bin \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f /etc/cni/net.d \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\" cni config uninitialized /opt/cni/bin \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f /etc/cni/net.d \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni config uninitialized\" \u53c2\u8003 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go https://cyberagent.ai/blog/tech/4036/ kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b https://downloadkubernetes.com/ https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/ Node Authorization https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822 https://kubernetes.io/docs/reference/access-authn-authz/node/ https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ static pod https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/policy/pod-security-policy/ https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver PodSecurityPolicy \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf( false \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f true \u306b\u76f4\u3059) https://github.com/kubernetes/kubernetes/issues/70952","title":"01. bootstrapping kubelet"},{"location":"setup/06_master/01_bootstrapping_kubelet/#bootstrapping-kubeletmasterworker","text":"kubelet \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002","title":"bootstrapping kubelet(master/worker \u5171\u901a)"},{"location":"setup/06_master/01_bootstrapping_kubelet/#worker-node","text":"Reserve Compute Resources for System Daemons https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9 = Node resource - system-reserved - kube-reserved - eviction-threshold \u3089\u3057\u3044 name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi","title":"worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubelet","text":"VERSION=\"v1.22.0\" ARCH=\"arm64\" sudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet sudo chmod +x /usr/bin/kubelet","title":"kubelet \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeconfig","text":"# host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" sudo install -o root -g root -m 755 -d /etc/kubelet.d sudo install -o root -g root -m 755 -d /var/lib/kubernetes sudo install -o root -g root -m 755 -d /var/lib/kubelet sudo cp ca.pem /var/lib/kubernetes/ sudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/ sudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig","title":"kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e"},{"location":"setup/06_master/01_bootstrapping_kubelet/#varlibkubeletkubelet-configyaml","text":"clusterDNS \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b podCIDR \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 # https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ staticPodPath: /etc/kubelet.d # kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f # - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b # - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981 authentication: anonymous: enabled: true webhook: enabled: false cacheTTL: \"2m\" x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" # kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a # - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow # - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981 authorization: mode: AlwaysAllow clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" podCIDR: \"10.200.0.0/24\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${host}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\" # Reserve Compute Resources for System Daemons # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # # Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044 # # system-reserved # - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b # # kube-reserved # - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b enforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"] cgroupsPerQOS: true cgroupDriver: systemd cgroupRoot: / systemCgroups: /systemd/system.slice systemReservedCgroup: /system.slice systemReserved: cpu: 256m memory: 256Mi runtimeCgroups: /kube.slice/containerd.service kubeletCgroups: /kube.slice/kubelet.service kubeReservedCgroup: /kube.slice kubeReserved: cpu: 1024m memory: 1024Mi EOF","title":"/var/lib/kubelet/kubelet-config.yaml \u3092\u4f5c\u6210\u3059\u308b"},{"location":"setup/06_master/01_bootstrapping_kubelet/#etcsystemdsystemkubeletservice","text":"cat << 'EOF' | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] Restart=on-failure RestartSec=5 ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/kube.slice \\ /sys/fs/cgroup/system.slice \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/system.slice \\ /sys/fs/cgroup/hugetlb/system.slice \\ /sys/fs/cgroup/hugetlb/kube.slice ExecStart=/usr/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --container-runtime=remote \\ --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\ --register-node=true \\ --v=2 [Install] WantedBy=multi-user.target EOF","title":"/etc/systemd/system/kubelet.service \u3092\u914d\u7f6e"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeletservice","text":"sudo systemctl enable kubelet.service sudo systemctl start kubelet.service","title":"kubelet.service \u3092\u8d77\u52d5"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_2","text":"","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cgroup","text":"kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist kubelet \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( --v 10 ) cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice] \u5bfe\u5fdc kubelet.service \u306e ExecStartPre \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice","title":"cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cgroupsystemreserved-memory-size","text":"\u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy","title":"cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeconfig-cn-node","text":"360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\" kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: k8s-master \u304c\u6b63\u3057\u3044\u306e\u306b CN = system:node:k8s-node1 \u3068\u306a\u3063\u3066\u3044\u305f root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master","title":"kubeconfig \u306e\u8a3c\u660e\u66f8\u306e CN \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b"},{"location":"setup/06_master/01_bootstrapping_kubelet/#node-specpodcidr-cidr","text":"\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044 flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f... kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u30ed\u30b0 Set node k8s-node1 PodCIDR to [10.200.0.0/24] \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8 kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b --allocate-node-cidrs=true \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist range_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24] ttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\" controller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}] controller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes node_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\" controller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1 node_lifecycle_controller.go:1429] Initializing eviction metric for zone: node_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp. event.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\" node_lifecycle_controller.go:1245] Controller detected that zone is now in state Normal.","title":"Node \u30ea\u30bd\u30fc\u30b9\u306e spec.podCIDR \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044"},{"location":"setup/06_master/01_bootstrapping_kubelet/#webhook-authentication","text":"I0214 07:03:56.822586 1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\" F0214 07:03:56.822637 1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication goroutine 1 [running]: https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication","title":"Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cni-plugin-etccninetd-cni-plugin","text":"kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized CNI Plugin\u3092 /etc/cni/net.d \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b https://github.com/containernetworking/plugins/releases","title":"CNI Plugin\u3092 /etc/cni/net.d \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubelet-cannot-determine-cpu-online-state","text":"sysinfo.go:203] Nodes topology is not available, providing CPU topology sysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory gce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:72] Cannot read number of physical cores correctly, number of cores set to 0 machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:86] Cannot read number of sockets correctly, number of sockets set to 0 container_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name: \u65e2\u77e5\u3089\u3057\u3044 https://github.com/kubernetes/kubernetes/issues/95039","title":"Kubelet cannot determine CPU online state"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cni-plugin-not-initialized","text":"/opt/cni/bin \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f /etc/cni/net.d \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"","title":"cni plugin not initialized"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cni-config-uninitialized","text":"/opt/cni/bin \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f /etc/cni/net.d \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni config uninitialized\"","title":"cni config uninitialized"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_3","text":"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go https://cyberagent.ai/blog/tech/4036/ kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b https://downloadkubernetes.com/ https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/ Node Authorization https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822 https://kubernetes.io/docs/reference/access-authn-authz/node/ https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ static pod https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/policy/pod-security-policy/ https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver PodSecurityPolicy \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf( false \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f true \u306b\u76f4\u3059) https://github.com/kubernetes/kubernetes/issues/70952","title":"\u53c2\u8003"},{"location":"setup/06_master/02_bootstrapping_etcd/","text":"bootstrapping etcd coreos\u304cetcd docker image \u3092\u63d0\u4f9b \u3057\u3066\u3044\u307e\u3059\u304c\u3001Raspberry Pi\u306b\u642d\u8f09\u3055\u308c\u3066\u3044\u308bARM CPU\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3 armv8(64bit) \u3067\u5229\u7528\u53ef\u80fd\u306aimage\u306f\u63d0\u4f9b\u3057\u3066\u3044\u306a\u3044\u305f\u3081\u3001image\u3092build\u3057\u307e\u3059\u3002 \u624b\u9806 Dockerfile_etcd.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_etcd.armhf cat << 'EOF' > Dockerfile_etcd.armhf FROM quay.io/coreos/etcd:v3.4.20 COPY ca.pem /etc/etcd/ COPY kubernetes-key.pem /etc/etcd/ COPY kubernetes.pem /etc/etcd/ ENV ETCD_UNSUPPORTED_ARCH=arm64 EXPOSE 2379 2380 VOLUME [\"/etcd-data\"] ENTRYPOINT [\"/usr/local/bin/etcd\"] EOF image build sudo mkdir -p /etcd-data sudo nerdctl build --namespace k8s.io -t k8s-etcd --file=Dockerfile_etcd.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/etcd.yaml cat << EOF | sudo tee /etc/kubelet.d/etcd.yaml --- apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://k8s-master:2379 name: etcd namespace: kube-system labels: tier: control-plane component: etcd spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true volumes: - name: etcd-data-volume hostPath: path: /etcd-data type: Directory containers: - name: etcd image: k8s-etcd:latest imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /etcd-data name: etcd-data-volume env: - name: ETCD_UNSUPPORTED_ARCH value: \"arm64\" resources: requests: cpu: 0.5 memory: \"384Mi\" limits: cpu: 1 memory: \"384Mi\" command: - /usr/local/bin/etcd - --data-dir=/etcd-data - --advertise-client-urls=https://k8s-master:2379,https://k8s-master:2380 - --listen-client-urls=https://0.0.0.0:2379 - --initial-advertise-peer-urls=https://k8s-master:2380 - --listen-peer-urls=https://0.0.0.0:2380 - --name=etcd0 - --cert-file=/etc/etcd/kubernetes.pem - --key-file=/etc/etcd/kubernetes-key.pem - --peer-cert-file=/etc/etcd/kubernetes.pem - --peer-key-file=/etc/etcd/kubernetes-key.pem - --trusted-ca-file=/etc/etcd/ca.pem - --peer-trusted-ca-file=/etc/etcd/ca.pem - --peer-client-cert-auth - --client-cert-auth - --initial-cluster-token=etcd-cluster-1 - --initial-cluster=etcd0=https://k8s-master:2380 - --initial-cluster-state=new EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name etcd CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 72f58248ec087 6e8b8110dc13cfe61d75f867a22c39766a397989413570500f51dedf94be7a12 25 seconds ago Running etcd 0 206c5b952097a \u53c2\u8003\u6587\u732e https://etcd.io/docs/v2/docker_guide/ https://quay.io/repository/coreos/etcd?tag=latest&tab=tags https://github.com/etcd-io/etcd","title":"02. bootstrapping etcd"},{"location":"setup/06_master/02_bootstrapping_etcd/#bootstrapping-etcd","text":"coreos\u304cetcd docker image \u3092\u63d0\u4f9b \u3057\u3066\u3044\u307e\u3059\u304c\u3001Raspberry Pi\u306b\u642d\u8f09\u3055\u308c\u3066\u3044\u308bARM CPU\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3 armv8(64bit) \u3067\u5229\u7528\u53ef\u80fd\u306aimage\u306f\u63d0\u4f9b\u3057\u3066\u3044\u306a\u3044\u305f\u3081\u3001image\u3092build\u3057\u307e\u3059\u3002","title":"bootstrapping etcd"},{"location":"setup/06_master/02_bootstrapping_etcd/#_1","text":"Dockerfile_etcd.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_etcd.armhf cat << 'EOF' > Dockerfile_etcd.armhf FROM quay.io/coreos/etcd:v3.4.20 COPY ca.pem /etc/etcd/ COPY kubernetes-key.pem /etc/etcd/ COPY kubernetes.pem /etc/etcd/ ENV ETCD_UNSUPPORTED_ARCH=arm64 EXPOSE 2379 2380 VOLUME [\"/etcd-data\"] ENTRYPOINT [\"/usr/local/bin/etcd\"] EOF image build sudo mkdir -p /etcd-data sudo nerdctl build --namespace k8s.io -t k8s-etcd --file=Dockerfile_etcd.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/etcd.yaml cat << EOF | sudo tee /etc/kubelet.d/etcd.yaml --- apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://k8s-master:2379 name: etcd namespace: kube-system labels: tier: control-plane component: etcd spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true volumes: - name: etcd-data-volume hostPath: path: /etcd-data type: Directory containers: - name: etcd image: k8s-etcd:latest imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /etcd-data name: etcd-data-volume env: - name: ETCD_UNSUPPORTED_ARCH value: \"arm64\" resources: requests: cpu: 0.5 memory: \"384Mi\" limits: cpu: 1 memory: \"384Mi\" command: - /usr/local/bin/etcd - --data-dir=/etcd-data - --advertise-client-urls=https://k8s-master:2379,https://k8s-master:2380 - --listen-client-urls=https://0.0.0.0:2379 - --initial-advertise-peer-urls=https://k8s-master:2380 - --listen-peer-urls=https://0.0.0.0:2380 - --name=etcd0 - --cert-file=/etc/etcd/kubernetes.pem - --key-file=/etc/etcd/kubernetes-key.pem - --peer-cert-file=/etc/etcd/kubernetes.pem - --peer-key-file=/etc/etcd/kubernetes-key.pem - --trusted-ca-file=/etc/etcd/ca.pem - --peer-trusted-ca-file=/etc/etcd/ca.pem - --peer-client-cert-auth - --client-cert-auth - --initial-cluster-token=etcd-cluster-1 - --initial-cluster=etcd0=https://k8s-master:2380 - --initial-cluster-state=new EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name etcd CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 72f58248ec087 6e8b8110dc13cfe61d75f867a22c39766a397989413570500f51dedf94be7a12 25 seconds ago Running etcd 0 206c5b952097a","title":"\u624b\u9806"},{"location":"setup/06_master/02_bootstrapping_etcd/#_2","text":"https://etcd.io/docs/v2/docker_guide/ https://quay.io/repository/coreos/etcd?tag=latest&tab=tags https://github.com/etcd-io/etcd","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/","text":"bootstrapping kube-apiserver \u624b\u9806 Dockerfile_kube-apiserver.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-apiserver.armhf cat << 'EOF' > Dockerfile_kube-apiserver.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget --quiet -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-apiserver \\ && chmod +x /usr/bin/kube-apiserver \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config \\ && install -o root -g root -m 755 -d /etc/kubernetes/webhook COPY ca.pem \\ ca-key.pem \\ kubernetes-key.pem \\ kubernetes.pem \\ service-account-key.pem \\ service-account.pem \\ encryption-config.yaml \\ front-proxy-ca.pem \\ front-proxy.pem \\ front-proxy-key.pem \\ /var/lib/kubernetes/ COPY authorization-config.yaml /etc/kubernetes/webhook/ EXPOSE 6443 ENTRYPOINT [\"/usr/bin/kube-apiserver\"] EOF encryption-provider-config \u3092\u4f5c\u6210\u3059\u308b --encryption-provider-config \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u3066secret\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u6697\u53f7\u5316\u3059\u308b\u305f\u3081\u306e\u9375\u3092\u5b9a\u7fa9\u3059\u308b https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#encrypting-your-data https://access.redhat.com/documentation/ja-jp/openshift_container_platform/3.11/html/cluster_administration/admin-guide-encrypting-data-at-datastore encryption-config.yaml ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat << EOF > encryption-config.yaml --- kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF webbhook config\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b --authorization-webhook-config-file \u3067\u6307\u5b9a\u3059\u308b\u30d5\u30a1\u30a4\u30eb authorization-config.yaml KUBE_API_SERVER_ADDRESS=k8s-master cat << EOF > authorization-config.yaml --- apiVersion: v1 # kind of the API object kind: Config # clusters refers to the remote service. clusters: - name: kubernetes cluster: certificate-authority: /var/lib/kubernetes/ca.pem # CA for verifying the remote service. server: https://${KUBE_API_SERVER_ADDRESS}:6443/authenticate # URL of remote service to query. Must use 'https'. # users refers to the API server's webhook configuration. users: - name: api-server-webhook user: client-certificate: /var/lib/kubernetes/kubernetes.pem # cert for the webhook plugin to use client-key: /var/lib/kubernetes/kubernetes-key.pem # key matching the cert # kubeconfig files require a context. Provide one for the API server. current-context: webhook contexts: - context: cluster: kubernetes user: api-server-webhook name: webhook EOF image build sudo nerdctl build --namespace k8s.io -t k8s-kube-apiserver --file=Dockerfile_kube-apiserver.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b --advertise-address \u30aa\u30d7\u30b7\u30e7\u30f3\u306fIP\u30a2\u30c9\u30ec\u30b9\u3067\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b(hostname\u3067\u306f\u8d77\u52d5\u3057\u306a\u304b\u3063\u305f) /etc/kubelet.d/kube-api-server.yaml KUBE_API_SERVER_ADDRESS=192.168.3.50 cat << EOF | sudo tee /etc/kubelet.d/kube-api-server.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system annotations: seccomp.security.alpha.kubernetes.io/pod: runtime/default labels: tier: control-plane component: kube-apiserver spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-apiserver image: k8s-kube-apiserver:latest imagePullPolicy: IfNotPresent resources: requests: memory: \"512Mi\" limits: memory: \"1024Mi\" command: - /usr/bin/kube-apiserver - --advertise-address=k8s-master - --allow-privileged=true - --anonymous-auth=false - --apiserver-count=1 - --audit-log-maxage=30 - --audit-log-maxbackup=3 - --audit-log-maxsize=100 - --audit-log-path=/var/log/audit.log - --authorization-mode=Node,RBAC,Webhook - --authorization-webhook-config-file=/etc/kubernetes/webhook/authorization-config.yaml - --authentication-token-webhook-cache-ttl=2m - --authentication-token-webhook-version=v1 - --bind-address=0.0.0.0 - --client-ca-file=/var/lib/kubernetes/ca.pem - --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,RuntimeClass - --etcd-cafile=/var/lib/kubernetes/ca.pem - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem - --etcd-servers=https://k8s-master:2379 - --event-ttl=1h - --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml - --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem - --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem - --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem - --runtime-config=authentication.k8s.io/v1beta1=true - --feature-gates=APIPriorityAndFairness=false - --service-account-key-file=/var/lib/kubernetes/service-account.pem - --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem - --service-account-issuer=api - --service-account-api-audiences=api - --service-cluster-ip-range=10.32.0.0/24 - --service-node-port-range=30000-32767 - --tls-cert-file=/var/lib/kubernetes/kubernetes.pem - --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem - --http2-max-streams-per-connection=3000 - --max-requests-inflight=3000 - --max-mutating-requests-inflight=1000 - --enable-aggregator-routing=true - --requestheader-client-ca-file=/var/lib/kubernetes/front-proxy-ca.pem - --requestheader-allowed-names=front-proxy-ca - --requestheader-extra-headers-prefix=X-Remote-Extra - --requestheader-group-headers=X-Remote-Group - --requestheader-username-headers=X-Remote-User - --proxy-client-cert-file=/var/lib/kubernetes/front-proxy.pem - --proxy-client-key-file=/var/lib/kubernetes/front-proxy-key.pem - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-apiserver CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 82c371fd9d99e 83e685a0b921ef5dd91eb3cdf208ba70690c1dd7decfc39bb3903be6ede752e6 24 seconds ago Running kube-apiserver 0 6af4d1b99fa37 master node\u306bPod\u304cschedule\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u308b taint\u3092\u8a2d\u5b9a\u3059\u308b https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/ https://kubernetes.io/ja/docs/concepts/scheduling-eviction/taint-and-toleration/ kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule $ kubectl get node k8s-master -o=jsonpath='{.spec.taints}' [{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\"}] \u30a8\u30e9\u30fc\u4e8b\u4f8b failed creating mandatory flowcontrol settings: failed getting mandatory FlowSchema exempt due to the server was unable to return a response in the time allotted, but may still be processing the request https://github.com/kubernetes/kubernetes/issues/97525#issuecomment-753022219 kube-apiserver\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u4ee5\u4e0b2\u3064\u3092\u4ed8\u52a0\u3059\u308b --feature-gates=APIPriorityAndFairness=false --runtime-config=flowcontrol.apiserver.k8s.io/v1beta1=false failed to run Kubelet: no client provided, cannot use webhook authentication kubelet\u304cWebhook\u8a8d\u8a3c\u3092\u671f\u5f85\u3057\u3066\u3044\u308b\u306e\u306bkube-api-server\u3067Webhook\u8a8d\u8a3c\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408 Webhook\u8a8d\u8a3c\u3092\u6709\u52b9\u306b\u3059\u308b https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication Failed creating a mirror pod for \"kube-scheduler-k8s-master_kube-system(a4a914cd05761a5a4335e2510ca075aa)\": pods \"kube-scheduler-k8s-master\" is forbidden: PodSecurityPolicy: no providers available to validate pod request StaticPod\u3092\u8d77\u52d5\u3057\u305f\u969b\u306bkubelet\u304b\u3089kube-apiserver\u3078mirror pod\u60c5\u5831\u3092\u767b\u9332\u3057\u3088\u3046\u3068\u3057\u3066PodSecurityPolicy\u306b\u3088\u308a\u62d2\u5426\u3055\u308c\u305f \u53c2\u8003\u6587\u732e https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/ old default\u306ePodSecurityPolicy(PSP)\u3092\u4f5c\u6210\u3059\u308b staticPod \u3092\u4f5c\u6210\u3059\u308b\u969b\u306bkubelet\u304b\u3089mirror pod\u4f5c\u6210\u30ea\u30af\u30a8\u30b9\u30c8\u304c\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3059 ( \u53c2\u8003 ) PSP / ClusterRole / ClusterRoleBinding cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: annotations: apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default' name: default spec: # allowedCapabilities: [] # default set of capabilities are implicitly allowed allowedCapabilities: - '*' # - NET_ADMIN # - NET_RAW # - SYS_ADMIN fsGroup: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 hostIPC: true hostNetwork: true hostPID: true privileged: true allowPrivilegeEscalation: true readOnlyRootFilesystem: true runAsUser: rule: 'MustRunAsNonRoot' seLinux: rule: 'RunAsNonRoot' supplementalGroups: rule: 'RunAsNonRoot' ranges: # Forbid adding the root group. - min: 1 max: 65535 volumes: - 'configMap' - 'downwardAPI' - 'emptyDir' - 'persistentVolumeClaim' - 'projected' - 'secret' - 'hostPath' hostNetwork: true runAsUser: rule: 'RunAsAny' seLinux: rule: 'RunAsAny' supplementalGroups: rule: 'RunAsAny' fsGroup: rule: 'RunAsAny' --- # Cluster role which grants access to the default pod security policy apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: default-psp rules: - apiGroups: - policy resourceNames: - default resources: - podsecuritypolicies verbs: - use --- # Cluster role binding for default pod security policy granting all authenticated users access apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default-psp roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: default-psp subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:authenticated EOF $ cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f - <\u7701\u7565> podsecuritypolicy.policy/default created clusterrole.rbac.authorization.k8s.io/default-psp created clusterrolebinding.rbac.authorization.k8s.io/default-psp created","title":"03. bootstrapping kube-apiserver"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#bootstrapping-kube-apiserver","text":"","title":"bootstrapping kube-apiserver"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_1","text":"Dockerfile_kube-apiserver.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-apiserver.armhf cat << 'EOF' > Dockerfile_kube-apiserver.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget --quiet -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-apiserver \\ && chmod +x /usr/bin/kube-apiserver \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config \\ && install -o root -g root -m 755 -d /etc/kubernetes/webhook COPY ca.pem \\ ca-key.pem \\ kubernetes-key.pem \\ kubernetes.pem \\ service-account-key.pem \\ service-account.pem \\ encryption-config.yaml \\ front-proxy-ca.pem \\ front-proxy.pem \\ front-proxy-key.pem \\ /var/lib/kubernetes/ COPY authorization-config.yaml /etc/kubernetes/webhook/ EXPOSE 6443 ENTRYPOINT [\"/usr/bin/kube-apiserver\"] EOF encryption-provider-config \u3092\u4f5c\u6210\u3059\u308b --encryption-provider-config \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u3066secret\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u6697\u53f7\u5316\u3059\u308b\u305f\u3081\u306e\u9375\u3092\u5b9a\u7fa9\u3059\u308b https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#encrypting-your-data https://access.redhat.com/documentation/ja-jp/openshift_container_platform/3.11/html/cluster_administration/admin-guide-encrypting-data-at-datastore encryption-config.yaml ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat << EOF > encryption-config.yaml --- kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF webbhook config\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b --authorization-webhook-config-file \u3067\u6307\u5b9a\u3059\u308b\u30d5\u30a1\u30a4\u30eb authorization-config.yaml KUBE_API_SERVER_ADDRESS=k8s-master cat << EOF > authorization-config.yaml --- apiVersion: v1 # kind of the API object kind: Config # clusters refers to the remote service. clusters: - name: kubernetes cluster: certificate-authority: /var/lib/kubernetes/ca.pem # CA for verifying the remote service. server: https://${KUBE_API_SERVER_ADDRESS}:6443/authenticate # URL of remote service to query. Must use 'https'. # users refers to the API server's webhook configuration. users: - name: api-server-webhook user: client-certificate: /var/lib/kubernetes/kubernetes.pem # cert for the webhook plugin to use client-key: /var/lib/kubernetes/kubernetes-key.pem # key matching the cert # kubeconfig files require a context. Provide one for the API server. current-context: webhook contexts: - context: cluster: kubernetes user: api-server-webhook name: webhook EOF image build sudo nerdctl build --namespace k8s.io -t k8s-kube-apiserver --file=Dockerfile_kube-apiserver.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b --advertise-address \u30aa\u30d7\u30b7\u30e7\u30f3\u306fIP\u30a2\u30c9\u30ec\u30b9\u3067\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b(hostname\u3067\u306f\u8d77\u52d5\u3057\u306a\u304b\u3063\u305f) /etc/kubelet.d/kube-api-server.yaml KUBE_API_SERVER_ADDRESS=192.168.3.50 cat << EOF | sudo tee /etc/kubelet.d/kube-api-server.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system annotations: seccomp.security.alpha.kubernetes.io/pod: runtime/default labels: tier: control-plane component: kube-apiserver spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-apiserver image: k8s-kube-apiserver:latest imagePullPolicy: IfNotPresent resources: requests: memory: \"512Mi\" limits: memory: \"1024Mi\" command: - /usr/bin/kube-apiserver - --advertise-address=k8s-master - --allow-privileged=true - --anonymous-auth=false - --apiserver-count=1 - --audit-log-maxage=30 - --audit-log-maxbackup=3 - --audit-log-maxsize=100 - --audit-log-path=/var/log/audit.log - --authorization-mode=Node,RBAC,Webhook - --authorization-webhook-config-file=/etc/kubernetes/webhook/authorization-config.yaml - --authentication-token-webhook-cache-ttl=2m - --authentication-token-webhook-version=v1 - --bind-address=0.0.0.0 - --client-ca-file=/var/lib/kubernetes/ca.pem - --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,RuntimeClass - --etcd-cafile=/var/lib/kubernetes/ca.pem - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem - --etcd-servers=https://k8s-master:2379 - --event-ttl=1h - --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml - --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem - --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem - --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem - --runtime-config=authentication.k8s.io/v1beta1=true - --feature-gates=APIPriorityAndFairness=false - --service-account-key-file=/var/lib/kubernetes/service-account.pem - --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem - --service-account-issuer=api - --service-account-api-audiences=api - --service-cluster-ip-range=10.32.0.0/24 - --service-node-port-range=30000-32767 - --tls-cert-file=/var/lib/kubernetes/kubernetes.pem - --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem - --http2-max-streams-per-connection=3000 - --max-requests-inflight=3000 - --max-mutating-requests-inflight=1000 - --enable-aggregator-routing=true - --requestheader-client-ca-file=/var/lib/kubernetes/front-proxy-ca.pem - --requestheader-allowed-names=front-proxy-ca - --requestheader-extra-headers-prefix=X-Remote-Extra - --requestheader-group-headers=X-Remote-Group - --requestheader-username-headers=X-Remote-User - --proxy-client-cert-file=/var/lib/kubernetes/front-proxy.pem - --proxy-client-key-file=/var/lib/kubernetes/front-proxy-key.pem - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-apiserver CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 82c371fd9d99e 83e685a0b921ef5dd91eb3cdf208ba70690c1dd7decfc39bb3903be6ede752e6 24 seconds ago Running kube-apiserver 0 6af4d1b99fa37 master node\u306bPod\u304cschedule\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u308b taint\u3092\u8a2d\u5b9a\u3059\u308b https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/ https://kubernetes.io/ja/docs/concepts/scheduling-eviction/taint-and-toleration/ kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule $ kubectl get node k8s-master -o=jsonpath='{.spec.taints}' [{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\"}]","title":"\u624b\u9806"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_2","text":"failed creating mandatory flowcontrol settings: failed getting mandatory FlowSchema exempt due to the server was unable to return a response in the time allotted, but may still be processing the request https://github.com/kubernetes/kubernetes/issues/97525#issuecomment-753022219 kube-apiserver\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u4ee5\u4e0b2\u3064\u3092\u4ed8\u52a0\u3059\u308b --feature-gates=APIPriorityAndFairness=false --runtime-config=flowcontrol.apiserver.k8s.io/v1beta1=false failed to run Kubelet: no client provided, cannot use webhook authentication kubelet\u304cWebhook\u8a8d\u8a3c\u3092\u671f\u5f85\u3057\u3066\u3044\u308b\u306e\u306bkube-api-server\u3067Webhook\u8a8d\u8a3c\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408 Webhook\u8a8d\u8a3c\u3092\u6709\u52b9\u306b\u3059\u308b https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication Failed creating a mirror pod for \"kube-scheduler-k8s-master_kube-system(a4a914cd05761a5a4335e2510ca075aa)\": pods \"kube-scheduler-k8s-master\" is forbidden: PodSecurityPolicy: no providers available to validate pod request StaticPod\u3092\u8d77\u52d5\u3057\u305f\u969b\u306bkubelet\u304b\u3089kube-apiserver\u3078mirror pod\u60c5\u5831\u3092\u767b\u9332\u3057\u3088\u3046\u3068\u3057\u3066PodSecurityPolicy\u306b\u3088\u308a\u62d2\u5426\u3055\u308c\u305f","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_3","text":"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#old","text":"default\u306ePodSecurityPolicy(PSP)\u3092\u4f5c\u6210\u3059\u308b staticPod \u3092\u4f5c\u6210\u3059\u308b\u969b\u306bkubelet\u304b\u3089mirror pod\u4f5c\u6210\u30ea\u30af\u30a8\u30b9\u30c8\u304c\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3059 ( \u53c2\u8003 ) PSP / ClusterRole / ClusterRoleBinding cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: annotations: apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default' name: default spec: # allowedCapabilities: [] # default set of capabilities are implicitly allowed allowedCapabilities: - '*' # - NET_ADMIN # - NET_RAW # - SYS_ADMIN fsGroup: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 hostIPC: true hostNetwork: true hostPID: true privileged: true allowPrivilegeEscalation: true readOnlyRootFilesystem: true runAsUser: rule: 'MustRunAsNonRoot' seLinux: rule: 'RunAsNonRoot' supplementalGroups: rule: 'RunAsNonRoot' ranges: # Forbid adding the root group. - min: 1 max: 65535 volumes: - 'configMap' - 'downwardAPI' - 'emptyDir' - 'persistentVolumeClaim' - 'projected' - 'secret' - 'hostPath' hostNetwork: true runAsUser: rule: 'RunAsAny' seLinux: rule: 'RunAsAny' supplementalGroups: rule: 'RunAsAny' fsGroup: rule: 'RunAsAny' --- # Cluster role which grants access to the default pod security policy apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: default-psp rules: - apiGroups: - policy resourceNames: - default resources: - podsecuritypolicies verbs: - use --- # Cluster role binding for default pod security policy granting all authenticated users access apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default-psp roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: default-psp subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:authenticated EOF $ cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f - <\u7701\u7565> podsecuritypolicy.policy/default created clusterrole.rbac.authorization.k8s.io/default-psp created clusterrolebinding.rbac.authorization.k8s.io/default-psp created","title":"old"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/","text":"bootstrapping kube-controller-manager \u624b\u9806 Dockerfile_kube-controller-manager.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-controller-manager.armhf cat << 'EOF' > Dockerfile_kube-controller-manager.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-controller-manager \\ && chmod +x /usr/bin/kube-controller-manager \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY ca.pem \\ ca-key.pem \\ service-account-key.pem \\ kube-controller-manager.kubeconfig \\ /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-controller-manager\"] EOF image build sudo nerdctl build --namespace k8s.io -t k8s-kube-controller-manager --file=Dockerfile_kube-controller-manager.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b --allocate-node-cidrs=true Node resource\u306e spec.podCIDR \u3078CIDR\u304c\u8a2d\u5b9a\u3055\u308c\u308b kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' spec.podCIDR \u306e\u5024\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044node instance\u3067\u306fCNI Plugin(flannel)\u304c\u6b63\u5e38\u52d5\u4f5c\u3057\u306a\u304b\u3063\u305f /etc/kubelet.d/kube-controller-manager.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-controller-manager.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-controller-manager namespace: kube-system labels: tier: control-plane component: kube-controller-manager spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-controller-manager image: k8s-kube-controller-manager:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" memory: \"128Mi\" limits: cpu: \"384m\" memory: \"128Mi\" command: - /usr/bin/kube-controller-manager - --bind-address=0.0.0.0 - --cluster-cidr=10.200.0.0/16 - --allocate-node-cidrs=true - --node-cidr-mask-size=24 - --cluster-name=kubernetes - --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem - --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem - --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig - --leader-elect=false - --root-ca-file=/var/lib/kubernetes/ca.pem - --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem - --service-cluster-ip-range=10.32.0.0/24 - --use-service-account-credentials=true - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-controller-manager CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a72cec7323686 4ada5d332b2c795b6333b8b6c538491dec96fb80f81b600359615651725b0ccf 20 seconds ago Running kube-controller-manager 0 526d7f2e9d3cb \u30a8\u30e9\u30fc\u4e8b\u4f8b Client.Timeout\u3092\u8d85\u3048\u305f\u305f\u3081\u3001kube-control-manager\u3068kube-scheduler\u304c\u30ed\u30c3\u30af\u3092\u53d6\u5f97\u3067\u304d\u306a\u3044 \u767a\u751f\u3057\u305f\u3089\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u518d\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067\u56de\u5fa9\u3059\u308b kube-apiserver\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u304c\u4e0a\u304c\u308b\u3068\u767a\u751f\u3057\u6613\u304f\u306a\u308b E0325 11:08:47.205570 1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://192.168.10.50:6443/apis/coordination.k8s.io/v1/namespaces/kube- system/leases/kube-controller-manager?timeout=10s\": context deadline exceeded I0325 11:08:47.205695 1 leaderelection.go:278] failed to renew lease kube-system/kube-controller-manager: timed out waiting for the condition F0325 11:08:47.205929 1 controllermanager.go:294] leaderelection lost \u53c2\u8003\u6587\u732e https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/ node(flannel)\u306e Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned \u30a8\u30e9\u30fc\u306b\u95a2\u3057\u3066 https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/ https://devops.stackexchange.com/questions/5898/how-to-get-kubernetes-pod-network-cidr","title":"04. bootstrapping kube-controller-manager"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#bootstrapping-kube-controller-manager","text":"","title":"bootstrapping kube-controller-manager"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_1","text":"Dockerfile_kube-controller-manager.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-controller-manager.armhf cat << 'EOF' > Dockerfile_kube-controller-manager.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-controller-manager \\ && chmod +x /usr/bin/kube-controller-manager \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY ca.pem \\ ca-key.pem \\ service-account-key.pem \\ kube-controller-manager.kubeconfig \\ /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-controller-manager\"] EOF image build sudo nerdctl build --namespace k8s.io -t k8s-kube-controller-manager --file=Dockerfile_kube-controller-manager.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b --allocate-node-cidrs=true Node resource\u306e spec.podCIDR \u3078CIDR\u304c\u8a2d\u5b9a\u3055\u308c\u308b kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' spec.podCIDR \u306e\u5024\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044node instance\u3067\u306fCNI Plugin(flannel)\u304c\u6b63\u5e38\u52d5\u4f5c\u3057\u306a\u304b\u3063\u305f /etc/kubelet.d/kube-controller-manager.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-controller-manager.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-controller-manager namespace: kube-system labels: tier: control-plane component: kube-controller-manager spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-controller-manager image: k8s-kube-controller-manager:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" memory: \"128Mi\" limits: cpu: \"384m\" memory: \"128Mi\" command: - /usr/bin/kube-controller-manager - --bind-address=0.0.0.0 - --cluster-cidr=10.200.0.0/16 - --allocate-node-cidrs=true - --node-cidr-mask-size=24 - --cluster-name=kubernetes - --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem - --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem - --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig - --leader-elect=false - --root-ca-file=/var/lib/kubernetes/ca.pem - --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem - --service-cluster-ip-range=10.32.0.0/24 - --use-service-account-credentials=true - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-controller-manager CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a72cec7323686 4ada5d332b2c795b6333b8b6c538491dec96fb80f81b600359615651725b0ccf 20 seconds ago Running kube-controller-manager 0 526d7f2e9d3cb","title":"\u624b\u9806"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_2","text":"Client.Timeout\u3092\u8d85\u3048\u305f\u305f\u3081\u3001kube-control-manager\u3068kube-scheduler\u304c\u30ed\u30c3\u30af\u3092\u53d6\u5f97\u3067\u304d\u306a\u3044 \u767a\u751f\u3057\u305f\u3089\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u518d\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067\u56de\u5fa9\u3059\u308b kube-apiserver\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u304c\u4e0a\u304c\u308b\u3068\u767a\u751f\u3057\u6613\u304f\u306a\u308b E0325 11:08:47.205570 1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://192.168.10.50:6443/apis/coordination.k8s.io/v1/namespaces/kube- system/leases/kube-controller-manager?timeout=10s\": context deadline exceeded I0325 11:08:47.205695 1 leaderelection.go:278] failed to renew lease kube-system/kube-controller-manager: timed out waiting for the condition F0325 11:08:47.205929 1 controllermanager.go:294] leaderelection lost","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_3","text":"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/ node(flannel)\u306e Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned \u30a8\u30e9\u30fc\u306b\u95a2\u3057\u3066 https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/ https://devops.stackexchange.com/questions/5898/how-to-get-kubernetes-pod-network-cidr","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/","text":"bootstrapping kube-scheduler \u624b\u9806 Dockerfile_kube-scheduler.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-scheduler.armhf cat << 'EOF' > Dockerfile_kube-scheduler.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-scheduler \\ && chmod +x /usr/bin/kube-scheduler \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY kube-scheduler.yaml /etc/kubernetes/config/ COPY kube-scheduler.kubeconfig /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-scheduler\"] EOF kube-scheduler\u306econfig\u751f\u6210 k8s 1.19.0 \u3067 KubeSchedulerConfiguration \u304c beta\u306bupdate\u3055\u308c\u3066\u3044\u307e\u3059 https://qiita.com/everpeace/items/7dbf14773db82e765370 kube-scheduler.yaml cat << EOF > kube-scheduler.yaml --- apiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: false EOF image build sudo nerdctl build --namespace k8s.io -t k8s-kube-scheduler --file=Dockerfile_kube-scheduler.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/kube-scheduler.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-scheduler.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-scheduler namespace: kube-system labels: tier: control-plane component: kube-scheduler spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-scheduler image: k8s-kube-scheduler:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" memory: \"128Mi\" limits: cpu: \"384m\" memory: \"128Mi\" command: - /usr/bin/kube-scheduler - --config=/etc/kubernetes/config/kube-scheduler.yaml - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-scheduler CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a19648dec2d54 70e852515b3c74175bb3ad4855287cb81101921b2b1f5a890fa4ebd0eeeee684 15 seconds ago Running kube-scheduler 0 da1d0572bc2b1 \u53c2\u8003\u6587\u732e https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/","title":"05. bootstrapping kube-scheduler"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/#bootstrapping-kube-scheduler","text":"","title":"bootstrapping kube-scheduler"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/#_1","text":"Dockerfile_kube-scheduler.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-scheduler.armhf cat << 'EOF' > Dockerfile_kube-scheduler.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-scheduler \\ && chmod +x /usr/bin/kube-scheduler \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY kube-scheduler.yaml /etc/kubernetes/config/ COPY kube-scheduler.kubeconfig /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-scheduler\"] EOF kube-scheduler\u306econfig\u751f\u6210 k8s 1.19.0 \u3067 KubeSchedulerConfiguration \u304c beta\u306bupdate\u3055\u308c\u3066\u3044\u307e\u3059 https://qiita.com/everpeace/items/7dbf14773db82e765370 kube-scheduler.yaml cat << EOF > kube-scheduler.yaml --- apiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: false EOF image build sudo nerdctl build --namespace k8s.io -t k8s-kube-scheduler --file=Dockerfile_kube-scheduler.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/kube-scheduler.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-scheduler.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-scheduler namespace: kube-system labels: tier: control-plane component: kube-scheduler spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-scheduler image: k8s-kube-scheduler:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" memory: \"128Mi\" limits: cpu: \"384m\" memory: \"128Mi\" command: - /usr/bin/kube-scheduler - --config=/etc/kubernetes/config/kube-scheduler.yaml - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-scheduler CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a19648dec2d54 70e852515b3c74175bb3ad4855287cb81101921b2b1f5a890fa4ebd0eeeee684 15 seconds ago Running kube-scheduler 0 da1d0572bc2b1","title":"\u624b\u9806"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/#_2","text":"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/","text":"kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b kubectl \u3084\u4ed6Client tool\u3067\u306fkube-apiserver\u3078\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u6295\u3052\u307e\u3059\u3002 kube-apiserver \u3067\u306fetcd\u306b\u683c\u7d0d\u3055\u308c\u305f\u60c5\u5831\u3092\u57fa\u306b\u5404worker node(\u306e kubelet ) \u3068\u3084\u308a\u3068\u308a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002(\u4f8b\u3048\u3070exec,top,logs\u306a\u3069) kube-apiserver \u304b\u3089 kubelet \u306e\u5fc5\u8981\u306a\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u3092\u4ed8\u4e0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 \u624b\u9806 ClusterRole system:kube-apiserver-to-kubelet \u3092\u4f5c\u6210 rbac.authorization.kubernetes.io/autoupdate annotations \u8d77\u52d5\u3059\u308b\u305f\u3073\u306b\u3001API\u30b5\u30fc\u30d0\u30fc\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRole\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a29\u9650\u3067\u66f4\u65b0\u3057\u3001 \u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRoleBinding\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308bsubjects\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u8aa4\u3063\u305f\u5909\u66f4\u3092\u30af\u30e9\u30b9\u30bf\u304c\u4fee\u5fa9\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001 \u65b0\u3057\u3044Kubernetes\u30ea\u30ea\u30fc\u30b9\u3067\u6a29\u9650\u3068subjects\u304c\u5909\u66f4\u3055\u308c\u3066\u3082\u3001 Role\u3068RoleBinding\u3092\u6700\u65b0\u306e\u72b6\u614b\u306b\u4fdd\u3064\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 kubernetes.io/bootstrapping: rbac-defaults labels k8s\u306e\u65e2\u5b9a\u30af\u30e9\u30b9\u30bf\u30ed\u30fc\u30eb\u3068\u65e2\u5b9a\u30ed\u30fc\u30eb\u30d0\u30a4\u30f3\u30c9\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3059 cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:kube-apiserver-to-kubelet annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF Kubernetes \u30e6\u30fc\u30b6\u3078 system:kube-apiserver-to-kubelet ClusterRole\u3092\u7d10\u4ed8\u3051\u308b roleRef \u3067\u7d10\u4ed8\u3051\u305fRole\u3092\u6307\u5b9a\u3059\u308b subjects \u3067Role\u3092\u7d10\u4ed8\u3051\u308bAccount\u3092\u6307\u5b9a\u3059\u308b cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: Kubernetes EOF \u3053\u306e\u7d10\u4ed8\u3051\u304c\u6b63\u3057\u304f\u306a\u3044\u3001\u3082\u3057\u304f\u306f\u672a\u8a2d\u5b9a\u306e\u5834\u5408\u3001token\u4ed8\u304d\u3067kubectl\u3092\u5229\u7528\u3057\u305f\u5834\u5408\u306b\u4ee5\u4e0b\u30a8\u30e9\u30fc\u3068\u306a\u308b Error from server (Forbidden): Forbidden (user=Kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-proxy) \u53c2\u8003\u8cc7\u6599 https://kubernetes.io/ja/docs/reference/access-authn-authz/rbac/ https://qiita.com/sheepland/items/67a5bb9b19d8686f389d","title":"06. kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/#kube-apiserver-kubelet","text":"kubectl \u3084\u4ed6Client tool\u3067\u306fkube-apiserver\u3078\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u6295\u3052\u307e\u3059\u3002 kube-apiserver \u3067\u306fetcd\u306b\u683c\u7d0d\u3055\u308c\u305f\u60c5\u5831\u3092\u57fa\u306b\u5404worker node(\u306e kubelet ) \u3068\u3084\u308a\u3068\u308a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002(\u4f8b\u3048\u3070exec,top,logs\u306a\u3069) kube-apiserver \u304b\u3089 kubelet \u306e\u5fc5\u8981\u306a\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u3092\u4ed8\u4e0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002","title":"kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/#_1","text":"ClusterRole system:kube-apiserver-to-kubelet \u3092\u4f5c\u6210 rbac.authorization.kubernetes.io/autoupdate annotations \u8d77\u52d5\u3059\u308b\u305f\u3073\u306b\u3001API\u30b5\u30fc\u30d0\u30fc\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRole\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a29\u9650\u3067\u66f4\u65b0\u3057\u3001 \u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRoleBinding\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308bsubjects\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u8aa4\u3063\u305f\u5909\u66f4\u3092\u30af\u30e9\u30b9\u30bf\u304c\u4fee\u5fa9\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001 \u65b0\u3057\u3044Kubernetes\u30ea\u30ea\u30fc\u30b9\u3067\u6a29\u9650\u3068subjects\u304c\u5909\u66f4\u3055\u308c\u3066\u3082\u3001 Role\u3068RoleBinding\u3092\u6700\u65b0\u306e\u72b6\u614b\u306b\u4fdd\u3064\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 kubernetes.io/bootstrapping: rbac-defaults labels k8s\u306e\u65e2\u5b9a\u30af\u30e9\u30b9\u30bf\u30ed\u30fc\u30eb\u3068\u65e2\u5b9a\u30ed\u30fc\u30eb\u30d0\u30a4\u30f3\u30c9\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3059 cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:kube-apiserver-to-kubelet annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF Kubernetes \u30e6\u30fc\u30b6\u3078 system:kube-apiserver-to-kubelet ClusterRole\u3092\u7d10\u4ed8\u3051\u308b roleRef \u3067\u7d10\u4ed8\u3051\u305fRole\u3092\u6307\u5b9a\u3059\u308b subjects \u3067Role\u3092\u7d10\u4ed8\u3051\u308bAccount\u3092\u6307\u5b9a\u3059\u308b cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: Kubernetes EOF \u3053\u306e\u7d10\u4ed8\u3051\u304c\u6b63\u3057\u304f\u306a\u3044\u3001\u3082\u3057\u304f\u306f\u672a\u8a2d\u5b9a\u306e\u5834\u5408\u3001token\u4ed8\u304d\u3067kubectl\u3092\u5229\u7528\u3057\u305f\u5834\u5408\u306b\u4ee5\u4e0b\u30a8\u30e9\u30fc\u3068\u306a\u308b Error from server (Forbidden): Forbidden (user=Kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-proxy)","title":"\u624b\u9806"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/#_2","text":"https://kubernetes.io/ja/docs/reference/access-authn-authz/rbac/ https://qiita.com/sheepland/items/67a5bb9b19d8686f389d","title":"\u53c2\u8003\u8cc7\u6599"},{"location":"setup/06_master/07_controller_health_check/","text":"Kubernetes API \u306e\u30d8\u30eb\u30b9\u30c1\u30a7\u30c3\u30af \u624b\u9806 \u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u8d77\u52d5\u78ba\u8a8d kubectl get pods -n kube-system \u5b9f\u884c\u4f8b $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE etcd-k8s-master 1/1 Running 0 5m56s kube-apiserver-k8s-master 1/1 Running 0 6m7s kube-controller-manager-k8s-master 1/1 Running 0 4m2s kube-scheduler-k8s-master 1/1 Running 0 2m48s master node\u4e0a\u306eresource\u78ba\u8a8d kubectl get nodes kubectl describe node <pod_name> \u5b9f\u884c\u4f8b $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready <none> 7m57s v1.20.1 $ kubectl describe node k8s-master Name: k8s-master Roles: <none> Labels: beta.kubernetes.io/arch=arm64 beta.kubernetes.io/os=linux kubernetes.io/arch=arm64 kubernetes.io/hostname=k8s-master kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Sat, 17 Apr 2021 15:13:42 +0000 Taints: node-role.kubernetes.io/master:NoSchedule Unschedulable: false Lease: HolderIdentity: k8s-master AcquireTime: <unset> RenewTime: Sat, 17 Apr 2021 16:34:29 +0000 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:52 +0000 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192.168.10.50 Hostname: k8s-master Capacity: cpu: 4 ephemeral-storage: 30459624Ki memory: 1892528Ki pods: 110 Allocatable: cpu: 3400m ephemeral-storage: 28071589432 memory: 1380528Ki pods: 110 System Info: Machine ID: 58f6de70444c4198b56b30122b6c77dc System UUID: 58f6de70444c4198b56b30122b6c77dc Boot ID: 79af3428-cf70-4189-a447-0b917a035a42 Kernel Version: 5.4.0-1032-raspi OS Image: Ubuntu 20.04.2 LTS Operating System: linux Architecture: arm64 Container Runtime Version: cri-o://1.20.2 Kubelet Version: v1.20.1 Kube-Proxy Version: v1.20.1 PodCIDR: 10.200.0.0/24 PodCIDRs: 10.200.0.0/24 Non-terminated Pods: (4 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system etcd-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-apiserver-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-controller-manager-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 76m kube-system kube-scheduler-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 75m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1200m (35%) 2600m (76%) memory 768Mi (56%) 1280Mi (94%) ephemeral-storage 0 (0%) 0 (0%) Events: <none> health checks kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067 --anonymous-auth=false \u3092\u4ed8\u52a0\u3057\u3066\u3044\u308b\u305f\u3081 https://localhost:6443 \u3078\u306eanonymous\u30a2\u30ab\u30a6\u30f3\u30c8\u3067\u306e\u78ba\u8a8d\u306f\u884c\u308f\u305a\u306b kubectl \u3067\u78ba\u8a8d\u3059\u308b API endpoints for health kubectl get --raw='/readyz?verbose' \u5b9f\u884c\u4f8b $ kubectl get --raw='/readyz?verbose' [+]ping ok [+]log ok [+]etcd ok [+]informer-sync ok [+]poststarthook/start-kube-apiserver-admission-initializer ok [+]poststarthook/generic-apiserver-start-informers ok [+]poststarthook/max-in-flight-filter ok [+]poststarthook/start-apiextensions-informers ok [+]poststarthook/start-apiextensions-controllers ok [+]poststarthook/crd-informer-synced ok [+]poststarthook/bootstrap-controller ok [+]poststarthook/rbac/bootstrap-roles ok [+]poststarthook/scheduling/bootstrap-system-priority-classes ok [+]poststarthook/priority-and-fairness-config-producer ok [+]poststarthook/start-cluster-authentication-info-controller ok [+]poststarthook/start-kube-aggregator-informers ok [+]poststarthook/apiservice-registration-controller ok [+]poststarthook/apiservice-status-available-controller ok [+]poststarthook/kube-apiserver-autoregistration ok [+]autoregister-completion ok [+]poststarthook/apiservice-openapi-controller ok [+]shutdown ok readyz check passed Individual health checks kubectl get --raw='/livez/etcd' \u5b9f\u884c\u4f8b $ kubectl get --raw='/livez/etcd' ok $ kubectl get --raw='/livez/poststarthook/start-apiextensions-controllers' ok \u53c2\u8003\u8cc7\u6599 https://kubernetes.io/docs/reference/using-api/health-checks/","title":"07. Kubernetes API \u306e\u30d8\u30eb\u30b9\u30c1\u30a7\u30c3\u30af"},{"location":"setup/06_master/07_controller_health_check/#kubernetes-api","text":"","title":"Kubernetes API \u306e\u30d8\u30eb\u30b9\u30c1\u30a7\u30c3\u30af"},{"location":"setup/06_master/07_controller_health_check/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/06_master/07_controller_health_check/#_2","text":"kubectl get pods -n kube-system \u5b9f\u884c\u4f8b $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE etcd-k8s-master 1/1 Running 0 5m56s kube-apiserver-k8s-master 1/1 Running 0 6m7s kube-controller-manager-k8s-master 1/1 Running 0 4m2s kube-scheduler-k8s-master 1/1 Running 0 2m48s","title":"\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u8d77\u52d5\u78ba\u8a8d"},{"location":"setup/06_master/07_controller_health_check/#master-noderesource","text":"kubectl get nodes kubectl describe node <pod_name> \u5b9f\u884c\u4f8b $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready <none> 7m57s v1.20.1 $ kubectl describe node k8s-master Name: k8s-master Roles: <none> Labels: beta.kubernetes.io/arch=arm64 beta.kubernetes.io/os=linux kubernetes.io/arch=arm64 kubernetes.io/hostname=k8s-master kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Sat, 17 Apr 2021 15:13:42 +0000 Taints: node-role.kubernetes.io/master:NoSchedule Unschedulable: false Lease: HolderIdentity: k8s-master AcquireTime: <unset> RenewTime: Sat, 17 Apr 2021 16:34:29 +0000 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:52 +0000 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192.168.10.50 Hostname: k8s-master Capacity: cpu: 4 ephemeral-storage: 30459624Ki memory: 1892528Ki pods: 110 Allocatable: cpu: 3400m ephemeral-storage: 28071589432 memory: 1380528Ki pods: 110 System Info: Machine ID: 58f6de70444c4198b56b30122b6c77dc System UUID: 58f6de70444c4198b56b30122b6c77dc Boot ID: 79af3428-cf70-4189-a447-0b917a035a42 Kernel Version: 5.4.0-1032-raspi OS Image: Ubuntu 20.04.2 LTS Operating System: linux Architecture: arm64 Container Runtime Version: cri-o://1.20.2 Kubelet Version: v1.20.1 Kube-Proxy Version: v1.20.1 PodCIDR: 10.200.0.0/24 PodCIDRs: 10.200.0.0/24 Non-terminated Pods: (4 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system etcd-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-apiserver-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-controller-manager-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 76m kube-system kube-scheduler-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 75m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1200m (35%) 2600m (76%) memory 768Mi (56%) 1280Mi (94%) ephemeral-storage 0 (0%) 0 (0%) Events: <none>","title":"master node\u4e0a\u306eresource\u78ba\u8a8d"},{"location":"setup/06_master/07_controller_health_check/#health-checks","text":"kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067 --anonymous-auth=false \u3092\u4ed8\u52a0\u3057\u3066\u3044\u308b\u305f\u3081 https://localhost:6443 \u3078\u306eanonymous\u30a2\u30ab\u30a6\u30f3\u30c8\u3067\u306e\u78ba\u8a8d\u306f\u884c\u308f\u305a\u306b kubectl \u3067\u78ba\u8a8d\u3059\u308b API endpoints for health kubectl get --raw='/readyz?verbose' \u5b9f\u884c\u4f8b $ kubectl get --raw='/readyz?verbose' [+]ping ok [+]log ok [+]etcd ok [+]informer-sync ok [+]poststarthook/start-kube-apiserver-admission-initializer ok [+]poststarthook/generic-apiserver-start-informers ok [+]poststarthook/max-in-flight-filter ok [+]poststarthook/start-apiextensions-informers ok [+]poststarthook/start-apiextensions-controllers ok [+]poststarthook/crd-informer-synced ok [+]poststarthook/bootstrap-controller ok [+]poststarthook/rbac/bootstrap-roles ok [+]poststarthook/scheduling/bootstrap-system-priority-classes ok [+]poststarthook/priority-and-fairness-config-producer ok [+]poststarthook/start-cluster-authentication-info-controller ok [+]poststarthook/start-kube-aggregator-informers ok [+]poststarthook/apiservice-registration-controller ok [+]poststarthook/apiservice-status-available-controller ok [+]poststarthook/kube-apiserver-autoregistration ok [+]autoregister-completion ok [+]poststarthook/apiservice-openapi-controller ok [+]shutdown ok readyz check passed Individual health checks kubectl get --raw='/livez/etcd' \u5b9f\u884c\u4f8b $ kubectl get --raw='/livez/etcd' ok $ kubectl get --raw='/livez/poststarthook/start-apiextensions-controllers' ok","title":"health checks"},{"location":"setup/06_master/07_controller_health_check/#_3","text":"https://kubernetes.io/docs/reference/using-api/health-checks/","title":"\u53c2\u8003\u8cc7\u6599"},{"location":"setup/07_worker/01_bootstrapping_kubelet/","text":"bootstrapping kubelet(master/worker \u5171\u901a) kubelet \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002 worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206 Reserve Compute Resources for System Daemons https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9 = Node resource - system-reserved - kube-reserved - eviction-threshold \u3089\u3057\u3044 name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi \u624b\u9806 kubelet \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 VERSION=\"v1.22.0\" ARCH=\"arm64\" sudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet sudo chmod +x /usr/bin/kubelet kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" sudo install -o root -g root -m 755 -d /etc/kubelet.d sudo install -o root -g root -m 755 -d /var/lib/kubernetes sudo install -o root -g root -m 755 -d /var/lib/kubelet sudo cp ca.pem /var/lib/kubernetes/ sudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/ sudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig /var/lib/kubelet/kubelet-config.yaml \u3092\u4f5c\u6210\u3059\u308b clusterDNS \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b podCIDR \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 # https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ staticPodPath: /etc/kubelet.d # kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f # - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b # - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981 authentication: anonymous: enabled: true webhook: enabled: false cacheTTL: \"2m\" x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" # kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a # - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow # - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981 authorization: mode: AlwaysAllow clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" podCIDR: \"10.200.0.0/24\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${host}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\" # Reserve Compute Resources for System Daemons # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # # Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044 # # system-reserved # - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b # # kube-reserved # - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b enforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"] cgroupsPerQOS: true cgroupDriver: systemd cgroupRoot: / systemCgroups: /systemd/system.slice systemReservedCgroup: /system.slice systemReserved: cpu: 256m memory: 256Mi runtimeCgroups: /kube.slice/containerd.service kubeletCgroups: /kube.slice/kubelet.service kubeReservedCgroup: /kube.slice kubeReserved: cpu: 1024m memory: 1024Mi EOF /etc/systemd/system/kubelet.service \u3092\u914d\u7f6e cat << 'EOF' | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] Restart=on-failure RestartSec=5 ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/kube.slice \\ /sys/fs/cgroup/system.slice \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/system.slice \\ /sys/fs/cgroup/hugetlb/system.slice \\ /sys/fs/cgroup/hugetlb/kube.slice ExecStart=/usr/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --container-runtime=remote \\ --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\ --register-node=true \\ --v=2 [Install] WantedBy=multi-user.target EOF kubelet.service \u3092\u8d77\u52d5 sudo systemctl enable kubelet.service sudo systemctl start kubelet.service \u30a8\u30e9\u30fc\u4e8b\u4f8b cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408 kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist kubelet \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( --v 10 ) cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice] \u5bfe\u5fdc kubelet.service \u306e ExecStartPre \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f \u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy kubeconfig \u306e\u8a3c\u660e\u66f8\u306e CN \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b 360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\" kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: k8s-master \u304c\u6b63\u3057\u3044\u306e\u306b CN = system:node:k8s-node1 \u3068\u306a\u3063\u3066\u3044\u305f root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master Node \u30ea\u30bd\u30fc\u30b9\u306e spec.podCIDR \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044 \u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044 flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f... kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u30ed\u30b0 Set node k8s-node1 PodCIDR to [10.200.0.0/24] \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8 kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b --allocate-node-cidrs=true \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist range_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24] ttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\" controller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}] controller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes node_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\" controller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1 node_lifecycle_controller.go:1429] Initializing eviction metric for zone: node_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp. event.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\" node_lifecycle_controller.go:1245] Controller detected that zone is now in state Normal. Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044 I0214 07:03:56.822586 1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\" F0214 07:03:56.822637 1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication goroutine 1 [running]: https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication CNI Plugin\u3092 /etc/cni/net.d \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044 kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized CNI Plugin\u3092 /etc/cni/net.d \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b https://github.com/containernetworking/plugins/releases Kubelet cannot determine CPU online state sysinfo.go:203] Nodes topology is not available, providing CPU topology sysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory gce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:72] Cannot read number of physical cores correctly, number of cores set to 0 machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:86] Cannot read number of sockets correctly, number of sockets set to 0 container_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name: \u65e2\u77e5\u3089\u3057\u3044 https://github.com/kubernetes/kubernetes/issues/95039 cni plugin not initialized /opt/cni/bin \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f /etc/cni/net.d \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\" cni config uninitialized /opt/cni/bin \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f /etc/cni/net.d \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni config uninitialized\" \u53c2\u8003 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go https://cyberagent.ai/blog/tech/4036/ kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b https://downloadkubernetes.com/ https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/ Node Authorization https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822 https://kubernetes.io/docs/reference/access-authn-authz/node/ https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ static pod https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/policy/pod-security-policy/ https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver PodSecurityPolicy \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf( false \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f true \u306b\u76f4\u3059) https://github.com/kubernetes/kubernetes/issues/70952","title":"01. bootstrapping kubelet"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#bootstrapping-kubeletmasterworker","text":"kubelet \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002","title":"bootstrapping kubelet(master/worker \u5171\u901a)"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#worker-node","text":"Reserve Compute Resources for System Daemons https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9 = Node resource - system-reserved - kube-reserved - eviction-threshold \u3089\u3057\u3044 name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi","title":"worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubelet","text":"VERSION=\"v1.22.0\" ARCH=\"arm64\" sudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet sudo chmod +x /usr/bin/kubelet","title":"kubelet \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeconfig","text":"# host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" sudo install -o root -g root -m 755 -d /etc/kubelet.d sudo install -o root -g root -m 755 -d /var/lib/kubernetes sudo install -o root -g root -m 755 -d /var/lib/kubelet sudo cp ca.pem /var/lib/kubernetes/ sudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/ sudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig","title":"kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#varlibkubeletkubelet-configyaml","text":"clusterDNS \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b podCIDR \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" cat << EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 # https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ staticPodPath: /etc/kubelet.d # kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f # - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b # - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981 authentication: anonymous: enabled: true webhook: enabled: false cacheTTL: \"2m\" x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" # kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a # - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow # - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981 authorization: mode: AlwaysAllow clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" podCIDR: \"10.200.0.0/24\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${host}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\" # Reserve Compute Resources for System Daemons # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # # Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044 # # system-reserved # - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b # # kube-reserved # - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b enforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"] cgroupsPerQOS: true cgroupDriver: systemd cgroupRoot: / systemCgroups: /systemd/system.slice systemReservedCgroup: /system.slice systemReserved: cpu: 256m memory: 256Mi runtimeCgroups: /kube.slice/containerd.service kubeletCgroups: /kube.slice/kubelet.service kubeReservedCgroup: /kube.slice kubeReserved: cpu: 1024m memory: 1024Mi EOF","title":"/var/lib/kubelet/kubelet-config.yaml \u3092\u4f5c\u6210\u3059\u308b"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#etcsystemdsystemkubeletservice","text":"cat << 'EOF' | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] Restart=on-failure RestartSec=5 ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/kube.slice \\ /sys/fs/cgroup/system.slice \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/system.slice \\ /sys/fs/cgroup/hugetlb/system.slice \\ /sys/fs/cgroup/hugetlb/kube.slice ExecStart=/usr/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --container-runtime=remote \\ --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\ --register-node=true \\ --v=2 [Install] WantedBy=multi-user.target EOF","title":"/etc/systemd/system/kubelet.service \u3092\u914d\u7f6e"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeletservice","text":"sudo systemctl enable kubelet.service sudo systemctl start kubelet.service","title":"kubelet.service \u3092\u8d77\u52d5"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_2","text":"","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cgroup","text":"kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist kubelet \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( --v 10 ) cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice] \u5bfe\u5fdc kubelet.service \u306e ExecStartPre \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice","title":"cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cgroupsystemreserved-memory-size","text":"\u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy","title":"cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeconfig-cn-node","text":"360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\" kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: k8s-master \u304c\u6b63\u3057\u3044\u306e\u306b CN = system:node:k8s-node1 \u3068\u306a\u3063\u3066\u3044\u305f root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master","title":"kubeconfig \u306e\u8a3c\u660e\u66f8\u306e CN \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#node-specpodcidr-cidr","text":"\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044 flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f... kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u30ed\u30b0 Set node k8s-node1 PodCIDR to [10.200.0.0/24] \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8 kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b --allocate-node-cidrs=true \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist range_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24] ttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\" controller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}] controller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes node_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\" controller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1 node_lifecycle_controller.go:1429] Initializing eviction metric for zone: node_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp. event.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\" node_lifecycle_controller.go:1245] Controller detected that zone is now in state Normal.","title":"Node \u30ea\u30bd\u30fc\u30b9\u306e spec.podCIDR \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#webhook-authentication","text":"I0214 07:03:56.822586 1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\" F0214 07:03:56.822637 1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication goroutine 1 [running]: https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication","title":"Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cni-plugin-etccninetd-cni-plugin","text":"kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized CNI Plugin\u3092 /etc/cni/net.d \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b https://github.com/containernetworking/plugins/releases","title":"CNI Plugin\u3092 /etc/cni/net.d \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubelet-cannot-determine-cpu-online-state","text":"sysinfo.go:203] Nodes topology is not available, providing CPU topology sysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory gce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:72] Cannot read number of physical cores correctly, number of cores set to 0 machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:86] Cannot read number of sockets correctly, number of sockets set to 0 container_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name: \u65e2\u77e5\u3089\u3057\u3044 https://github.com/kubernetes/kubernetes/issues/95039","title":"Kubelet cannot determine CPU online state"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cni-plugin-not-initialized","text":"/opt/cni/bin \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f /etc/cni/net.d \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"","title":"cni plugin not initialized"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cni-config-uninitialized","text":"/opt/cni/bin \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f /etc/cni/net.d \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni config uninitialized\"","title":"cni config uninitialized"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_3","text":"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go https://cyberagent.ai/blog/tech/4036/ kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b https://downloadkubernetes.com/ https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/ Node Authorization https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822 https://kubernetes.io/docs/reference/access-authn-authz/node/ https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ static pod https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/policy/pod-security-policy/ https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver PodSecurityPolicy \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf( false \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f true \u306b\u76f4\u3059) https://github.com/kubernetes/kubernetes/issues/70952","title":"\u53c2\u8003"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/","text":"bootstrapping kube-proxy kube-proxy \u3068\u306f kube-proxy \u3068\u306f\u5404worker node\u3067\u52d5\u4f5c\u3059\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30d7\u30ed\u30ad\u30b7\u3092\u5b9f\u73fe\u3059\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3067\u3059\u3002\u5177\u4f53\u7684\u306b\u306f Sertvice \u30ea\u30bd\u30fc\u30b9\u3067\u4f5c\u6210\u3055\u308c\u308bCluster IP\u3084Node Port\u306e\u7ba1\u7406\u3068\u305d\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30c6\u30fc\u30d6\u30eb\u306e\u7ba1\u7406\u3001\u307e\u305fnginx ingress controller\u3092\u5229\u7528\u3057\u305fIngress\u30ea\u30bd\u30fc\u30b9\u3067\u306fPod\u3078\u306e\u8ca0\u8377\u5206\u6563\u306bkube-proxy\u3092\u6d3b\u7528\u6307\u5b9a\u305f\u308a\u3059\u308b\u305d\u3046\u3067\u3059( With NGINX, we\u2019ll use the DNS name or virtual IP address to identify the service, and rely on kube-proxy to perform the internal load-balancing across the pool of pods. ) \u624b\u9806 Dockerfile_kube-proxy.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-proxy.armhf cat << 'EOF' > Dockerfile_kube-proxy.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-proxy \\ && chmod +x /usr/bin/kube-proxy \\ && install -o root -g root -m 755 -d /var/lib/kube-proxy \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY kube-proxy.kubeconfig /var/lib//kube-proxy/kubeconfig ENTRYPOINT [\"/usr/bin/kube-proxy\"] EOF image build sudo nerdctl build --namespace k8s.io -f Dockerfile_kube-proxy.armhf -t k8s-kube-proxy ./ kernel parameter cat <<EOF | sudo tee /etc/sysctl.d/kubelet.conf # kube-proxy net.ipv4.conf.all.route_localnet = 1 net.netfilter.nf_conntrack_max = 131072 net.netfilter.nf_conntrack_tcp_timeout_established = 86400 net.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600 EOF sudo sysctl --system cat <<EOF | sudo tee /etc/modprobe.d/kube-proxy.conf options nf_conntrack hashsize=32768 EOF sudo /sbin/modprobe nf_conntrack hashsize=32768 pod manifests\u3092 /etc/kubernetes/manifests/ \u3078\u4f5c\u6210\u3059\u308b /etc/kubernetes/manifests/kube-proxy.yaml cluster_cidr=\"10.200.0.0/16\" sudo mkdir -p /etc/kubernetes/manifests cat << EOF | sudo tee /etc/kubernetes/manifests/kube-proxy.yaml --- apiVersion: v1 kind: ConfigMap metadata: labels: app: kube-proxy name: kube-proxy-configuration namespace: kube-system data: config.conf: |- --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"${cluster_cidr}\" # https://kubernetes.io/docs/reference/config-api/kube-proxy-config.v1alpha1/ # metricsBindAddress: 127.0.0.1:10249 metricsBindAddress: 0.0.0.0:10249 --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-proxy namespace: kube-system labels: component: kube-proxy # TODO # master node\u306baddon-manager\u3092\u5c0e\u5165\u3057\u305f\u3089\u30b3\u30e1\u30f3\u30c8\u5916\u3059 # addonmanager.kubernetes.io/mode=Reconcile spec: selector: matchLabels: name: kube-proxy # https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#performing-a-rolling-update updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: # template \u4ee5\u4e0b\u306fpod templates # (apiVersion\u3084kind\u3092\u3082\u305f\u306a\u3044\u3053\u3068\u3092\u9664\u3044\u3066\u306f\u3001Pod\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3068\u540c\u3058\u30b9\u30ad\u30fc\u30de) # https://kubernetes.io/ja/docs/concepts/workloads/controllers/daemonset/ metadata: labels: name: kube-proxy annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-proxy image: localhost/k8s-kube-proxy:latest securityContext: capabilities: add: - SYS_ADMIN - NET_ADMIN - NET_RAW command: - /usr/bin/kube-proxy - --config=/var/lib/kube-proxy/kube-proxy-config.yaml imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" volumeMounts: - name: kube-proxy-configuration-volume mountPath: /var/lib/kube-proxy/kube-proxy-config.yaml - name: conntrack-command mountPath: /usr/sbin/conntrack - name: iptables-command mountPath: /usr/sbin/iptables - name: iptables-restore-command mountPath: /usr/sbin/iptables-restore - name: iptables-save-command mountPath: /usr/sbin/iptables-save - name: xtables-lock-file mountPath: /run/xtables.lock - name: usr-lib-dir mountPath: /usr/lib - name: lib-dir mountPath: /lib - name: sys-dir mountPath: /sys volumes: - name: kube-proxy-configuration configMap: name: kube-proxy-configuration - name: conntrack-command hostPath: path: /usr/sbin/conntrack - name: iptables-command hostPath: path: /usr/sbin/iptables - name: iptables-restore-command hostPath: path: /usr/sbin/iptables-restore - name: iptables-save-command hostPath: path: /usr/sbin/iptables-save - name: xtables-lock-file hostPath: path: /run/xtables.lock - name: usr-lib-dir hostPath: path: /usr/lib - name: lib-dir hostPath: path: /lib - name: sys-dir hostPath: path: /sys EOF pod\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b kubectl apply -f /etc/kubernetes/manifests/kube-proxy.yaml","title":"02. bootstrapping kube-proxy"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/#bootstrapping-kube-proxy","text":"","title":"bootstrapping kube-proxy"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/#kube-proxy","text":"kube-proxy \u3068\u306f\u5404worker node\u3067\u52d5\u4f5c\u3059\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30d7\u30ed\u30ad\u30b7\u3092\u5b9f\u73fe\u3059\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3067\u3059\u3002\u5177\u4f53\u7684\u306b\u306f Sertvice \u30ea\u30bd\u30fc\u30b9\u3067\u4f5c\u6210\u3055\u308c\u308bCluster IP\u3084Node Port\u306e\u7ba1\u7406\u3068\u305d\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30c6\u30fc\u30d6\u30eb\u306e\u7ba1\u7406\u3001\u307e\u305fnginx ingress controller\u3092\u5229\u7528\u3057\u305fIngress\u30ea\u30bd\u30fc\u30b9\u3067\u306fPod\u3078\u306e\u8ca0\u8377\u5206\u6563\u306bkube-proxy\u3092\u6d3b\u7528\u6307\u5b9a\u305f\u308a\u3059\u308b\u305d\u3046\u3067\u3059( With NGINX, we\u2019ll use the DNS name or virtual IP address to identify the service, and rely on kube-proxy to perform the internal load-balancing across the pool of pods. )","title":"kube-proxy \u3068\u306f"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/#_1","text":"Dockerfile_kube-proxy.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-proxy.armhf cat << 'EOF' > Dockerfile_kube-proxy.armhf FROM arm64v8/ubuntu:bionic ARG VERSION=\"v1.22.0\" ARG ARCH=\"arm64\" RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-proxy \\ && chmod +x /usr/bin/kube-proxy \\ && install -o root -g root -m 755 -d /var/lib/kube-proxy \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY kube-proxy.kubeconfig /var/lib//kube-proxy/kubeconfig ENTRYPOINT [\"/usr/bin/kube-proxy\"] EOF image build sudo nerdctl build --namespace k8s.io -f Dockerfile_kube-proxy.armhf -t k8s-kube-proxy ./ kernel parameter cat <<EOF | sudo tee /etc/sysctl.d/kubelet.conf # kube-proxy net.ipv4.conf.all.route_localnet = 1 net.netfilter.nf_conntrack_max = 131072 net.netfilter.nf_conntrack_tcp_timeout_established = 86400 net.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600 EOF sudo sysctl --system cat <<EOF | sudo tee /etc/modprobe.d/kube-proxy.conf options nf_conntrack hashsize=32768 EOF sudo /sbin/modprobe nf_conntrack hashsize=32768 pod manifests\u3092 /etc/kubernetes/manifests/ \u3078\u4f5c\u6210\u3059\u308b /etc/kubernetes/manifests/kube-proxy.yaml cluster_cidr=\"10.200.0.0/16\" sudo mkdir -p /etc/kubernetes/manifests cat << EOF | sudo tee /etc/kubernetes/manifests/kube-proxy.yaml --- apiVersion: v1 kind: ConfigMap metadata: labels: app: kube-proxy name: kube-proxy-configuration namespace: kube-system data: config.conf: |- --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"${cluster_cidr}\" # https://kubernetes.io/docs/reference/config-api/kube-proxy-config.v1alpha1/ # metricsBindAddress: 127.0.0.1:10249 metricsBindAddress: 0.0.0.0:10249 --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-proxy namespace: kube-system labels: component: kube-proxy # TODO # master node\u306baddon-manager\u3092\u5c0e\u5165\u3057\u305f\u3089\u30b3\u30e1\u30f3\u30c8\u5916\u3059 # addonmanager.kubernetes.io/mode=Reconcile spec: selector: matchLabels: name: kube-proxy # https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#performing-a-rolling-update updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: # template \u4ee5\u4e0b\u306fpod templates # (apiVersion\u3084kind\u3092\u3082\u305f\u306a\u3044\u3053\u3068\u3092\u9664\u3044\u3066\u306f\u3001Pod\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3068\u540c\u3058\u30b9\u30ad\u30fc\u30de) # https://kubernetes.io/ja/docs/concepts/workloads/controllers/daemonset/ metadata: labels: name: kube-proxy annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-proxy image: localhost/k8s-kube-proxy:latest securityContext: capabilities: add: - SYS_ADMIN - NET_ADMIN - NET_RAW command: - /usr/bin/kube-proxy - --config=/var/lib/kube-proxy/kube-proxy-config.yaml imagePullPolicy: IfNotPresent resources: requests: cpu: \"256m\" volumeMounts: - name: kube-proxy-configuration-volume mountPath: /var/lib/kube-proxy/kube-proxy-config.yaml - name: conntrack-command mountPath: /usr/sbin/conntrack - name: iptables-command mountPath: /usr/sbin/iptables - name: iptables-restore-command mountPath: /usr/sbin/iptables-restore - name: iptables-save-command mountPath: /usr/sbin/iptables-save - name: xtables-lock-file mountPath: /run/xtables.lock - name: usr-lib-dir mountPath: /usr/lib - name: lib-dir mountPath: /lib - name: sys-dir mountPath: /sys volumes: - name: kube-proxy-configuration configMap: name: kube-proxy-configuration - name: conntrack-command hostPath: path: /usr/sbin/conntrack - name: iptables-command hostPath: path: /usr/sbin/iptables - name: iptables-restore-command hostPath: path: /usr/sbin/iptables-restore - name: iptables-save-command hostPath: path: /usr/sbin/iptables-save - name: xtables-lock-file hostPath: path: /run/xtables.lock - name: usr-lib-dir hostPath: path: /usr/lib - name: lib-dir hostPath: path: /lib - name: sys-dir hostPath: path: /sys EOF pod\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b kubectl apply -f /etc/kubernetes/manifests/kube-proxy.yaml","title":"\u624b\u9806"},{"location":"setup/08_flannel/bootstrapping_flannel/","text":"bootstrapping flannel \u53c2\u8003\u6587\u732e https://github.com/flannel-io/flannel/ https://github.com/flannel-io/flannel/blob/master/Documentation/troubleshooting.md /opt/bin/flanneld \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3 https://github.com/flannel-io/flannel/blob/master/main.go#L110-L132 \u624b\u9806 vxlan module\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b sudo apt install -y linux-modules-extra-raspi locate sudo updatedb sudo modprobe vxlan sudo lsmod | grep vxlan flannel k8s manifests\u3092\u516c\u5f0f\u304b\u3089\u53d6\u5f97\u3059\u308b master\u30d6\u30e9\u30f3\u30c1\u304b\u3089\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u304c2021/03/06 \u6642\u70b9\u3067\u306f release tag v0.13.1-rc2 \u306e\u5185\u5bb9 sudo mkdir -p /etc/kubernetes/manifests sudo curl -o /etc/kubernetes/manifests/kube-flannel.yml -sSL https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml manifests\u3092\u4fee\u6b63\u3059\u308b net-conf.json \u306e Network \u3092controller-manager\u306e --cluster-cidr \u3067\u6307\u5b9a\u3057\u305f\u5024\u306b\u5909\u66f4\u3059\u308b etcd\u306b\u7528\u3044\u305fcertificate\u3084admin\u306ekubeconfig\u3092kube-flannel\u30b3\u30f3\u30c6\u30ca\u3078bind mount\u3059\u308b /opt/bin/flanneld \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3 --kubeconfig-file --etcd-endpoints --etcd-prefix --etcd-keyfile --etcd-certfile --etcd-cafile --v sudo vim /etc/kubernetes/manifests/kube-flannel.yml \u4fee\u6b63\u5f8c\u306e /etc/kubernetes/manifests/kube-flannel.yml --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \"/etc/cni/net.d\" - pathPrefix: \"/etc/kube-flannel\" - pathPrefix: \"/run/flannel\" readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN', 'NET_RAW'] defaultAddCapabilities: [] required seLinux: # SELinux is unused in CaaSP rule: 'RunAsAny' --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel rules: - apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged'] - apiGroups: - \"\" resources: - pods verbs: - get - apiGroups: - \"\" resources: - nodes verbs: - list - watch - apiGroups: - \"\" resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } net-conf.json: | { \"Network\": \"10.200.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni-plugin #image: flannelcni/flannel-cni-plugin:v1.0.1 for ppc64le and mips64le (dockerhub limitations may apply) image: rancher/mirrored-flannelcni-flannel-cni-plugin:v1.0.1 command: - cp args: - -f - /flannel - /opt/cni/bin/flannel volumeMounts: - name: cni-plugin mountPath: /opt/cni/bin - name: install-cni #image: flannelcni/flannel:v0.16.3 for ppc64le and mips64le (dockerhub limitations may apply) image: rancher/mirrored-flannelcni-flannel:v0.16.3 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel #image: flannelcni/flannel:v0.16.3 for ppc64le and mips64le (dockerhub limitations may apply) image: rancher/mirrored-flannelcni-flannel:v0.16.3 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig - --etcd-endpoints=https://k8s-master:4001 - --etcd-prefix=/coreos.com/network - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem - --etcd-cafile=/var/lib/kubernetes/ca.pem - --v=10 resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: false capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ - name: xtables-lock mountPath: /run/xtables.lock - name: var-lib-kubernetes-dir mountPath: /var/lib/kubernetes/ volumes: - name: run hostPath: path: /run/flannel - name: cni-plugin hostPath: path: /opt/cni/bin - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg - name: xtables-lock hostPath: path: /run/xtables.lock type: FileOrCreate - name: var-lib-kubernetes-dir hostPath: path: /var/lib/kubernetes flannel Pod\u3092\u30c7\u30d7\u30ed\u30a4 kubectl apply -f /etc/kubernetes/manifests/kube-flannel.yml \u30a8\u30e9\u30fc\u4e8b\u4f8b flannel\u304c\u53c2\u7167\u3059\u308bkubeconfig\u304c\u6b63\u3057\u304f\u306a\u3044 flannel-io/flannel Documentation/kube-flannel.yml \u305d\u306e\u307e\u307e\u3060\u3068\u767a\u751f\u3057\u305f Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. error creating inClusterConfig, falling back to default config: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory Failed to create SubnetManager: fail to create kubernetes config: invalid configuration: no configuration has been provided, try setting KUBERNETES_MASTER environment variable Failed to create SubnetManager: fail to create kubernetes config: stat \"/var/lib/kubernetes/admin.kubeconfig\": no such file or directory --kubeconfig-file \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u305fkubeconfig path\u304c\u6b63\u3057\u304f\u306a\u3044 \u79c1\u306e\u30b1\u30fc\u30b9\u3067\u306f --kubeconfig-file=\"/var/lib/kubernetes/admin.kubeconfig\" \u3068\u3057\u3066\u3044\u308b\u3068\u30c0\u30d6\u30eb\u30af\u30a9\u30fc\u30c8(\") \u304c\u30d1\u30b9\u306b\u542b\u307e\u308c\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6c17\u4ed8\u304f\u306e\u306b\u6642\u9593\u304b\u304b\u308a\u307e\u3057\u305f(\u6b63\u3057\u304f\u306f --kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig ) Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned Node\u30ea\u30bd\u30fc\u30b9\u306e .spec.podCIDR \u304c\u767b\u9332\u3055\u308c\u3066\u3044\u306a\u3044 \u78ba\u8a8d\u65b9\u6cd5 kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u8a2d\u5b9a\u4e0d\u5099\u306e\u53ef\u80fd\u6027 \u4ee5\u4e0b\u8a2d\u5b9a\u5909\u66f4\u5f8c\u3001kubelet\u306e\u8d77\u52d5\u524d\u306b kubectl delete node <NODE> \u3092\u5b9f\u884c\u3059\u308b https://github.com/flannel-io/flannel/issues/728#issuecomment-325347810 podCIDR\u5272\u308a\u5f53\u3066\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044 --cluster-cidr=<CIDR> --allocate-node-cidrs=true https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/ --service-cluster-ip-range=<CIDR> \u3068 (flannnel\u306e) --pod-cidr=<CIDR> \u304c\u88ab\u3063\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b (kube-controller-manager \u304c\u6b63\u3057\u3044\u5834\u5408) Node\u30ea\u30bd\u30fc\u30b9\u306e .spec.podCIDR \u3092\u624b\u52d5\u8a2d\u5b9a\u3057\u3066\u56de\u907f\u3059\u308b kubectl patch node <NODE_NAME> -p '{\"spec\":{\"podCIDR\":\"<SUBNET>\"}}' Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again. kubectl delete pod kube-flannel-... \u3067 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9 flannel.1 \u304c\u958b\u653e\u3055\u308c\u306a\u3044 https://github.com/flannel-io/flannel/issues/1060 \u53e4\u3044 flannel.1 \u3092\u524a\u9664 ip a sudo ip link delete flannel.1","title":"08. bootstrapping flannel"},{"location":"setup/08_flannel/bootstrapping_flannel/#bootstrapping-flannel","text":"","title":"bootstrapping flannel"},{"location":"setup/08_flannel/bootstrapping_flannel/#_1","text":"https://github.com/flannel-io/flannel/ https://github.com/flannel-io/flannel/blob/master/Documentation/troubleshooting.md /opt/bin/flanneld \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3 https://github.com/flannel-io/flannel/blob/master/main.go#L110-L132","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/08_flannel/bootstrapping_flannel/#_2","text":"vxlan module\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b sudo apt install -y linux-modules-extra-raspi locate sudo updatedb sudo modprobe vxlan sudo lsmod | grep vxlan flannel k8s manifests\u3092\u516c\u5f0f\u304b\u3089\u53d6\u5f97\u3059\u308b master\u30d6\u30e9\u30f3\u30c1\u304b\u3089\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u304c2021/03/06 \u6642\u70b9\u3067\u306f release tag v0.13.1-rc2 \u306e\u5185\u5bb9 sudo mkdir -p /etc/kubernetes/manifests sudo curl -o /etc/kubernetes/manifests/kube-flannel.yml -sSL https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml manifests\u3092\u4fee\u6b63\u3059\u308b net-conf.json \u306e Network \u3092controller-manager\u306e --cluster-cidr \u3067\u6307\u5b9a\u3057\u305f\u5024\u306b\u5909\u66f4\u3059\u308b etcd\u306b\u7528\u3044\u305fcertificate\u3084admin\u306ekubeconfig\u3092kube-flannel\u30b3\u30f3\u30c6\u30ca\u3078bind mount\u3059\u308b /opt/bin/flanneld \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3 --kubeconfig-file --etcd-endpoints --etcd-prefix --etcd-keyfile --etcd-certfile --etcd-cafile --v sudo vim /etc/kubernetes/manifests/kube-flannel.yml \u4fee\u6b63\u5f8c\u306e /etc/kubernetes/manifests/kube-flannel.yml --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \"/etc/cni/net.d\" - pathPrefix: \"/etc/kube-flannel\" - pathPrefix: \"/run/flannel\" readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN', 'NET_RAW'] defaultAddCapabilities: [] required seLinux: # SELinux is unused in CaaSP rule: 'RunAsAny' --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel rules: - apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged'] - apiGroups: - \"\" resources: - pods verbs: - get - apiGroups: - \"\" resources: - nodes verbs: - list - watch - apiGroups: - \"\" resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } net-conf.json: | { \"Network\": \"10.200.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni-plugin #image: flannelcni/flannel-cni-plugin:v1.0.1 for ppc64le and mips64le (dockerhub limitations may apply) image: rancher/mirrored-flannelcni-flannel-cni-plugin:v1.0.1 command: - cp args: - -f - /flannel - /opt/cni/bin/flannel volumeMounts: - name: cni-plugin mountPath: /opt/cni/bin - name: install-cni #image: flannelcni/flannel:v0.16.3 for ppc64le and mips64le (dockerhub limitations may apply) image: rancher/mirrored-flannelcni-flannel:v0.16.3 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel #image: flannelcni/flannel:v0.16.3 for ppc64le and mips64le (dockerhub limitations may apply) image: rancher/mirrored-flannelcni-flannel:v0.16.3 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig - --etcd-endpoints=https://k8s-master:4001 - --etcd-prefix=/coreos.com/network - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem - --etcd-cafile=/var/lib/kubernetes/ca.pem - --v=10 resources: requests: cpu: \"100m\" memory: \"50Mi\" limits: cpu: \"100m\" memory: \"50Mi\" securityContext: privileged: false capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ - name: xtables-lock mountPath: /run/xtables.lock - name: var-lib-kubernetes-dir mountPath: /var/lib/kubernetes/ volumes: - name: run hostPath: path: /run/flannel - name: cni-plugin hostPath: path: /opt/cni/bin - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg - name: xtables-lock hostPath: path: /run/xtables.lock type: FileOrCreate - name: var-lib-kubernetes-dir hostPath: path: /var/lib/kubernetes flannel Pod\u3092\u30c7\u30d7\u30ed\u30a4 kubectl apply -f /etc/kubernetes/manifests/kube-flannel.yml","title":"\u624b\u9806"},{"location":"setup/08_flannel/bootstrapping_flannel/#_3","text":"flannel\u304c\u53c2\u7167\u3059\u308bkubeconfig\u304c\u6b63\u3057\u304f\u306a\u3044 flannel-io/flannel Documentation/kube-flannel.yml \u305d\u306e\u307e\u307e\u3060\u3068\u767a\u751f\u3057\u305f Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. error creating inClusterConfig, falling back to default config: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory Failed to create SubnetManager: fail to create kubernetes config: invalid configuration: no configuration has been provided, try setting KUBERNETES_MASTER environment variable Failed to create SubnetManager: fail to create kubernetes config: stat \"/var/lib/kubernetes/admin.kubeconfig\": no such file or directory --kubeconfig-file \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u305fkubeconfig path\u304c\u6b63\u3057\u304f\u306a\u3044 \u79c1\u306e\u30b1\u30fc\u30b9\u3067\u306f --kubeconfig-file=\"/var/lib/kubernetes/admin.kubeconfig\" \u3068\u3057\u3066\u3044\u308b\u3068\u30c0\u30d6\u30eb\u30af\u30a9\u30fc\u30c8(\") \u304c\u30d1\u30b9\u306b\u542b\u307e\u308c\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6c17\u4ed8\u304f\u306e\u306b\u6642\u9593\u304b\u304b\u308a\u307e\u3057\u305f(\u6b63\u3057\u304f\u306f --kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig ) Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned Node\u30ea\u30bd\u30fc\u30b9\u306e .spec.podCIDR \u304c\u767b\u9332\u3055\u308c\u3066\u3044\u306a\u3044 \u78ba\u8a8d\u65b9\u6cd5 kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u8a2d\u5b9a\u4e0d\u5099\u306e\u53ef\u80fd\u6027 \u4ee5\u4e0b\u8a2d\u5b9a\u5909\u66f4\u5f8c\u3001kubelet\u306e\u8d77\u52d5\u524d\u306b kubectl delete node <NODE> \u3092\u5b9f\u884c\u3059\u308b https://github.com/flannel-io/flannel/issues/728#issuecomment-325347810 podCIDR\u5272\u308a\u5f53\u3066\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044 --cluster-cidr=<CIDR> --allocate-node-cidrs=true https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/ --service-cluster-ip-range=<CIDR> \u3068 (flannnel\u306e) --pod-cidr=<CIDR> \u304c\u88ab\u3063\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b (kube-controller-manager \u304c\u6b63\u3057\u3044\u5834\u5408) Node\u30ea\u30bd\u30fc\u30b9\u306e .spec.podCIDR \u3092\u624b\u52d5\u8a2d\u5b9a\u3057\u3066\u56de\u907f\u3059\u308b kubectl patch node <NODE_NAME> -p '{\"spec\":{\"podCIDR\":\"<SUBNET>\"}}' Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again. kubectl delete pod kube-flannel-... \u3067 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9 flannel.1 \u304c\u958b\u653e\u3055\u308c\u306a\u3044 https://github.com/flannel-io/flannel/issues/1060 \u53e4\u3044 flannel.1 \u3092\u524a\u9664 ip a sudo ip link delete flannel.1","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/09_coredns/bootstrapping_coredns/","text":"bootstrapping CoreDNS CoreDNS \u3068\u306f CoreDNS \u306f CNCF\u306egraduated project \u3068\u3057\u3066\u30db\u30b9\u30c8\u3055\u308c\u3066\u3044\u308bDNS\u30b5\u30fc\u30d0\u3067\u3001 Kubernetes 1.13 \u4ee5\u964d\u306b\u3066\u30c7\u30d5\u30a9\u30eb\u30c8DNS\u30b5\u30fc\u30d0\u3068\u3057\u3066\u63a1\u7528 \u3055\u308c\u3066\u304a\u308a\u3001Cluster\u5185\u3067\u306eService\u30ea\u30bd\u30fc\u30b9\u306e\u540d\u524d\u89e3\u6c7a\u306b\u5229\u7528\u3057\u3066\u3044\u307e\u3059 ( Kubernetes DNS-Based Service Discovery )\u3002 AWS Load Balancer Controller \u306a\u3069Cluster\u5916\u3078\u30a8\u30f3\u30c9\u30dd\u30a4\u30f3\u30c8\u3092\u516c\u958b\u3059\u308b\u5834\u5408\u306f\u5225\u9014\u5916\u90e8DNS\u30b5\u30fc\u30d0(Route53\u306a\u3069)\u3092\u5229\u7528\u3057\u307e\u3059(CoreDNS\u3092Cluster\u5916\u90e8\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd)\u3002 CoreDNS\u306f\u3042\u3089\u3086\u308b\u51e6\u7406\u3092Plugin\u3068\u3057\u3066\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002CoreDNS\u5358\u4f53\u306fDNS\u30af\u30a8\u30ea\u30fc\u3092\u89e3\u91c8\u3057\u3066\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb( ./Corefile )\u306b\u8a18\u8ff0\u3055\u308c\u305fPlugin\u306b\u51e6\u7406\u3092\u53d7\u3051\u6e21\u3057\u307e\u3059\u3002( \u53c2\u8003 ) kubernetes plugin \u306f Kubernetes DNS-Based Service Discovery Specification. \u306e\u5b9f\u88c5\u3067\u3059\u3002 Corefile \u3067kubernetes plugin\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u5229\u7528\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u8a2d\u5b9a\u306f kubernetes/coredns.yaml.sed \u306e\u5185\u5bb9\u3067\u3059\u3002 fallthrough \u3068\u306fNXDOMAIN(\u4e0d\u5728\u5fdc\u7b54)\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408\u306b\u51e6\u7406\u3092\u4e0b\u6d41\u306ePlugin\u306b\u6e21\u3057\u3066\u304f\u308c\u307e\u3059(\u3053\u306e\u8a2d\u5b9a\u3067\u306f\u9006\u5f15\u304d\u3067NXDOMAIN\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408)\u3002 .:53 { kubernetes cluster.local in-addr.arpa ip6.arpa { fallthrough in-addr.arpa ip6.arpa } } \u53c2\u8003\u6587\u732e https://coredns.io/ https://github.com/coredns/coredns https://github.com/coredns/deployment https://github.com/coredns/helm https://engineer.retty.me/entry/2020/12/15/161544 https://www.netone.co.jp/knowledge-center/netone-blog/20191226-1/ https://www.scsk.jp/sp/sysdig/blog/container_monitoring/coredns.html \u624b\u9806 coredns k8s manifests\u3092 \u516c\u5f0f \u304b\u3089\u53d6\u5f97\u3059\u308b git clone https://github.com/coredns/deployment.git coredns_deployment cd coredns_deployment/kubernetes/ bash deploy.sh -i 10.32.0.10 -s -t coredns.yaml.sed | kubectl apply -f - CoreDNS\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u53d6\u5f97\u306b\u3064\u3044\u3066 Corefile \u3067\u4ee5\u4e0b\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u304a\u304f\u3053\u3068\u3067Prometheus\u7528\u306b 9153/TCP \u3067\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u516c\u958b\u3067\u304d\u307e\u3059 .:53 { prometheus :9153 } \u30a8\u30e9\u30fc\u4e8b\u4f8b kubectl get pods -n kube-system \u3067\u3044\u3064\u307e\u3067\u7d4c\u3063\u3066\u3082 ContainerCreating \u306e\u307e\u307e kubectl describe pod -n kube-system <POD_ID> Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"<CONTAINER_ID>\" network for pod \"<POD_ID>\": networkPlugin cni failed to set up pod \"<<POD_NAME>\" network: failed to Statfs \"/proc/15875/ns/net\": no such file or directory, failed to clean up sandbox container \"<CONTAINER_ID>\" network for pod \"<POD_ID>\": networkPlugin cni failed to teardown pod \"<POD_NAME>\" network: neither iptables nor ip6tables usable] controller-manager https://github.com/kubernetes/kubernetes/blob/v1.20.2/pkg/controller/endpointslice/utils.go#L407-L415 couldn't find ipfamilies for headless service: kube-system/kube-dns. This could happen if controller manager is connected to an old apiserver that does not support ip families yet. EndpointSlices for this Service will use IPv4 as the IP Family based on familyOf(ClusterIP:10.32.0.10). $ kubectl get service -n kube-system -o jsonpath='{.items[*].spec.clusterIP}' 10.32.0.10 cni0(flannel)\u306e\u8d77\u52d5\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b failed to set bridge addr: \"cni0\" already has an IP address different from 10.200.1.1/24 \u540d\u524d\u89e3\u6c7a\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b https://coredns.io/plugins/loop/#troubleshooting [FATAL] plugin/loop: Loop (127.0.0.1:36286 -> :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 1048258276942848743.906062863108256161.\" i/o timeout [ERROR] plugin/errors: 2 1233258971421873826.4416823678189275919. HINFO: read udp 10.200.0.5:35249->8.8.4.4:53: i/o timeout Pod\u306e\u30b3\u30f3\u30c6\u30ca\u304b\u3089\u540d\u524d\u89e3\u6c7a\u304c\u3067\u304d\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b /etc/resolv.conf \u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u3044\u304b\u78ba\u8a8d\u3059\u308b kubectl run nginx --image=nginx POD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\"{.items[0].metadata.name}\") kubectl exec -it $POD_NAME -- bash cat /etc/resolv.conf apt-get update # \u5916\u306b\u3059\u3089\u51fa\u308c\u306a\u3044\u5834\u5408\u306f\u5931\u6557\u3059\u308b apt-get install dnsutils nslookup kubernetes kube-apiserver \u3078\u306e\u758e\u901a\u304c\u3067\u304d\u3066\u3044\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b kube-proxy -> flannel \u306e\u9806\u3067pod\u306e\u518d\u8d77\u52d5\u3092\u884c\u3063\u3066\u307f\u308b","title":"09. bootstrapping coredns"},{"location":"setup/09_coredns/bootstrapping_coredns/#bootstrapping-coredns","text":"","title":"bootstrapping CoreDNS"},{"location":"setup/09_coredns/bootstrapping_coredns/#coredns","text":"CoreDNS \u306f CNCF\u306egraduated project \u3068\u3057\u3066\u30db\u30b9\u30c8\u3055\u308c\u3066\u3044\u308bDNS\u30b5\u30fc\u30d0\u3067\u3001 Kubernetes 1.13 \u4ee5\u964d\u306b\u3066\u30c7\u30d5\u30a9\u30eb\u30c8DNS\u30b5\u30fc\u30d0\u3068\u3057\u3066\u63a1\u7528 \u3055\u308c\u3066\u304a\u308a\u3001Cluster\u5185\u3067\u306eService\u30ea\u30bd\u30fc\u30b9\u306e\u540d\u524d\u89e3\u6c7a\u306b\u5229\u7528\u3057\u3066\u3044\u307e\u3059 ( Kubernetes DNS-Based Service Discovery )\u3002 AWS Load Balancer Controller \u306a\u3069Cluster\u5916\u3078\u30a8\u30f3\u30c9\u30dd\u30a4\u30f3\u30c8\u3092\u516c\u958b\u3059\u308b\u5834\u5408\u306f\u5225\u9014\u5916\u90e8DNS\u30b5\u30fc\u30d0(Route53\u306a\u3069)\u3092\u5229\u7528\u3057\u307e\u3059(CoreDNS\u3092Cluster\u5916\u90e8\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd)\u3002 CoreDNS\u306f\u3042\u3089\u3086\u308b\u51e6\u7406\u3092Plugin\u3068\u3057\u3066\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002CoreDNS\u5358\u4f53\u306fDNS\u30af\u30a8\u30ea\u30fc\u3092\u89e3\u91c8\u3057\u3066\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb( ./Corefile )\u306b\u8a18\u8ff0\u3055\u308c\u305fPlugin\u306b\u51e6\u7406\u3092\u53d7\u3051\u6e21\u3057\u307e\u3059\u3002( \u53c2\u8003 ) kubernetes plugin \u306f Kubernetes DNS-Based Service Discovery Specification. \u306e\u5b9f\u88c5\u3067\u3059\u3002 Corefile \u3067kubernetes plugin\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u5229\u7528\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u8a2d\u5b9a\u306f kubernetes/coredns.yaml.sed \u306e\u5185\u5bb9\u3067\u3059\u3002 fallthrough \u3068\u306fNXDOMAIN(\u4e0d\u5728\u5fdc\u7b54)\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408\u306b\u51e6\u7406\u3092\u4e0b\u6d41\u306ePlugin\u306b\u6e21\u3057\u3066\u304f\u308c\u307e\u3059(\u3053\u306e\u8a2d\u5b9a\u3067\u306f\u9006\u5f15\u304d\u3067NXDOMAIN\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408)\u3002 .:53 { kubernetes cluster.local in-addr.arpa ip6.arpa { fallthrough in-addr.arpa ip6.arpa } }","title":"CoreDNS \u3068\u306f"},{"location":"setup/09_coredns/bootstrapping_coredns/#_1","text":"https://coredns.io/ https://github.com/coredns/coredns https://github.com/coredns/deployment https://github.com/coredns/helm https://engineer.retty.me/entry/2020/12/15/161544 https://www.netone.co.jp/knowledge-center/netone-blog/20191226-1/ https://www.scsk.jp/sp/sysdig/blog/container_monitoring/coredns.html","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/09_coredns/bootstrapping_coredns/#_2","text":"coredns k8s manifests\u3092 \u516c\u5f0f \u304b\u3089\u53d6\u5f97\u3059\u308b git clone https://github.com/coredns/deployment.git coredns_deployment cd coredns_deployment/kubernetes/ bash deploy.sh -i 10.32.0.10 -s -t coredns.yaml.sed | kubectl apply -f -","title":"\u624b\u9806"},{"location":"setup/09_coredns/bootstrapping_coredns/#coredns_1","text":"Corefile \u3067\u4ee5\u4e0b\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u304a\u304f\u3053\u3068\u3067Prometheus\u7528\u306b 9153/TCP \u3067\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u516c\u958b\u3067\u304d\u307e\u3059 .:53 { prometheus :9153 }","title":"CoreDNS\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u53d6\u5f97\u306b\u3064\u3044\u3066"},{"location":"setup/09_coredns/bootstrapping_coredns/#_3","text":"kubectl get pods -n kube-system \u3067\u3044\u3064\u307e\u3067\u7d4c\u3063\u3066\u3082 ContainerCreating \u306e\u307e\u307e kubectl describe pod -n kube-system <POD_ID> Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"<CONTAINER_ID>\" network for pod \"<POD_ID>\": networkPlugin cni failed to set up pod \"<<POD_NAME>\" network: failed to Statfs \"/proc/15875/ns/net\": no such file or directory, failed to clean up sandbox container \"<CONTAINER_ID>\" network for pod \"<POD_ID>\": networkPlugin cni failed to teardown pod \"<POD_NAME>\" network: neither iptables nor ip6tables usable] controller-manager https://github.com/kubernetes/kubernetes/blob/v1.20.2/pkg/controller/endpointslice/utils.go#L407-L415 couldn't find ipfamilies for headless service: kube-system/kube-dns. This could happen if controller manager is connected to an old apiserver that does not support ip families yet. EndpointSlices for this Service will use IPv4 as the IP Family based on familyOf(ClusterIP:10.32.0.10). $ kubectl get service -n kube-system -o jsonpath='{.items[*].spec.clusterIP}' 10.32.0.10 cni0(flannel)\u306e\u8d77\u52d5\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b failed to set bridge addr: \"cni0\" already has an IP address different from 10.200.1.1/24 \u540d\u524d\u89e3\u6c7a\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b https://coredns.io/plugins/loop/#troubleshooting [FATAL] plugin/loop: Loop (127.0.0.1:36286 -> :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 1048258276942848743.906062863108256161.\" i/o timeout [ERROR] plugin/errors: 2 1233258971421873826.4416823678189275919. HINFO: read udp 10.200.0.5:35249->8.8.4.4:53: i/o timeout Pod\u306e\u30b3\u30f3\u30c6\u30ca\u304b\u3089\u540d\u524d\u89e3\u6c7a\u304c\u3067\u304d\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b /etc/resolv.conf \u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u3044\u304b\u78ba\u8a8d\u3059\u308b kubectl run nginx --image=nginx POD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\"{.items[0].metadata.name}\") kubectl exec -it $POD_NAME -- bash cat /etc/resolv.conf apt-get update # \u5916\u306b\u3059\u3089\u51fa\u308c\u306a\u3044\u5834\u5408\u306f\u5931\u6557\u3059\u308b apt-get install dnsutils nslookup kubernetes kube-apiserver \u3078\u306e\u758e\u901a\u304c\u3067\u304d\u3066\u3044\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b kube-proxy -> flannel \u306e\u9806\u3067pod\u306e\u518d\u8d77\u52d5\u3092\u884c\u3063\u3066\u307f\u308b","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/","text":"bootstrapping nginx ingress controller \u53c2\u8003 https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/ https://kubernetes.github.io/ingress-nginx/ https://github.com/nginxinc/kubernetes-ingress \u624b\u9806 \u69cb\u7bc9 https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters \u52d5\u4f5c\u78ba\u8a8d nginx IngressClass \u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl describe ingressclasses nginx Name: nginx Labels: app.kubernetes.io/component=controller app.kubernetes.io/instance=ingress-nginx app.kubernetes.io/name=ingress-nginx app.kubernetes.io/part-of=ingress-nginx app.kubernetes.io/version=1.3.0 Annotations: <none> Controller: k8s.io/ingress-nginx Events: <none> Ingress\u3092\u4f5c\u6210\u3057\u3066worker node\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml sudo tee /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml << EOF > /dev/null --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test-deployment spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort ports: - name: node-port protocol: TCP port: 8080 targetPort: 80 nodePort: 30011 selector: app: nginx --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-test-ingress annotations: external-dns.alpha.kubernetes.io/hostname: <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> spec: ingressClassName: nginx defaultBackend: service: name: nginx-service port: number: 8080 rules: - http: paths: - path: / pathType: Prefix backend: service: name: nginx-service port: number: 8080 # external-dns.alpha.kubernetes.io/hostname annotations\u3092\u6307\u5b9a\u305b\u305arule\u6bce\u306b\u8a2d\u5b9a\u3059\u308b\u4f8b # - host: <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> # http: # paths: # - path: / # pathType: Prefix # backend: # service: # name: nginx-service # port: # number: 8080 EOF <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> \u306e\u7b87\u6240\u3092\u4fee\u6b63\u3059\u308b sudo vim /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml \u30ea\u30bd\u30fc\u30b9\u4f5c\u6210 kubectl apply -f /etc.kubernetes/manifests/04_nginx_ingress_controller.yaml service\u3067node port(30011/TCP) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl describe services nginx-service Name: nginx-service Namespace: default Labels: <none> Annotations: <none> Selector: app=nginx Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.32.0.145 IPs: 10.32.0.145 Port: node-port 8080/TCP TargetPort: 80/TCP NodePort: node-port 30011/TCP Endpoints: 10.200.1.184:80 Session Affinity: None External Traffic Policy: Cluster Events: <none> ingress\u3067worker node\u306eIP\u30a2\u30c9\u30ec\u30b9(wlan0: 192.168.10.51 ) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl describe ingresses nginx-test-ingress Name: nginx-test-ingress Namespace: default Address: 192.168.10.51 Default backend: nginx-service:8080 (10.200.1.184:80) Rules: Host Path Backends ---- ---- -------- * / nginx-service:8080 (10.200.1.184:80) Annotations: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 5m39s (x7 over 25h) nginx-ingress-controller Scheduled for sync MacBookPro\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u3067curl\u306b\u3066\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d $ curl --include http://192.168.10.51:30011/ HTTP/1.1 200 OK Date: Tue, 21 Sep 2021 16:35:07 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Tue, 04 Dec 2018 14:44:49 GMT ETag: \"5c0692e1-264\" Accept-Ranges: bytes <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> \u30a8\u30e9\u30fc\u4e8b\u4f8b Error when getting IngressClass nginx: the server could not find the requested resource https://github.com/nginxinc/kubernetes-ingress/issues/1906 https://qiita.com/smallpalace/items/7a6844651d1d7b43b411 https://github.com/kubernetes/ingress-nginx/issues/7448 https://github.com/kubernetes/ingress-nginx/blob/3c0bfc1ca3eb48246b12e77d40bde1162633efae/deploy/static/provider/baremetal/deploy.yaml error validating data: ValidationError --validate=false \u3092\u4ed8\u52a0\u3059\u308b ValidatingWebhookConfiguration \u3092\u524a\u9664\u3059\u308b ( issue comment )","title":"10. bootstrapping nginx ingress controller"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#bootstrapping-nginx-ingress-controller","text":"","title":"bootstrapping nginx ingress controller"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_1","text":"https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/ https://kubernetes.github.io/ingress-nginx/ https://github.com/nginxinc/kubernetes-ingress","title":"\u53c2\u8003"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_2","text":"","title":"\u624b\u9806"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_3","text":"https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters","title":"\u69cb\u7bc9"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_4","text":"","title":"\u52d5\u4f5c\u78ba\u8a8d"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#nginx-ingressclass","text":"$ kubectl describe ingressclasses nginx Name: nginx Labels: app.kubernetes.io/component=controller app.kubernetes.io/instance=ingress-nginx app.kubernetes.io/name=ingress-nginx app.kubernetes.io/part-of=ingress-nginx app.kubernetes.io/version=1.3.0 Annotations: <none> Controller: k8s.io/ingress-nginx Events: <none>","title":"nginx IngressClass \u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#ingressworker-nodeip","text":"manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml sudo tee /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml << EOF > /dev/null --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test-deployment spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort ports: - name: node-port protocol: TCP port: 8080 targetPort: 80 nodePort: 30011 selector: app: nginx --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-test-ingress annotations: external-dns.alpha.kubernetes.io/hostname: <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> spec: ingressClassName: nginx defaultBackend: service: name: nginx-service port: number: 8080 rules: - http: paths: - path: / pathType: Prefix backend: service: name: nginx-service port: number: 8080 # external-dns.alpha.kubernetes.io/hostname annotations\u3092\u6307\u5b9a\u305b\u305arule\u6bce\u306b\u8a2d\u5b9a\u3059\u308b\u4f8b # - host: <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> # http: # paths: # - path: / # pathType: Prefix # backend: # service: # name: nginx-service # port: # number: 8080 EOF <Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN> \u306e\u7b87\u6240\u3092\u4fee\u6b63\u3059\u308b sudo vim /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml \u30ea\u30bd\u30fc\u30b9\u4f5c\u6210 kubectl apply -f /etc.kubernetes/manifests/04_nginx_ingress_controller.yaml service\u3067node port(30011/TCP) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl describe services nginx-service Name: nginx-service Namespace: default Labels: <none> Annotations: <none> Selector: app=nginx Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.32.0.145 IPs: 10.32.0.145 Port: node-port 8080/TCP TargetPort: 80/TCP NodePort: node-port 30011/TCP Endpoints: 10.200.1.184:80 Session Affinity: None External Traffic Policy: Cluster Events: <none> ingress\u3067worker node\u306eIP\u30a2\u30c9\u30ec\u30b9(wlan0: 192.168.10.51 ) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ kubectl describe ingresses nginx-test-ingress Name: nginx-test-ingress Namespace: default Address: 192.168.10.51 Default backend: nginx-service:8080 (10.200.1.184:80) Rules: Host Path Backends ---- ---- -------- * / nginx-service:8080 (10.200.1.184:80) Annotations: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 5m39s (x7 over 25h) nginx-ingress-controller Scheduled for sync MacBookPro\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u3067curl\u306b\u3066\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d $ curl --include http://192.168.10.51:30011/ HTTP/1.1 200 OK Date: Tue, 21 Sep 2021 16:35:07 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Tue, 04 Dec 2018 14:44:49 GMT ETag: \"5c0692e1-264\" Accept-Ranges: bytes <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>","title":"Ingress\u3092\u4f5c\u6210\u3057\u3066worker node\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_5","text":"Error when getting IngressClass nginx: the server could not find the requested resource https://github.com/nginxinc/kubernetes-ingress/issues/1906 https://qiita.com/smallpalace/items/7a6844651d1d7b43b411 https://github.com/kubernetes/ingress-nginx/issues/7448 https://github.com/kubernetes/ingress-nginx/blob/3c0bfc1ca3eb48246b12e77d40bde1162633efae/deploy/static/provider/baremetal/deploy.yaml error validating data: ValidationError --validate=false \u3092\u4ed8\u52a0\u3059\u308b ValidatingWebhookConfiguration \u3092\u524a\u9664\u3059\u308b ( issue comment )","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/11_external_dns/bootstrapping_external_dns/","text":"bootstrapping external-dns \u53c2\u8003\u60c5\u5831 https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/integrations/external_dns/ kubernetes 1.22 \u5bfe\u5fdc\u72b6\u6cc1\u306b\u3064\u3044\u3066 v0.10.0 \u3067\u5bfe\u5fdc\u6e08\u307f \u30aa\u30f3\u30d7\u30ec\u3067AWS Route53\u5411\u3051external-dns controller\u306e\u8d77\u52d5\u65b9\u6cd5\u306b\u3064\u3044\u3066 https://stackoverflow.com/questions/60267737/is-it-possible-to-use-aws-route-53-as-a-dns-provider-for-a-bare-metal-k8s-cluste https://github.com/kubernetes-sigs/external-dns/issues/539 \u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306eWAN IP\u30a2\u30c9\u30ec\u30b9\u3092DNS Provider\u306b\u901a\u77e5\u3057\u305f\u3044 https://github.com/kubernetes-sigs/external-dns/issues/1394 \u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306b\u304a\u3051\u308bPublic IP\u3092\u6255\u3044\u51fa\u3057\u3064\u3064route53\u3078\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3059\u308b k8s cluster\u5185\u90e8\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u306f\u306a\u304f\u3001worker node\u306einterface(wlan0)\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u308bIP\u30a2\u30c9\u30ec\u30b9(\u4fbf\u5b9c\u7684\u306bpublic ip\u3068\u4eee\u5b9a)\u3068\u540c\u3058\u30ec\u30f3\u30b8\u3067IP\u30a2\u30c9\u30ec\u30b9\u3092\u6255\u3044\u51fa\u3059\u3088\u3046\u306a\u69cb\u6210 \u3053\u308c\u306f\u5c06\u6765\u7528 metallb \u3067k8s cluster\u306e\u5916\u5074\u306bLoadBalancer\u3092\u4f5c\u6210\u3057Public IP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b https://blog.web-apps.tech/type-loadbalancer_by_metallb/ \u69cb\u7bc9\u624b\u9806 1. AWS IAM Policy\u4f5c\u6210 external-dns-controller-policy-document.json \u3092\u4f5c\u6210 external-dns-controller-policy-document.json ``` sudo tee external-dns-controller-policy-document.json << EOF > /dev/null { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\" ], \"Resource\": [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ListHostedZones\", \"route53:ListResourceRecordSets\" ], \"Resource\": [ \"*\" ] } ] } EOF ``` policy\u3092\u4f5c\u6210 aws iam create-policy --policy-name k8s-external-dns-policy --policy-document file://external-dns-controller-policy-document.json 2. AWS IAM User\u4f5c\u6210 user\u3092\u4f5c\u6210 aws iam create-user --user-name k8s-external-dns \u4f5c\u6210\u3057\u305fIAM Policy\u3092\u30a2\u30bf\u30c3\u30c1\u3059\u308b aws iam attach-user-policy --user-name k8s-external-dns --policy-arn arn:aws:iam::<AWS_ACCOUNT_ID>:policy/k8s-external-dns-policy \u4f5c\u6210\u3057\u305fIAM User\u306ecredential\u3092\u78ba\u8a8d\u3059\u308b(Deployments\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u74b0\u5883\u5909\u6570\u3068\u3057\u3066\u30bb\u30c3\u30c8\u3059\u308b) AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY 3. external-dns controller\u3092\u30c7\u30d7\u30ed\u30a4 point https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md \u3092\u30d9\u30fc\u30b9\u306bbare-metal\u5411\u3051\u306b\u4fee\u6b63 namespace\u306f kube-system aws credential\u306f k8s-external-dns iam user\u306e\u3082\u306e hosted_zone_id \u306fexternal-dns\u3067\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3055\u305b\u305f\u3044Route53 zone manifests\u306b\u4ee3\u5165\u3059\u308b\u5909\u6570\u3092\u5b9a\u7fa9 variable name description DOMAIN external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306e\u30c9\u30e1\u30a4\u30f3 HOSTED_ZONE_ID external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306eHosted Zone ID AWS_ACCESS_KEY_ID external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 AWS_SECRET_ACCESS_KEY external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 AWS_DEFAULT_REGION external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 DOMAIN=\"XXXXXXX.com\" HOSTED_ZONE_ID=\"XXXXXXX\" AWS_ACCESS_KEY_ID=\"XXXXXXX\" AWS_SECRET_ACCESS_KEY=\"XXXXXXX\" AWS_DEFAULT_REGION=\"XXXXXXX\" manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 /etc/kubernetes/manifests/external-dns.yaml ``` AWS_DEFAULT_REGION=\"ap-north-east-1\" sudo tee /etc/kubernetes/manifests/external-dns.yaml << EOF > /dev/null --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: external-dns rules: - apiGroups: [\"\"] resources: [\"services\",\"endpoints\",\"pods\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"extensions\",\"networking.k8s.io\"] resources: [\"ingresses\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"list\",\"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: external-dns-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: kube-system spec: strategy: type: Recreate selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: k8s.gcr.io/external-dns/external-dns:v0.10.0 env: - name: AWS_ACCESS_KEY_ID value: <k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_ACCESS_KEY_ID> - name: AWS_SECRET_ACCESS_KEY value: <k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_SECRET_ACCESS_KEY> - name: AWS_DEFAULT_REGION value: \"${AWS_DEFAULT_REGION}\" args: - --source=service - --source=ingress - --domain-filter=${DOMAIN} # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=<HOSTED_ZONE_ID> - --txt-prefix=prefix_ - --log-level=debug securityContext: fsGroup: 65534 # For ExternalDNS to be able to read Kubernetes and AWS token files EOF ``` \u30c7\u30d7\u30ed\u30a4 kubectl apply -f /etc/kubernetes/manifests/external-dns.yaml \u52d5\u4f5c\u78ba\u8a8d external-dns \u30b3\u30f3\u30c6\u30ca\u30ed\u30b0 \u30c7\u30d7\u30ed\u30a4\u6e08\u307fingress\u306ehostname\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3092\u78ba\u8a8d time=\"2021-09-26T04:59:24Z\" level=info msg=\"Instantiating new Kubernetes client\" time=\"2021-09-26T04:59:24Z\" level=debug msg=\"apiServerURL: \" time=\"2021-09-26T04:59:24Z\" level=debug msg=\"kubeConfig: \" time=\"2021-09-26T04:59:24Z\" level=info msg=\"Using inCluster-config based on serviceaccount-token\" time=\"2021-09-26T04:59:24Z\" level=info msg=\"Created Kubernetes client https://10.32.0.1:443\" time=\"2021-09-26T04:59:30Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/kube-dns\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/metrics-server\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/kubernetes\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/nginx-service\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller-admission\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN A 192.168.10.51 []]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com A [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com TXT [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:32Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>] were successfully updated\" route53 record set \u5bfe\u8c61\u306ehosted zone\u306bingress\u306ehost\u3067\u6307\u5b9a\u3057\u305fhostname\u3067\u30ec\u30b3\u30fc\u30c9\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ aws route53 list-resource-record-sets --output json --hosted-zone-id <HOSTED_ZONE_ID> | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\"))' [ { \"Name\": \"dev1.example.com.\", \"Type\": \"A\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"192.168.10.51\" } ] }, { \"Name\": \"dev1.example.com.\", \"Type\": \"TXT\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"\\\"heritage=external-dns,external-dns/owner=<HOSTED_ZONE_ID>,external-dns/resource=ingress/default/nginx-test-ingress\\\"\" } ] } ] \u767b\u9332\u3055\u308c\u305fA\u30ec\u30b3\u30fc\u30c9\u306ehostname\u304c\u540d\u524d\u89e3\u6c7a\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d k8s service\u30ea\u30bd\u30fc\u30b9\u304b\u3089\u898b\u308b\u3068node port\u306b\u5bfe\u3059\u308bnode address\u306f\u81ea\u5b85\u74b0\u5883\u306eWifi\u30eb\u30fc\u30bf\u3067\u6255\u3044\u51fa\u3059\u30ec\u30f3\u30b8(192.168.10.0/24)\u306a\u306e\u3067\u60f3\u5b9a\u901a\u308a $ dig +noall +answer dev1.example.com dev1.example.com. 283 IN A 192.168.10.51 Appendix Ingress\u30ea\u30bd\u30fc\u30b9\u3067\u4f7f\u7528\u53ef\u80fd\u306aannotations https://github.com/kubernetes-sigs/external-dns/blob/v0.10.0/source/source.go#L40-L68 annotations describe external-dns.alpha.kubernetes.io/controller \u8907\u6570\u306eDNS Controller\u304c\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u3069\u306eDNS Controller\u304c\u8cac\u4efb\u3092\u8ca0\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u628a\u63e1\u3059\u308b\u305f\u3081\u306b\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/hostname \u4f7f\u7528\u3059\u308b\u30db\u30b9\u30c8\u540d\u3092\u6307\u5b9a\u3059\u308b Service\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u3067\u6307\u5b9a\u3059\u308b Ingress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u304brule\u306ehost attr\u3067\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/access \u30d1\u30d6\u30ea\u30c3\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u3068\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u306e\u3069\u3061\u3089\u3092\u4f7f\u7528\u3059\u308b\u304b\u3092\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/target CNAME record\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\u306bCNAME record\u306evalue\u3068\u306a\u308b\u5024\u3092\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/ttl DNS record\u306eTTL\u3092\u6307\u5b9a\u3059\u308b(default: 300) external-dns.alpha.kubernetes.io/alias true \u3067ALIAS record\u3092\u4f5c\u6210\u3059\u308b external-dns.alpha.kubernetes.io/ingress-hostname-source Ingress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306bhostname\u306e\u6307\u5b9a\u65b9\u6cd5\u3092annotations\u304brule\u306ehost attr\u304b\u3092\u9650\u5b9a\u3067\u304d\u308b external-dns.alpha.kubernetes.io/internal-hostname Target IP\u30a2\u30c9\u30ec\u30b9\u3092Cluster IP\u30a2\u30c9\u30ec\u30b9\u3068\u3059\u308b\u5834\u5408\u306b\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/set-identifier AWS Route53\u306b\u304a\u3044\u3066DNS Name\u3068Type\u304c\u540c\u3058\u5834\u5408\u306b\u91cd\u307f\u4ed8\u3051\u306a\u3069\u306b\u3088\u308b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30dd\u30ea\u30b7\u30fc\u3092\u5b9a\u7fa9\u3059\u308b\u969b\u306e\u8b58\u5225\u5b50 Record ID\u3068\u306a\u308b\u5024 external-dns\u3067A\u30ec\u30b3\u30fc\u30c9\u3067\u306f\u306a\u304fCNAME\u30ec\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3059\u308b controller\u306e\u8d77\u52d5\u5f15\u6570\u306b --txt-prefix=<prefix\u6587\u5b57\u5217> \u3092\u8ffd\u52a0 \u6307\u5b9a\u3057\u305f\u6587\u5b57\u5217\u304cTXT record\u306e\u30ec\u30b3\u30fc\u30c9\u540d(A\u30ec\u30b3\u30fc\u30c9\u306e\u5834\u5408\u306fA\u30ec\u30b3\u30fc\u30c9\u3068\u540c\u540d)\u306eprefix\u3068\u3057\u3066\u8ffd\u52a0\u3055\u308c\u307e\u3059 CNAME\u30ec\u30b3\u30fc\u30c9\u306f(TXT\u30ec\u30b3\u30fc\u30c9\u3067\u3042\u3063\u3066\u3082)\u4ed6\u306e\u30ec\u30b3\u30fc\u30c9\u3068\u5171\u5b58\u3067\u304d\u306a\u3044\u4ed5\u69d8\u3067\u3059( RFC 1034\u30bb\u30af\u30b7\u30e7\u30f33.6.2 \u3001 RFC 1912\u30bb\u30af\u30b7\u30e7\u30f32.4 https://github.com/kubernetes-sigs/external-dns/blob/master/docs/faq.md#im-using-an-elb-with-txt-registry-but-the-cname-record-clashes-with-the-txt-record-how-to-avoid-this Ingress\u30ea\u30bd\u30fc\u30b9\u306eannotations\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/hostname \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eFQDN\u3092\u6307\u5b9a external-dns.alpha.kubernetes.io/target \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eValue\u3068\u306a\u308b\u53c2\u7167\u5148\u306eFQDN\u307e\u305f\u306fIP\u30a2\u30c9\u30ec\u30b9\u306a\u3069\u6307\u5b9a external-dns.alpha.kubernetes.io/hostname: dev1.example.com external-dns.alpha.kubernetes.io/target: alias1.example.com --txt-prefix=prefix_ \u3067\u52d5\u4f5c\u78ba\u8a8d Ingress\u30ea\u30bd\u30fc\u30b9 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-test-ingress annotations: external-dns.alpha.kubernetes.io/hostname: dev1.example.com external-dns.alpha.kubernetes.io/target: alias1.example.com spec: ingressClassName: nginx defaultBackend: service: name: nginx-service port: number: 8080 rules: - host: dev1.example.com http: paths: - path: / pathType: Prefix backend: service: name: nginx-service port: number: 8080 external-dns log time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN CNAME alias1.example.com [] dev1.example.com 0 IN CNAME alias1.example.com []]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Removing duplicate endpoint dev1.example.com 0 IN CNAME alias1.example.com []\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding prefix_dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE dev1.example.com CNAME [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE prefix_dev1.example.com TXT [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:19Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>] were successfully updated\" route53 \u30ec\u30b3\u30fc\u30c9\u78ba\u8a8d $ aws route53 list-resource-record-sets --output json --hosted-zone-id <HOSTED_ZONE_ID> | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\" or .Name == \"prefix_dev1.example.com.\"))' [ { \"Name\": \"prefix_dev1.example.com.\", \"Type\": \"TXT\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"\\\"heritage=external-dns,external-dns/owner=<HOSTED_ZONE_ID>,external-dns/resource=ingress/default/nginx-test-ingress\\\"\" } ] }, { \"Name\": \"dev1.example.com.\", \"Type\": \"CNAME\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"alias1.example.com\" } ] } ] $ dig +noall +answer dev1.example.com dev1.example.com. 300 IN CNAME alias1.example.com.","title":"11. bootstrapping external-dns"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#bootstrapping-external-dns","text":"","title":"bootstrapping external-dns"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_1","text":"https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/integrations/external_dns/","title":"\u53c2\u8003\u60c5\u5831"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#kubernetes-122","text":"v0.10.0 \u3067\u5bfe\u5fdc\u6e08\u307f","title":"kubernetes 1.22 \u5bfe\u5fdc\u72b6\u6cc1\u306b\u3064\u3044\u3066"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#aws-route53external-dns-controller","text":"https://stackoverflow.com/questions/60267737/is-it-possible-to-use-aws-route-53-as-a-dns-provider-for-a-bare-metal-k8s-cluste https://github.com/kubernetes-sigs/external-dns/issues/539","title":"\u30aa\u30f3\u30d7\u30ec\u3067AWS Route53\u5411\u3051external-dns controller\u306e\u8d77\u52d5\u65b9\u6cd5\u306b\u3064\u3044\u3066"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#wan-ipdns-provider","text":"https://github.com/kubernetes-sigs/external-dns/issues/1394","title":"\u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306eWAN IP\u30a2\u30c9\u30ec\u30b9\u3092DNS Provider\u306b\u901a\u77e5\u3057\u305f\u3044"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#public-iproute53","text":"k8s cluster\u5185\u90e8\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u306f\u306a\u304f\u3001worker node\u306einterface(wlan0)\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u308bIP\u30a2\u30c9\u30ec\u30b9(\u4fbf\u5b9c\u7684\u306bpublic ip\u3068\u4eee\u5b9a)\u3068\u540c\u3058\u30ec\u30f3\u30b8\u3067IP\u30a2\u30c9\u30ec\u30b9\u3092\u6255\u3044\u51fa\u3059\u3088\u3046\u306a\u69cb\u6210 \u3053\u308c\u306f\u5c06\u6765\u7528 metallb \u3067k8s cluster\u306e\u5916\u5074\u306bLoadBalancer\u3092\u4f5c\u6210\u3057Public IP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b https://blog.web-apps.tech/type-loadbalancer_by_metallb/","title":"\u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306b\u304a\u3051\u308bPublic IP\u3092\u6255\u3044\u51fa\u3057\u3064\u3064route53\u3078\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3059\u308b"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_2","text":"","title":"\u69cb\u7bc9\u624b\u9806"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#1-aws-iam-policy","text":"external-dns-controller-policy-document.json \u3092\u4f5c\u6210 external-dns-controller-policy-document.json ``` sudo tee external-dns-controller-policy-document.json << EOF > /dev/null { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\" ], \"Resource\": [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ListHostedZones\", \"route53:ListResourceRecordSets\" ], \"Resource\": [ \"*\" ] } ] } EOF ``` policy\u3092\u4f5c\u6210 aws iam create-policy --policy-name k8s-external-dns-policy --policy-document file://external-dns-controller-policy-document.json","title":"1. AWS IAM Policy\u4f5c\u6210"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#2-aws-iam-user","text":"user\u3092\u4f5c\u6210 aws iam create-user --user-name k8s-external-dns \u4f5c\u6210\u3057\u305fIAM Policy\u3092\u30a2\u30bf\u30c3\u30c1\u3059\u308b aws iam attach-user-policy --user-name k8s-external-dns --policy-arn arn:aws:iam::<AWS_ACCOUNT_ID>:policy/k8s-external-dns-policy \u4f5c\u6210\u3057\u305fIAM User\u306ecredential\u3092\u78ba\u8a8d\u3059\u308b(Deployments\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u74b0\u5883\u5909\u6570\u3068\u3057\u3066\u30bb\u30c3\u30c8\u3059\u308b) AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY","title":"2. AWS IAM User\u4f5c\u6210"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#3-external-dns-controller","text":"point https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md \u3092\u30d9\u30fc\u30b9\u306bbare-metal\u5411\u3051\u306b\u4fee\u6b63 namespace\u306f kube-system aws credential\u306f k8s-external-dns iam user\u306e\u3082\u306e hosted_zone_id \u306fexternal-dns\u3067\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3055\u305b\u305f\u3044Route53 zone manifests\u306b\u4ee3\u5165\u3059\u308b\u5909\u6570\u3092\u5b9a\u7fa9 variable name description DOMAIN external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306e\u30c9\u30e1\u30a4\u30f3 HOSTED_ZONE_ID external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306eHosted Zone ID AWS_ACCESS_KEY_ID external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 AWS_SECRET_ACCESS_KEY external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 AWS_DEFAULT_REGION external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 DOMAIN=\"XXXXXXX.com\" HOSTED_ZONE_ID=\"XXXXXXX\" AWS_ACCESS_KEY_ID=\"XXXXXXX\" AWS_SECRET_ACCESS_KEY=\"XXXXXXX\" AWS_DEFAULT_REGION=\"XXXXXXX\" manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 /etc/kubernetes/manifests/external-dns.yaml ``` AWS_DEFAULT_REGION=\"ap-north-east-1\" sudo tee /etc/kubernetes/manifests/external-dns.yaml << EOF > /dev/null --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: external-dns rules: - apiGroups: [\"\"] resources: [\"services\",\"endpoints\",\"pods\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"extensions\",\"networking.k8s.io\"] resources: [\"ingresses\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"list\",\"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: external-dns-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: kube-system spec: strategy: type: Recreate selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: k8s.gcr.io/external-dns/external-dns:v0.10.0 env: - name: AWS_ACCESS_KEY_ID value: <k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_ACCESS_KEY_ID> - name: AWS_SECRET_ACCESS_KEY value: <k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_SECRET_ACCESS_KEY> - name: AWS_DEFAULT_REGION value: \"${AWS_DEFAULT_REGION}\" args: - --source=service - --source=ingress - --domain-filter=${DOMAIN} # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=<HOSTED_ZONE_ID> - --txt-prefix=prefix_ - --log-level=debug securityContext: fsGroup: 65534 # For ExternalDNS to be able to read Kubernetes and AWS token files EOF ``` \u30c7\u30d7\u30ed\u30a4 kubectl apply -f /etc/kubernetes/manifests/external-dns.yaml","title":"3. external-dns controller\u3092\u30c7\u30d7\u30ed\u30a4"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_3","text":"external-dns \u30b3\u30f3\u30c6\u30ca\u30ed\u30b0 \u30c7\u30d7\u30ed\u30a4\u6e08\u307fingress\u306ehostname\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3092\u78ba\u8a8d time=\"2021-09-26T04:59:24Z\" level=info msg=\"Instantiating new Kubernetes client\" time=\"2021-09-26T04:59:24Z\" level=debug msg=\"apiServerURL: \" time=\"2021-09-26T04:59:24Z\" level=debug msg=\"kubeConfig: \" time=\"2021-09-26T04:59:24Z\" level=info msg=\"Using inCluster-config based on serviceaccount-token\" time=\"2021-09-26T04:59:24Z\" level=info msg=\"Created Kubernetes client https://10.32.0.1:443\" time=\"2021-09-26T04:59:30Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/kube-dns\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/metrics-server\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/kubernetes\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/nginx-service\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller-admission\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN A 192.168.10.51 []]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com A [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com TXT [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-09-26T04:59:32Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>] were successfully updated\" route53 record set \u5bfe\u8c61\u306ehosted zone\u306bingress\u306ehost\u3067\u6307\u5b9a\u3057\u305fhostname\u3067\u30ec\u30b3\u30fc\u30c9\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ aws route53 list-resource-record-sets --output json --hosted-zone-id <HOSTED_ZONE_ID> | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\"))' [ { \"Name\": \"dev1.example.com.\", \"Type\": \"A\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"192.168.10.51\" } ] }, { \"Name\": \"dev1.example.com.\", \"Type\": \"TXT\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"\\\"heritage=external-dns,external-dns/owner=<HOSTED_ZONE_ID>,external-dns/resource=ingress/default/nginx-test-ingress\\\"\" } ] } ] \u767b\u9332\u3055\u308c\u305fA\u30ec\u30b3\u30fc\u30c9\u306ehostname\u304c\u540d\u524d\u89e3\u6c7a\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d k8s service\u30ea\u30bd\u30fc\u30b9\u304b\u3089\u898b\u308b\u3068node port\u306b\u5bfe\u3059\u308bnode address\u306f\u81ea\u5b85\u74b0\u5883\u306eWifi\u30eb\u30fc\u30bf\u3067\u6255\u3044\u51fa\u3059\u30ec\u30f3\u30b8(192.168.10.0/24)\u306a\u306e\u3067\u60f3\u5b9a\u901a\u308a $ dig +noall +answer dev1.example.com dev1.example.com. 283 IN A 192.168.10.51","title":"\u52d5\u4f5c\u78ba\u8a8d"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#appendix","text":"","title":"Appendix"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#ingressannotations","text":"https://github.com/kubernetes-sigs/external-dns/blob/v0.10.0/source/source.go#L40-L68 annotations describe external-dns.alpha.kubernetes.io/controller \u8907\u6570\u306eDNS Controller\u304c\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u3069\u306eDNS Controller\u304c\u8cac\u4efb\u3092\u8ca0\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u628a\u63e1\u3059\u308b\u305f\u3081\u306b\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/hostname \u4f7f\u7528\u3059\u308b\u30db\u30b9\u30c8\u540d\u3092\u6307\u5b9a\u3059\u308b Service\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u3067\u6307\u5b9a\u3059\u308b Ingress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u304brule\u306ehost attr\u3067\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/access \u30d1\u30d6\u30ea\u30c3\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u3068\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u306e\u3069\u3061\u3089\u3092\u4f7f\u7528\u3059\u308b\u304b\u3092\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/target CNAME record\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\u306bCNAME record\u306evalue\u3068\u306a\u308b\u5024\u3092\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/ttl DNS record\u306eTTL\u3092\u6307\u5b9a\u3059\u308b(default: 300) external-dns.alpha.kubernetes.io/alias true \u3067ALIAS record\u3092\u4f5c\u6210\u3059\u308b external-dns.alpha.kubernetes.io/ingress-hostname-source Ingress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306bhostname\u306e\u6307\u5b9a\u65b9\u6cd5\u3092annotations\u304brule\u306ehost attr\u304b\u3092\u9650\u5b9a\u3067\u304d\u308b external-dns.alpha.kubernetes.io/internal-hostname Target IP\u30a2\u30c9\u30ec\u30b9\u3092Cluster IP\u30a2\u30c9\u30ec\u30b9\u3068\u3059\u308b\u5834\u5408\u306b\u6307\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/set-identifier AWS Route53\u306b\u304a\u3044\u3066DNS Name\u3068Type\u304c\u540c\u3058\u5834\u5408\u306b\u91cd\u307f\u4ed8\u3051\u306a\u3069\u306b\u3088\u308b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30dd\u30ea\u30b7\u30fc\u3092\u5b9a\u7fa9\u3059\u308b\u969b\u306e\u8b58\u5225\u5b50 Record ID\u3068\u306a\u308b\u5024","title":"Ingress\u30ea\u30bd\u30fc\u30b9\u3067\u4f7f\u7528\u53ef\u80fd\u306aannotations"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#external-dnsacname","text":"controller\u306e\u8d77\u52d5\u5f15\u6570\u306b --txt-prefix=<prefix\u6587\u5b57\u5217> \u3092\u8ffd\u52a0 \u6307\u5b9a\u3057\u305f\u6587\u5b57\u5217\u304cTXT record\u306e\u30ec\u30b3\u30fc\u30c9\u540d(A\u30ec\u30b3\u30fc\u30c9\u306e\u5834\u5408\u306fA\u30ec\u30b3\u30fc\u30c9\u3068\u540c\u540d)\u306eprefix\u3068\u3057\u3066\u8ffd\u52a0\u3055\u308c\u307e\u3059 CNAME\u30ec\u30b3\u30fc\u30c9\u306f(TXT\u30ec\u30b3\u30fc\u30c9\u3067\u3042\u3063\u3066\u3082)\u4ed6\u306e\u30ec\u30b3\u30fc\u30c9\u3068\u5171\u5b58\u3067\u304d\u306a\u3044\u4ed5\u69d8\u3067\u3059( RFC 1034\u30bb\u30af\u30b7\u30e7\u30f33.6.2 \u3001 RFC 1912\u30bb\u30af\u30b7\u30e7\u30f32.4 https://github.com/kubernetes-sigs/external-dns/blob/master/docs/faq.md#im-using-an-elb-with-txt-registry-but-the-cname-record-clashes-with-the-txt-record-how-to-avoid-this Ingress\u30ea\u30bd\u30fc\u30b9\u306eannotations\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b external-dns.alpha.kubernetes.io/hostname \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eFQDN\u3092\u6307\u5b9a external-dns.alpha.kubernetes.io/target \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eValue\u3068\u306a\u308b\u53c2\u7167\u5148\u306eFQDN\u307e\u305f\u306fIP\u30a2\u30c9\u30ec\u30b9\u306a\u3069\u6307\u5b9a external-dns.alpha.kubernetes.io/hostname: dev1.example.com external-dns.alpha.kubernetes.io/target: alias1.example.com --txt-prefix=prefix_ \u3067\u52d5\u4f5c\u78ba\u8a8d Ingress\u30ea\u30bd\u30fc\u30b9 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-test-ingress annotations: external-dns.alpha.kubernetes.io/hostname: dev1.example.com external-dns.alpha.kubernetes.io/target: alias1.example.com spec: ingressClassName: nginx defaultBackend: service: name: nginx-service port: number: 8080 rules: - host: dev1.example.com http: paths: - path: / pathType: Prefix backend: service: name: nginx-service port: number: 8080 external-dns log time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN CNAME alias1.example.com [] dev1.example.com 0 IN CNAME alias1.example.com []]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Removing duplicate endpoint dev1.example.com 0 IN CNAME alias1.example.com []\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/<HOSTED_ZONE_ID> (domain: example.com.)\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding prefix_dev1.example.com. to zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE dev1.example.com CNAME [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE prefix_dev1.example.com TXT [Id: /hostedzone/<HOSTED_ZONE_ID>]\" time=\"2021-10-01T09:05:19Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/<HOSTED_ZONE_ID>] were successfully updated\" route53 \u30ec\u30b3\u30fc\u30c9\u78ba\u8a8d $ aws route53 list-resource-record-sets --output json --hosted-zone-id <HOSTED_ZONE_ID> | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\" or .Name == \"prefix_dev1.example.com.\"))' [ { \"Name\": \"prefix_dev1.example.com.\", \"Type\": \"TXT\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"\\\"heritage=external-dns,external-dns/owner=<HOSTED_ZONE_ID>,external-dns/resource=ingress/default/nginx-test-ingress\\\"\" } ] }, { \"Name\": \"dev1.example.com.\", \"Type\": \"CNAME\", \"TTL\": 300, \"ResourceRecords\": [ { \"Value\": \"alias1.example.com\" } ] } ] $ dig +noall +answer dev1.example.com dev1.example.com. 300 IN CNAME alias1.example.com.","title":"external-dns\u3067A\u30ec\u30b3\u30fc\u30c9\u3067\u306f\u306a\u304fCNAME\u30ec\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3059\u308b"},{"location":"static_pod/","text":"8\u6708 31 12:57:50 k8s-master kubelet[1912]: E0831 12:57:50.411795 1912 kubelet.go:1635] Failed creating a mirror pod for \"etcd-k8s-master_kube-system(8fff3e1f31b52ebeb520767ae50cc739)\": pods \"etcd-k8s-master\" is forbidden: PodSecurityPolicy: no providers available to validate pod request 8\u6708 31 12:58:55 k8s-master kubelet[1912]: I0831 12:58:55.446865 1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-apiserver-k8s-master_kube-system(b2f9759c-9ffe-436c-9ebb-0b5f9c68a452)\" 8\u6708 31 12:58:58 k8s-master kubelet[1912]: I0831 12:58:58.433779 1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-scheduler-k8s-master_kube-system(b502a669-2571-41f7-a705-3aa194ade526)\" 8\u6708 31 12:59:00 k8s-master kubelet[1912]: I0831 12:59:00.432199 1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-controller-manager-k8s-master_kube-system(f834a62b-d802-40c5-90df-24a49a38efb6)\" 8\u6708 31 12:59:11 k8s-master kubelet[1912]: I0831 12:59:11.421459 1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"etcd-k8s-master_kube-system(e612e1ab-a907-4955-8b4c-97d5a37b11fa)\" 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849447 1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/etcd-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849581 1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-apiserver-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849654 1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-controller-manager-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849719 1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-scheduler-k8s-master\" status=Running","title":"Index"}]}