{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"container-runtime/","title":"Container Runtime","text":""},{"location":"container-runtime/#high-level-container-runtime","title":"High Level Container Runtime","text":"<ul> <li>https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/</li> <li>Container Runtime Interfaces(CRI)<ul> <li>kubelet\u304b\u3089\u547c\u3070\u308c\u308bContainer Runtime</li> <li>contanerd \u3084 cri-o \u306a\u3069\u304cCRI\u306b\u8a72\u5f53\u3059\u308b</li> </ul> </li> </ul>"},{"location":"container-runtime/#cri-o","title":"cri-o","text":"<ul> <li>https://github.com/cri-o/cri-o</li> <li>RedHat\u304c\u4e3b\u5c0e\u3067\u958b\u767a</li> <li>\u7d14\u7c8b\u306b\u30b3\u30f3\u30c6\u30ca\u3092\u8d77\u52d5\u3055\u305b\u308b\u305f\u3081\u306e\u5b9f\u88c5\u306a\u306e\u3067\u8efd\u91cf\u304b\u3064\u30bb\u30ad\u30e5\u30a2\u3089\u3057\u3044</li> <li> <p>image \u306e build \u3084 push \u306f\u975e\u30b5\u30dd\u30fc\u30c8</p> <p>What is not in scope for this project?</p> <ul> <li>Building, signing and pushing images to various image storages</li> </ul> </li> </ul>"},{"location":"container-runtime/#containerd","title":"containerd","text":"<ul> <li>https://github.com/containerd/containerd</li> <li>CNCF\u306egraduated project</li> <li>dockerd\u304b\u3089CRI Interface\u90e8\u5206\u3092\u5206\u96e2(containerd v1.1\u3067native\u306b\u5bfe\u5fdc)</li> <li>image \u306e build \u3084 push \u3092\u30b5\u30dd\u30fc\u30c8</li> </ul>"},{"location":"container-runtime/#low-level-container-runtime","title":"Low Level Container Runtime","text":""},{"location":"container-runtime/#runc","title":"runc","text":"<ul> <li>\u30b3\u30f3\u30c6\u30ca\u4ed5\u69d8\u306e\u6a19\u6e96\u5316\u56e3\u4f53\u3067\u3042\u308b Open Container Initiative(OCI) \u304c\u516c\u958b</li> <li><code>containerd</code> \u3084 <code>cri-o</code> \u304b\u3089\u5b9f\u884c\u3055\u308c\u308b</li> </ul>"},{"location":"container-runtime/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://thinkit.co.jp/article/18024</li> <li>https://medium.com/nttlabs/container-runtime-d3e25189f67a</li> <li>https://qiita.com/mamomamo/items/ed5db2ab1555078f8a24</li> <li>https://www.kimullaa.com/entry/2021/05/07/204706</li> <li>https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/</li> <li>https://www.publickey1.jp/blog/20/firecrackergvisorunikernel_container_runtime_meetup_2.html</li> </ul>"},{"location":"addons/metallb/","title":"MetalLB","text":""},{"location":"addons/metallb/#about-metallb","title":"About MetalLB","text":"<p>https://metallb.universe.tf/ https://github.com/metallb/metallb</p> <p>Bare Metal\u306a\u74b0\u5883\u3067LoadBlancer\u30b5\u30fc\u30d3\u30b9\u3092\u63d0\u4f9b\u3059\u308baddon\u3067\u3059\u3002</p> <p>CloudProvider\u304c\u63d0\u4f9b\u3059\u308bKubernetes\u30b5\u30fc\u30d3\u30b9\u3067\u306f\u5f53\u8a72CloudProvider\u306eLoadBlancer\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3067\u304d\u307e\u3059\u3002 AWS\u3067\u306f AWS Load Balancer Controller \u3092\u5c0e\u5165\u3059\u308b\u3053\u3068\u3067Ingress\u30ea\u30bd\u30fc\u30b9 \u3084 Service\u30ea\u30bd\u30fc\u30b9 \u3067Elastic Load Balancer\u306e\u4f5c\u6210\u3092\u884c\u3046\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002</p> <p>Bare Metal\u306a\u74b0\u5883\u3067Nginx Ingress Controller\u3092\u5c0e\u5165\u3057\u305f\u5834\u5408\u3001Cluster\u5185\u306ePod Network\u306eIP\u30a2\u30c9\u30ec\u30b9\u304c\u30a2\u30b5\u30a4\u30f3\u3055\u308c\u307e\u3059\u3002</p> <p>\u79c1\u306fRaspberry pi(ubuntu server)\u3067Kubernetes Cluster\u3092\u69cb\u7bc9\u3057\u3066\u304a\u308a\u3001Node IP\u30a2\u30c9\u30ec\u30b9\u306f\u81ea\u5b85\u306eWiFi\u30eb\u30fc\u30bf(<code>192.168.3.0/24</code>)\u304b\u3089\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u3002 Cluster Cidr(<code>10.200.0.0/16</code>)\u3067\u306fMacBook\u306a\u3069Cluster\u5916\u306e\u30d6\u30e9\u30a6\u30b6\u30a2\u30af\u30bb\u30b9\u304c\u3067\u304d\u305a\u3001(NAT\u5909\u63db\u306a\u3069\u3092\u99c6\u4f7f\u3059\u308c\u3070\u53ef\u80fd\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c) \u5c11\u3005\u4e0d\u4fbf\u3067\u3059\u3002</p> <p>MetalLB\u3067\u306f <code>Service</code> \u30ea\u30bd\u30fc\u30b9\u3067 <code>type: LoadBalancer</code> \u3092\u6307\u5b9a\u53ef\u80fd\u3068\u3057\u3001\u304b\u3064<code>192.168.3.200/27</code> \u306a\u3069Kubernetes Cluster\u306e\u5916\u90e8\u306b\u516c\u958b\u53ef\u80fd\u306aIP\u30a2\u30c9\u30ec\u30b9\u306e\u5272\u308a\u5f53\u3066\u304c\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"addons/metallb/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://blog.framinal.life/entry/2020/04/16/022042</li> <li>https://garafu.blogspot.com/2019/06/install-metallb.html</li> </ul>"},{"location":"addons/metallb/#installation","title":"Installation","text":"<ul> <li>https://metallb.universe.tf/installation/</li> </ul>"},{"location":"addons/metallb/#configuration","title":"Configuration","text":"<ul> <li> <p>https://metallb.universe.tf/configuration/</p> <ul> <li><code>IPAddressPool</code><ul> <li><code>type: LoadBalancer</code> \u3092\u6307\u5b9a\u3057\u305fService\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306e\u30d7\u30fc\u30eb\u3092\u5b9a\u7fa9</li> </ul> </li> <li><code>L2Advertisement</code><ul> <li>IP\u30a2\u30c9\u30ec\u30b9\u306eAdvertisement\u3092\u884c\u3046k8s node\u3092\u6307\u5b9a\u3059\u308b</li> <li>https://metallb.universe.tf/configuration/_advanced_l2_configuration/</li> </ul> </li> </ul> <p>manifests <pre><code>---\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: ip-pool\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.3.200-192.168.3.210\n\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: ip-pool-advertisement\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n  - ip-pool\n  nodeSelectors:\n  - matchLabels:\n      kubernetes.io/hostname: k8s-master\n  - matchLabels:\n      kubernetes.io/hostname: k8s-node1\n  - matchLabels:\n      kubernetes.io/hostname: k8s-node2\n</code></pre> </p> </li> </ul>"},{"location":"addons/metallb/#kubernetes-dashboard-metallbip","title":"Kubernetes Dashboard \u3092MetalLB\u3067\u6255\u3044\u51fa\u3057\u305fIP\u30a2\u30c9\u30ec\u30b9\u3067\u30a2\u30af\u30bb\u30b9\u3057\u3066\u307f\u308b","text":"<ol> <li> <p>edit of kubernetes-dashboard manifests</p> <ul> <li>bootstrapping kubernetes-dashboard \u3067\u4f5c\u6210\u3057\u305f\u74b0\u5883<ul> <li>Service\u30ea\u30bd\u30fc\u30b9\u306eType\u3092 <code>LoadBalancer</code> \u306b\u5909\u66f4</li> <li><code>metallb.universe.tf/address-pool</code> annotations\u3092\u8ffd\u8a18     /etc/kubernetes/manifests/kubernetes-dashboard.yaml \u306e\u4fee\u6b63\u5f8c\u306ediff <pre><code>@@ -36,7 +36,10 @@\n     k8s-app: kubernetes-dashboard\n   name: kubernetes-dashboard\n   namespace: kubernetes-dashboard\n+  annotations:\n+    metallb.universe.tf/address-pool: ip-pool\n spec:\n+  type: LoadBalancer\n   ports:\n     - port: 443\n       targetPort: 8443\n</code></pre> </li> </ul> </li> </ul> </li> <li> <p>apply kubernetes-dashboard manifests     <pre><code>kubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard.yaml\n</code></pre></p> </li> <li> <p>check service and ingress</p> <p>service <pre><code>$ kubectl describe svc -n kubernetes-dashboard kubernetes-dashboard\nName:                     kubernetes-dashboard\nNamespace:                kubernetes-dashboard\nLabels:                   k8s-app=kubernetes-dashboard\nAnnotations:              metallb.universe.tf/address-pool: ip-pool\nSelector:                 k8s-app=kubernetes-dashboard\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.32.0.177\nIPs:                      10.32.0.177\nLoadBalancer Ingress:     192.168.3.201\nPort:                     &lt;unset&gt;  443/TCP\nTargetPort:               8443/TCP\nNodePort:                 &lt;unset&gt;  30522/TCP\nEndpoints:                10.200.2.78:8443\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:\n  Type    Reason        Age                 From                Message\n  ----    ------        ----                ----                -------\n  Normal  IPAllocated   53m                 metallb-controller  Assigned IP [\"192.168.3.201\"]\n  Normal  nodeAssigned  50s (x34 over 53m)  metallb-speaker     announcing from node \"k8s-master\" with protocol \"layer2\"\n</code></pre> </p> <p>ingress <pre><code>$ kubectl describe ingress -n kubernetes-dashboard dashboard-ingress\nName:             dashboard-ingress\nLabels:           &lt;none&gt;\nNamespace:        kubernetes-dashboard\nAddress:          192.168.3.200\nIngress Class:    &lt;none&gt;\nDefault backend:  &lt;default&gt;\nTLS:\n  dashboard-secret-tls terminates k8s-dashboard.local\nRules:\n  Host                 Path  Backends\n  ----                 ----  --------\n  k8s-dashboard.local\n                       /   kubernetes-dashboard:443 (10.200.2.78:8443)\nAnnotations:           kubernetes.io/ingress.class: nginx\n                       nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n                       nginx.ingress.kubernetes.io/ssl-passthrough: true\nEvents:\n  Type    Reason  Age                From                      Message\n  ----    ------  ----               ----                      -------\n  Normal  Sync    53m (x2 over 54m)  nginx-ingress-controller  Scheduled for sync\n</code></pre> </p> </li> <li> <p>Service\u306b\u5272\u308a\u5f53\u3066\u3089\u308c\u3066\u3044\u308b <code>192.168.3.201</code> \u3067\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> <ul> <li>NodePort\u306a\u3069\u306e\u6307\u5b9a\u306f\u4e0d\u8981\u306a\u306e\u3067\u30a2\u30af\u30bb\u30b9\u304c\u624b\u8efd</li> <li></li> </ul> </li> </ol>"},{"location":"addons/argo/argo-rollouts/about/","title":"About","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/</li> <li>https://github.com/argoproj/argo-rollouts</li> <li>https://techstep.hatenablog.com/entry/2020/10/13/084905</li> </ul> <p>Argo Rollouts\u306f\u3001<code>Blue/Green</code> \u3084 <code>Canary</code> \u3068\u3044\u3063\u305fdeploy strategies\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> <p>Kubernetes Deployment\u6a19\u6e96\u306edeploy strategies(<code>RollingUpdate</code>) \u3068\u6bd4\u8f03\u3057\u9ad8\u5ea6\u306a\u30c7\u30d7\u30ed\u30a4\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002 (\u53c2\u8003: Why Argo Rollouts?)</p>"},{"location":"addons/argo/argo-rollouts/about/#architecture","title":"Architecture","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/architecture/ </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#argo-rollout-controller","title":"<code>Argo Rollout Controller</code>","text":"<ul> <li><code>Rollout</code> resource type\u3092\u76e3\u8996\u3057\u5909\u66f4\u304c\u751f\u3058\u308b\u3068\u5b9a\u7fa9\u3068\u540c\u3058\u72b6\u614b\u306b\u3059\u308b\u5f79\u5272\u3092\u6301\u3064\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9</li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#rollout-resource","title":"<code>Rollout</code> resource","text":"<ul> <li><code>Argo Rollout</code> \u306eCustomResource<ul> <li>Deployment resource type\u3068\u307b\u307c\u4e92\u63db\u6027\u304c\u3042\u308a\u307e\u3059\u304c\u3001deployment strategies\u3068\u3057\u3066canary\u3084blue-green\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u305d\u308c\u3089\u306e\u305f\u3081\u306e\u30d5\u30a3\u30fc\u30eb\u30c9\u3092\u6301\u3064</li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#replicaset","title":"<code>ReplicaSet</code>","text":"<ul> <li>Kubernetes\u306e\u6a19\u6e96\u7684\u306aReplicaSet<ul> <li><code>Argo Rollout</code> \u304c\u4f5c\u6210\u30fb\u7ba1\u7406\u3057\u307e\u3059</li> <li>rollout\u3067\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u3082\u8ffd\u52a0\u3055\u308c\u307e\u3059</li> <li>\u4e0a\u56f3\u3067\u306f<code>Canary ReplicaSet</code> \u3084 <code>Stable ReplicaSet</code> \u3068\u8868\u8a18\u3055\u308c\u3066\u3044\u308b\u7b87\u6240<ul> <li><code>\u65e7version application</code>\u304c\u52d5\u4f5c\u3059\u308b<code>Stable ReplicaSet</code></li> <li>stable\u6607\u683c\u524d\u306e\u65b0version application\u304c\u52d5\u4f5c\u3059\u308b<code>Canary ReplicaSet</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#ingress-service","title":"<code>Ingress</code> / <code>Service</code>","text":"<ul> <li><code>Service</code>\u306b\u5bfe\u3059\u308btraffic\u3092\u3069\u306eReplicaSet\u3078routing\u3059\u308b\u306e\u304b\u3092selector\u3067\u7ba1\u7406\u3057\u307e\u3059</li> <li><code>Ingress</code> \u3068\u9023\u643a\u3059\u308b\u3053\u3068\u3067canary service\u306b\u5bfe\u3059\u308btraffic routing\u306e\u6bd4\u91cd\u3084routing rule\u3092\u5236\u5fa1\u3067\u304d\u307e\u3059<ul> <li><code>Service</code> \u306e\u307f\u306e\u5834\u5408\u3001stable \u3068 canary \u306ePod\u6570\u3067\u306e\u307ftraffic routing\u306e\u6bd4\u91cd\u3092\u5236\u5fa1\u3057\u307e\u3059</li> <li> <p><code>Ingress</code> \u306e\u5834\u5408\u3001\u30d1\u30fc\u30bb\u30f3\u30c6\u30fc\u30b8 / HTTP Header / Mirror \u3068\u3044\u3063\u305f\u624b\u6cd5\u3067\u306etraffic routing\u3092\u5b9f\u73fe\u3067\u304d\u307e\u3059</p> <p>Warning</p> <p>HTTP Header \u3084 Mirror\u306fIngress Controller\u306b\u3088\u3063\u3066\u30b5\u30dd\u30fc\u30c8\u72b6\u6cc1\u304c\u7570\u306a\u308a\u307e\u3059</p> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#analysistemplate-analysisrun","title":"<code>AnalysisTemplate</code> / <code>AnalysisRun</code>","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/analysis/</li> <li><code>Analysis</code> \u3068\u306f<code>Argo Rollout</code>\u304cdeploy\u4e2d\u306bMetricsProvider\u306b\u63a5\u7d9a\u3057\u7279\u5b9a\u306eMetrics\u306b\u5bfe\u3059\u308b\u95be\u5024\u3092\u8a2d\u5b9a\u3057\u3066\u304a\u304f\u3053\u3068\u3067\u65b0version application\u304c\u6b63\u5e38\u306b\u52d5\u3044\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3059\u308b\u4ed5\u7d44\u307f\u3067\u3059<ul> <li>Metrics\u304c\u826f\u597d\u306a\u5834\u5408\u306frollout\u306fdeploy\u3092\u7d99\u7d9a\u3057\u3001\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306fdeploy\u3092\u4e2d\u65ad\u3057rollback\u3057\u307e\u3059</li> </ul> </li> <li><code>Analysis</code> \u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306eCutomResource\u3068\u3057\u3066 <code>AnalysisTemplate</code> \u3068 <code>AnalysisRun</code> \u304c\u3042\u308a\u307e\u3059<ul> <li><code>AnalysisTemplate</code><ul> <li>MetricsProvider\u3084Analysis\u306b\u4f7f\u7528\u3059\u308bMetrics\u306b\u95a2\u3059\u308b\u8a2d\u5b9a</li> <li>Rollout Resource\u306b\u76f4\u63a5\u8a2d\u5b9a\u3001<code>AnalysisTemplate</code> \u3082\u3057\u304f\u306f <code>ClusterAnalysisTemplate</code> Resource\u3068\u3057\u3066\u5b9a\u7fa9\u3059\u308b</li> </ul> </li> <li><code>AnalysisRun</code><ul> <li><code>AnalysisTemplate</code> \u306e\u5b9f\u884c\u7d50\u679c</li> <li>\u7279\u5b9a\u306e <code>Rollout</code> Resource\u306bscope\u3055\u308c\u307e\u3059</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#deploy-strategies","title":"Deploy Strategies","text":"<p>rollout CRD\u306e <code>.spec.strategy</code> \u3067 <code>blueGreen</code> \u3082\u3057\u304f\u306f <code>canary</code> \u3092\u6307\u5b9a\u3057\u307e\u3059\u3002</p>"},{"location":"addons/argo/argo-rollouts/about/#bluegreen","title":"Blue/Green","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/bluegreen/</li> <li>rollout\u306eBlue/Green\u30c7\u30d7\u30ed\u30a4\u3067\u306f<code>activeService</code>\u3068<code>previewService</code>\u3068\u3044\u30462\u3064\u306eService\u3092\u6307\u5b9a\u3057\u307e\u3059<ul> <li><code>activeService</code> \u306f\u65e7version application ReplicaSet\u3078traffic\u3092routing\u3057\u307e\u3059</li> <li><code>previewService</code> \u306f\u65b0version application ReplicaSet\u3078traffic\u3092routing\u3057\u307e\u3059</li> </ul> </li> <li>rollout\u306e <code>.spec.template</code> \u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u65b0ReplicaSet\u3092\u4f5c\u6210\u3057\u307e\u3059<ul> <li><code>activeService</code> \u306btraffic\u304c\u6d41\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3059\u3050\u306b\u65b0ReplicaSet\u3078\u5207\u308a\u66ff\u3048\u3001\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u65b0ReplicaSet\u304c\u5229\u7528\u53ef\u80fd\u306b\u306a\u308b\u307e\u3067\u306f\u65e7ReplicaSet\u3078routing\u3057\u307e\u3059</li> <li>\u65b0ReplicaSet\u304c\u5229\u7528\u53ef\u80fd\u306b\u306a\u3063\u305f\u3089<code>activeService</code>\u3092\u65b0ReplicaSet\u3078routing\u3092\u5207\u308a\u66ff\u3048\u307e\u3059</li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#canary","title":"Canary","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/canary/</li> <li>rollout\u306eCanary\u30c7\u30d7\u30ed\u30a4\u3067\u306f\u65e7Service\u306b\u306f\u65e7version application\u306b\u5bfe\u3059\u308btraffic\u3092routing\u3057\u3064\u3064traffic\u3092\u5f90\u3005\u306b\u65b0Service\u306brouting\u3057\u307e\u3059<ul> <li>rollout CRD\u306e <code>spec.strategy.canary.steps</code> \u3067\u65b0Service\u3078traffic\u3092routing\u3059\u308b\u6bd4\u91cd\u3068\u79fb\u884c\u9593\u9694\u3092\u6307\u5b9a\u3057\u307e\u3059</li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#progressive-delivery","title":"Progressive Delivery","text":"<p><code>Progressive Delivery</code> \u3068\u306fContinuous Delivery\u3092\u767a\u5c55\u3055\u305b\u305f\u8003\u3048\u65b9\u3067\u3001canary deploy\u9014\u4e2d\u3067\u65b0version application\u3092\u89e3\u6790\u3057\u6b63\u5e38\u3067\u3042\u308c\u3070deploy\u3092\u7d99\u7d9a\u3001\u7570\u5e38\u3067\u3042\u308c\u3070\u65e7version application\u3078rollback\u3059\u308b\u3068\u3044\u3063\u305f\u8003\u3048\u65b9\u3084\u4ed5\u7d44\u307f\u3092\u6307\u3057\u307e\u3059\u3002Argo Rollouts\u3067\u306f <code>Analysis</code> \u6a5f\u80fd\u3092\u5229\u7528\u3057\u307e\u3059\u3002</p> <p>\u5f15\u7528: Leveling Up Your CD: Unlocking Progressive Delivery on Kubernetes \u2013 Daniel Thomson &amp; Jesse Suen, Intuit</p> <p></p> \u5f93\u6765\u306eCD <p></p> progressive delivery"},{"location":"addons/argo/argo-rollouts/about/#analysis","title":"Analysis","text":"<ul> <li>Rollout \u306e <code>sepc.strategy.canary.analysis</code></li> <li>blue/green strategy\u3067\u3082<code>Analysis</code> \u3092\u5229\u7528\u53ef\u80fd<ul> <li>\u65b0version application\u306eReplicaSet\u306escale\u304c\u5b8c\u4e86\u3057\u30c8\u30e9\u30d5\u30a3\u30c3\u30af\u3092\u65b0\u3057\u3044\u30d0\u30fc\u30b8\u30e7\u30f3\u306b\u5207\u308a\u66ff\u3048\u308b\u524d\u5f8c\u3067 <code>AnalysisRun</code> \u3092\u8d77\u52d5\u3067\u304d\u307e\u3059</li> <li>https://argoproj.github.io/argo-rollouts/features/analysis/#bluegreen-pre-promotion-analysis</li> <li>https://argoproj.github.io/argo-rollouts/features/analysis/#bluegreen-post-promotion-analysis</li> <li>https://aws.amazon.com/jp/blogs/architecture/use-amazon-eks-and-argo-rollouts-for-progressive-delivery/</li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#analysis_1","title":"Analysis\u306b\u95a2\u3059\u308b\u30ab\u30b9\u30bf\u30e0\u30ea\u30bd\u30fc\u30b9","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/analysis/#custom-resource-definitions<ul> <li><code>AnalysisTemplate</code><ul> <li>\u89e3\u6790\u65b9\u6cd5\u306b\u3064\u3044\u3066\u306e\u5b9a\u7fa9\u3057\u305f\u30ab\u30b9\u30bf\u30e0\u30ea\u30bd\u30fc\u30b9</li> <li>namespace\u3054\u3068</li> </ul> </li> <li><code>ClusterAnalysisTemplate</code><ul> <li><code>AnalysisTemplate</code> \u3068\u540c\u3058</li> <li>cluster wide</li> </ul> </li> <li><code>AnalysisRun</code><ul> <li><code>AnalysisTemplate</code> \u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3055\u308c\u305f\u3082\u306e</li> <li>Kubernetes\u306e <code>Job</code> \u30ea\u30bd\u30fc\u30b9\u3068\u4f3c\u3066\u3044\u3066\u6700\u7d42\u7684\u306b\u5b8c\u4e86\u3057\u307e\u3059</li> <li>\u5b9f\u884c\u7d50\u679c\u3068\u3057\u3066\u306f <code>Successful</code>\u3001<code>Failed</code>\u3001<code>Inconclusive</code> \u304c\u3042\u308a\u3001\u305d\u308c\u305e\u308c\u304crollout\u306edeploy\u304c\u7d99\u7d9a\u3001\u4e2d\u65ad\u3001\u307e\u305f\u306f\u4e00\u6642\u505c\u6b62\u3055\u308c\u308b\u304b\u306b\u5f71\u97ff\u3057\u307e\u3059\u3002</li> </ul> </li> <li><code>Experiment</code><ul> <li>\u5f8c\u8ff0</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/about/#experimentation","title":"<code>experimentation</code>","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/experiment/</li> <li>1\u3064\u307e\u305f\u306f\u8907\u6570\u306eReplicaSet\u3092\u30a8\u30d5\u30a7\u30e1\u30e9\u30eb\u306a\u30ea\u30bd\u30fc\u30b9\u3068\u3057\u3066\u8d77\u52d5\u3055\u305b\u3001background\u3067AnalysisRun\u3092\u5b9f\u884c\u3055\u305b\u308b\u3053\u3068\u3067\u65b0version application\u306e\u6b63\u5e38\u6027\u78ba\u8a8d\u3092\u884c\u3048\u307e\u3059<ul> <li><code>Experiment</code> \u30671\u3064\u307e\u305f\u306f\u8907\u6570\u306eReplicaSet\u3068<code>AnalysisTemplate</code>\u306e\u6307\u5b9a\u3092\u884c\u3044\u307e\u3059</li> <li><code>Rollout</code>\u30ea\u30bd\u30fc\u30b9\u5185\u306esteps\u3068\u3057\u3066<code>experiment</code>\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059<ul> <li>https://argoproj.github.io/argo-rollouts/features/experiment/#integration-with-rollouts</li> <li>https://github.com/argoproj/argo-rollouts/blob/master/examples/rollout-experiment-step.yaml</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/basic_usage/","title":"Basic Usage","text":""},{"location":"addons/argo/argo-rollouts/basic_usage/#canary","title":"canary","text":""},{"location":"addons/argo/argo-rollouts/basic_usage/#manifests","title":"manifests","text":"<ul> <li>\u4eca\u56de\u306fService\u3084Ingress\u3092\u6307\u5b9a\u3057\u306a\u3044\u305f\u3081Pod\u6570\u306e\u5897\u6e1b\u3068\u306a\u308a\u307e\u3059</li> <li>canary \u3078\u306etraffic shift \u304c\u5206\u304b\u308a\u3084\u3059\u3044\u3088\u3046\u306b <code>.spec.replicas</code> \u306f <code>5</code> \u3068\u3057\u307e\u3059</li> <li> <p>\u4eca\u56de\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306astep\u3067canary traffic \u3078\u306e\u5272\u308a\u632f\u308a\u3092\u884c\u3044\u307e\u3059</p> # step 0 20%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b 1 60sec pause 2 80%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b 3 60sec pause 4 100%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b 5 canary \u3092 stable \u306b\u5207\u308a\u66ff\u3048 <p>argo-rollouts/nginx.yaml <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rollout-test-canary-nginx-conf\ndata:\n  nginx.conf: |\n    user nginx;\n    worker_processes  1;\n    error_log  /var/log/nginx/error.log;\n    events {\n      worker_connections  1024;\n    }\n    http {\n      server {\n          listen       80;\n          server_name  _;\n\n          location / {\n              root   html;\n              index  index.html index.htm;\n          }\n\n          location /nginx_status {\n              stub_status on;\n              access_log off;\n              allow 127.0.0.1;\n              deny all;\n          }\n\n      }\n    }\n\n---\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: rollout-test-canary-nginx\nspec:\n  strategy:\n    canary:\n      steps:\n      - setWeight: 20\n      - pause: {duration: 60}\n      - setWeight: 80\n      - pause: {duration: 60}\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 5\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n      labels:\n        app: nginx\n    spec:\n      volumes:\n      - name: nginx-conf\n        configMap:\n          name: rollout-test-canary-nginx-conf\n          items:\n            - key: nginx.conf\n              path: nginx.conf\n      containers:\n      - name: nginx\n        image: nginx:1.14.1\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx/nginx.conf\n          readOnly: true\n          name: nginx-conf\n          subPath: nginx.conf\n      - name: nginx-exporter\n        image: nginx/nginx-prometheus-exporter:0.11.0\n        args:\n          - -nginx.scrape-uri=http://localhost/nginx_status\n        ports:\n          - containerPort: 9113\n</code></pre> </p> </li> </ul>"},{"location":"addons/argo/argo-rollouts/basic_usage/#deploy","title":"\u521d\u56dedeploy","text":"<ol> <li> <p>deploy\u5f8c\u306e\u8d77\u52d5\u78ba\u8a8d     list <pre><code>$ kubectl argo rollouts list rollouts\nNAME                STRATEGY   STATUS        STEP  SET-WEIGHT  READY  DESIRED  UP-TO-DATE  AVAILABLE\nrollout-test-canary-nginx  Canary     Healthy       4/4   100         5/5    5        5           5\n</code></pre> </p> <p>get <pre><code>$ kubectl argo rollouts get rollout rollout-test-canary-nginx\nName:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        Canary\n  Step:          4/4\n  SetWeight:     100\n  ActualWeight:  100\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (stable)\n                 nginx:1.14.1 (stable)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                            KIND        STATUS     AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u2714 Healthy  2m58s\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2714 Healthy  2m58s  stable\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-47h6s  Pod         \u2714 Running  2m58s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u2714 Running  2m58s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-ph8pf  Pod         \u2714 Running  2m58s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-pw8md  Pod         \u2714 Running  2m58s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-rqbr5  Pod         \u2714 Running  2m58s  ready:2/2\n</code></pre> </p> </li> </ol>"},{"location":"addons/argo/argo-rollouts/basic_usage/#2deploy","title":"2\u56de\u76ee\u306edeploy","text":"<ol> <li><code>nginx:1.14.2</code> \u3078bump up\u3057\u3066deploy</li> <li> <p><code>kubectl argo rollouts get rollout rollout-test-canary-nginx -w</code> \u3067\u72b6\u614b\u9077\u79fb\u3092\u78ba\u8a8d     0. 20%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u25cc Progressing\nMessage:         more replicas need to be updated\nStrategy:        Canary\n  Step:          0/4\n  SetWeight:     20\n  ActualWeight:  0\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary, stable)\n                 nginx:1.14.1 (stable)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       1\n  Ready:         4\n  Available:     4\n\nNAME                                            KIND        STATUS         AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u25cc Progressing  4m57s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89            ReplicaSet  \u25cc Progressing  7s     canary\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb   Pod         \u2714 Running      6s     ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2714 Healthy      4m57s  stable\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u2714 Running      4m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-ph8pf  Pod         \u2714 Running      4m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-pw8md  Pod         \u2714 Running      4m57s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-rqbr5  Pod         \u2714 Running      4m57s  ready:2/2\n</code></pre> </p> <p>1. 60sec pause <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u0965 Paused\nMessage:         CanaryPauseStep\nStrategy:        Canary\n  Step:          1/4\n  SetWeight:     20\n  ActualWeight:  20\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary, stable)\n                 nginx:1.14.1 (stable)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       1\n  Ready:         5\n  Available:     5\n\nNAME                                            KIND        STATUS     AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u0965 Paused   4m57s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89            ReplicaSet  \u2714 Healthy  7s     canary\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb   Pod         \u2714 Running  6s     ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2714 Healthy  4m57s  stable\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u2714 Running  4m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-ph8pf  Pod         \u2714 Running  4m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-pw8md  Pod         \u2714 Running  4m57s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-rqbr5  Pod         \u2714 Running  4m57s  ready:2/2\n</code></pre> </p> <p>2. 80%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u25cc Progressing\nMessage:         more replicas need to be updated\nStrategy:        Canary\n  Step:          2/4\n  SetWeight:     80\n  ActualWeight:  25\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary, stable)\n                 nginx:1.14.1 (stable)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       4\n  Updated:       1\n  Ready:         4\n  Available:     4\n\nNAME                                            KIND        STATUS               AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u25cc Progressing        5m57s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89            ReplicaSet  \u25cc Progressing        67s    canary\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb   Pod         \u2714 Running            66s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-9dcqt   Pod         \u25cc Pending            0s     ready:0/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-cgf6w   Pod         \u25cc ContainerCreating  0s     ready:0/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-hjlrp   Pod         \u25cc Pending            0s     ready:0/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2714 Healthy            5m57s  stable\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u2714 Running            5m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-ph8pf  Pod         \u2714 Running            5m57s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-pw8md  Pod         \u25cc Terminating        5m57s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-rqbr5  Pod         \u2714 Running            5m57s  ready:2/2\n</code></pre> </p> <p>3. 60sec pause <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u0965 Paused\nMessage:         CanaryPauseStep\nStrategy:        Canary\n  Step:          3/4\n  SetWeight:     80\n  ActualWeight:  80\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary, stable)\n                 nginx:1.14.1 (stable)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       4\n  Ready:         5\n  Available:     5\n\nNAME                                            KIND        STATUS         AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u0965 Paused       6m14s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89            ReplicaSet  \u2714 Healthy      84s    canary\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb   Pod         \u2714 Running      83s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-9dcqt   Pod         \u2714 Running      17s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-cgf6w   Pod         \u2714 Running      17s    ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-hjlrp   Pod         \u2714 Running      17s    ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2714 Healthy      6m14s  stable\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u2714 Running      6m14s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-ph8pf  Pod         \u25cc Terminating  6m14s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-rqbr5  Pod         \u25cc Terminating  6m14s  ready:2/2\n</code></pre> </p> <p>4. 100%\u306etraffic\u3092canary\u306b\u5272\u308a\u632f\u308b <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u25cc Progressing\nMessage:         more replicas need to be updated\nStrategy:        Canary\n  Step:          4/4\n  SetWeight:     100\n  ActualWeight:  100\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary, stable)\n                 nginx:1.14.1 (stable)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       4\n  Ready:         5\n  Available:     5\n\nNAME                                            KIND        STATUS         AGE    INFO\n\u27f3 rollout-test-canary-nginx                            Rollout     \u25cc Progressing  7m13s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89            ReplicaSet  \u25cc Progressing  2m23s  canary\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb   Pod         \u2714 Running      2m22s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-9dcqt   Pod         \u2714 Running      76s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-cgf6w   Pod         \u2714 Running      76s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-hjlrp   Pod         \u2714 Running      76s    ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-ct9xs   Pod         \u25cc Pending      0s     ready:0/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8           ReplicaSet  \u2022 ScaledDown   7m13s  stable\n      \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-598c9cf7c8-dkq5v  Pod         \u25cc Terminating  7m13s  ready:2/2\n\n\nsnip...\n\n\nName:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u25cc Progressing\nMessage:         updated replicas are still becoming available\nStrategy:        Canary\n  Step:          4/4\n  SetWeight:     100\n  ActualWeight:  100\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (canary)\n                 nginx:1.14.2 (canary)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       5\n  Ready:         4\n  Available:     4\n\nNAME                                           KIND        STATUS         AGE    INFO\n\u27f3 rollout-test-canary-nginx                           Rollout     \u25cc Progressing  7m22s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89           ReplicaSet  \u25cc Progressing  2m32s  canary\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb  Pod         \u2714 Running      2m31s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-9dcqt  Pod         \u2714 Running      85s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-cgf6w  Pod         \u2714 Running      85s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-hjlrp  Pod         \u2714 Running      85s    ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-ct9xs  Pod         \u2714 Running      9s     ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8          ReplicaSet  \u2022 ScaledDown   7m22s  stable\n</code></pre> </p> <p>5. canary \u3092 stable \u306b\u5207\u308a\u66ff\u3048 <pre><code>Name:            rollout-test-canary-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        Canary\n  Step:          4/4\n  SetWeight:     100\n  ActualWeight:  100\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (stable)\n                 nginx:1.14.2 (stable)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                           KIND        STATUS        AGE    INFO\n\u27f3 rollout-test-canary-nginx                           Rollout     \u2714 Healthy     7m23s\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-bc478cd89           ReplicaSet  \u2714 Healthy     2m33s  stable\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-w7clb  Pod         \u2714 Running     2m32s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-9dcqt  Pod         \u2714 Running     86s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-cgf6w  Pod         \u2714 Running     86s    ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-hjlrp  Pod         \u2714 Running     86s    ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-canary-nginx-bc478cd89-ct9xs  Pod         \u2714 Running     10s    ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-canary-nginx-598c9cf7c8          ReplicaSet  \u2022 ScaledDown  7m23s\n</code></pre> </p> </li> </ol>"},{"location":"addons/argo/argo-rollouts/basic_usage/#bluegreen","title":"blue/green","text":""},{"location":"addons/argo/argo-rollouts/basic_usage/#manifests_1","title":"manifests","text":"<ul> <li> <p>Argo Rollout\u306e <code>blue/green</code> \u306fService\u304b\u3089traffic routing\u3059\u308bReplicaSet\u3092\u5207\u308a\u66ff\u3048\u308b\u3053\u3068\u3067\u5b9f\u73fe\u3057\u307e\u3059\u3002   \u305d\u306e\u305f\u3081\u3001<code>Service</code>\u3082\u4f75\u305b\u3066\u4f5c\u6210\u3057\u307e\u3059\u3002</p> <p>argo-rollouts/nginx_blue-green.yaml <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rollout-test-bluegreen-nginx-conf\ndata:\n  nginx.conf: |\n    user nginx;\n    worker_processes  1;\n    error_log  /var/log/nginx/error.log;\n    events {\n      worker_connections  1024;\n    }\n    http {\n      server {\n          listen       80;\n          server_name  _;\n\n          location / {\n              root   html;\n              index  index.html index.htm;\n          }\n\n          location /nginx_status {\n              stub_status on;\n              access_log off;\n              allow 127.0.0.1;\n              deny all;\n          }\n\n      }\n    }\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: rollout-test-bluegreen-active\nspec:\n  selector:\n    app: rollout-test-bluegreen-nginx\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: rollout-test-bluegreen-preview\nspec:\n  selector:\n    app: rollout-test-bluegreen-nginx\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n\n---\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: rollout-test-bluegreen-nginx\nspec:\n  strategy:\n    blueGreen:\n      activeService: rollout-test-bluegreen-active\n      previewService: rollout-test-bluegreen-preview\n\n  selector:\n    matchLabels:\n      app: rollout-test-bluegreen-nginx\n  replicas: 5\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n      labels:\n        app: rollout-test-bluegreen-nginx\n    spec:\n      volumes:\n      - name: nginx-conf\n        configMap:\n          name: rollout-test-bluegreen-nginx-conf\n          items:\n            - key: nginx.conf\n              path: nginx.conf\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx/nginx.conf\n          readOnly: true\n          name: nginx-conf\n          subPath: nginx.conf\n      - name: nginx-exporter\n        image: nginx/nginx-prometheus-exporter:0.11.0\n        args:\n          - -nginx.scrape-uri=http://localhost/nginx_status\n        ports:\n          - containerPort: 9113\n</code></pre> </p> <p>Warning</p> <ul> <li><code>Service</code>\u3088\u308a\u3082\u5148\u306b<code>Rollout</code>\u304c\u4f5c\u6210\u3055\u308c\u305f\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b<code>Degraded</code>\u3068\u306a\u308a\u307e\u3059\u306e\u3067\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059<ul> <li><code>Rollout</code> \u3088\u308a\u3082\u5148\u306b <code>Service</code> \u3092\u4f5c\u6210\u3059\u308b</li> <li>\u540c\u3058manifests\u30d5\u30a1\u30a4\u30eb\u306b\u8a18\u8f09\u3059\u308b\u5834\u5408\u3001\u4e0a\u304b\u3089\u9806\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3057\u3066\u3044\u304f\u305f\u3081\u8a18\u8f09\u9806\u3082\u6ce8\u610f\u3059\u308b\u3053\u3068</li> <li>https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/ <p>The resources will be created in the order they appear in the file. Therefore, it's best to specify the service first, since that will ensure the scheduler can spread the pods associated with the service as they are created by the controller(s), such as Deployment.</p> </li> </ul> </li> </ul> <pre><code>$ kubectl argo rollouts get rollout rollout-test-bluegreen-nginx\nName:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2716 Degraded\nMessage:         InvalidSpec: The Rollout \"rollout-test-bluegreen-nginx\" is invalid: spec.strategy.blueGreen.activeService: Invalid value: \"rollout-test-bluegreen-active\": service \"rollout-test-bl\nuegreen-active\" not found\nStrategy:        BlueGreen\nReplicas:\n  Desired:       5\n  Current:       0\n  Updated:       0\n  Ready:         0\n  Available:     0\n</code></pre> </li> </ul>"},{"location":"addons/argo/argo-rollouts/basic_usage/#deploy_1","title":"\u521d\u56dedeploy","text":"<ol> <li> <p>deploy\u5f8c\u306e\u8d77\u52d5\u78ba\u8a8d     get <pre><code>$ kubectl argo rollouts get rollout rollout-test-bluegreen-nginx\nName:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (stable, active)\n                 nginx:1.14.1 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS     AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy  16s\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2714 Healthy  16s  stable,active\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u2714 Running  15s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u2714 Running  15s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u2714 Running  15s  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u2714 Running  15s  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u2714 Running  15s  ready:2/2\n</code></pre> </p> <p>Service <pre><code>$ kubectl get service\nNAME                             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE\n\nsnip...\n\nrollout-test-bluegreen-active    ClusterIP   10.32.0.33    &lt;none&gt;        80/TCP           46s\nrollout-test-bluegreen-preview   ClusterIP   10.32.0.253   &lt;none&gt;        80/TCP           46s\n</code></pre> </p> </li> </ol>"},{"location":"addons/argo/argo-rollouts/basic_usage/#2deploy_1","title":"2\u56de\u76ee\u306edeploy","text":"<ol> <li><code>nginx:1.14.2</code> \u3078bump up\u3057\u3066deploy</li> <li> <p><code>kubectl argo rollouts get rollout rollout-test-bluegreen-nginx -w</code> \u3067\u72b6\u614b\u9077\u79fb\u3092\u78ba\u8a8d     1. \u65b0\u3057\u3044ReplicaSet\u304c\u4f5c\u6210\u3055\u308c\u3001Pod\u304c\u8d77\u52d5\u3059\u308b <pre><code>Name:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u25cc Progressing\nMessage:         active service cutover pending\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (active, preview, stable)\n                 nginx:1.14.1 (stable, active)\n                 nginx:1.14.2 (preview)\nReplicas:\n  Desired:       5\n  Current:       10\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS         AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u25cc Progressing  14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u25cc Progressing  13s  preview\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running      13s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running      13s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running      13s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running      12s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running      12s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2714 Healthy      14m  stable,active\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u2714 Running      14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u2714 Running      14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u2714 Running      14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u2714 Running      14m  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u2714 Running      14m  ready:2/2\n</code></pre> </p> <p>2. active service\u304c\u65b0\u3057\u3044ReplicaSet\u306b\u5411\u304d\u3001\u53e4\u3044ReplicaSet\u304cscale down\u3059\u308b\u307e\u3067scaleDownDelaySeconds` sec(default: 30) delay <pre><code>Name:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (active, stable)\n                 nginx:1.14.1\n                 nginx:1.14.2 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       10\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS     AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy  14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u2714 Healthy  14s  stable,active\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running  14s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running  14s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running  14s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running  13s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running  13s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2714 Healthy  14m  delay:28s\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u2714 Running  14m  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u2714 Running  14m  ready:2/2\n\n\nName:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (active, stable)\n                 nginx:1.14.1\n                 nginx:1.14.2 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       10\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS     AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy  14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u2714 Healthy  42s  stable,active\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running  42s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running  42s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running  42s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running  41s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running  41s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2714 Healthy  14m  delay:0s\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u2714 Running  14m  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u2714 Running  14m  ready:2/2\n</code></pre> </p> <p>4. <code>scaleDownDelaySeconds</code> sec\u304c\u7d4c\u904e <pre><code>Name:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (active, stable)\n                 nginx:1.14.1\n                 nginx:1.14.2 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       10\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS     AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy  14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u2714 Healthy  43s  stable,active\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running  43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running  43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running  43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running  42s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running  42s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2714 Healthy  14m  delay:passed\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u2714 Running  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u2714 Running  14m  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u2714 Running  14m  ready:2/2\n</code></pre> </p> <p>5. \u53e4\u3044ReplicaSet\u304cscale down\u3059\u308b <pre><code>Name:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (stable, active)\n                 nginx:1.14.2 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       10\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS         AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy      14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u2714 Healthy      43s  stable,active\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running      43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running      43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running      43s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running      42s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running      42s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2022 ScaledDown   14m\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-6rpsr  Pod         \u25cc Terminating  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9jcz9  Pod         \u25cc Terminating  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-9v7vb  Pod         \u25cc Terminating  14m  ready:2/2\n      \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-gg6kv  Pod         \u25cc Terminating  14m  ready:2/2\n      \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-5bb9dbdb65-s87hs  Pod         \u25cc Terminating  14m  ready:2/2\n</code></pre> </p> <p>6. \u53e4\u3044ReplicaSet\u306escale down\u304c\u5b8c\u4e86 <pre><code>Name:            rollout-test-bluegreen-nginx\nNamespace:       default\nStatus:          \u2714 Healthy\nStrategy:        BlueGreen\nImages:          nginx/nginx-prometheus-exporter:0.11.0 (stable, active)\n                 nginx:1.14.2 (stable, active)\nReplicas:\n  Desired:       5\n  Current:       5\n  Updated:       5\n  Ready:         5\n  Available:     5\n\nNAME                                                      KIND        STATUS        AGE  INFO\n\u27f3 rollout-test-bluegreen-nginx                            Rollout     \u2714 Healthy     14m\n\u251c\u2500\u2500# revision:2\n\u2502  \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-559fd99986           ReplicaSet  \u2714 Healthy     52s  stable,active\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-4kr6t  Pod         \u2714 Running     52s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-jbgnb  Pod         \u2714 Running     52s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-pzpdn  Pod         \u2714 Running     52s  ready:2/2\n\u2502     \u251c\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-9p276  Pod         \u2714 Running     51s  ready:2/2\n\u2502     \u2514\u2500\u2500\u25a1 rollout-test-bluegreen-nginx-559fd99986-n87x2  Pod         \u2714 Running     51s  ready:2/2\n\u2514\u2500\u2500# revision:1\n   \u2514\u2500\u2500\u29c9 rollout-test-bluegreen-nginx-5bb9dbdb65           ReplicaSet  \u2022 ScaledDown  14m\n</code></pre> </p> </li> </ol>"},{"location":"addons/argo/argo-rollouts/install/","title":"Install","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/installation/</li> <li>https://argoproj.github.io/argo-rollouts/features/helm/</li> <li> <p>https://github.com/argoproj/argo-helm/tree/main/charts/argo-rollouts</p> <p>Info</p> <p><code>.Values.keepCRDs</code> \u304c <code>default: True</code> \u306e\u305f\u3081 <code>helm uninstall</code> \u5b9f\u884c\u6642\u306b <code>CustomResourceDefinition</code> Resource\u306f\u6b8b\u308a\u307e\u3059</p> <p>refs Chart Values</p> <pre><code>$ kubectl get CustomResourceDefinition | grep argoproj.io\nanalysisruns.argoproj.io                         2023-01-29T08:45:21Z\nanalysistemplates.argoproj.io                    2023-01-29T08:45:20Z\nclusteranalysistemplates.argoproj.io             2023-01-29T08:45:21Z\nexperiments.argoproj.io                          2023-01-29T08:45:21Z\nrollouts.argoproj.io                             2023-01-29T08:45:21Z\n</code></pre> </li> </ul>"},{"location":"addons/argo/argo-rollouts/install/#controller-install","title":"controller install","text":"<ol> <li> <p>install with helm</p> <ul> <li>https://github.com/argoproj/argo-helm/tree/main/charts/argo-rollouts <pre><code>helm repo add argo https://argoproj.github.io/argo-helm\nhelm upgrade -i argo-rollouts argo/argo-rollouts --namespace argo-rollouts --create-namespace\n</code></pre></li> </ul> </li> <li> <p><code>argo-rollouts</code> controller\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> <p>deployment <pre><code>$ kubectl describe deployments -n argo-rollouts argo-rollouts\nName:               argo-rollouts\nNamespace:          argo-rollouts\nCreationTimestamp:  Sun, 29 Jan 2023 17:45:21 +0900\nLabels:             app.kubernetes.io/component=rollouts-controller\n                    app.kubernetes.io/instance=argo-rollouts\n                    app.kubernetes.io/managed-by=Helm\n                    app.kubernetes.io/name=argo-rollouts\n                    app.kubernetes.io/part-of=argo-rollouts\n                    app.kubernetes.io/version=v1.4.0\n                    helm.sh/chart=argo-rollouts-2.22.1\nAnnotations:        deployment.kubernetes.io/revision: 1\n                    meta.helm.sh/release-name: argo-rollouts\n                    meta.helm.sh/release-namespace: argo-rollouts\nSelector:           app.kubernetes.io/component=rollouts-controller,app.kubernetes.io/instance=argo-rollouts,app.kubernetes.io/name=argo-rollouts\nReplicas:           2 desired | 2 updated | 2 total | 2 available | 0 unavailable\nStrategyType:       Recreate\nMinReadySeconds:    0\nPod Template:\n  Labels:           app.kubernetes.io/component=rollouts-controller\n                    app.kubernetes.io/instance=argo-rollouts\n                    app.kubernetes.io/name=argo-rollouts\n  Service Account:  argo-rollouts\n  Containers:\n   argo-rollouts:\n    Image:       quay.io/argoproj/argo-rollouts:v1.4.0\n    Ports:       8090/TCP, 8080/TCP\n    Host Ports:  0/TCP, 0/TCP\n    Args:\n      --leader-elect\n    Liveness:     http-get http://:healthz/healthz delay=30s timeout=10s period=20s #success=1 #failure=3\n    Readiness:    http-get http://:metrics/metrics delay=15s timeout=4s period=5s #success=1 #failure=3\n    Environment:  &lt;none&gt;\n    Mounts:       &lt;none&gt;\n  Volumes:        &lt;none&gt;\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   argo-rollouts-648bff954f (2/2 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  118s  deployment-controller  Scaled up replica set argo-rollouts-648bff954f to 2\n</code></pre> </p> <p>pods <pre><code>$ kubectl get pods -n argo-rollouts\nNAME                             READY   STATUS    RESTARTS   AGE\nargo-rollouts-648bff954f-4svmt   1/1     Running   0          4m22s\nargo-rollouts-648bff954f-bm994   1/1     Running   0          4m22s\n</code></pre> </p> </li> <li> <p>argo-rollouts controller\u8d77\u52d5\u6642\u306e\u30ed\u30b0\u3092\u78ba\u8a8d</p> <ul> <li> <p><code>leader election</code> \u304c\u52d5\u4f5c\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d     leader\u3068\u306a\u3063\u305fcontroller log <pre><code>$ kubectl logs -n argo-rollouts argo-rollouts-648bff954f-4svmt\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Argo Rollouts starting\" version=v1.4.0+e40c9fe\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Creating event broadcaster\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Setting up event handlers\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Setting up experiments event handlers\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Setting up analysis event handlers\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Leaderelection get id argo-rollouts-648bff954f-4svmt_01e7fcf9-d81e-4e6b-b311-15706341d8af\"\nI0129 08:46:03.959656       1 leaderelection.go:248] attempting to acquire leader lease argo-rollouts/argo-rollouts-controller-lock...\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Starting Healthz Server at 0.0.0.0:8080\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Starting Metric Server at 0.0.0.0:8090\"\nI0129 08:46:03.999015       1 leaderelection.go:258] successfully acquired lease argo-rollouts/argo-rollouts-controller-lock\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"New leader elected: argo-rollouts-648bff954f-4svmt_01e7fcf9-d81e-4e6b-b311-15706341d8af\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"I am the new leader: argo-rollouts-648bff954f-4svmt_01e7fcf9-d81e-4e6b-b311-15706341d8af\"\ntime=\"2023-01-29T08:46:03Z\" level=info msg=\"Starting Controllers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Waiting for controller's informer caches to sync\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started controller\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Starting analysis workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started 30 analysis workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Starting Experiment workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started Experiment workers\"\ntime=\"2023-01-29T08:46:04Z\" level=warning msg=\"Controller is running.\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Starting Ingress workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Starting Service workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started Ingress workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started Service workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Starting Rollout workers\"\ntime=\"2023-01-29T08:46:04Z\" level=info msg=\"Started rollout workers\"\n</code></pre> </p> <p>leader\u3067\u306f\u306a\u304f\u5f85\u6a5f\u3068\u306a\u3063\u305fcontroller log <pre><code>$ kubectl logs -n argo-rollouts argo-rollouts-648bff954f-bm994\ntime=\"2023-01-29T08:46:06Z\" level=info msg=\"Argo Rollouts starting\" version=v1.4.0+e40c9fe\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Creating event broadcaster\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Setting up event handlers\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Setting up experiments event handlers\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Setting up analysis event handlers\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Leaderelection get id argo-rollouts-648bff954f-bm994_e3f2195c-58be-4086-8d08-1264405dcd59\"\nI0129 08:46:07.397481       1 leaderelection.go:248] attempting to acquire leader lease argo-rollouts/argo-rollouts-controller-lock...\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Starting Healthz Server at 0.0.0.0:8080\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"Starting Metric Server at 0.0.0.0:8090\"\ntime=\"2023-01-29T08:46:07Z\" level=info msg=\"New leader elected: argo-rollouts-648bff954f-4svmt_01e7fcf9-d81e-4e6b-b311-15706341d8af\"\n</code></pre> </p> </li> </ul> </li> </ol>"},{"location":"addons/argo/argo-rollouts/install/#kubectl-plugin","title":"kubectl plugin","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/kubectl-plugin/<ul> <li> <p><code>kubectl argo rollouts</code> \u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u3059\u308b     <pre><code>curl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-linux-arm64\nchmod +x kubectl-argo-rollouts-linux-arm64\nsudo mv kubectl-argo-rollouts-linux-arm64 /usr/local/bin/kubectl-argo-rollouts\n</code></pre></p> <pre><code>$ kubectl argo rollouts version\nkubectl-argo-rollouts: v1.4.0+e40c9fe\n  BuildDate: 2023-01-09T20:20:38Z\n  GitCommit: e40c9fe8a2f7fee9d8ee1c56b4c6c7b983fce135\n  GitTreeState: clean\n  GoVersion: go1.19.4\n  Compiler: gc\n  Platform: linux/arm64\n</code></pre> </li> </ul> </li> </ul>"},{"location":"addons/argo/argo-rollouts/install/#ui-dashboard","title":"UI Dashboard","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/dashboard/</li> <li>https://argoproj.github.io/argo-rollouts/generated/kubectl-argo-rollouts/kubectl-argo-rollouts_dashboard/</li> <li>https://github.com/argoproj/argo-helm/tree/main/charts/argo-rollouts#ui-dashboard</li> <li>install<ol> <li>Values\u3092\u8ffd\u52a0\u3057\u3066helm install\u3059\u308b<ul> <li><code>dashboard.enabled</code></li> <li><code>dashboard.service.type</code> (metallb\u306b\u3088\u308bIP\u30a2\u30c9\u30ec\u30b9\u3092\u6255\u3044\u51fa\u3057\u3001Service\u306eLoadBalancer IP\u30a2\u30c9\u30ec\u30b9\u3068\u3059\u308b)     <pre><code>helm upgrade -i argo-rollouts argo/argo-rollouts --namespace argo-rollouts --create-namespace \\\n  --set dashboard.enabled=true --set dashboard.service.type=LoadBalancer\n</code></pre></li> </ul> </li> <li>Pod     <pre><code>$ kubectl get pods -n argo-rollouts\nNAME                                       READY   STATUS    RESTARTS       AGE\nargo-rollouts-648bff954f-4svmt             1/1     Running   1 (4h5m ago)   4h37m\nargo-rollouts-648bff954f-bm994             1/1     Running   0              4h37m\nargo-rollouts-dashboard-6cc9c45468-2hxlq   1/1     Running   0              12m\n</code></pre></li> <li>Service     <pre><code>$ kubectl get service -n argo-rollouts\nNAME                      TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)          AGE\nargo-rollouts-dashboard   LoadBalancer   10.32.0.153   192.168.3.203   3100:30809/TCP   51s\n\n$ kubectl describe service -n argo-rollouts\nName:                     argo-rollouts-dashboard\nNamespace:                argo-rollouts\nLabels:                   app.kubernetes.io/component=rollouts-dashboard\n                          app.kubernetes.io/instance=argo-rollouts\n                          app.kubernetes.io/managed-by=Helm\n                          app.kubernetes.io/name=argo-rollouts\n                          app.kubernetes.io/part-of=argo-rollouts\n                          app.kubernetes.io/version=v1.4.0\n                          helm.sh/chart=argo-rollouts-2.22.1\nAnnotations:              meta.helm.sh/release-name: argo-rollouts\n                          meta.helm.sh/release-namespace: argo-rollouts\nSelector:                 app.kubernetes.io/component=rollouts-dashboard,app.kubernetes.io/instance=argo-rollouts,app.kubernetes.io/name=argo-rollouts\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.32.0.153\nIPs:                      10.32.0.153\nLoadBalancer Ingress:     192.168.3.203\nPort:                     dashboard  3100/TCP\nTargetPort:               3100/TCP\nNodePort:                 dashboard  30809/TCP\nEndpoints:                &lt;none&gt;\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:\n  Type    Reason       Age   From                Message\n  ----    ------       ----  ----                -------\n  Normal  IPAllocated  90s   metallb-controller  Assigned IP [\"192.168.3.203\"]\n</code></pre></li> <li>\u30d6\u30e9\u30a6\u30b6<ul> <li></li> <li></li> </ul> </li> </ol> </li> </ul>"},{"location":"addons/argo/argo-rollouts/traffic-management/","title":"Traffic Management","text":""},{"location":"addons/argo/argo-rollouts/traffic-management/#traffic-management","title":"Traffic Management","text":""},{"location":"addons/argo/argo-rollouts/traffic-management/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://argoproj.github.io/argo-rollouts/features/traffic-management/</li> <li>https://argoproj.github.io/argo-rollouts/features/traffic-management/alb/</li> <li>https://argoproj.github.io/argo-rollouts/getting-started/alb/</li> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/</li> <li>https://aws.amazon.com/jp/blogs/news/using-aws-load-balancer-controller-for-blue-green-deployment-canary-deployment-and-a-b-testing/</li> </ul>"},{"location":"addons/argo/argo-rollouts/traffic-management/#traffic-management_1","title":"Traffic Management\u3068\u306f","text":"<p>Argo Rollouts\u3067deploy\u4e2d\u306e\u65b0version application\u3078\u6d41\u3059traffic\u306e\u5272\u5408\u3092\u5236\u5fa1\u3067\u304d\u307e\u3059\u3002 getting-started \u306e\u4f8b\u3067\u3082\u3042\u308b\u901a\u308a\u3001default\u3067\u306f\u65b0version application\u3068\u65e7version application\u306eService selecttor\u306b\u7d10\u4ed8\u304fReplicaSet\u306ePod\u6570\u3092\u5909\u52d5\u3055\u305b\u308b\u3053\u3068\u3067traffic\u306e\u5272\u5408\u3092\u5236\u5fa1\u3057\u307e\u3059\u3002 Argo Rollouts\u306e <code>Traffic Management</code> \u3067\u306f\u30b5\u30fc\u30d3\u30b9\u30e1\u30c3\u30b7\u30e5(<code>AWS ALB Ingress Controller</code> \u3084 <code>Nginx Ingress Controller</code> \u542b\u3080) \u3068\u9023\u643a\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002</p> <p>Traffic Management\u306f\u5927\u304d\u304f3\u3064\u306e\u624b\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002</p> <ol> <li>Raw percentages<ul> <li>\u65b0version\u306bN%\u3001\u65e7version\u306bN% \u3068\u3044\u3063\u305f\u5272\u5408\u3067\u6307\u5b9a\u3059\u308b</li> <li>ALB Ingress, Nginx Ingress\u3082\u5bfe\u5fdc<ul> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/nginx/nginx.go</li> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/alb/alb.go</li> </ul> </li> </ul> </li> <li>Header-based routing<ul> <li>\u4efb\u610f\u306eheader\u304c\u30ea\u30af\u30a8\u30b9\u30c8\u306b\u542b\u307e\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u65b0version\u306b\u6d41\u3059</li> <li>ALB Ingress\u5bfe\u5fdc<ul> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/nginx/nginx.go</li> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/alb/alb.go</li> </ul> </li> </ul> </li> <li>Mirrored traffic<ul> <li>\u307e\u3063\u305f\u304f\u540c\u3058traffic\u3092\u65b0version\u306b\u3082\u6d41\u3059\u3002\u305f\u3060\u3057\u3001response\u306f\u7121\u8996\u3055\u308c\u308b</li> <li>ALB Ingress, Nginx Ingress\u3082\u975e\u5bfe\u5fdc<ul> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/nginx/nginx.go</li> <li>https://github.com/argoproj/argo-rollouts/blob/v1.4.0/rollout/trafficrouting/alb/alb.go</li> </ul> </li> </ul> </li> </ol>"},{"location":"addons/argo/argo-rollouts/traffic-management/#_2","title":"\u8a2d\u5b9a","text":""},{"location":"addons/argo/argo-rollouts/traffic-management/#alb","title":"ALB\u306e\u5834\u5408","text":"<ul> <li> <p>https://argoproj.github.io/argo-rollouts/features/traffic-management/alb/#usage</p> <ul> <li> <p>Rollout</p> <ul> <li><code>spec.strategy.canary.canaryService</code></li> <li><code>spec.strategy.canary.stableService</code></li> <li> <p><code>spec.strategy.canary.trafficRouting</code> (ServiceMesh\u306b\u3088\u3063\u3066parameter\u304c\u7570\u306a\u308b)</p> <pre><code>spec:\n  ...\n  strategy:\n    canary:\n      canaryService: canary-service\n      stableService: stable-service\n      trafficRouting:\n        alb:\n          ingress: ingress\n          servicePort: 443\n</code></pre> </li> </ul> </li> <li> <p>Ingress</p> <ul> <li> <p><code>spec.rules</code> \u4ee5\u4e0b\u306erule\u304c Rollout \u306e <code>spec.strategy.canary.stableService</code> \u3068\u4e00\u81f4\u3059\u308brule\u3067deploy\u3055\u308c\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</p> <pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress\n  annotations:\n    kubernetes.io/ingress.class: alb\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /*\n        backend:\n          # serviceName must match either: canary.trafficRouting.alb.rootService (if specified),\n          # or canary.stableService (if rootService is omitted)\n          serviceName: stable-service\n          # servicePort must be the value: use-annotation\n          # This instructs AWS Load Balancer Controller to look to annotations on how to direct traffic\n          servicePort: use-annotation\n</code></pre> </li> <li> <p>rollout\u304cdeploy\u4e2d</p> <ul> <li>ALB Controller\u304c\u7406\u89e3\u3067\u304d\u308bJSON payload\u3092\u542b\u3080 <code>alb.ingress.kubernetes.io/actions.&lt;SERVICE-NAME&gt;</code> annotation\u3092Rollouts Controller\u304cIngress\u30ea\u30bd\u30fc\u30b9\u306b\u6ce8\u5165\u3057\u307e\u3059     <pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/actions.stable-service: |\n      {\n        \"Type\":\"forward\",\n        \"ForwardConfig\":{\n          \"TargetGroups\":[\n            {\n                \"Weight\":10,\n                \"ServiceName\":\"canary-service\",\n                \"ServicePort\":\"80\"\n            },\n            {\n                \"Weight\":90,\n                \"ServiceName\":\"stable-service\",\n                \"ServicePort\":\"80\"\n            }\n          ]\n        }\n      }\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /*\n        backend:\n          serviceName: stable-service\n          servicePort: use-annotation\n</code></pre></li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/cert-manager/about/","title":"cert-manager","text":""},{"location":"addons/cert-manager/about/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://cert-manager.io</li> <li>https://github.com/cert-manager</li> <li>https://aws.amazon.com/jp/about-aws/whats-new/2022/01/acm-kubernetes-cert-manager-plugin-production/</li> <li>https://blog.manabusakai.com/2021/10/custom-resources-for-cert-manager/</li> <li>https://zenn.dev/masaaania/articles/e54119948bbaa2</li> </ul>"},{"location":"addons/cert-manager/about/#about","title":"About","text":"<ul> <li>TLS\u8a3c\u660e\u66f8\u3084\u767a\u884c\u8005(Let's Encrypt \u306a\u3069)\u3092kubernetes resources\u3068\u3057\u3066\u8868\u73fe\u3067\u304d\u308bkubernetes addons<ul> <li>\u8a3c\u660e\u66f8\u304c\u6709\u52b9\u3067\u6700\u65b0\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3001\u6709\u52b9\u671f\u9650\u304c\u5207\u308c\u308b\u524d\u306b\u8a2d\u5b9a\u3055\u308c\u305f\u6642\u9593\u3067\u8a3c\u660e\u66f8\u306e\u66f4\u65b0\u3092\u8a66\u307f\u307e\u3059</li> <li>\u767a\u884c\u8005(issuers)\u306f\u8907\u6570\u306eProvider\u3092\u30b5\u30dd\u30fc\u30c8     </li> </ul> </li> </ul>"},{"location":"addons/cert-manager/about/#acmeautomated-certificate-management-environment","title":"ACME(Automated Certificate Management Environment)","text":"<ul> <li>https://cert-manager.io/docs/configuration/acme/</li> <li><code>ACME</code> \u3068\u306fX.509\u8a3c\u660e\u66f8\u306e\u30c9\u30e1\u30a4\u30f3\u691c\u8a3c\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3001\u304a\u3088\u3073\u7ba1\u7406\u3092\u81ea\u52d5\u5316\u3059\u308b\u305f\u3081\u306e\u6a19\u6e96\u30d7\u30ed\u30c8\u30b3\u30eb<ul> <li>X.509\u8a3c\u660e\u66f8\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306b\u306fCSR\u3092\u8a8d\u8a3c\u5c40\u306b\u9001\u4fe1\u3059\u308b\u306a\u3069\u69d8\u3005\u306a\u624b\u7d9a\u304d\u304c\u3042\u308a\u3001\u66f4\u65b0\u4f5c\u696d\u542b\u3081\u3066\u7169\u96d1\u3067\u3059\u3002   <code>ACME</code> \u306f\u305d\u308c\u3089\u306e\u624b\u7d9a\u304d\u3092\u81ea\u52d5\u5316\u3057\u305f\u3082\u306e\u3067\u3059\u3002</li> <li><code>cert-manager</code> \u306f<code>ACME</code> \u306b\u5bfe\u5fdc\u3057\u305fIssuer(Let's Encrypt \u306a\u3069) \u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001   User\u306f <code>Certificate</code> resource\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3067 <code>cert-manager</code> \u304cX.509\u8a3c\u660e\u66f8\u3092\u81ea\u52d5\u7684\u306b\u53d6\u5f97\u3057\u307e\u3059\u3002(\u5f8c\u8ff0\u3059\u308bChallenges\u306f\u884c\u3046\u5fc5\u8981\u306f\u3042\u308b)</li> </ul> </li> </ul>"},{"location":"addons/cert-manager/about/#challenges","title":"Challenges","text":"<ul> <li>https://cert-manager.io/docs/configuration/acme/#solving-challenges</li> <li><code>Challenges</code> \u3068\u306f\u8a3c\u660e\u66f8\u3092\u8981\u6c42\u3057\u305f\u30af\u30e9\u30a4\u30a2\u30f3\u30c8(user)\u304c\u5f53\u8a72\u30c9\u30e1\u30a4\u30f3\u306e\u6240\u6709\u8005\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306e\u4ed5\u7d44\u307f\u3067\u3059<ul> <li>cert-manager\u306f\u3001<code>HTTP01</code> \u304a\u3088\u3073 <code>DNS01</code> challenges\u3068\u3044\u3046\u691c\u8a3c\u65b9\u6cd5\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u307e\u3059 (refs: Which ACME Challenge Type Should I Use? HTTP-01 or DNS-01?)<ul> <li>https://cert-manager.io/docs/configuration/acme/http01/<ul> <li>Issuer\u304cToken\u3092\u767a\u884c\u3057\u3001\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u304c\u767a\u884c\u3055\u308c\u305fToken\u3084\u9375\u306efingerprint\u306a\u3069\u5fc5\u8981\u306a\u60c5\u5831\u3092\u8a18\u8f09\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u3092\u6240\u6709\u30c9\u30e1\u30a4\u30f3\u306e <code>http://&lt;YOUR_DOMAIN&gt;/.well-known/acme-challenge/&lt;TOKEN&gt;</code> \u306b\u914d\u7f6e\u3059\u308b\u3053\u3068\u3067Issuer\u306f\u8a3c\u660e\u66f8\u3092\u8981\u6c42\u3057\u305f\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3068\u30c9\u30e1\u30a4\u30f3\u306e\u6240\u6709\u8005\u304c\u540c\u4e00\u3067\u3042\u308b\u3053\u3068\u3092\u691c\u8a3c\u3057\u307e\u3059</li> <li>\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u304c\u6240\u6709\u3059\u308b\u30c9\u30e1\u30a4\u30f3\u306eWeb\u30b5\u30fc\u30d0\u4e0a\u306b\u30d5\u30a1\u30a4\u30eb\u3092\u914d\u7f6e\u3057\u3001\u304b\u3064 <code>80/HTTP</code> \u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</li> </ul> </li> <li>https://cert-manager.io/docs/configuration/acme/dns01/<ul> <li>Issuer\u304cToken\u3092\u767a\u884c\u3057\u3001\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u304c\u767a\u884c\u3055\u308c\u305fToken\u3084\u9375\u306efingerprint\u306a\u3069\u5fc5\u8981\u306a\u60c5\u5831\u3092\u8a18\u8f09\u3057\u305fTXT\u30ec\u30b3\u30fc\u30c9 <code>_acme-challenge.&lt;YOUR_DOMAIN&gt;</code> \u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u3067Issuer\u306f\u8a3c\u660e\u66f8\u3092\u8981\u6c42\u3057\u305f\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3068\u30c9\u30e1\u30a4\u30f3\u306e\u6240\u6709\u8005\u304c\u540c\u4e00\u3067\u3042\u308b\u3053\u3068\u3092\u691c\u8a3c\u3057\u307e\u3059</li> <li><code>HTTP01</code> \u3068\u306f\u7570\u306a\u308aWeb\u30b5\u30fc\u30d0\u3078\u306e <code>80/HTTP</code> \u30a2\u30af\u30bb\u30b9\u306f\u4e0d\u8981</li> <li>\u30c9\u30e1\u30a4\u30f3\u3092\u7ba1\u7406\u3059\u308bDNS\u30b5\u30fc\u30d0\u3067TXT\u30ec\u30b3\u30fc\u30c9\u306e\u7ba1\u7406\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</li> <li>wildcard\u8a3c\u660e\u66f8\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/cert-manager/about/#lets-encrypt","title":"\u8a3c\u660e\u66f8\u306e\u767a\u884c\u8005\u3068\u3057\u3066 <code>Let's Encrypt</code> \u3092\u767b\u9332\u3059\u308b","text":"<ul> <li>https://cert-manager.io/docs/configuration/acme/</li> <li>https://letsencrypt.org/ja/docs/challenge-types/</li> </ul>"},{"location":"addons/cert-manager/about/#_2","title":"\u8a3c\u660e\u66f8\u306e\u53d6\u5f97","text":"<ul> <li>https://cert-manager.io/docs/concepts/certificate/</li> </ul>"},{"location":"addons/cert-manager/about/#_3","title":"About","text":"<ul> <li>https://cert-manager.io/docs/configuration/acme/dns01/route53/</li> </ul>"},{"location":"addons/cert-manager/install/","title":"cert-manager","text":""},{"location":"addons/cert-manager/install/#_1","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ul> <li>https://cert-manager.io/docs/installation/helm/</li> <li>https://cert-manager.io/docs/usage/kubectl-plugin/</li> </ul>"},{"location":"addons/keda/about_keda/","title":"KEDA","text":""},{"location":"addons/keda/about_keda/#references","title":"References","text":"<ul> <li>https://keda.sh/</li> <li>https://github.com/kedacore/keda</li> <li>https://github.com/kedacore/keda/blob/main/CREATE-NEW-SCALER.md</li> <li>https://tech.quickguard.jp/posts/pod-autoscaling-using-prometheus/</li> <li>https://learn.microsoft.com/ja-jp/azure/aks/keda-about</li> <li>https://www.bionconsulting.com/blog/using-keda-to-trigger-hpa-with-prometheus-metrics</li> <li>https://aws.amazon.com/jp/blogs/mt/proactive-autoscaling-kubernetes-workloads-keda-metrics-ingested-into-aws-amp/</li> </ul>"},{"location":"addons/keda/about_keda/#about-keda","title":"About KEDA","text":"<ul> <li> <p><code>KEDA(Kubernetes Event-driven Autoscaling)</code></p> <ul> <li>\u30a4\u30d9\u30f3\u30c8\u99c6\u52d5\u306ePod Autoscaler</li> <li>Architecture<ul> <li>https://keda.sh/docs/2.9/concepts/#architecture </li> </ul> </li> <li> <p>KEDA \u306f <code>HPA(Horizontal Pod Autoscaler)</code> \u3092\u7f6e\u304d\u63db\u3048\u308b\u3082\u306e\u3067\u306f\u306a\u304f\u3001HPA\u3068\u5354\u8abf\u3057\u3066\u52d5\u4f5c\u3057\u307e\u3059</p> <ul> <li> <p><code>ScaledObject</code> \u30ea\u30bd\u30fc\u30b9\u4f5c\u6210\u6642\u306bHPA\u30ea\u30bd\u30fc\u30b9\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408\u306f\u4f75\u305b\u3066\u4f5c\u6210\u3057\u3066\u304f\u308c\u307e\u3059</p> <p>Info</p> <p>KEDA\u306b\u3088\u3063\u3066\u4f5c\u6210\u3055\u308c\u305fHPA\u30ea\u30bd\u30fc\u30b9\u306f<code>ScaledObject</code> \u30ea\u30bd\u30fc\u30b9\u524a\u9664\u6642\u306b\u4f75\u305b\u3066\u524a\u9664\u3055\u308c\u307e\u3059</p> <ul> <li>KEDA\u306fHPA\u4f5c\u6210\u6642\u306b.metadata.ownerReferences \u3092\u4ed8\u52a0\u3057\u307e\u3059\u3002<code>.metadata.ownerReferences</code> \u306f\u81ea\u8eab\u304c\u5f93\u5c5e\u3059\u308b\u89aa\u30ea\u30bd\u30fc\u30b9\u306b\u95a2\u3059\u308b\u60c5\u5831\u304c\u683c\u7d0d\u3055\u308c\u3001kube-controller-manager\u306e Kubernetes garbage-collection \u306b\u3088\u3063\u3066\u89aa\u30ea\u30bd\u30fc\u30b9\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u524a\u9664\u3055\u308c\u307e\u3059\u3002</li> <li> <p>KEDA\u304c\u4f5c\u6210\u3057\u305fHPA\u306emanifests\u306b <code>.metadata.ownerReferences</code> \u304c\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d   HPA\u30ea\u30bd\u30fc\u30b9\u306e.metadata.ownerReferences <pre><code>$ kubectl get hpa keda-hpa-nginx-scaledobject -o yaml | yq .metadata.ownerReferences -y\n- apiVersion: keda.sh/v1alpha1\n  blockOwnerDeletion: true\n  controller: true\n  kind: ScaledObject\n  name: nginx-scaledobject\n  uid: 38ffaf3a-8033-4d97-837a-a919e7f9eaec\n</code></pre> </p> </li> <li> <p><code>ScaledObject</code> \u30ea\u30bd\u30fc\u30b9\u524a\u9664\u76f4\u5f8c\u306bgarbage-collection\u306b\u3088\u308aHPA\u3082\u524a\u9664\u3055\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d   <code>ScaledObject</code> \u30ea\u30bd\u30fc\u30b9\u524a\u9664\u76f4\u5f8c\u306ekube-controler-manager garbage-collection log <pre><code>I1217 16:16:29.579726       1 garbagecollector.go:379] according to the absentOwnerCache, object 84edf0d8-5d3f-418a-996c-24c218549866's owner keda.sh/v1alpha1/ScaledObject, nginx-scaledobject d\noes not exist in namespace default\nI1217 16:16:29.579819       1 garbagecollector.go:518] classify references of [autoscaling/v1/HorizontalPodAutoscaler, namespace: default, name: keda-hpa-nginx-scaledobject, uid: 84edf0d8-5d3f-418a-996c-24c218549866].\nsolid: []v1.OwnerReference(nil)\ndangling: []v1.OwnerReference{v1.OwnerReference{APIVersion:\"keda.sh/v1alpha1\", Kind:\"ScaledObject\", Name:\"nginx-scaledobject\", UID:\"1e910cb9-5cf0-4109-966f-599c3676e7ff\", Controller:(*bool)(0x4000b5b0e5), BlockOwnerDeletion:(*bool)(0x4000b5b0e6)}}\nwaitingForDependentsDeletion: []v1.OwnerReference(nil)\nI1217 16:16:29.579916       1 garbagecollector.go:580] \"Deleting object\" object=\"default/keda-hpa-nginx-scaledobject\" objectUID=84edf0d8-5d3f-418a-996c-24c218549866 kind=\"HorizontalPodAutoscaler\" propagationPolicy=Background\nI1217 16:16:29.580128       1 request.go:1181] Request Body: {\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"preconditions\":{\"uid\":\"84edf0d8-5d3f-418a-996c-24c218549866\"},\"propagationPolicy\":\"Background\"}\nI1217 16:16:29.581134       1 round_trippers.go:435] curl -v -XDELETE  -H \"User-Agent: kube-controller-manager/v1.22.0 (linux/arm64) kubernetes/c2b5237/system:serviceaccount:kube-system:generic-garbage-collector\" -H \"Accept: application/vnd.kubernetes.protobuf,application/json\" 'https://k8s-master:6443/apis/autoscaling/v1/namespaces/default/horizontalpodautoscalers/keda-hpa-nginx-scaledobject'\nI1217 16:16:29.584188       1 graph_builder.go:632] GraphBuilder process object: events.k8s.io/v1/Event, namespace default, name nginx-scaledobject.1731a0d8c5b4e8ed, uid 424a0f3c-9565-4950-9c61-ef8fb1d8a081, event type add, virtual=false\nI1217 16:16:29.593137       1 graph_builder.go:632] GraphBuilder process object: autoscaling/v1/HorizontalPodAutoscaler, namespace default, name keda-hpa-nginx-scaledobject, uid 84edf0d8-5d3f-418a-996c-24c218549866, event type delete, virtual=false\nI1217 16:16:29.593255       1 resource_quota_monitor.go:355] QuotaMonitor process object: autoscaling/v1, Resource=horizontalpodautoscalers, namespace default, name keda-hpa-nginx-scaledobject, uid 84edf0d8-5d3f-418a-996c-24c218549866, event type delete\nI1217 16:16:29.593686       1 round_trippers.go:454] DELETE https://k8s-master:6443/apis/autoscaling/v1/namespaces/default/horizontalpodautoscalers/keda-hpa-nginx-scaledobject 200 OK in 12 milliseconds\n</code></pre> </p> </li> </ul> </li> <li> <p><code>ScaledObject</code> \u30ea\u30bd\u30fc\u30b9\u3067\u65e2\u306b\u4f5c\u6210\u6e08\u307fHPA\u30ea\u30bd\u30fc\u30b9\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u306a\u306e\u3067\u3001\u65e2\u306bHPA\u304c\u52d5\u4f5c\u3059\u308b\u74b0\u5883\u306b\u5bfe\u3057\u3066\u3082\u5c0e\u5165\u53ef\u80fd\u3067\u3059</p> </li> </ul> </li> </ul> </li> <li> <p>KEDA\u306e\u5f79\u5272</p> <ol> <li> <p>Agent</p> <ul> <li> <p>Kubernetes Deployment\u3092\u30a2\u30af\u30c6\u30a3\u30d6\u306b\u3057\u305f\u308a\u3001\u975e\u30a2\u30af\u30c6\u30a3\u30d6\u306b\u3057\u305f\u308a\u3057\u3066\u3001\u30a4\u30d9\u30f3\u30c8\u306a\u3057\u3067\u30bc\u30ed\u304b\u3089\u30b9\u30b1\u30fc\u30eb\u3055\u305b\u307e\u3059   \u3053\u308c\u306f\u3001KEDA\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u969b\u306b\u5b9f\u884c\u3055\u308c\u308b <code>keda-operator</code> \u30b3\u30f3\u30c6\u30ca\u306e\u4e3b\u8981\u306a\u5f79\u5272\u306e1\u3064\u3067\u3059\u3002</p> <p>Info</p> <p>KEDA\u304c\u76f4\u63a5HPA\u306eReplicas\u3092\u66f4\u65b0\u3059\u308b\u8a33\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002 Metrics Adapter\u306e\u9805\u3067\u3082\u8a18\u8f09\u306e\u901a\u308a\u3001HPA\u306fKEDA\u304c\u516c\u958b\u3059\u308bcustom metrics\u3092\u53c2\u7167\u3057scale\u3057\u307e\u3059\u3002</p> </li> </ul> </li> <li> <p>Metrics Adapter</p> <ul> <li>https://keda.sh/docs/2.9/operate/metrics-server/</li> <li>HPA\u3067\u5229\u7528\u53ef\u80fd\u306ametrics\u3068\u3057\u3066kubernetes-sigs/metrics-server \u3067\u53d6\u5f97\u53ef\u80fd\u306aCPU\u3084Memory\u306e\u4f7f\u7528\u7387(metrics.k8s.io)\u306e\u4ed6\u306b\u3082custom metrics(custom.metrics.k8s.io, or external.metrics.k8s.io)(e.g. Prometheus\u3084KEDA) \u306b\u3082\u5bfe\u5fdc\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3089custom metrics\u3092HPA\u304c\u53c2\u7167\u3059\u308b\u305f\u3081\u306b\u306f\u4ef2\u4ecb\u5f79\u3068\u306a\u308bMetrics Adapter\u304c\u5fc5\u8981\u3067\u3059\u3002(refs https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics)</li> <li> <p>metrics-server\u3068\u3057\u3066event source\u304b\u3089\u53d6\u5f97\u3057\u305f\u30c7\u30fc\u30bf\u3092\u516c\u958b\u3057\u307e\u3059   HPA\u306fKEDA\u304c\u516c\u958b\u3059\u308bmetrics server\u304b\u3089\u5024\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002   \u3053\u308c\u306f\u3001KEDA\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u969b\u306b\u5b9f\u884c\u3055\u308c\u308b <code>keda-operator-metrics-apiserver</code> \u30b3\u30f3\u30c6\u30ca\u306e\u5f79\u5272\u3067\u3059\u3002</p> <p>Warning</p> <p>KEDA v2.9.0 \u3067 keda-operator-metrics-apiserver\u306f <code>deprecated</code> \u3068\u306a\u308a\u3001<code>keda-operator</code> \u30b3\u30f3\u30c6\u30ca\u304cmetrics server\u306e\u5f79\u5272\u3082\u62c5\u3044\u307e\u3059</p> </li> </ul> </li> </ol> </li> </ul>"},{"location":"addons/keda/about_keda/#custom-resources","title":"Custom Resources","text":"<ul> <li>https://keda.sh/docs/2.9/concepts/scaling-deployments/</li> </ul>"},{"location":"addons/keda/about_keda/#scaledobject","title":"ScaledObject","text":"<ul> <li>https://keda.sh/docs/2.9/concepts/scaling-deployments/</li> <li>https://github.com/kedacore/keda/blob/main/apis/keda/v1alpha1/scaledobject_types.go</li> <li><code>Deployments</code> \u3084 <code>StatefulSets</code> \u306a\u3069\u306eworkloads\u3092scale\u3055\u305b\u308b\u305f\u3081\u306etriiger\u3084\u5bfe\u8c61workloads\u3092\u6307\u5b9a\u3059\u308bcustom resources<ul> <li>custom resource workloads(e.g. argo-rollouts) \u306escale\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059<ul> <li>KEDA scales any CustomResource that implements Scale subresource #703</li> <li>\u5bfe\u8c61\u3068\u306a\u308bCustom Resource\u304c/scale subresource\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/keda/about_keda/#scaledjob","title":"ScaledJob","text":"<ul> <li><code>Job</code> workloads\u3092scale\u3055\u305b\u308b\u305f\u3081\u306etriiger\u3084\u5bfe\u8c61workloads\u3092\u6307\u5b9a\u3059\u308bcustom resources</li> <li>https://keda.sh/docs/2.0/concepts/scaling-jobs/#scaledjob-spec</li> <li>https://github.com/kedacore/keda/blob/main/apis/keda/v1alpha1/scaledjob_types.go</li> </ul>"},{"location":"addons/keda/about_keda/#authentication","title":"Authentication","text":"<ul> <li>https://keda.sh/docs/2.9/concepts/authentication/</li> <li>event source\u304b\u3089metrics\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306b\u8a8d\u8a3c\u60c5\u5831\u3084secret\u60c5\u5831\u304c\u5fc5\u8981\u3068\u306a\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059(e.g. datadog\u306eapp key/api key)<ol> <li>ScaledObject\u3054\u3068\u306b\u8a2d\u5b9a\u3059\u308b<ul> <li>\u30b3\u30f3\u30c6\u30ca\u304b\u3089\u53c2\u7167\u53ef\u80fd\u306asecret\u3092\u5229\u7528\u53ef\u80fd(ScaledObject\u304b\u3089secret\u3092\u76f4\u63a5\u53c2\u7167\u306f\u3067\u304d\u306a\u3044)</li> <li>\u5229\u7528\u53ef\u80fd\u304b\u3069\u3046\u304b\u306fscaler\u306b\u4f9d\u308a\u305d\u3046<ul> <li>MySQL scaler\u306e<code>passwordFromEnv</code></li> <li>RabbitMQ scaler \u306e <code>host</code> (\u63a5\u7d9a\u30db\u30b9\u30c8\u304a\u3088\u3073user/pass \u3092\u8a18\u8f09\u3057\u305f\u6587\u5b57\u5217) \u306esecret\u3092\u4f5c\u6210\u3057\u3001host parameter\u306bsecret-key-name\u3092\u6307\u5b9a\u3059\u308b</li> </ul> </li> </ul> </li> <li>namespace\u3054\u3068\u306b\u5229\u7528\u53ef\u80fd\u306a <code>TriggerAuthentication</code> \u30ea\u30bd\u30fc\u30b9\u3092\u5229\u7528\u3059\u308b</li> <li>cluster wide\u306b\u5229\u7528\u53ef\u80fd\u306a <code>ClusterTriggerAuthentication</code> \u30ea\u30bd\u30fc\u30b9\u3092\u5229\u7528\u3059\u308b</li> </ol> </li> </ul>"},{"location":"addons/keda/about_keda/#triggerauthentication","title":"TriggerAuthentication","text":"<ul> <li>https://github.com/kedacore/keda/blob/main/apis/keda/v1alpha1/triggerauthentication_types.go</li> </ul>"},{"location":"addons/keda/about_keda/#clustertriggerauthentication","title":"ClusterTriggerAuthentication","text":"<ul> <li>https://github.com/kedacore/keda/blob/main/apis/keda/v1alpha1/triggerauthentication_types.go</li> </ul>"},{"location":"addons/keda/fallback/","title":"KEDA","text":"<p>\u696d\u52d9\u3067fallback\u306b\u3064\u3044\u3066\u8abf\u67fb\u3059\u308b\u6a5f\u4f1a\u304c\u3042\u308a\u307e\u3057\u305f\u306e\u3067\u81ea\u5206\u306a\u308a\u306b\u6574\u7406\u3057\u3066\u304a\u304d\u307e\u3059\u3002</p>"},{"location":"addons/keda/fallback/#fallback","title":"Fallback","text":"<ul> <li> <p>https://keda.sh/docs/2.9/concepts/scaling-deployments/#fallback</p> <ul> <li>Trigger\u3067\u6307\u5b9a\u3057\u305fevent source\u304b\u3089metrics\u304c\u53d6\u5f97\u3067\u304d\u306a\u3044\u5834\u5408\u306e\u4ee3\u66ff\u52d5\u4f5c</li> <li>KEDA\u304c\u516c\u958b\u3059\u308bcustom metrics(<code>/metrics</code>) \u3067\u53d6\u5f97\u53ef\u80fd\u306a <code>metric value</code> \u3068 <code>replicas</code> \u306e\u5024\u3092(\u6b63\u3057\u304f\u53d6\u5f97\u30fb\u8a08\u7b97\u3067\u304d\u306a\u3044\u305f\u3081) \u6b63\u898f\u5316\u3057\u305f\u5024\u3067\u8fd4\u3059</li> </ul> <p>Warning</p> <p>fallback\u3092\u4f7f\u7528\u3059\u308b\u969b\u306e\u6ce8\u610f\u70b9</p> <ul> <li><code>spec.triggers.metricType</code> \u304c <code>AverageValue</code> \u3067\u3042\u308b\u5834\u5408\u306b\u9650\u3089\u308c\u307e\u3059<ul> <li>refs v2.9.1</li> <li>CPU/Memory scaler\u3084 <code>spec.triggers.metricType</code> \u304c <code>Value</code> \u3067\u3042\u308bscaler\u306f\u672a\u30b5\u30dd\u30fc\u30c8</li> </ul> </li> <li><code>ScaledObjects</code> \u3067\u306e\u307f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u307e\u3059<ul> <li><code>ScaledJobs</code> \u306f\u672a\u30b5\u30dd\u30fc\u30c8</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/keda/fallback/#fallback-behavior","title":"Fallback behavior","text":"<ol> <li>event source\u304b\u3089metrics\u53d6\u5f97\u306b\u5931\u6557\u3057\u305f\u5834\u5408\u3001<code>NumberOfFailures</code> \u306b1\u3092\u52a0\u7b97\u3059\u308b<ul> <li>https://github.com/kedacore/keda/blob/v2.9.1/pkg/fallback/fallback.go#L63</li> </ul> </li> <li><code>NumberOfFailures &gt; spec.fallback.failureThreshold</code> \u306e\u5834\u5408\u3001fallback\u51e6\u7406\u3092\u547c\u3073\u51fa\u3059<ul> <li>https://github.com/kedacore/keda/blob/v2.9.1/pkg/fallback/fallback.go#L74-L75</li> </ul> </li> <li><code>metric value</code> \u3068 <code>replicas</code> \u3068\u3057\u3066\u6b63\u898f\u5316\u3057\u305f\u5024\u3092\u8fd4\u3059<ul> <li>\u6b63\u898f\u5316\u306e\u305f\u3081\u306e\u6570\u5f0f\u306f\u5f8c\u8ff0</li> </ul> </li> </ol>"},{"location":"addons/keda/fallback/#metric-value","title":"<code>metric value</code>","text":"<ul> <li> <p>\u4ee5\u4e0b\u8a08\u7b97\u5f0f\u3067\u6c42\u3081\u307e\u3059</p> <ol> <li> <p>https://keda.sh/docs/2.9/concepts/scaling-deployments/#fallback</p> <ul> <li> <p>https://github.com/kedacore/keda/blob/v2.9.1/pkg/fallback/fallback.go#L105 <pre><code>target metric value * fallback replicas\n</code></pre></p> <p>fallback\u52d5\u4f5c\u6642\u306e <code>target metric value</code> \u306f <code>AverageValue</code> \u3067\u3042\u308b</p> <ul> <li>https://github.com/kedacore/keda/blob/v2.9.1/pkg/fallback/fallback.go#L102</li> <li>https://github.com/kedacore/keda/blob/v2.9.1/pkg/scaling/cache/scalers_cache.go#L344-L361</li> </ul> <p>e.g. (cloudwatch scaler)</p> <pre><code>spec.triggers.targetMetricValue: 100\ntarget metric value: 50\nfallback replicas: 2\n</code></pre> <p>(100 / 2) * 2 = 100</p> </li> </ul> </li> <li> <p>https://keda.sh/docs/2.9/concepts/scaling-deployments/#triggers</p> <ul> <li>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</li> <li><code>With AverageValue, the value returned from the custom metrics API is divided by the number of Pods before being compared to the target.</code></li> <li> <p>\u6700\u521d\u306btarget metric value\u306e <code>AverageValue</code> \u3092\u6c42\u3081\u3001fallback replicas\u3067\u639b\u3051\u3066\u3044\u308b</p> <ul> <li>https://github.com/kedacore/keda/blob/v2.9.1/pkg/fallback/fallback.go#L102</li> <li>https://github.com/kedacore/keda/blob/v2.9.1/pkg/scaling/cache/scalers_cache.go#L344-L361<ul> <li>metricSpecs ExternalMetricSource \u304c\u5165\u308b\u305f\u3081\u8981\u7d20\u6570\u306f\u5e38\u306b <code>2</code> \u306b\u306a\u308b?(\u63a8\u6e2c)</li> </ul> </li> </ul> <pre><code>metric value / number of pod\n</code></pre> <p>e.g.</p> <pre><code>target metric value: 100\nfallback replicas: 2\n</code></pre> <p>(100 * 2) / 2 = 100</p> <p>\u3053\u306eexample\u306e\u5834\u5408\u3001fallback\u304c\u52d5\u4f5c\u3057\u305f\u5834\u5408\u306emetric value\u306f <code>100</code> \u3068\u306a\u308a\u307e\u3059</p> </li> </ul> </li> </ol> </li> </ul>"},{"location":"addons/keda/fallback/#replicas","title":"<code>replicas</code>","text":"<ul> <li>\u4ee5\u4e0b\u8a08\u7b97\u5f0f\u3067\u6c42\u3081\u307e\u3059<ol> <li> <p>https://keda.sh/docs/2.9/concepts/scaling-deployments/#triggers <pre><code>metric value / target metric value\n(`spec.fallback.replicas` \u3068\u540c\u5024\u3068\u306a\u308b)\n</code></pre></p> <p>e.g.</p> <pre><code>target metric value: 100\nfallback replicas: 2\n</code></pre> <p>(100 * 2) / 100 = 2</p> <p>\u3053\u306eexample\u306e\u5834\u5408\u3001fallback\u304c\u52d5\u4f5c\u3057\u305f\u5834\u5408\u306ereplicas\u306f <code>2</code> \u3068\u306a\u308a\u307e\u3059</p> </li> </ol> </li> </ul>"},{"location":"addons/keda/fallback/#_1","title":"\u691c\u8a3c","text":"<ol> <li>deploy<ul> <li>Install KEDA &gt; \u52d5\u4f5c\u78ba\u8a8d \u3067\u4f7f\u7528\u3057\u305fmanifests\u3092\u4f7f\u7528     <pre><code>kubectl apply -f deployment.yaml\n</code></pre></li> </ul> </li> <li> <p>Deployment\u30ea\u30bd\u30fc\u30b9\u306e <code>Replicas</code> \u3092\u524a\u9664</p> <ul> <li>HPA\u30ea\u30bd\u30fc\u30b9\u306e <code>Replicas</code> \u3068 Deployment\u30ea\u30bd\u30fc\u30b9\u306e <code>Replicas</code> \u304c\u7af6\u5408\u3057\u3066\u3057\u307e\u3046</li> <li> <p>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#migrating-deployments-and-statefulsets-to-horizontal-autoscaling</p> <p>1. <code>kubectl.kubernetes.io/last-applied-configuration</code> annotations\u306b <code>spec.replicas</code> \u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d <pre><code>$ kubectl get deployment nginx-deployment -o yaml | yq .metadata.annotations\ndeployment.kubernetes.io/revision: \"1\"\nkubectl.kubernetes.io/last-applied-configuration: |\n  {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"name\":\"nginx-deployment\",\"namespace\":\"default\"},\"spec\":{\"replicas\":2,\"selector\":{\"matchLabels\":{\"app\":\"nginx\"}},\"template\":{\"metadata\":{\"annotations\":{\"prometheus.io/port\":\"9113\",\"prometheus.io/scrape\":\"true\"},\"labels\":{\"app\":\"nginx\"}},\"spec\":{\"containers\":[{\"image\":\"nginx:1.14.2\",\"name\":\"nginx\",\"ports\":[{\"containerPort\":80}],\"volumeMounts\":[{\"mountPath\":\"/etc/nginx/nginx.conf\",\"name\":\"nginx-conf\",\"readOnly\":true,\"subPath\":\"nginx.conf\"}]},{\"args\":[\"-nginx.scrape-uri=http://localhost/nginx_status\"],\"image\":\"nginx/nginx-prometheus-exporter:0.11.0\",\"name\":\"nginx-exporter\",\"ports\":[{\"containerPort\":9113}]}],\"volumes\":[{\"configMap\":{\"items\":[{\"key\":\"nginx.conf\",\"path\":\"nginx.conf\"}],\"name\":\"nginx-conf\"},\"name\":\"nginx-conf\"}]}}}}\n</code></pre> </p> <p>2. <code>kubectl.kubernetes.io/last-applied-configuration</code> annotations\u306e <code>spec.replicas</code> \u3092\u524a\u9664 <pre><code>kubectl apply edit-last-applied deployment/nginx-deployment\n</code></pre> </p> <p>3. <code>kubectl.kubernetes.io/last-applied-configuration</code> annotations\u304b\u3089 <code>spec.replicas</code> \u304c\u524a\u9664\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d <pre><code>$ kubectl get deployment nginx-deployment -o yaml | yq .metadata.annotations\ndeployment.kubernetes.io/revision: \"1\"\nkubectl.kubernetes.io/last-applied-configuration: |\n  {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"name\":\"nginx-deployment\",\"namespace\":\"default\"},\"spec\":{\"selector\":{\"matchLabels\":{\"app\":\"nginx\"}},\"template\":{\"metadata\":{\"annotations\":{\"prometheus.io/port\":\"9113\",\"prometheus.io/scrape\":\"true\"},\"labels\":{\"app\":\"nginx\"}},\"spec\":{\"containers\":[{\"image\":\"nginx:1.14.2\",\"name\":\"nginx\",\"ports\":[{\"containerPort\":80}],\"volumeMounts\":[{\"mountPath\":\"/etc/nginx/nginx.conf\",\"name\":\"nginx-conf\",\"readOnly\":true,\"subPath\":\"nginx.conf\"}]},{\"args\":[\"-nginx.scrape-uri=http://localhost/nginx_status\"],\"image\":\"nginx/nginx-prometheus-exporter:0.11.0\",\"name\":\"nginx-exporter\",\"ports\":[{\"containerPort\":9113}]}],\"volumes\":[{\"configMap\":{\"items\":[{\"key\":\"nginx.conf\",\"path\":\"nginx.conf\"}],\"name\":\"nginx-conf\"},\"name\":\"nginx-conf\"}]}}}}\n</code></pre> </p> <p>4. <code>deployment.yaml</code> \u306eDeployment manifests\u304b\u3089 <code>spec.replicas</code> \u3092\u524a\u9664\u3059\u308b <pre><code>vim deployment.yaml\n\n\u3084\n\nsed -i -e '/replicas:\\s2/d' deployment.yaml\n\n\u306a\u3069\n</code></pre> </p> </li> </ul> </li> <li> <p>scaledObject\u3092\u78ba\u8a8d</p> <ul> <li><code>Spec</code> \u306e\u8a2d\u5b9a\u304c\u60f3\u5b9a\u901a\u308a\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d</li> <li><code>Status &gt; Conditions</code> \u306e <code>Type: Fallback</code> \u3067Fallback\u304c\u767a\u751f\u3057\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d     scaledObject <pre><code>$ kubectl describe scaledObject nginx-scaledobject\nName:         nginx-scaledobject\nNamespace:    default\nLabels:       deploymentName=nginx-deployment\n              scaledobject.keda.sh/name=nginx-scaledobject\nAnnotations:  &lt;none&gt;\nAPI Version:  keda.sh/v1alpha1\nKind:         ScaledObject\nMetadata:\n  Creation Timestamp:  2023-02-03T14:44:15Z\n  Finalizers:\n    finalizer.keda.sh\n  Generation:  1\n  Managed Fields:\n    API Version:  keda.sh/v1alpha1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:finalizers:\n          .:\n          v:\"finalizer.keda.sh\":\n        f:labels:\n          f:scaledobject.keda.sh/name:\n    Manager:      keda\n    Operation:    Update\n    Time:         2023-02-03T14:44:15Z\n    API Version:  keda.sh/v1alpha1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        .:\n        f:externalMetricNames:\n        f:hpaName:\n        f:originalReplicaCount:\n        f:scaleTargetGVKR:\n          .:\n          f:group:\n          f:kind:\n          f:resource:\n          f:version:\n        f:scaleTargetKind:\n    Manager:      keda\n    Operation:    Update\n    Subresource:  status\n    Time:         2023-02-03T14:44:15Z\n    API Version:  keda.sh/v1alpha1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          .:\n          f:kubectl.kubernetes.io/last-applied-configuration:\n        f:labels:\n          .:\n          f:deploymentName:\n      f:spec:\n        .:\n        f:fallback:\n          .:\n          f:failureThreshold:\n          f:replicas:\n        f:maxReplicaCount:\n        f:minReplicaCount:\n        f:scaleTargetRef:\n          .:\n          f:name:\n        f:triggers:\n    Manager:      kubectl-client-side-apply\n    Operation:    Update\n    Time:         2023-02-03T14:44:15Z\n    API Version:  keda.sh/v1alpha1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        f:conditions:\n        f:health:\n          .:\n          f:s0-prometheus-nginx_http_requests_total:\n            .:\n            f:numberOfFailures:\n            f:status:\n    Manager:         keda-adapter\n    Operation:       Update\n    Subresource:     status\n    Time:            2023-02-03T14:44:31Z\n  Resource Version:  19302409\n  UID:               20ef1bf8-5476-4e17-ac31-2b33e73c758a\nSpec:\n  Fallback:\n    Failure Threshold:  3\n    Replicas:           5\n  Max Replica Count:    5\n  Min Replica Count:    1\n  Scale Target Ref:\n    Name:  nginx-deployment\n  Triggers:\n    Metadata:\n      Metric Name:     nginx_http_requests_total\n      Query:           sum(rate(nginx_http_requests_total{app=\"nginx\"}[2m]))\n      Server Address:  http://prometheus-server.monitoring.svc.cluster.local\n      Threshold:       3\n    Type:              prometheus\nStatus:\n  Conditions:\n    Message:  ScaledObject is defined correctly and is ready for scaling\n    Reason:   ScaledObjectReady\n    Status:   True\n    Type:     Ready\n    Message:  Scaling is not performed because triggers are not active\n    Reason:   ScalerNotActive\n    Status:   False\n    Type:     Active\n    Message:  No fallbacks are active on this scaled object\n    Reason:   NoFallbackFound\n    Status:   False\n    Type:     Fallback\n  External Metric Names:\n    s0-prometheus-nginx_http_requests_total\n  Health:\n    s0-prometheus-nginx_http_requests_total:\n      Number Of Failures:  0\n      Status:              Happy\n  Hpa Name:                keda-hpa-nginx-scaledobject\n  Original Replica Count:  1\n  Scale Target GVKR:\n    Group:            apps\n    Kind:             Deployment\n    Resource:         deployments\n    Version:          v1\n  Scale Target Kind:  apps/v1.Deployment\nEvents:\n  Type    Reason              Age   From           Message\n  ----    ------              ----  ----           -------\n  Normal  KEDAScalersStarted  36m   keda-operator  Started scalers watch\n  Normal  ScaledObjectReady   36m   keda-operator  ScaledObject is ready for scaling\n</code></pre> </li> </ul> </li> <li> <p>MetricsProvider\u306e<code>Prometheus</code> \u3092down\u3055\u305b\u307e\u3059</p> <ul> <li> <p><code>prometheus-server</code> \u306ereplicas\u3092 <code>0</code> \u306b\u5909\u66f4\u3059\u308b     1. prometheus-server pod\u304c1\u3064\u8d77\u52d5\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d <pre><code>$ kubectl get deployments -n monitoring prometheus-server\nNAME                READY   UP-TO-DATE   AVAILABLE   AGE\nprometheus-server   1/1     1            1           96d\n</code></pre> </p> <p>2. prometheus-server deployment\u306ereplicas\u30920\u306b\u5909\u66f4\u3059\u308b <pre><code>$ kubectl edit deployments -n monitoring prometheus-server\ndeployment.apps/prometheus-server edited\n</code></pre> </p> <p>3. prometheus-server pod\u304c0\u3068\u306a\u3063\u305f\u3053\u3068\u3092\u78ba\u8a8d <pre><code>$ kubectl get deployments -n monitoring prometheus-server\nNAME                READY   UP-TO-DATE   AVAILABLE   AGE\nprometheus-server   0/0     0            0           96d\n</code></pre> </p> </li> </ul> </li> <li> <p>Fallback\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> <ul> <li> <p><code>scaledObject</code> \u30ea\u30bd\u30fc\u30b9\u306e <code>Status</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u306b\u5909\u5316\u3092\u78ba\u8a8d     1. <code>Health</code> \u306e <code>Number Of Failures</code> \u304c <code>1</code> \u306b\u30ab\u30a6\u30f3\u30c8\u3055\u308c\u3001<code>Status</code> \u304c <code>Happy</code> \u304b\u3089 <code>Failing</code> \u306b\u5909\u5316 <pre><code>$ kubectl describe scaledObject nginx-scaledobject\n\nsnip...\n\nStatus:\n  Conditions:\n    Message:  ScaledObject is defined correctly and is ready for scaling\n    Reason:   ScaledObjectReady\n    Status:   True\n    Type:     Ready\n    Message:  Scaling is not performed because triggers are not active\n    Reason:   ScalerNotActive\n    Status:   False\n    Type:     Active\n    Message:  No fallbacks are active on this scaled object\n    Reason:   NoFallbackFound\n    Status:   False\n    Type:     Fallback\n  External Metric Names:\n    s0-prometheus-nginx_http_requests_total\n  Health:\n    s0-prometheus-nginx_http_requests_total:\n      Number Of Failures:  1\n      Status:              Failing\n</code></pre> </p> <p>2. <code>Number Of Failures</code>\u304c<code>2</code>\u306b\u30ab\u30a6\u30f3\u30c8\u30a2\u30c3\u30d7\u3057\u3001<code>Events</code>\u30d5\u30a3\u30fc\u30eb\u30c9\u306b<code>prometheus-server</code>\u3078\u63a5\u7d9a\u3067\u304d\u306a\u304b\u3063\u305f\u65e8\u306e\u30a8\u30e9\u30fc\u304c\u8a18\u9332\u3055\u308c\u305f\u3002\u304b\u3064\u3001<code>Status &gt; Conditions &gt; Type: Fallback</code> \u3067 <code>Status: True</code> \u3078\u5909\u5316\u304c\u3042\u3063\u305f <pre><code>$ kubectl describe scaledObject nginx-scaledobject\n\nsnip...\n\nStatus:\n  Conditions:\n    Message:  ScaledObject is defined correctly and is ready for scaling\n    Reason:   ScaledObjectReady\n    Status:   True\n    Type:     Ready\n    Message:  Scaling is not performed because triggers are not active\n    Reason:   ScalerNotActive\n    Status:   False\n    Type:     Active\n    Message:  At least one trigger is falling back on this scaled object\n    Reason:   FallbackExists\n    Status:   True\n    Type:     Fallback\n  External Metric Names:\n    s0-prometheus-nginx_http_requests_total\n  Health:\n    s0-prometheus-nginx_http_requests_total:\n      Number Of Failures:  2\n      Status:              Failing\n\nsnip...\n\nEvents:\n  Type     Reason              Age   From           Message\n  ----     ------              ----  ----           -------\n  Normal   KEDAScalersStarted  43m   keda-operator  Started scalers watch\n  Normal   ScaledObjectReady   43m   keda-operator  ScaledObject is ready for scaling\n  Warning  KEDAScalerFailed    1s    keda-operator  Get \"http://prometheus-server.monitoring.svc.cluster.local/api/v1/query?query=sum%28rate%28nginx_http_requests_total%7Bapp%3D%22nginx%22%7D%5B2m\n%5D%29%29&amp;time=2023-02-03T15:27:18Z\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\n</code></pre> </p> <p>3. <code>Number Of Failures</code>\u304c<code>3</code>\u306b\u30ab\u30a6\u30f3\u30c8\u30a2\u30c3\u30d7 <pre><code>$ kubectl describe scaledObject nginx-scaledobject\n\nsnip...\n\nStatus:\n  Conditions:\n    Message:  ScaledObject is defined correctly and is ready for scaling\n    Reason:   ScaledObjectReady\n    Status:   True\n    Type:     Ready\n    Message:  Scaling is not performed because triggers are not active\n    Reason:   ScalerNotActive\n    Status:   False\n    Type:     Active\n    Message:  No fallbacks are active on this scaled object\n    Reason:   NoFallbackFound\n    Status:   False\n    Type:     Fallback\n  External Metric Names:\n    s0-prometheus-nginx_http_requests_total\n  Health:\n    s0-prometheus-nginx_http_requests_total:\n      Number Of Failures:  3\n      Status:              Failing\n</code></pre> </p> </li> </ul> </li> <li> <p>HPA\u306eReplicas\u304cFallback\u306e\u8a2d\u5b9a\u901a\u308a\u3068\u306a\u3063\u305f\u3053\u3068\u3092\u78ba\u8a8d     get <pre><code>$ kubectl get hpa\nNAME                          REFERENCE                     TARGETS     MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-nginx-scaledobject   Deployment/nginx-deployment   0/3 (avg)   1         5         5          43m\n</code></pre> </p> <p>describe <pre><code>$ kubectl describe hpa keda-hpa-nginx-scaledobject\nName:                                                                keda-hpa-nginx-scaledobject\nNamespace:                                                           default\nLabels:                                                              app.kubernetes.io/managed-by=keda-operator\n                                                                     app.kubernetes.io/name=keda-hpa-nginx-scaledobject\n                                                                     app.kubernetes.io/part-of=nginx-scaledobject\n                                                                     app.kubernetes.io/version=2.8.1\n                                                                     deploymentName=nginx-deployment\n                                                                     scaledobject.keda.sh/name=nginx-scaledobject\nAnnotations:                                                         &lt;none&gt;\nCreationTimestamp:                                                   Fri, 03 Feb 2023 14:44:15 +0000\nReference:                                                           Deployment/nginx-deployment\nMetrics:                                                             ( current / target )\n  \"s0-prometheus-nginx_http_requests_total\" (target average value):  0 / 3\nMin replicas:                                                        1\nMax replicas:                                                        5\nDeployment pods:                                                     5 current / 5 desired\nConditions:\n  Type            Status  Reason            Message\n  ----            ------  ------            -------\n  AbleToScale     True    SucceededRescale  the HPA controller was able to update the target scale to 1\n  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from external metric s0-prometheus-nginx_http_requests_total(&amp;LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name: nginx-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},})\n  ScalingLimited  True    TooFewReplicas    the desired replica count is less than the minimum replica count\nEvents:\n  Type     Reason                        Age                From                       Message\n  ----     ------                        ----               ----                       -------\n  Warning  FailedGetExternalMetric       29s (x3 over 68s)  horizontal-pod-autoscaler  unable to get external metric default/s0-prometheus-nginx_http_requests_total/&amp;LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name: nginx-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},}: unable to fetch metrics from external metrics API: no matching metrics found for s0-prometheus-nginx_http_requests_total\n  Warning  FailedComputeMetricsReplicas  29s (x3 over 68s)  horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get s0-prometheus-nginx_http_requests_total external metric: unable to get external metric default/s0-prometheus-nginx_http_requests_total/&amp;LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name: nginx-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},}: unable to fetch metrics from external metrics API: no matching metrics found for s0-prometheus-nginx_http_requests_total\n  Normal   SuccessfulRescale             14s                horizontal-pod-autoscaler  New size: 1; reason: All metrics below target\n</code></pre> </p> </li> <li> <p>keda-operator log\u304b\u3089Fallback\u304c\u767a\u751f\u3057\u305f\u3053\u3068\u3092\u78ba\u8a8d</p> <ul> <li>\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u7e8f\u307e\u308a\u306elog</li> <li><code>Successfully set ScaleTarget replicas count to ScaledObject fallback.replicas</code> keda-operator log <pre><code>2023-02-03T15:27:18Z    ERROR   prometheus_scaler       error executing prometheus query        {\"type\": \"ScaledObject\", \"namespace\": \"default\", \"name\": \"nginx-scaledobject\", \"error\": \"Get \\\"http://prometheus-server.monitoring.svc.cluster.local/api/v1/query?query=sum%28rate%28nginx_http_requests_total%7Bapp%3D%22nginx%22%7D%5B2m%5D%29%29&amp;time=2023-02-03T15:27:15Z\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\ngithub.com/kedacore/keda/v2/pkg/scaling/cache.(*ScalersCache).IsScaledObjectActive\n        /workspace/pkg/scaling/cache/scalers_cache.go:89\ngithub.com/kedacore/keda/v2/pkg/scaling.(*scaleHandler).checkScalers\n        /workspace/pkg/scaling/scale_handler.go:278\ngithub.com/kedacore/keda/v2/pkg/scaling.(*scaleHandler).startScaleLoop\n        /workspace/pkg/scaling/scale_handler.go:149\n\n2023-02-03T15:27:22Z    ERROR   prometheus_scaler       error executing prometheus query        {\"type\": \"ScaledObject\", \"namespace\": \"default\", \"name\": \"nginx-scaledobject\", \"error\": \"Get \\\"http://prometheus-server.monitoring.svc.cluster.local/api/v1/query?query=sum%28rate%28nginx_http_requests_total%7Bapp%3D%22nginx%22%7D%5B2m%5D%29%29&amp;time=2023-02-03T15:27:18Z\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\ngithub.com/kedacore/keda/v2/pkg/scaling/cache.(*ScalersCache).IsScaledObjectActive\n        /workspace/pkg/scaling/cache/scalers_cache.go:94\ngithub.com/kedacore/keda/v2/pkg/scaling.(*scaleHandler).checkScalers\n        /workspace/pkg/scaling/scale_handler.go:278\ngithub.com/kedacore/keda/v2/pkg/scaling.(*scaleHandler).startScaleLoop\n        /workspace/pkg/scaling/scale_handler.go:149\n\n2023-02-03T15:27:22Z    ERROR   scalehandler    Error getting scale decision    {\"scaledobject.Name\": \"nginx-scaledobject\", \"scaledObject.Namespace\": \"default\", \"scaleTarget.Name\": \"nginx-deployment\", \"error\": \"Get \\\"http://prometheus-server.monitoring.svc.cluster.local/api/v1/query?query=sum%28rate%28nginx_http_requests_total%7Bapp%3D%22nginx%22%7D%5B2m%5D%29%29&amp;time=2023-02-03T15:27:18Z\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\ngithub.com/kedacore/keda/v2/pkg/scaling.(*scaleHandler).checkScalers\n        /workspace/pkg/scaling/scale_handler.go:278\ngithub.com/kedacore/keda/v2/pkg/scaling.(*scaleHandler).startScaleLoop\n        /workspace/pkg/scaling/scale_handler.go:149\n\n2023-02-03T15:27:22Z    DEBUG   events  Warning {\"object\": {\"kind\":\"ScaledObject\",\"namespace\":\"default\",\"name\":\"nginx-scaledobject\",\"uid\":\"20ef1bf8-5476-4e17-ac31-2b33e73c758a\",\"apiVersion\":\"keda.sh/v1alpha1\",\"resourceVersion\":\"19308916\"}, \"reason\": \"KEDAScalerFailed\", \"message\": \"Get \\\"http://prometheus-server.monitoring.svc.cluster.local/api/v1/query?query=sum%28rate%28nginx_http_requests_total%7Bapp%3D%22nginx%22%7D%5B2m%5D%29%29&amp;time=2023-02-03T15:27:18Z\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"}\n\n2023-02-03T15:27:22Z    INFO    scaleexecutor   Successfully set ScaleTarget replicas count to ScaledObject fallback.replicas   {\"scaledobject.Name\": \"nginx-scaledobject\", \"scaledObject.Namespace\": \"default\", \"scaleTarget.Name\": \"nginx-deployment\", \"Original Replicas Count\": 1, \"New Replicas Count\": 5}\n</code></pre> </li> </ul> </li> <li> <p>MetricsProvider\u304c\u5fa9\u65e7\u3057\u305f\u3089Metrics\u306b\u57fa\u3065\u3044\u3066Replicas\u304c\u6307\u5b9a\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059</p> </li> </ol>"},{"location":"addons/keda/install/","title":"Install KEDA","text":""},{"location":"addons/keda/install/#install","title":"Install","text":"<ul> <li>https://keda.sh/docs/latest/deploy/</li> </ul> <pre><code>helm repo add kedacore https://kedacore.github.io/charts\nhelm repo update\nhelm install keda kedacore/keda --namespace keda --create-namespace\n</code></pre> <pre><code>$ kubectl get pods -n keda -w\nNAME                                               READY   STATUS    RESTARTS      AGE\nkeda-operator-54bcdc6446-rf9rc                     1/1     Running   8 (30h ago)   2d21h\nkeda-operator-metrics-apiserver-74487bb99f-ztqcr   1/1     Running   0             2d21h\n</code></pre>"},{"location":"addons/keda/install/#_1","title":"\u52d5\u4f5c\u78ba\u8a8d","text":""},{"location":"addons/keda/install/#nginx-podscale-outin","title":"nginx pod\u3067\u30a2\u30af\u30bb\u30b9\u5897\u6e1b\u306b\u3088\u308bscale out/in \u3092\u78ba\u8a8d\u3059\u308b","text":""},{"location":"addons/keda/install/#point","title":"Point","text":"<ul> <li>HPA\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b\u524d\u306bDeployment\u30ea\u30bd\u30fc\u30b9\u306e <code>spec.replica</code> \u3092\u524a\u9664\u3059\u308b<ul> <li>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#migrating-deployments-and-statefulsets-to-horizontal-autoscaling</li> </ul> </li> <li>nginx stub_status\u3092\u6709\u52b9\u306b\u3059\u308b<ul> <li>http://nginx.org/en/docs/http/ngx_http_stub_status_module.html</li> </ul> </li> <li>nginx exporter\u3092side car\u3068\u3057\u3066\u8ffd\u52a0\u3059\u308b<ul> <li>https://github.com/nginxinc/nginx-prometheus-exporter</li> <li><code>-nginx.scrape-uri=http://localhost/nginx_status</code> \u3092\u8ffd\u52a0\u3059\u308b</li> <li>scrape\u7528Port\u3068\u3057\u3066 <code>9113</code> \u3092export\u3059\u308b</li> </ul> </li> </ul>"},{"location":"addons/keda/install/#_2","title":"\u624b\u9806","text":"<ol> <li> <p>deploy</p> <p>deployment.yaml <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-conf\ndata:\n  nginx.conf: |\n    user nginx;\n    worker_processes  1;\n    error_log  /var/log/nginx/error.log;\n    events {\n      worker_connections  1024;\n    }\n    http {\n      server {\n          listen       80;\n          server_name  _;\n\n          location / {\n              root   html;\n              index  index.html index.htm;\n          }\n\n          location /nginx_status {\n              stub_status on;\n              access_log off;\n              allow 127.0.0.1;\n              deny all;\n          }\n\n      }\n    }\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n\n  # HPA\u5c0e\u5165\u6642\u306b\u306fDeployment\u306espec.replicas\u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u63a8\u5968\u3055\u308c\u3066\u3044\u308b\n  # refs https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#api-object\n  # replicas: 2\n\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n      labels:\n        app: nginx\n    spec:\n      volumes:\n      - name: nginx-conf\n        configMap:\n          name: nginx-conf\n          items:\n            - key: nginx.conf\n              path: nginx.conf\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /etc/nginx/nginx.conf\n          readOnly: true\n          name: nginx-conf\n          subPath: nginx.conf\n      - name: nginx-exporter\n        image: nginx/nginx-prometheus-exporter:0.11.0\n        args:\n          - -nginx.scrape-uri=http://localhost/nginx_status\n        ports:\n          - containerPort: 9113\n\n---\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: nginx-scaledobject\n  labels:\n    deploymentName: nginx-deployment\n\nspec:\n  fallback:\n    failureThreshold: 3\n    replicas: 5\n  minReplicaCount: 1\n  maxReplicaCount: 5\n  scaleTargetRef:\n    name: nginx-deployment\n  triggers:\n  - type: prometheus\n    metadata:\n      serverAddress: http://prometheus-server.monitoring.svc.cluster.local\n      metricName: nginx_http_requests_total\n      query: sum(rate(nginx_http_requests_total{app=\"nginx\"}[2m]))\n      threshold: '3'\n</code></pre> </p> <pre><code>kubectl apply -f deployment.yaml\n</code></pre> </li> <li> <p>Deployment\u30ea\u30bd\u30fc\u30b9\u306e <code>Replicas</code> \u3092\u524a\u9664</p> <ul> <li>HPA\u30ea\u30bd\u30fc\u30b9\u306e <code>Replicas</code> \u3068 Deployment\u30ea\u30bd\u30fc\u30b9\u306e <code>Replicas</code> \u304c\u7af6\u5408\u3057\u3066\u3057\u307e\u3046</li> <li> <p>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#migrating-deployments-and-statefulsets-to-horizontal-autoscaling</p> <p>1. <code>kubectl.kubernetes.io/last-applied-configuration</code> annotations\u306b <code>spec.replicas</code> \u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d <pre><code>$ kubectl get deployment nginx-deployment -o yaml | yq .metadata.annotations\ndeployment.kubernetes.io/revision: \"1\"\nkubectl.kubernetes.io/last-applied-configuration: |\n  {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"name\":\"nginx-deployment\",\"namespace\":\"default\"},\"spec\":{\"replicas\":2,\"selector\":{\"matchLabels\":{\"app\":\"nginx\"}},\"template\":{\"metadata\":{\"annotations\":{\"prometheus.io/port\":\"9113\",\"prometheus.io/scrape\":\"true\"},\"labels\":{\"app\":\"nginx\"}},\"spec\":{\"containers\":[{\"image\":\"nginx:1.14.2\",\"name\":\"nginx\",\"ports\":[{\"containerPort\":80}],\"volumeMounts\":[{\"mountPath\":\"/etc/nginx/nginx.conf\",\"name\":\"nginx-conf\",\"readOnly\":true,\"subPath\":\"nginx.conf\"}]},{\"args\":[\"-nginx.scrape-uri=http://localhost/nginx_status\"],\"image\":\"nginx/nginx-prometheus-exporter:0.11.0\",\"name\":\"nginx-exporter\",\"ports\":[{\"containerPort\":9113}]}],\"volumes\":[{\"configMap\":{\"items\":[{\"key\":\"nginx.conf\",\"path\":\"nginx.conf\"}],\"name\":\"nginx-conf\"},\"name\":\"nginx-conf\"}]}}}}\n</code></pre> </p> <p>2. <code>kubectl.kubernetes.io/last-applied-configuration</code> annotations\u306e <code>spec.replicas</code> \u3092\u524a\u9664 <pre><code>kubectl apply edit-last-applied deployment/nginx-deployment\n</code></pre> </p> <p>3. <code>kubectl.kubernetes.io/last-applied-configuration</code> annotations\u304b\u3089 <code>spec.replicas</code> \u304c\u524a\u9664\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d <pre><code>$ kubectl get deployment nginx-deployment -o yaml | yq .metadata.annotations\ndeployment.kubernetes.io/revision: \"1\"\nkubectl.kubernetes.io/last-applied-configuration: |\n  {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"name\":\"nginx-deployment\",\"namespace\":\"default\"},\"spec\":{\"selector\":{\"matchLabels\":{\"app\":\"nginx\"}},\"template\":{\"metadata\":{\"annotations\":{\"prometheus.io/port\":\"9113\",\"prometheus.io/scrape\":\"true\"},\"labels\":{\"app\":\"nginx\"}},\"spec\":{\"containers\":[{\"image\":\"nginx:1.14.2\",\"name\":\"nginx\",\"ports\":[{\"containerPort\":80}],\"volumeMounts\":[{\"mountPath\":\"/etc/nginx/nginx.conf\",\"name\":\"nginx-conf\",\"readOnly\":true,\"subPath\":\"nginx.conf\"}]},{\"args\":[\"-nginx.scrape-uri=http://localhost/nginx_status\"],\"image\":\"nginx/nginx-prometheus-exporter:0.11.0\",\"name\":\"nginx-exporter\",\"ports\":[{\"containerPort\":9113}]}],\"volumes\":[{\"configMap\":{\"items\":[{\"key\":\"nginx.conf\",\"path\":\"nginx.conf\"}],\"name\":\"nginx-conf\"},\"name\":\"nginx-conf\"}]}}}}\n</code></pre> </p> <p>4. <code>deployment.yaml</code> \u306eDeployment manifests\u304b\u3089 <code>spec.replicas</code> \u3092\u524a\u9664\u3059\u308b <pre><code>vim deployment.yaml\n\n\u3084\n\nsed -i -e '/replicas:\\s2/d' deployment.yaml\n\n\u306a\u3069\n</code></pre> </p> </li> </ul> </li> <li> <p>keda-operator log\u3067<code>ScaledObject</code> \u3068 <code>HPA</code> \u304c\u4f5c\u6210\u3055\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d     keda-operator logs <pre><code>$ kubectl logs -f Deployment/keda-operator -n keda\n\nsnip...\n\n2022-11-26T15:37:46Z    INFO    Reconciling ScaledObject        {\"controller\": \"scaledobject\", \"controllerGroup\": \"keda.sh\", \"controllerKind\": \"ScaledObject\", \"scaledObject\": {\"name\":\"nginx-scaledobject\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"nginx-scaledobject\", \"reconcileID\": \"0085c4c3-03ac-48d9-8b1d-86201dfadf60\"}\n2022-11-26T15:37:46Z    INFO    Adding Finalizer for the ScaledObject   {\"controller\": \"scaledobject\", \"controllerGroup\": \"keda.sh\", \"controllerKind\": \"ScaledObject\", \"scaledObject\": {\"name\":\"nginx-scaledobject\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"nginx-scaledobject\", \"reconcileID\": \"0085c4c3-03ac-48d9-8b1d-86201dfadf60\"}\n2022-11-26T15:37:46Z    INFO    Detected resource targeted for scaling  {\"controller\": \"scaledobject\", \"controllerGroup\": \"keda.sh\", \"controllerKind\": \"ScaledObject\", \"scaledObject\": {\"name\":\"nginx-scaledobject\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"nginx-scaledobject\", \"reconcileID\": \"0085c4c3-03ac-48d9-8b1d-86201dfadf60\", \"resource\": \"apps/v1.Deployment\", \"name\": \"nginx-deployment\"}\n2022-11-26T15:37:46Z    INFO    Creating a new HPA      {\"controller\": \"scaledobject\", \"controllerGroup\": \"keda.sh\", \"controllerKind\": \"ScaledObject\", \"scaledObject\": {\"name\":\"nginx-scaledobject\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"nginx-scaledobject\", \"reconcileID\": \"0085c4c3-03ac-48d9-8b1d-86201dfadf60\", \"HPA.Namespace\": \"default\", \"HPA.Name\": \"keda-hpa-nginx-scaledobject\"}\n2022-11-26T15:37:46Z    INFO    Initializing Scaling logic according to ScaledObject Specification      {\"controller\": \"scaledobject\", \"controllerGroup\": \"keda.sh\", \"controllerKind\": \"ScaledObject\", \"scaledObject\": {\"name\":\"nginx-scaledobject\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"nginx-scaledobject\", \"reconcileID\": \"0085c4c3-03ac-48d9-8b1d-86201dfadf60\"}\n2022-11-26T15:37:46Z    INFO    Reconciling ScaledObject        {\"controller\": \"scaledobject\", \"controllerGroup\": \"keda.sh\", \"controllerKind\": \"ScaledObject\", \"scaledObject\": {\"name\":\"nginx-scaledobject\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"nginx-scaledobject\", \"reconcileID\": \"76eb89b7-5ffb-4a53-9165-9a37e184010e\"}\n2022-11-26T15:37:46Z    INFO    Reconciling ScaledObject        {\"controller\": \"scaledobject\", \"controllerGroup\": \"keda.sh\", \"controllerKind\": \"ScaledObject\", \"scaledObject\": {\"name\":\"nginx-scaledobject\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"nginx-scaledobject\", \"reconcileID\": \"645b6f98-fcb8-4710-80b2-2754e673d93b\"}\n2022-11-26T15:38:01Z    INFO    Reconciling ScaledObject        {\"controller\": \"scaledobject\", \"controllerGroup\": \"keda.sh\", \"controllerKind\": \"ScaledObject\", \"scaledObject\": {\"name\":\"nginx-scaledobject\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"nginx-scaledobject\", \"reconcileID\": \"8265c60e-948d-42f7-ae35-11cb7bf7d46f\"}\n\nsnip...\n</code></pre> </p> </li> <li> <p>\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u8a8d\u3059\u308b     HPA <pre><code>$ kubectl get hpa\nNAME                          REFERENCE                     TARGETS       MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-nginx-scaledobject   Deployment/nginx-deployment   17m/3 (avg)   1         5         2          2m45s\n\n$ kubectl get hpa keda-hpa-nginx-scaledobject -o yaml\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  annotations:\n    autoscaling.alpha.kubernetes.io/conditions: '[{\"type\":\"AbleToScale\",\"status\":\"True\",\"lastTransitionTime\":\"2022-11-26T15:38:01Z\",\"reason\":\"ReadyForNewScale\",\"message\":\"recommended\n      size matches current size\"},{\"type\":\"ScalingActive\",\"status\":\"True\",\"lastTransitionTime\":\"2022-12-10T06:28:49Z\",\"reason\":\"ValidMetricFound\",\"message\":\"the\n      HPA was able to successfully calculate a replica count from external metric\n      s0-prometheus-nginx_http_requests_total(\\u0026LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name:\n      nginx-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},})\"},{\"type\":\"ScalingLimited\",\"status\":\"False\",\"lastTransitionTime\":\"2022-11-26T16:27:54Z\",\"reason\":\"DesiredWithinRange\",\"message\":\"the\n      desired count is within the acceptable range\"}]'\n    autoscaling.alpha.kubernetes.io/current-metrics: '[{\"type\":\"External\",\"external\":{\"metricName\":\"s0-prometheus-nginx_http_requests_total\",\"metricSelector\":{\"matchLabels\":{\"scaledobject.keda.sh/name\":\"nginx-scaledobject\"}},\"currentValue\":\"0\",\"currentAverageValue\":\"16m\"}}]'\n    autoscaling.alpha.kubernetes.io/metrics: '[{\"type\":\"External\",\"external\":{\"metricName\":\"s0-prometheus-nginx_http_requests_total\",\"metricSelector\":{\"matchLabels\":{\"scaledobject.keda.sh/name\":\"nginx-scaledobject\"}},\"targetAverageValue\":\"3\"}}]'\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"keda.sh/v1alpha1\",\"kind\":\"ScaledObject\",\"metadata\":{\"annotations\":{},\"labels\":{\"deploymentName\":\"nginx-deployment\"},\"name\":\"nginx-scaledobject\",\"namespace\":\"default\"},\"spec\":{\"fallback\":{\"failureThreshold\":3,\"replicas\":5},\"maxReplicaCount\":5,\"minReplicaCount\":1,\"scaleTargetRef\":{\"name\":\"nginx-deployment\"},\"triggers\":[{\"metadata\":{\"metricName\":\"nginx_http_requests_total\",\"query\":\"sum(rate(nginx_http_requests_total{app=\\\"nginx\\\"}[2m]))\",\"serverAddress\":\"http://prometheus-server.monitoring.svc.cluster.local\",\"threshold\":\"3\"},\"type\":\"prometheus\"}]}}\n  creationTimestamp: \"2022-11-26T15:37:46Z\"\n  labels:\n    app.kubernetes.io/managed-by: keda-operator\n    app.kubernetes.io/name: keda-hpa-nginx-scaledobject\n    app.kubernetes.io/part-of: nginx-scaledobject\n    app.kubernetes.io/version: 2.8.1\n    deploymentName: nginx-deployment\n    scaledobject.keda.sh/name: nginx-scaledobject\n  name: keda-hpa-nginx-scaledobject\n  namespace: default\n  ownerReferences:\n  - apiVersion: keda.sh/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: ScaledObject\n    name: nginx-scaledobject\n    uid: 38ffaf3a-8033-4d97-837a-a919e7f9eaec\n  resourceVersion: \"12875576\"\n  uid: 02e4603e-bd98-45b2-9c39-2e89f81393a2\nspec:\n  maxReplicas: 5\n  minReplicas: 1\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\nstatus:\n  currentReplicas: 1\n  desiredReplicas: 1\n  lastScaleTime: \"2022-12-03T07:40:28Z\"metadata:\n  annotations:\n    autoscaling.alpha.kubernetes.io/conditions: '[{\"type\":\"AbleToScale\",\"status\":\"True\",\"lastTransitionTime\":\"2022-11-26T15:38:01Z\",\"reason\":\"ReadyForNewScale\",\"message\":\"recommended\n      size matches current size\"},{\"type\":\"ScalingActive\",\"status\":\"True\",\"lastTransitionTime\":\"2022-12-10T06:28:49Z\",\"reason\":\"ValidMetricFound\",\"message\":\"the\n      HPA was able to successfully calculate a replica count from external metric\n      s0-prometheus-nginx_http_requests_total(\\u0026LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name:\n      nginx-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},})\"},{\"type\":\"ScalingLimited\",\"status\":\"False\",\"lastTransitionTime\":\"2022-11-26T16:27:54Z\",\"reason\":\"DesiredWithinRange\",\"message\":\"the\n      desired count is within the acceptable range\"}]'\n    autoscaling.alpha.kubernetes.io/current-metrics: '[{\"type\":\"External\",\"external\":{\"metricName\":\"s0-prometheus-nginx_http_requests_total\",\"metricSelector\":{\"matchLabels\":{\"scaledobject.keda.sh/name\":\"nginx-scaledobject\"}},\"currentValue\":\"0\",\"currentAverageValue\":\"16m\"}}]'\n    autoscaling.alpha.kubernetes.io/metrics: '[{\"type\":\"External\",\"external\":{\"metricName\":\"s0-prometheus-nginx_http_requests_total\",\"metricSelector\":{\"matchLabels\":{\"scaledobject.keda.sh/name\":\"nginx-scaledobject\"}},\"targetAverageValue\":\"3\"}}]'\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"keda.sh/v1alpha1\",\"kind\":\"ScaledObject\",\"metadata\":{\"annotations\":{},\"labels\":{\"deploymentName\":\"nginx-deployment\"},\"name\":\"nginx-scaledobject\",\"namespace\":\"default\"},\"spec\":{\"fallback\":{\"failureThreshold\":3,\"replicas\":5},\"maxReplicaCount\":5,\"minReplicaCount\":1,\"scaleTargetRef\":{\"name\":\"nginx-deployment\"},\"triggers\":[{\"metadata\":{\"metricName\":\"nginx_http_requests_total\",\"query\":\"sum(rate(nginx_http_requests_total{app=\\\"nginx\\\"}[2m]))\",\"serverAddress\":\"http://prometheus-server.monitoring.svc.cluster.local\",\"threshold\":\"3\"},\"type\":\"prometheus\"}]}}\n  creationTimestamp: \"2022-11-26T15:37:46Z\"\n  labels:\n    app.kubernetes.io/managed-by: keda-operator\n    app.kubernetes.io/name: keda-hpa-nginx-scaledobject\n    app.kubernetes.io/part-of: nginx-scaledobject\n    app.kubernetes.io/version: 2.8.1\n    deploymentName: nginx-deployment\n    scaledobject.keda.sh/name: nginx-scaledobject\n  name: keda-hpa-nginx-scaledobject\n  namespace: default\n  ownerReferences:\n  - apiVersion: keda.sh/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: ScaledObject\n    name: nginx-scaledobject\n    uid: 38ffaf3a-8033-4d97-837a-a919e7f9eaec\n  resourceVersion: \"12875576\"\n  uid: 02e4603e-bd98-45b2-9c39-2e89f81393a2\nspec:\n  maxReplicas: 5\n  minReplicas: 1\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\nstatus:\n  currentReplicas: 1\n  desiredReplicas: 1\n  lastScaleTime: \"2022-12-03T07:40:28Z\"\n\n$ kubectl describe hpa keda-hpa-nginx-scaledobject\nName:                                                                keda-hpa-nginx-scaledobject\nNamespace:                                                           default\nLabels:                                                              app.kubernetes.io/managed-by=keda-operator\n                                                                     app.kubernetes.io/name=keda-hpa-nginx-scaledobject\n                                                                     app.kubernetes.io/part-of=nginx-scaledobject\n                                                                     app.kubernetes.io/version=2.8.1\n                                                                     deploymentName=nginx-deployment\n                                                                     scaledobject.keda.sh/name=nginx-scaledobject\nAnnotations:                                                         &lt;none&gt;\nCreationTimestamp:                                                   Sun, 27 Nov 2022 00:37:46 +0900\nReference:                                                           Deployment/nginx-deployment\nMetrics:                                                             ( current / target )\n  \"s0-prometheus-nginx_http_requests_total\" (target average value):  17m / 3\nMin replicas:                                                        1\nMax replicas:                                                        5\nDeployment pods:                                                     2 current / 2 desired\nConditions:\n  Type            Status  Reason               Message\n  ----            ------  ------               -------\n  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation\n  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from external metric s0-prometheus-nginx_http_requests_total(&amp;LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name: nginx-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},})\n  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range\nEvents:           &lt;none&gt;\n</code></pre> </p> <p>ScaledObject <pre><code>$ kubectl get scaledobject\nNAME                 SCALETARGETKIND      SCALETARGETNAME    MIN   MAX   TRIGGERS     AUTHENTICATION   READY   ACTIVE   FALLBACK   AGE\nnginx-scaledobject   apps/v1.Deployment   nginx-deployment   1     5     prometheus                    True    True     False      2m58s\n\n$ kubectl describe scaledobject nginx-scaledobject\nName:         nginx-scaledobject\nNamespace:    default\nLabels:       deploymentName=nginx-deployment\n              scaledobject.keda.sh/name=nginx-scaledobject\nAnnotations:  &lt;none&gt;\nAPI Version:  keda.sh/v1alpha1\nKind:         ScaledObject\nMetadata:\n  Creation Timestamp:  2022-11-26T15:37:46Z\n  Finalizers:\n    finalizer.keda.sh\n  Generation:  1\n  Managed Fields:\n    API Version:  keda.sh/v1alpha1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:finalizers:\n          .:\n          v:\"finalizer.keda.sh\":\n        f:labels:\n          f:scaledobject.keda.sh/name:\n    Manager:      keda\n    Operation:    Update\n    Time:         2022-11-26T15:37:46Z\n    API Version:  keda.sh/v1alpha1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          .:\n          f:kubectl.kubernetes.io/last-applied-configuration:\n        f:labels:\n          .:\n          f:deploymentName:\n      f:spec:\n        .:\n        f:fallback:\n          .:\n          f:failureThreshold:\n          f:replicas:\n        f:maxReplicaCount:\n        f:minReplicaCount:\n        f:scaleTargetRef:\n          .:\n          f:name:\n        f:triggers:\n    Manager:      kubectl-client-side-apply\n    Operation:    Update\n    Time:         2022-11-26T15:37:46Z\n    API Version:  keda.sh/v1alpha1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        f:health:\n          .:\n          f:s0-prometheus-nginx_http_requests_total:\n            .:\n            f:numberOfFailures:\n            f:status:\n    Manager:      keda-adapter\n    Operation:    Update\n    Subresource:  status\n    Time:         2022-11-26T15:38:01Z\n    API Version:  keda.sh/v1alpha1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:status:\n        .:\n        f:conditions:\n        f:externalMetricNames:\n        f:hpaName:\n        f:lastActiveTime:\n        f:originalReplicaCount:\n        f:scaleTargetGVKR:\n          .:\n          f:group:\n          f:kind:\n          f:resource:\n          f:version:\n        f:scaleTargetKind:\n    Manager:         keda\n    Operation:       Update\n    Subresource:     status\n    Time:            2022-11-26T15:39:46Z\n  Resource Version:  10027225\n  UID:               38ffaf3a-8033-4d97-837a-a919e7f9eaec\nSpec:\n  Fallback:\n    Failure Threshold:  3\n    Replicas:           5\n  Max Replica Count:    5\n  Min Replica Count:    1\n  Scale Target Ref:\n    Name:  nginx-deployment\n  Triggers:\n    Metadata:\n      Metric Name:     nginx_http_requests_total\n      Query:           sum(rate(nginx_http_requests_total{app=\"nginx\"}[2m]))\n      Server Address:  http://prometheus-server.monitoring.svc.cluster.local\n      Threshold:       3\n    Type:              prometheus\nStatus:\n  Conditions:\n    Message:  ScaledObject is defined correctly and is ready for scaling\n    Reason:   ScaledObjectReady\n    Status:   True\n    Type:     Ready\n    Message:  Scaling is performed because triggers are active\n    Reason:   ScalerActive\n    Status:   True\n    Type:     Active\n    Message:  No fallbacks are active on this scaled object\n    Reason:   NoFallbackFound\n    Status:   False\n    Type:     Fallback\n  External Metric Names:\n    s0-prometheus-nginx_http_requests_total\n  Health:\n    s0-prometheus-nginx_http_requests_total:\n      Number Of Failures:  0\n      Status:              Happy\n  Hpa Name:                keda-hpa-nginx-scaledobject\n  Last Active Time:        2022-11-26T15:41:16Z\n  Original Replica Count:  2\n  Scale Target GVKR:\n    Group:            apps\n    Kind:             Deployment\n    Resource:         deployments\n    Version:          v1\n  Scale Target Kind:  apps/v1.Deployment\nEvents:\n  Type    Reason              Age    From           Message\n  ----    ------              ----   ----           -------\n  Normal  KEDAScalersStarted  3m54s  keda-operator  Started scalers watch\n  Normal  ScaledObjectReady   3m54s  keda-operator  ScaledObject is ready for scaling\n</code></pre> </p> </li> <li> <p>\u8ca0\u8377\u3092\u304b\u3051\u308b</p> <ul> <li>nginx\u30b3\u30f3\u30c6\u30ca\u306battach\u3057\u3066 <code>http://localhost/nginx_status</code> \u306b\u5bfe\u3057\u3066\u306e\u30a2\u30af\u30bb\u30b9\u3092\u5897\u3084\u3057\u3066\u307f\u308b</li> </ul> </li> <li> <p>scaleout/scalein \u3092\u78ba\u8a8d      <pre><code>$ while true; do date ; kubectl describe hpa keda-hpa-nginx-scaledobject | grep -A 30 \"^Metrics:\" ; echo ; echo ; sleep 1 ; done\n\nsnip...\n\n2022\u5e74 11\u670827\u65e5 \u65e5\u66dc\u65e5 02\u664206\u520609\u79d2 JST\nMetrics:                                                             ( current / target )\n  \"s0-prometheus-nginx_http_requests_total\" (target average value):  33m / 3\nMin replicas:                                                        1\nMax replicas:                                                        5\nDeployment pods:                                                     1 current / 1 desired\nConditions:\n  Type            Status  Reason              Message\n  ----            ------  ------              -------\n  AbleToScale     True    ReadyForNewScale    recommended size matches current size\n  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from external metric s0-prometheus-nginx_http_requests_total(&amp;LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name: nginx-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},})\n  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range\nEvents:\n  Type    Reason             Age                From                       Message\n  ----    ------             ----               ----                       -------\n  Normal  SuccessfulRescale  59m                horizontal-pod-autoscaler  New size: 3; reason: external metric s0-prometheus-nginx_http_requests_total(&amp;LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name: nginx-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},}) above target\n  Normal  SuccessfulRescale  54m                horizontal-pod-autoscaler  New size: 4; reason: external metric s0-prometheus-nginx_http_requests_total(&amp;LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name: nginx-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},}) above target\n  Normal  SuccessfulRescale  51m                horizontal-pod-autoscaler  New size: 5; reason: external metric s0-prometheus-nginx_http_requests_total(&amp;LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name: nginx-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},}) above target\n  Normal  SuccessfulRescale  37m                horizontal-pod-autoscaler  New size: 3; reason: All metrics below target\n  Normal  SuccessfulRescale  5m45s              horizontal-pod-autoscaler  New size: 2; reason: All metrics below target\n  Normal  SuccessfulRescale  42s (x2 over 83m)  horizontal-pod-autoscaler  New size: 1; reason: All metrics below target\n</code></pre> </p> </li> <li> <p>grafana\u3067\u78ba\u8a8d</p> <ul> <li>green: <code>sum(rate(nginx_http_requests_total[10m]))</code></li> <li>yellow: <code>sum(irate(nginx_http_requests_total{app=\"nginx\"}[10m]))</code></li> <li>blue: <code>kube_deployment_status_replicas{deployment=\"nginx-deployment\"}</code> </li> </ul> </li> </ol>"},{"location":"addons/kyverno/about_kyverno/","title":"Kyverno","text":""},{"location":"addons/kyverno/about_kyverno/#about-kyverno","title":"About Kyverno","text":"<ul> <li>https://kyverno.io/docs/introduction/</li> <li>Kyverno \u306fKubernetes\u306e\u305f\u3081\u306b\u8a2d\u8a08\u3055\u308c\u305fPolicy Engine\u3067\u3059\u3002<ul> <li>Open Policy Agent(OPA) \u306eRego\u306e\u3088\u3046\u306b\u72ec\u81ea\u8a00\u8a9e\u3092\u899a\u3048\u308b\u5fc5\u8981\u306f\u306a\u304fCustomResourceDefinition(CRD)\u3067\u5b9a\u7fa9\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059</li> </ul> </li> </ul>"},{"location":"addons/kyverno/about_kyverno/#about-kyverno-policy","title":"About Kyverno Policy","text":"<ul> <li> <p>\u4ee5\u4e0b\u56f3\u306f \u3053\u3061\u3089 \u304b\u3089\u629c\u7c8b</p> <ul> <li></li> </ul> </li> <li> <p>Kyverno Policy\u306f1\u3064\u4ee5\u4e0a\u306e\u30eb\u30fc\u30eb\u306e\u30b3\u30ec\u30af\u30b7\u30e7\u30f3\u3067\u3059\u3002</p> </li> <li>1\u3064\u306e\u30eb\u30fc\u30eb\u306f2\u3064\u306e\u5ba3\u8a00\u3092\u6301\u3061\u307e\u3059<ol> <li>Policy\u9069\u7528\u5bfe\u8c61\u30ea\u30bd\u30fc\u30b9\u306e\u9078\u629e<ul> <li>Select Resources</li> </ul> </li> <li>Policy\u9069\u7528<ul> <li>Mutate Resources</li> <li>Validate Resources</li> <li>Generate Resources</li> <li>VerifyImages Resources</li> </ul> </li> </ol> </li> </ul>"},{"location":"addons/kyverno/about_kyverno/#kind-of-policy","title":"Kind of Policy","text":"<ul> <li>Cluster wide\u306b\u9069\u7528\u3055\u308c\u308b <code>ClusterPolicy</code> \u3068 namespace\u5358\u4f4d\u306b\u9069\u7528\u3055\u308c\u308b <code>Policy</code> \u304c\u3042\u308b<ul> <li>kyverno.io/v1.ClusterPolicy</li> <li>kyverno.io/v1.Policy</li> </ul> </li> </ul>"},{"location":"addons/kyverno/about_kyverno/#kind-of-resources","title":"Kind of Resources","text":""},{"location":"addons/kyverno/about_kyverno/#select-resources","title":"Select Resources","text":"<ul> <li>Mutate/VerifyImages/Validate/Generate \u306a\u3069\u306epolicy\u3092\u9069\u7528\u3059\u308b\u5bfe\u8c61\u30ea\u30bd\u30fc\u30b9\u3092\u6307\u5b9a\u3059\u308b<ul> <li>\u5bfe\u8c61\u3068\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9\u3092<code>match</code>\u3067\u3001\u5bfe\u8c61\u5916\u3068\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9\u3092<code>exclude</code> \u3067\u6307\u5b9a\u3059\u308b</li> <li>any(<code>OR</code>) \u3082\u3057\u304f\u306f all(<code>AND</code>)\u306e\u6761\u4ef6\u4e0b\u3067 resource filters \u3092\u6307\u5b9a<ul> <li><code>resources</code>: select resources by names, namespaces, kinds, label selectors, annotations, and namespace selectors.</li> <li><code>subjects</code>: select users, user groups, and service accounts</li> <li><code>roles</code>: select namespaced roles</li> <li><code>clusterRoles</code>: select cluster wide roles</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/kyverno/about_kyverno/#mutate-resources","title":"Mutate Resources","text":"<ul> <li>rule\u306b\u5fdc\u3058\u3066\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3092\u66f8\u304d\u63db\u3048\u308b<ul> <li>\u66f8\u304d\u63db\u3048\u305f\u3044\u5185\u5bb9\u306f <code>RFC 6902 JSON Patch</code>\u3001<code>a strategic merge patch</code>\u3001<code>an overlay pattern</code> \u306e\u3044\u305a\u308c\u304b\u3067\u5b9a\u7fa9\u53ef\u80fd</li> <li>RFC 6902 JSON Patch</li> <li>Strategic Merge Patch</li> <li>an overlay pattern<ul> <li>Conditional logic using anchors</li> <li>Mutate Rule Ordering</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/kyverno/about_kyverno/#validate-resources","title":"Validate Resources","text":"<ul> <li>\u3053\u308c\u304b\u3089\u9069\u7528\u3059\u308bmanifests\u3001\u307e\u305f\u306f\u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306b\u95a2\u3057\u3066\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u304c\u306a\u3044\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3059\u308b</li> <li><code>spec.validationFailureAction</code><ul> <li><code>enforce</code><ul> <li>\u65b0\u898f\u4f5c\u6210\u306e\u5834\u5408\u306f\u62d2\u5426\u3059\u308b</li> </ul> </li> <li><code>audit</code><ul> <li>\u4f5c\u6210\u306f\u62d2\u5426\u3057\u306a\u3044</li> <li><code>ClusterPolicyReport</code> \u307e\u305f\u306f <code>PolicyReport</code> \u3092\u4f5c\u6210\u3057\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u8a18\u9332\u3059\u308b</li> </ul> </li> </ul> </li> <li><code>spec.background</code><ul> <li>\u5f53\u8a72 <code>ClusterPolicy</code> \u307e\u305f\u306f <code>Policy</code> \u306b\u95a2\u3057\u3066\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u3057\u3066\u3044\u306a\u3044\u304b\u3069\u3046\u304b\u3092\u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306b\u691c\u8a3c\u3059\u308b</li> <li>(<code>enforce</code>\u306e\u5834\u5408\u3067\u3082) \u4f5c\u6210\u6e08\u307f\u30ea\u30bd\u30fc\u30b9\u306e\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u306f<code>ClusterPolicyReport</code> \u307e\u305f\u306f <code>PolicyReport</code> \u3092\u4f5c\u6210\u3057\u30dd\u30ea\u30b7\u30fc\u9055\u53cd\u3092\u8a18\u9332\u3059\u308b</li> </ul> </li> <li>Patterns<ul> <li><code>spec.rules[*].va1lidate.pattern</code></li> <li><code>spec.rules[*].va1lidate.anyPattern</code></li> </ul> </li> <li>\u30eb\u30fc\u30eb\u306e\u62d2\u5426<ul> <li><code>deny</code><ul> <li><code>validationFailureAction: enforce</code> \u3092\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b</li> <li><code>deny</code> rule\u306e <code>exclude</code> \u3067\u62d2\u5426\u3055\u305b\u305f\u304f\u306a\u3044subjects\u3084role\u3092\u6307\u5b9a\u3059\u308b\u306a\u3069\u3082\u53ef\u80fd</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/kyverno/about_kyverno/#generate-resources","title":"Generate Resources","text":"<ul> <li>\u30ea\u30bd\u30fc\u30b9\u306e\u4f5c\u6210\u30fb\u66f4\u65b0\u306b\u57fa\u3065\u3044\u3066\u8ffd\u52a0\u306e\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b</li> <li>\u30ea\u30bd\u30fc\u30b9\u4f5c\u6210\u65b9\u6cd5<ol> <li><code>spec.rules.generate.clone</code><ul> <li>Secret\u3084ConfigMap\u306a\u3069\u65e2\u5b58\u30ea\u30bd\u30fc\u30b9\u3092\u30b3\u30d4\u30fc\u3057\u305f\u3044\u5834\u5408</li> </ul> </li> <li><code>spec.rules.generate.data</code><ul> <li>rule manifests\u306b\u5b9a\u7fa9\u3055\u308c\u305f\u65b0\u898f\u4f5c\u6210\u3057\u305f\u3044\u30ea\u30bd\u30fc\u30b9</li> </ul> </li> </ol> </li> </ul> <p>Note</p> <p>kyverno policy rule\u306b\u3088\u3063\u3066\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u306f\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u7ba1\u7406\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u3002 <code>spec.rules.generate.synchronize=true</code> \u306e\u5834\u5408\u3001\u5fc5\u8981\u306a\u6a29\u9650\u3092\u6301\u3064\u30e6\u30fc\u30b6\u3084ServiceAccount\u306a\u3069\u304b\u3089\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u304c\u5909\u66f4\u30fb\u524a\u9664\u3055\u308c\u3066\u3082\u5143\u306e\u30ea\u30bd\u30fc\u30b9\u72b6\u614b\u306b\u623b\u3059\u3088\u3046\u306b\u66f4\u65b0\u30fb\u518d\u4f5c\u6210\u3057\u3066\u304f\u308c\u307e\u3059\u3002</p>"},{"location":"addons/kyverno/about_kyverno/#verifyimages-resources","title":"VerifyImages Resources","text":"<ul> <li>image\u306e\u30b7\u30b0\u30cd\u30c1\u30e3(\u7f72\u540d) \u3092\u30c1\u30a7\u30c3\u30af\u3057\u3001\u30c0\u30a4\u30b8\u30a7\u30b9\u30c8\u3092\u8ffd\u52a0\u3059\u308b</li> </ul>"},{"location":"addons/kyverno/installation_kyverno/","title":"Kyverno","text":""},{"location":"addons/kyverno/installation_kyverno/#installation-kyverno","title":"Installation Kyverno","text":""},{"location":"addons/kyverno/installation_kyverno/#kyverno-policy-engine","title":"Kyverno Policy Engine","text":"<ul> <li>https://kyverno.io/docs/installation/</li> <li>\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f\u5927\u304d\u304f\u4ee5\u4e0b2\u30d1\u30bf\u30fc\u30f3<ol> <li>helm chart</li> <li>manifests(yaml)</li> </ol> </li> </ul>"},{"location":"addons/kyverno/installation_kyverno/#_1","title":"\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8","text":"<ul> <li>\u4ee5\u4e0b\u56f3\u306f \u3053\u3061\u3089 \u304b\u3089\u629c\u7c8b<ul> <li></li> </ul> </li> </ul>"},{"location":"addons/kyverno/installation_kyverno/#tls-configuration","title":"TLS Configuration","text":"<ul> <li>Kyverno policy engine \u306fAdmission Webhook\u3068\u3057\u3066\u52d5\u4f5c\u3057\u3001kube-apiserver\u3068\u306e\u901a\u4fe1\u3067TLS\u901a\u4fe1\u3092\u884c\u3046\u305f\u3081\u306b\u8a8d\u8a3c\u5c40\u306e\u7f72\u540d\u304c\u3055\u308c\u305f\u8a3c\u660e\u66f8\u304c\u5fc5\u8981</li> <li>\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u306e\u4f5c\u6210<ul> <li>(helm charts\u306e) <code>createSelfSignedCert</code> (defailt: false)<ul> <li><code>false</code>: <code>kube-controller-manager</code> \u3067\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u4f5c\u6210</li> <li><code>true</code>: \u4f5c\u6210\u6e08\u307f\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u3092\u4f7f\u7528\u3059\u308b<ul> <li>https://kyverno.io/docs/installation/#option-2-use-your-own-ca-signed-certificate</li> </ul> </li> </ul> </li> </ul> </li> <li>\u53c2\u8003<ul> <li>https://kyverno.io/docs/installation/#customize-the-installation-of-kyverno</li> <li>https://github.com/kyverno/kyverno/blob/v1.5.1/charts/kyverno/README.md#tls-configuration</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/</li> <li>https://github.com/kyverno/kyverno/blob/v1.5.1/charts/kyverno/values.yaml#L228-L238</li> </ul> </li> </ul>"},{"location":"addons/kyverno/installation_kyverno/#webhook-configurations","title":"Webhook Configurations","text":"<ul> <li>ValicatingWebhookConfiguration</li> <li>MutatingWebhookConfiguration</li> <li>\u8d77\u52d5\u6642\u306b\u4f5c\u6210<ul> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/kyverno/main.go#L426</li> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/initContainer/main.go#L165</li> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/webhookconfig/monitor.go#L85</li> </ul> </li> <li>\u505c\u6b62\u6642\u306b\u524a\u9664<ul> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/cmd/kyverno/main.go#L533-L536</li> </ul> </li> </ul>"},{"location":"addons/kyverno/installation_kyverno/#mutatingwebhookconfigurations","title":"mutatingwebhookconfigurations","text":"<ul> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/config/config.go#L11-L66</li> <li>generateMutatingWebhook<ul> <li>constructVerifyMutatingWebhookConfig</li> <li>constructPolicyMutatingWebhookConfig</li> <li>constructDefaultMutatingWebhookConfig</li> </ul> </li> </ul>"},{"location":"addons/kyverno/installation_kyverno/#validatingwebhookconfigurations","title":"validatingwebhookconfigurations","text":"<ul> <li>https://github.com/kyverno/kyverno/blob/v1.5.2/pkg/config/config.go#L11-L66</li> <li>generateValidatingWebhook<ul> <li>constructPolicyValidatingWebhookConfig</li> <li>constructDefaultValidatingWebhookConfig</li> </ul> </li> </ul>"},{"location":"addons/kyverno/installation_kyverno/#installation-polcies","title":"Installation Polcies","text":"<ul> <li><code>Policy</code> \u3082\u3057\u304f\u306f <code>ClusterPolicy</code> \u306emanifests\u3092\u9069\u7528\u3057\u307e\u3059</li> <li>Policy\u306e\u66f8\u304d\u65b9\u3084\u63a8\u5968Policy\u306b\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u3092\u53c2\u7167<ul> <li>https://kyverno.io/policies/</li> <li>https://github.com/kyverno/policies</li> <li>Privilege mode\u3092\u8a31\u53ef\u3057\u306a\u3044Policy\u306esample</li> <li>https://aws.amazon.com/jp/blogs/news/easy-as-one-two-three-policy-management-with-kyverno-on-amazon-eks/</li> </ul> </li> </ul>"},{"location":"addons/kyverno/installation_kyverno/#uninstallation-kyverno","title":"Uninstallation Kyverno","text":"<ul> <li> <p>https://kyverno.io/docs/installation/#uninstalling-kyverno</p> <p>Warning</p> <p>https://github.com/kyverno/kyverno/issues/2750 https://github.com/kyverno/kyverno/issues/2623</p> <p>kyverno Pod\u304cdelete\u3055\u308c\u308b\u969b\u306b <code>mutatingwebhookconfigurations</code> \u3068 <code>validatingwebhookconfigurations</code> \u304c\u524a\u9664\u3055\u308c\u306a\u3044bug\u304c\u3042\u308a\u307e\u3059\u3002 (1.5.2-rc2 image\u3067fix\u3057\u305f\u3088\u3046\u3067\u3059)</p> <p>kyverno controller\u304c\u524a\u9664\u3055\u308cwebhook\u304c\u6b8b\u3063\u3066\u3044\u308b\u5834\u5408\u3001Pod\u3092\u8d77\u52d5\u3057\u3088\u3046\u3068\u3057\u305f\u969b\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u3002</p> <pre><code>warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\nError from server (InternalError): Internal error occurred: failed calling webhook \"validate.kyverno.svc-fail\": Post \"https://kyverno-svc.kyverno.svc:443/validate?timeout=10s\": service \"kyverno-svc\" not found\n</code></pre> <p>\u624b\u52d5\u3067webhook\u3092\u524a\u9664\u3059\u308b\u5834\u5408\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u3092\u53c2\u7167 https://kyverno.io/docs/installation/#clean-up-webhook-configurations</p> </li> </ul>"},{"location":"addons/kyverno/installation_kyverno/#appendix","title":"Appendix","text":""},{"location":"addons/kyverno/installation_kyverno/#kube-controller-manager","title":"<code>kube-controller-manager</code> \u3067\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u4f5c\u6210","text":"<ul> <li>https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/</li> </ul>"},{"location":"addons/openebs/about/","title":"About OpenEBS","text":""},{"location":"addons/openebs/about/#openebs","title":"OpenEBS","text":"<ul> <li><code>OpenEBS</code> \u306f\u3001Kubernetes\u30ef\u30fc\u30ab\u30fc\u30ce\u30fc\u30c9\u304c\u5229\u7528\u3067\u304d\u308b\u3042\u3089\u3086\u308b\u30b9\u30c8\u30ec\u30fc\u30b8(\u30ed\u30fc\u30ab\u30eb\u3084\u5206\u6563\u306a\u3069) \u3092Kubernetes Persistent Volumes\u306b\u5909\u63db\u3057\u307e\u3059</li> </ul>"},{"location":"addons/openebs/about/#reference","title":"Reference","text":"<ul> <li>https://openebs.io/</li> <li>https://github.com/openebs/openebs</li> <li>https://github.com/openebs/charts</li> <li>https://openebs.github.io/charts/</li> <li>https://blog.openebs.io/arming-kubernetes-with-openebs-1-b450f41e0c1f</li> <li>https://note.com/ryoma_0923/n/n7d2837212028</li> <li>https://qiita.com/ysakashita/items/8ca805cb6ac10df911be</li> <li>https://blog.cybozu.io/entry/2018/03/29/080000</li> <li>https://www.infoq.com/jp/news/2020/08/kubernetes-storage-kubera/</li> </ul>"},{"location":"addons/openebs/about/#openebs_1","title":"OpenEBS \u306b\u3064\u3044\u3066","text":"<ul> <li>MayaData\u304c\u5b9f\u88c5\u3092\u516c\u958b\u3057\u3001\u73fe\u5728\u306fCNCF sandbox project\u306eContainer Storage Interface (CSI) Provider\u3067\u3059</li> <li><code>Container Attached Storage(CAS)</code> \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u3092\u63a1\u7528\u3057\u3066\u3044\u308b</li> <li>Dynamic Provisioner\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u308b<ul> <li>Kubernetes Native\u306e local PersistentVolume volume \u306fStatic Provisioning\u306e\u307f\u3092\u30b5\u30dd\u30fc\u30c8</li> </ul> </li> </ul>"},{"location":"addons/openebs/about/#container-attached-storagecas","title":"<code>Container Attached Storage(CAS)</code>","text":"<ul> <li>https://openebs.io/docs/concepts/cas</li> <li>https://www.cncf.io/blog/2018/04/19/container-attached-storage-a-primer/</li> <li>https://www.cncf.io/blog/2020/09/22/container-attached-storage-is-cloud-native-storage-cas/</li> <li>https://www.cncf.io/online-programs/kubernetes-and-storage-kubernetes-for-storage-an-overview/</li> <li> <p>https://blog.mayadata.io/container-attached-storage-cas-vs.-shared-storage-which-one-to-choose</p> <p>In Kubernetes, shared storage is typically achieved by mounting volumes and connecting to an external filesystem or block storage solution. Container Attached Storage (CAS) is a relatively newer solution that allows Kubernetes administrators to deploy storage as containerized microservices in a cluster.</p> </li> </ul> <p><code>Container Attached Storage(CAS)</code> \u3068\u306fPod\u3067\u5229\u7528\u53ef\u80fd\u306a\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u30b3\u30f3\u30c6\u30ca\u5316\u3057\u305f\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9\u3068\u3057\u3066Kubernetes Cluster\u3078\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u4ed5\u7d44\u307f\u3067\u3059\u3002 <code>CAS</code> \u3067\u306fPersistent Volumes\u3092\u30de\u30a4\u30af\u30ed\u30b5\u30fc\u30d3\u30b9 \u30d9\u30fc\u30b9\u306e\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3068\u3057\u3066\u69cb\u6210\u3057\u307e\u3059\u3002\u305d\u306e\u969b\u3001\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3092\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30b9\u30c8\u30ec\u30fc\u30b8 \u30b3\u30f3\u30c8\u30ed\u30fc\u30e9\u3092\u3001\u72ec\u7acb\u3057\u3066\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u304a\u3088\u3073\u5b9f\u884c\u3067\u304d\u308b \u69cb\u6210\u30e6\u30cb\u30c3\u30c8\u3068\u3057\u3066\u30c7\u30d7\u30ed\u30a4\u3057\u307e\u3059\u3002(\u3064\u307e\u308a\u3001OpenEBS \u306b\u304a\u3051\u308bPV\u306fVolume\u3092\u30db\u30b9\u30c8\u3059\u308breplica pod \u3068 replica pod\u3092\u7ba1\u7406\u3059\u308bcontroller pod\u3067\u69cb\u6210\u3055\u308c\u307e\u3059)</p> <ul> <li>\u4ee5\u4e0b\u306f Deploy Jenkins with OpenEBS \u3067\u4f5c\u6210\u3055\u308c\u308b\u30b9\u30c8\u30ec\u30fc\u30b8 \u30ec\u30d7\u30ea\u30ab\u3068\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9     <pre><code>$ kubectl get pods -n openebs | grep pvc-2fee1acb\nopenebs                pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-ctrl-66d449fp8jp8   2/2     Running   0                3h58m   10.200.0.39    k8s-master   &lt;none&gt;           &lt;none&gt;\nopenebs                pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-0               1/1     Running   1 (3h34m ago)    4h16m   10.200.2.194   k8s-node2    &lt;none&gt;           &lt;none&gt;\nopenebs                pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-1               1/1     Running   1 (3h40m ago)    4h16m   10.200.0.38    k8s-master   &lt;none&gt;           &lt;none&gt;\nopenebs                pvc-2fee1acb-2d7f-4068-b56e-777eefa35e4a-jiva-rep-2               0/1     Pending   0                4h16m   &lt;none&gt;         &lt;none&gt;       &lt;none&gt;           &lt;none&gt;\n</code></pre></li> </ul>"},{"location":"addons/openebs/about/#node-disk-managerndm","title":"Node Disk Manager(NDM)","text":"<ul> <li>https://openebs.io/docs/main/concepts/ndm <ul> <li>CPU\u3084Memory\u3001Network\u306a\u3069\u3068\u540c\u3058\u3088\u3046\u306bNode\u4e0a\u306eblock device\u3092kubernetes resources(CustomResource)\u3068\u3057\u3066\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8</li> <li>DeamonSet\u3068\u3057\u3066\u5404Node\u306b\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u308b</li> <li>Node\u4e0a\u306eblock device\u3078\u306e\u30a2\u30af\u30bb\u30b9\u3059\u308b\u306e\u306b <code>/dev</code>, <code>/proc</code>, <code>/sys</code> \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u304c\u5fc5\u8981\u306a\u305f\u3081Privileged mode\u3067\u52d5\u4f5c\u3059\u308b</li> <li><code>Local PV</code> \u3068 <code>cStor PV</code> \u3067\u4f7f\u7528\u3055\u308c\u308b<ul> <li><code>JIVA PV</code> \u306fNDM\u3078\u30a2\u30af\u30bb\u30b9\u3057\u3066\u3044\u306a\u3044?</li> <li>https://openebs.io/docs/main/user-guides/ndm </li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/openebs/about/#_1","title":"\u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3","text":"<ul> <li> <p>https://openebs.io/docs/main/concepts/casengines</p> Storage Engine Status Description link <code>Local PV</code> Beta  \u5358\u4e00\u30ce\u30fc\u30c9\u3067\u5229\u7528\u53ef\u80fd\u306aVolume\u3092\u63d0\u4f9b Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8(Kubernetes Native\u306e local PersistentVolume volume \u306fStatic Provisioning(\u4e8b\u524d\u306ePV\u3092\u624b\u52d5\u4f5c\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b)) https://openebs.io/docs/concepts/localpvhttps://openebs.io/docs/main/user-guides/localpv-device <code>JIVA PV</code> Stable  \u8907\u6570\u30ce\u30fc\u30c9\u3067\u30ec\u30d7\u30ea\u30ab\u3092\u69cb\u6210\u3057\u305fVolume\u3092\u63d0\u4f9b(\u9ad8\u53ef\u7528\u6027) iSCSI Volume\u3092\u30a8\u30df\u30e5\u30ec\u30fc\u30c8 Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8 \u30b7\u30f3 \u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3092\u30b5\u30dd\u30fc\u30c8 https://openebs.io/docs/concepts/jiva <code>cStor PV</code> Beta  \u8907\u6570\u30ce\u30fc\u30c9\u3067\u30ec\u30d7\u30ea\u30ab\u3092\u69cb\u6210\u3057\u305fVolume\u3092\u63d0\u4f9b(\u9ad8\u53ef\u7528\u6027) iSCSI Volume\u3092\u30a8\u30df\u30e5\u30ec\u30fc\u30c8 Dynamic Provisioning\u3092\u30b5\u30dd\u30fc\u30c8 \u30b7\u30f3 \u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3092\u30b5\u30dd\u30fc\u30c8 Snapshot\u3092\u30b5\u30dd\u30fc\u30c8 https://openebs.io/docs/concepts/cstor <ul> <li>\u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3\u306e\u9078\u629e\u57fa\u6e96\u306b\u3064\u3044\u3066<ul> <li>https://openebs.io/docs/2.12.x/concepts/casengines#cstor-vs-jiva-vs-localpv-features-comparison</li> </ul> </li> </ul> </li> </ul>"},{"location":"addons/openebs/install/","title":"Installing OpenEBS","text":""},{"location":"addons/openebs/install/#installing-openebs","title":"Installing OpenEBS","text":"<ol> <li>helm \u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b<ul> <li>https://openebs.io/docs/user-guides/installation#installation-through-helm</li> <li>https://github.com/openebs/charts <pre><code>helm install openebs --namespace openebs openebs/openebs --create-namespace \\\n  --set cstor.enabled=true \\\n  --set jiva.enabled=true\n</code></pre></li> </ul> </li> <li> <p>StorageClass\u3092\u78ba\u8a8d\u3059\u308b     <pre><code>$ kubectl get sc\nNAME                       PROVISIONER           RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nopenebs-device             openebs.io/local      Delete          WaitForFirstConsumer   false                  28m\nopenebs-hostpath           openebs.io/local      Delete          WaitForFirstConsumer   false                  28m\nopenebs-jiva-csi-default   jiva.csi.openebs.io   Delete          Immediate              true                   28mS\n</code></pre></p> </li> <li> <p>OpenEBS\u306e\u5404Pod\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b     <pre><code>$ kubectl get pods -n openebs\nNAME                                            READY   STATUS    RESTARTS      AGE\nopenebs-cstor-admission-server-b74f5487-djhxj   1/1     Running   0             22m\nopenebs-cstor-csi-controller-0                  6/6     Running   0             21m\nopenebs-cstor-csi-node-54prc                    2/2     Running   0             28m\nopenebs-cstor-csi-node-g844x                    2/2     Running   0             22m\nopenebs-cstor-csi-node-vrm2w                    2/2     Running   0             95s\nopenebs-cstor-cspc-operator-84464fb479-vdh67    1/1     Running   0             22m\nopenebs-cstor-cvc-operator-646f6f676b-5t79q     1/1     Running   0             22m\nopenebs-jiva-csi-controller-0                   5/5     Running   3 (16m ago)   28m\nopenebs-jiva-csi-node-2hz6w                     3/3     Running   0             110s\nopenebs-jiva-csi-node-nt6jz                     3/3     Running   0             28m\nopenebs-jiva-csi-node-zm49x                     3/3     Running   0             22m\nopenebs-jiva-operator-f994f6868-w7xrl           1/1     Running   0             22m\nopenebs-localpv-provisioner-55b65f8b55-2mq9z    1/1     Running   3 (16m ago)   28m\nopenebs-ndm-4smgw                               1/1     Running   0             2m37s\nopenebs-ndm-6fpg7                               1/1     Running   0             9m7s\nopenebs-ndm-operator-6c944d87b6-5dq77           1/1     Running   0             22m\nopenebs-ndm-vm255                               1/1     Running   0             28m\n</code></pre></p> </li> </ol>"},{"location":"addons/openebs/install/#deploy-jenkins-with-openebs","title":"Deploy Jenkins with OpenEBS","text":""},{"location":"addons/openebs/install/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://blog.openebs.io/tagged/jenkins</li> <li>https://github.com/openebs/openebs/tree/main/k8s/demo/jenkins</li> </ul>"},{"location":"addons/openebs/install/#_2","title":"\u624b\u9806","text":"<ol> <li>download manifests     <pre><code>wget https://raw.githubusercontent.com/openebs/openebs/master/k8s/demo/jenkins/jenkins.yml\n</code></pre></li> <li> <p>modify manifests     <pre><code>vim jenkins.yml\n</code></pre></p> <p><pre><code>$ diff -u &lt;(curl -s https://raw.githubusercontent.com/openebs/openebs/master/k8s/demo/jenkins/jenkins.yml) &lt;(cat ./jenkins.yml)\n--- /dev/fd/63  2022-10-01 14:11:35.968241073 +0000\n+++ /dev/fd/62  2022-10-01 14:11:35.980240892 +0000\n@@ -3,7 +3,7 @@\n metadata:\n   name: jenkins-claim\n   annotations:\n-    volume.beta.kubernetes.io/storage-class: openebs-jiva-default\n+    volume.beta.kubernetes.io/storage-class: openebs-jiva-csi-default\n spec:\n   accessModes:\n     - ReadWriteOnce\n</code></pre> 1. apply manifests <pre><code>kubectl apply -f jenkins.yml\n</code></pre></p> </li> <li> <p>\u30ea\u30bd\u30fc\u30b9\u78ba\u8a8d     PersistentVolumeClaim <pre><code>$ kubectl describe PersistentVolumeClaim jenkins-claim\nName:          jenkins-claim\nNamespace:     default\nStorageClass:  openebs-jiva-csi-default\nStatus:        Bound\nVolume:        pvc-e69e2bbb-f945-4ec7-b61a-490a66596441\nLabels:        &lt;none&gt;\nAnnotations:   pv.kubernetes.io/bind-completed: yes\n               pv.kubernetes.io/bound-by-controller: yes\n                              volume.beta.kubernetes.io/storage-class: openebs-jiva-csi-default\n                                             volume.beta.kubernetes.io/storage-provisioner: jiva.csi.openebs.io\n                                             Finalizers:    [kubernetes.io/pvc-protection]\n                                             Capacity:      5G\n                                             Access Modes:  RWO\n                                             VolumeMode:    Filesystem\n                                             Used By:       jenkins-7f87d6d6d8-htjx9\n                                             Events:        &lt;none&gt;\n</code></pre> </p> <p>Service <pre><code>$ kubectl describe service jenkins-svc\nName:                     jenkins-svc\nNamespace:                default\nLabels:                   &lt;none&gt;\nAnnotations:              &lt;none&gt;\nSelector:                 app=jenkins-app\nType:                     NodePort\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.32.0.70\nIPs:                      10.32.0.70\nPort:                     &lt;unset&gt;  80/TCP\nTargetPort:               8080/TCP\nNodePort:                 &lt;unset&gt;  30792/TCP\nEndpoints:                10.200.0.45:8080\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n</code></pre> </p> <p>&gt;Deployment <pre><code>$ kubectl describe deployment jenkins\nName:                   jenkins\nNamespace:              default\nCreationTimestamp:      Fri, 30 Sep 2022 04:56:42 +0000\nLabels:                 &lt;none&gt;\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               app=jenkins-app\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=jenkins-app\n  Containers:\n   jenkins:\n    Image:        jenkins/jenkins:lts\n    Port:         8080/TCP\n    Host Port:    0/TCP\n    Environment:  &lt;none&gt;\n    Mounts:\n      /var/jenkins_home from jenkins-home (rw)\n  Volumes:\n   jenkins-home:\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n    ClaimName:  jenkins-claim\n    ReadOnly:   false\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Progressing    True    NewReplicaSetAvailable\n  Available      True    MinimumReplicasAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   jenkins-7f87d6d6d8 (1/1 replicas created)\nEvents:          &lt;none&gt;\n</code></pre> </p> <p>Pod <pre><code>$ kubectl describe pods jenkins-7f87d6d6d8-htjx9\nName:         jenkins-7f87d6d6d8-htjx9\nNamespace:    default\nPriority:     0\nNode:         k8s-master/192.168.3.50\nStart Time:   Sun, 02 Oct 2022 16:49:07 +0000\nLabels:       app=jenkins-app\n              pod-template-hash=7f87d6d6d8\nAnnotations:  &lt;none&gt;\nStatus:       Running\nIP:           10.200.0.45\nIPs:\n  IP:           10.200.0.45\nControlled By:  ReplicaSet/jenkins-7f87d6d6d8\nContainers:\n  jenkins:\n    Container ID:   containerd://9c039657f1a1dee2b4b480aaff1859c9385b9b9d2d874757ad6e0e6c47278bda\n    Image:          jenkins/jenkins:lts\n    Image ID:       docker.io/jenkins/jenkins@sha256:5508cb1317aa0ede06cb34767fb1ab3860d1307109ade577d5df871f62170214\n    Port:           8080/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 02 Oct 2022 16:49:38 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/jenkins_home from jenkins-home (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gzs4t (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             True\n  ContainersReady   True\n  PodScheduled      True\nVolumes:\n  jenkins-home:\n    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n    ClaimName:  jenkins-claim\n    ReadOnly:   false\n  kube-api-access-gzs4t:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       &lt;nil&gt;\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              &lt;none&gt;\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:                      &lt;none&gt;\n</code></pre> </p> </li> <li> <p>Web\u30a2\u30af\u30bb\u30b9\u78ba\u8a8d</p> image descrioption \u8868\u793a\u306b\u3042\u308b\u901a\u308a<code>/var/jenkins_home/secrets/initialAdminPassword</code>\u306e\u5185\u5bb9\u3092\u8cbc\u308a\u4ed8\u3051\u308b<code>kubectl exec -it jenkins-7f87d6d6d8-wx8nn -- cat /var/jenkins_home/secrets/initialAdminPassword</code> </li> </ol>"},{"location":"autoscaler/","title":"Index","text":"<ul> <li>https://speakerdeck.com/oracle4engineer/kubernetes-autoscale-deep-dive</li> </ul>"},{"location":"aws-eks/aws-load-balancer-controller/","title":"aws-load-balancer-controller","text":""},{"location":"aws-eks/aws-load-balancer-controller/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/</li> </ul>"},{"location":"cloud-native-buildpacks/about/","title":"About","text":""},{"location":"cloud-native-buildpacks/about/#cloud-nativeimage","title":"Cloud Native\u306aImage\u4e8b\u60c5","text":"<p>\u5f93\u6765\u3001container runtime\u306fDocker\u304c\u30c7\u30d5\u30a1\u30af\u30c8\u30b9\u30bf\u30f3\u30c0\u30fc\u30c9\u3067\u3057\u305f\u3002</p> <p>\u6700\u8fd1\u306eCloud Native\u306acontainer runtime\u4e8b\u60c5\u3068\u3057\u3066\u306f\u3001 Container Runtime \u306e\u30da\u30fc\u30b8\u3067\u8a18\u8f09\u3057\u305f\u901a\u308a\u3001container runtime\u306f<code>High Level Container Runtime</code> \u3068 <code>Low Level Container Runtime</code> \u3067\u5f79\u5272\u3092\u5206\u96e2\u3057\u3066\u3044\u307e\u3059\u3002 <code>Low Level Container Runtime</code> \u306fdaemon service\u3067\u306f\u306a\u304f\u30d0\u30a4\u30ca\u30ea\u3067\u63d0\u4f9b\u3055\u308c\u3001<code>High Level Container Runtime</code> \u304b\u3089\u5b9f\u884c\u3055\u308c\u307e\u3059\u3002<code>Low Level Container Runtime</code> \u306fOCI(Open Container Initiative)\u306b\u6e96\u62e0\u3057\u305f <code>config.json</code> \u306b\u751f\u6210\u3059\u3079\u304d\u30b3\u30f3\u30c6\u30ca\u306e\u30e1\u30bf\u60c5\u5831\u304c\u66f8\u304b\u308c\u3066\u3044\u308b\u306e\u3067\u305d\u308c\u3092\u57fa\u306b\u30b3\u30f3\u30c6\u30ca\u3092\u751f\u6210\u3057\u307e\u3059\u3002</p> <p>\u3053\u306e\u8fba\u308a\u306e\u8a71\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u306a\u3069\u304c\u308f\u304b\u308a\u3084\u3059\u304b\u3063\u305f\u3067\u3059\u3002</p> <ul> <li>https://www.publickey1.jp/blog/20/firecrackergvisorunikernel_container_runtime_meetup_2.html</li> <li>https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/</li> <li>https://qiita.com/mamomamo/items/ed5db2ab1555078f8a24</li> </ul> <p>\u3053\u306econtainer runtime\u306e\u8a71\u306f<code>config.json</code>\u306b\u66f8\u304b\u308c\u3066\u3044\u308b\u60c5\u5831\u3092\u57fa\u306b\u30b3\u30f3\u30c6\u30ca\u3092\u751f\u6210\u3059\u308b\u305f\u3081\u306e\u4ed5\u7d44\u307f\u3067\u3059\u3002\u30b3\u30f3\u30c6\u30ca\u60c5\u5831\u3092\u30dd\u30fc\u30bf\u30d6\u30eb\u306b\u6301\u3061\u51fa\u305b\u308b\u72b6\u614b(OCI image\u3068\u8a00\u308f\u308c\u307e\u3059\u304c\u5b9f\u614b\u306ftar archive)\u3068\u3057\u3066\u4fdd\u5b58\u3059\u308b\u4ed5\u7d44\u307f\u306fCRI(\u306e<code>High Level Container Runtime</code>)\u3068\u3057\u3066\u6a19\u6e96\u5b9f\u88c5\u3067\u306f\u306a\u3044\u305f\u3081\u5225\u9014\u8003\u616e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002(\u4f8b\u3048\u3070\u3001cri-o\u3067\u306fimage build\u3092\u975e\u30b5\u30dd\u30fc\u30c8\u3060\u3063\u305f\u308a\u3057\u307e\u3059)</p> <p>OCI image\u306eLayout\u306b\u3064\u3044\u3066\u306e\u516c\u5f0f\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u306f\u4ee5\u4e0b\u3067\u3059\u3002</p> <ul> <li>https://github.com/opencontainers/image-spec/blob/main/image-layout.md</li> </ul>"},{"location":"cloud-native-buildpacks/about/#cloud-native-buildpacks","title":"Cloud Native Buildpacks\u3068\u306f","text":"<ul> <li>https://buildpacks.io/</li> <li>https://github.com/buildpacks</li> <li>https://github.com/buildpacks/spec</li> </ul> <p><code>Cloud Native Buildpacks</code> \u306f\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092Dockerfile\u306a\u3057\u3067OCI\u6e96\u62e0\u306eimage\u3068\u3057\u3066build\u3059\u308b\u305f\u3081\u306e\u30c4\u30fc\u30eb\u3067\u3059\u3002</p> <p>\u5143\u3005\u306f <code>Buildpacks</code> \u3068\u3044\u3046\u540d\u524d\u3067Heroku\u304c\u5185\u88fd\u30c4\u30fc\u30eb\u3068\u3057\u3066\u4f5c\u3063\u305f\u3082\u306e\u3067\u3059\u304c\u30012018\u5e74\u306b CNCF Project \u3068\u3057\u3066\u516c\u958b\u3055\u308c\u307e\u3057\u305f\u3002 Heroku\u306e\u30c4\u30fc\u30eb\u540d\u3068\u898b\u5206\u3051\u308b\u305f\u3081\u3001 <code>Cloud Native Buildpacks(CNB)</code> \u3068\u8868\u73fe\u3055\u308c\u308b\u3053\u3068\u304c\u591a\u3044\u3088\u3046\u3067\u3059\u3002</p> <p></p> <p>\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304c\u914d\u7f6e\u3055\u308c\u3066\u3044\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067 Builder\u3068\u547c\u3070\u308c\u308bbuildpack\u3084build image, run image\u3092\u7d44\u307f\u5408\u308f\u305b\u305fcomponent\u3092\u6307\u5b9a\u3057\u3066 <code>pack build</code> \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u3067OCI\u6e96\u62e0\u306eimage\u3092build\u3067\u304d\u307e\u3059\u3002build\u6642\u306e\u51e6\u7406\u306f <code>Detect phase</code> \u3068 <code>Build phase</code> \u304c\u3042\u308a\u307e\u3059\u3002</p> <p>(\u4f59\u8ac7) <code>pack build</code> \u30b3\u30de\u30f3\u30c9\u3067 <code>--buildpack</code> \u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067builder\u306b\u542b\u307e\u308c\u306a\u3044buildpack\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd</p> <ol> <li>Detect phase<ul> <li>https://buildpacks.io/docs/concepts/#detect-phase</li> </ul> </li> <li>Build phase<ul> <li>https://buildpacks.io/docs/concepts/#build-phase</li> </ul> </li> </ol> <p><code>Builder</code> \u306f\u72ec\u81ea\u306b\u4f5c\u6210\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u304c\u3001Paketo Buildpacks \u306e\u3088\u3046\u306a\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u304c\u63d0\u4f9b\u3059\u308bBuilder\u3084Buildpack\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002</p> <p><code>pack builder suggest</code> \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u305f\u3068\u3053\u308d\u3001<code>Google</code>, <code>Heroku</code>, <code>Paketo Buildpacks</code> \u304csuggest\u3055\u308c\u307e\u3057\u305f\u3002</p> <pre><code>$ pack builder suggest\nSuggested builders:\n        Google:                gcr.io/buildpacks/builder:v1      Ubuntu 18 base image with buildpacks for .NET, Go, Java, Node.js, and Python\n        Heroku:                heroku/builder:22                 Base builder for Heroku-22 stack, based on ubuntu:22.04 base image\n        Heroku:                heroku/buildpacks:20              Base builder for Heroku-20 stack, based on ubuntu:20.04 base image\n        Paketo Buildpacks:     paketobuildpacks/builder:base     Ubuntu bionic base image with buildpacks for Java, .NET Core, NodeJS, Go, Python, Ruby, Apache HTTPD, NGINX and Procfile\n        Paketo Buildpacks:     paketobuildpacks/builder:full     Ubuntu bionic base image with buildpacks for Java, .NET Core, NodeJS, Go, Python, PHP, Ruby, Apache HTTPD, NGINX and Procfile\n        Paketo Buildpacks:     paketobuildpacks/builder:tiny     Tiny base image (bionic build image, distroless-like run image) with buildpacks for Java, Java Native Image and Go\n\nTip: Learn more about a specific builder with:\n        pack builder inspect &lt;builder-image&gt;\n</code></pre>"},{"location":"cloud-native-buildpacks/about/#components","title":"Components","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/</li> </ul>"},{"location":"cloud-native-buildpacks/about/#builder","title":"Builder","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/builder/ <ul> <li>1\u3064\u4ee5\u4e0a\u306ebuildpack</li> <li>stack</li> <li>build image</li> <li>run image</li> </ul> </li> <li><code>builder.toml</code><ul> <li>https://buildpacks.io/docs/reference/config/builder-config/</li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#buildpack","title":"Buildpack","text":"<ul> <li> <p>https://buildpacks.io/docs/concepts/components/buildpack/</p> <ul> <li><code>buildpack.toml</code></li> <li><code>bin/detect</code><ul> <li>\u5f53\u8a72Buildpack\u3092\u9069\u7528\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b</li> <li>e.g.<ul> <li>Ruby app\u306e\u5834\u5408\u3001 <code>Gemfile</code> \u306e\u5b58\u5728\u78ba\u8a8d\u3084 <code>.ruby-version</code> \u306eversion check\u306a\u3069 (e.g. sample)</li> </ul> </li> </ul> </li> <li><code>bin/build</code><ul> <li>\u5f53\u8a72Buildpack\u3092\u9069\u7528\u3057build\u3059\u308b</li> <li>e.g.<ul> <li>Ruby app\u306e\u5834\u5408\u3001<code>bundle install</code> \u5b9f\u884c\u306a\u3069 (e.g. sample)</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>https://buildpacks.io/docs/concepts/components/buildpack/#meta-buildpack</p> <ul> <li><code>meta-buildpack</code> \u3068\u306f\u4ed6\u306e<code>Buildpack</code>\u3092\u53c2\u7167\u3059\u308b\u9806\u5e8f\u8a2d\u5b9a\u3092\u6301\u3064 <code>buildpack.toml</code> \u3060\u3051\u6301\u3064\u3082\u306e\u3067\u3059</li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#stack","title":"Stack","text":"<ul> <li> <p>https://buildpacks.io/docs/concepts/components/stack/</p> <ul> <li><code>build image</code> \u3068 <code>run image</code> \u3067\u69cb\u6210\u3055\u308c\u307e\u3059<ul> <li><code>build image</code> \u3068\u306flifecycle(\u53ca\u3073\u305d\u308c\u306b\u3088\u308bbuildpack) \u304c\u5b9f\u884c\u3055\u308c\u308b\u30b3\u30f3\u30c6\u30ca\u74b0\u5883\u306ebase image</li> <li><code>run image</code> \u3068\u306fapplication image\u3092build\u3059\u308b\u305f\u3081\u306ebase image</li> </ul> </li> </ul> </li> <li> <p>https://buildpacks.io/docs/operator-guide/create-a-stack/</p> <ul> <li>custom stack\u3092\u4f5c\u6210\u3059\u308b\u306b\u306f<code>build image</code> \u3068 <code>run image</code>\u3092custom image\u3068\u3057\u3066\u7528\u610f\u3057\u307e\u3059<ol> <li>common base image\u306e\u4f5c\u6210</li> <li>run image\u306e\u4f5c\u6210</li> <li>build image\u306e\u4f5c\u6210</li> </ol> </li> <li> <p>Buildpack \u306e stack\u3067\u4f7f\u7528\u3059\u308bimage\u306f\u4ee5\u4e0b\u8a2d\u5b9a\u304c\u5fc5\u8981\u3067\u3059</p> <ol> <li><code>io.buildpacks.stack.id</code> Label</li> <li> <p>\u74b0\u5883\u5909\u6570</p> <ul> <li><code>CNB_USER_ID</code></li> <li><code>CNB_GROUP_ID</code></li> <li><code>CNB_STACK_ID</code></li> </ul> <p>e.g. Dockerfile <pre><code># 1. Set a common base\nFROM ubuntu:bionic as base\n\n# 2. Set required CNB information\nENV CNB_USER_ID=1000\nENV CNB_GROUP_ID=1000\nENV CNB_STACK_ID=\"io.buildpacks.samples.stacks.bionic\"\nLABEL io.buildpacks.stack.id=\"io.buildpacks.samples.stacks.bionic\"\n\n# 3. Create the user\nRUN groupadd cnb --gid ${CNB_GROUP_ID} &amp;&amp; \\\n  useradd --uid ${CNB_USER_ID} --gid ${CNB_GROUP_ID} -m -s /bin/bash cnb\n\n# 4. Install common packages\nRUN apt-get update &amp;&amp; \\\n  apt-get install -y xz-utils ca-certificates &amp;&amp; \\\n  rm -rf /var/lib/apt/lists/*\n\n# 5. Start a new run stage\nFROM base as run\n\n# 6. Set user and group (as declared in base image)\nUSER ${CNB_USER_ID}:${CNB_GROUP_ID}\n\n# 7. Start a new build stage\nFROM base as build\n\n# 8. Install packages that we want to make available at build time\nRUN apt-get update &amp;&amp; \\\n  apt-get install -y git wget jq &amp;&amp; \\\n  rm -rf /var/lib/apt/lists/* &amp;&amp; \\\n  wget https://github.com/sclevine/yj/releases/download/v5.0.0/yj-linux -O /usr/local/bin/yj &amp;&amp; \\\n  chmod +x /usr/local/bin/yj\n\n# ========== ADDED ===========\n# 9. Set user and group (as declared in base image)\nUSER ${CNB_USER_ID}:${CNB_GROUP_ID}\n</code></pre> </p> <pre><code>docker build . -t cnbs/sample-stack-build:bionic --target build\n</code></pre> </li> </ol> </li> <li> <p><code>io.buildpacks.stack.id</code> \u30e9\u30d9\u30eb\u3092\u8a2d\u5b9a\u3057\u305fimage\u3092builder\u3067\u6307\u5b9a\u3057\u307e\u3059     e.g. builder.toml <pre><code>[[buildpacks]]\n  # ...\n\n[[order]]\n  # ...\n\n[stack]\n  id = \"com.example.stack\"\n  build-image = \"example/build\"\n  run-image = \"example/run\"\n  run-image-mirrors = [\"gcr.io/example/run\", \"registry.example.com/example/run\"]\n</code></pre> </p> </li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#buildpack-group","title":"Buildpack Group","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/buildpack-group/<ul> <li>\u5b9f\u884c\u3059\u308b\u9806\u756a\u306b\u5b9a\u7fa9\u3055\u308c\u305fBuildpack\u306e\u30ea\u30b9\u30c8\u3067\u3059\u3002   <code>Buildpack</code> \u306f\u30e2\u30b8\u30e5\u30fc\u30eb\u5316\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u518d\u5229\u7528\u304c\u53ef\u80fd\u3067<code>Buildpack Group</code> \u3067\u8907\u6570\u306e<code>Buildpack</code>\u3092\u63a5\u7d9a\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002</li> <li> <p><code>buildpack.toml</code> \u3067\u4ed6Buildpack\u3092\u6307\u5b9a\u3057\u3066\u3044\u308b\u4f8b</p> <ul> <li> <p><code>samples/hello-world</code> \u3068 <code>samples/hello-moon</code> Buildpack\u3092\u6301\u3064 <code>samples/hello-universe</code> Buildpack Group</p> <ul> <li>https://github.com/buildpacks/samples/blob/main/buildpacks/hello-universe/buildpack.toml buildpack.toml <pre><code># Buildpack API version\napi = \"0.2\"\n\n# Buildpack ID and metadata\n[buildpack]\nid = \"samples/hello-universe\"\nversion = \"0.0.1\"\nname = \"Hello Universe Buildpack\"\nhomepage = \"https://github.com/buildpacks/samples/tree/main/buildpacks/hello-universe\"\n\n# Order used for detection\n[[order]]\n[[order.group]]\nid = \"samples/hello-world\"\nversion = \"0.0.1\"\n\n[[order.group]]\nid = \"samples/hello-moon\"\nversion = \"0.0.1\"\n</code></pre> </li> </ul> </li> <li> <p><code>Builder</code> \u304b\u3089 <code>[[buildpacks]]</code> \u3067 <code>samples/hello-universe</code> Buildpack Group\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067<code>samples/hello-world</code> \u3068 <code>samples/hello-moon</code> Buildpack\u3092\u5229\u7528\u3067\u304d\u308b</p> <ul> <li>https://buildpacks.io/docs/operator-guide/create-a-builder/#1-builder-configuration</li> <li><code>[[order]]</code> &gt; <code>[[order.group]]</code> \u306b\u4e26\u3093\u3067\u3044\u308b\u9806\u756a\u306bdetect/build\u304c\u5b9f\u884c\u3055\u308c\u308b     builder.toml <pre><code># Buildpacks to include in builder\n[[buildpacks]]\nuri = \"samples/buildpacks/hello-processes\"\n\n[[buildpacks]]\n# Packaged buildpacks to include in builder;\n# the \"hello-universe\" package contains the \"hello-world\" and \"hello-moon\" buildpacks\nuri = \"docker://cnbs/sample-package:hello-universe\"\n\n# Order used for detection\n[[order]]\n    # This buildpack will display build-time information (as a dependency)\n    [[order.group]]\n    id = \"samples/hello-world\"\n    version = \"0.0.1\"\n\n    # This buildpack will display build-time information (as a dependant)\n    [[order.group]]\n    id = \"samples/hello-moon\"\n    version = \"0.0.1\"\n\n    # This buildpack will create a process type \"sys-info\" to display runtime information\n    [[order.group]]\n    id = \"samples/hello-processes\"\n    version = \"0.0.1\"\n\n# Stack that will be used by the builder\n[stack]\nid = \"io.buildpacks.samples.stacks.bionic\"\n# This image is used at runtime\nrun-image = \"cnbs/sample-stack-run:bionic\"\n# This image is used at build-time\nbuild-image = \"cnbs/sample-stack-build:bionic\"\n</code></pre> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#lifecycle","title":"Lifecycle","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/lifecycle/<ul> <li>buildpack\u306e\u5b9f\u884c\u3092\u30aa\u30fc\u30b1\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\u3057\u3001application image\u3092build\u3057\u307e\u3059</li> <li>https://github.com/buildpacks/lifecycle</li> <li>https://github.com/buildpacks/spec/blob/main/platform.md#lifecycle-interface</li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#1-analyze","title":"1. Analyze","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/lifecycle/analyze/<ul> <li>Build\u304a\u3088\u3073Export\u30d5\u30a7\u30fc\u30ba\u3092\u6700\u9069\u5316\u3059\u308b\u305f\u3081\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u5fa9\u5143\u3057\u307e\u3059</li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#2-detect","title":"2. Detect","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/lifecycle/detect/<ul> <li>Build\u30d5\u30a7\u30fc\u30ba\u3067\u5b9f\u884c\u3059\u308bbuildpack\u306e\u9806\u5e8f\u3092\u691c\u51fa\u3057\u307e\u3059</li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#3-restore","title":"3. Restore","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/lifecycle/restore/<ul> <li>\u524d\u56de\u306eimage\u3068image cache\u304b\u3089layer\u306emetadata\u3092\u5fa9\u5143\u3057cache\u3055\u308c\u305fimage layer\u3092\u5fa9\u5143\u3057\u307e\u3059</li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#4-build","title":"4. Build","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/lifecycle/build/<ul> <li>application source code\u3092\u5b9f\u884c\u53ef\u80fd\u306aartifact\u3078\u5909\u63db\u3057\u30b3\u30f3\u30c6\u30ca\u306b\u683c\u7d0d\u3057\u307e\u3059</li> <li>https://buildpacks.io/docs/concepts/operations/build/<ul> <li></li> </ul> </li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#5-export","title":"5. Export","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/lifecycle/export/<ul> <li>OCI image\u3092\u4f5c\u6210\u3057\u307e\u3059</li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#6-create","title":"6. Create","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/lifecycle/create/<ul> <li>analyze, detect, restore, build, export \u30921\u30b3\u30de\u30f3\u30c9\u3067\u5b9f\u884c\u3057\u307e\u3059</li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#7-launch","title":"7. Launch","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/lifecycle/launch/<ul> <li>OCI image\u306eEntrypoint\u3092\u8a2d\u5b9a\u3057\u307e\u3059</li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#8-rebase","title":"8. Rebase","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/lifecycle/rebase/<ul> <li>\u524d\u306eimage\u304b\u3089base image(run image)\u3092\u66f4\u65b0\u3057\u305f\u3044\u5834\u5408\u306b\u3001\u8a72\u5f53\u306eimage layer\u3060\u3051\u5dee\u3057\u66ff\u3048\u307e\u3059</li> <li>https://buildpacks.io/docs/concepts/operations/rebase/</li> <li></li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/about/#platform","title":"Platform","text":"<ul> <li>https://buildpacks.io/docs/concepts/components/platform/<ul> <li>Cloud Native Buildpacks\u3092\u4f7f\u7528\u3057\u3066OCI image\u3092build\u3059\u308b\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0<ul> <li>CLI (e.g. Pack CLI)</li> <li>CI Service (e.g. Tekton buildpacks plugin)</li> <li>Cloud Service (e.g. kpack)</li> </ul> </li> </ul> </li> </ul>"},{"location":"cloud-native-buildpacks/paketo-buildpacks/ruby/","title":"Paketo Buildpacks\u3067Ruby on Rails\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092OCI image\u306bbuild\u3059\u308b","text":""},{"location":"cloud-native-buildpacks/paketo-buildpacks/ruby/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://paketo.io/docs/reference/ruby-reference/</li> <li>https://paketo.io/docs/howto/ruby/</li> <li>https://github.com/paketo-buildpacks/ruby</li> <li>https://github.com/paketo-buildpacks/full-builder</li> </ul> <p>Warning</p> <p><code>mysql2</code> library \u306e\u3088\u3046\u306bnative extention\u3092\u4e00\u7dd2\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5fc5\u8981\u306e\u3042\u308bgem library\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u305f\u3081\u306b\u306f full builder image(<code>paketobuildpacks/builder:full</code>) \u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 - https://github.com/paketo-buildpacks/ruby/issues/862</p> <pre><code>$ pack build puma-sample --buildpack paketo-buildpacks/ruby --builder paketobuildpacks/builder:full\n</code></pre> <p><code>paketobuildpacks/builder:base</code> \u3067\u306f\u4ee5\u4e0b\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3057\u305f <pre><code>      Gem::Ext::BuildError: ERROR: Failed to build gem native extension.\n\n      current directory:\n      /layers/paketo-buildpacks_bundle-install/launch-gems/ruby/3.2.0/gems/mysql2-0.5.5/ext/mysql2\n      /layers/paketo-buildpacks_mri/mri/bin/ruby -I\n      /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0 extconf.rb\n      checking for rb_absint_size()... yes\n      checking for rb_absint_singlebit_p()... yes\n      checking for rb_gc_mark_movable()... yes\n      checking for rb_wait_for_single_fd()... yes\n      checking for rb_enc_interned_str() in ruby.h... yes\n      *** extconf.rb failed ***\n      Could not create Makefile due to some reason, probably lack of necessary\n      libraries and/or headers.  Check the mkmf.log file for more details.  You may\n      need configuration options.\n\n      Provided configuration options:\n        --with-opt-dir\n        --without-opt-dir\n        --with-opt-include\n        --without-opt-include=${opt-dir}/include\n        --with-opt-lib\n        --without-opt-lib=${opt-dir}/lib\n        --with-make-prog\n        --without-make-prog\n        --srcdir=.\n        --curdir\n        --ruby=/layers/paketo-buildpacks_mri/mri/bin/$(RUBY_BASE_NAME)\n        --with-openssl-dir\n        --without-openssl-dir\n        --with-mysql-dir\n        --without-mysql-dir\n        --with-mysql-include\n        --without-mysql-include=${mysql-dir}/include\n        --with-mysql-lib\n        --without-mysql-lib=${mysql-dir}/lib\n        --with-mysql-config\n        --without-mysql-config\n        --with-mysqlclient-dir\n        --without-mysqlclient-dir\n        --with-mysqlclient-include\n        --without-mysqlclient-include=${mysqlclient-dir}/include\n        --with-mysqlclient-lib\n        --without-mysqlclient-lib=${mysqlclient-dir}/lib\n        --with-mysqlclientlib\n        --without-mysqlclientlib\n      /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/mkmf.rb:1083:in `block in\n      find_library': undefined method `split' for nil:NilClass (NoMethodError)\n\n          paths = paths.flat_map {|path| path.split(File::PATH_SEPARATOR)}\n                                             ^^^^^^\n        from /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/mkmf.rb:1083:in `each'\n      from /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/mkmf.rb:1083:in\n      `flat_map'\n      from /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/mkmf.rb:1083:in\n      `find_library'\n        from extconf.rb:131:in `&lt;main&gt;'\n\n      To see why this extension failed to compile, please check the mkmf.log which can\n      be found here:\n\n      /layers/paketo-buildpacks_bundle-install/launch-gems/ruby/3.2.0/extensions/x86_64-linux/3.2.0-static/mysql2-0.5.5/mkmf.log\n\n      extconf failed, exit code 1\n\n      Gem files will remain installed in\n      /layers/paketo-buildpacks_bundle-install/launch-gems/ruby/3.2.0/gems/mysql2-0.5.5\n      for inspection.\n      Results logged to\n      /layers/paketo-buildpacks_bundle-install/launch-gems/ruby/3.2.0/extensions/x86_64-linux/3.2.0-static/mysql2-0.5.5/gem_make.out\n\n      /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/rubygems/ext/builder.rb:102:in\n      `run'\n      /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/rubygems/ext/ext_conf_builder.rb:28:in\n      `build'\n      /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/rubygems/ext/builder.rb:170:in\n      `build_extension'\n      /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/rubygems/ext/builder.rb:204:in\n      `block in build_extensions'\n      /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/rubygems/ext/builder.rb:201:in\n      `each'\n      /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/rubygems/ext/builder.rb:201:in\n      `build_extensions'\n      /layers/paketo-buildpacks_mri/mri/lib/ruby/3.2.0/rubygems/installer.rb:843:in\n      `build_extensions'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/rubygems_gem_installer.rb:72:in\n      `build_extensions'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/rubygems_gem_installer.rb:28:in\n      `install'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/source/rubygems.rb:200:in\n      `install'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/installer/gem_installer.rb:54:in\n      `install'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/installer/gem_installer.rb:16:in\n      `install_from_spec'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/installer/parallel_installer.rb:155:in\n      `do_install'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/installer/parallel_installer.rb:146:in\n      `block in worker_pool'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/worker.rb:62:in\n      `apply_func'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/worker.rb:57:in\n      `block in process_queue'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/worker.rb:54:in\n      `loop'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/worker.rb:54:in\n      `process_queue'\n      /layers/paketo-buildpacks_bundler/bundler/gems/bundler-2.4.6/lib/bundler/worker.rb:90:in\n      `block (2 levels) in create_threads'\n\n      An error occurred while installing mysql2 (0.5.5), and Bundler cannot continue.\n\n      In Gemfile:\n        mysql2\nfailed to execute bundle install output:\n\nerror: exit status 5\nERROR: failed to build: exit status 1\nERROR: failed to build: executing lifecycle: failed with status code: 51\n</code></pre> </p>"},{"location":"cloud-native-buildpacks/paketo-buildpacks/ruby/#image-build","title":"image build","text":"<ol> <li>build     <code>paketobuildpacks/builder:full</code> builder\u3067rails app\u306eimage\u4f5c\u6210 <pre><code>$ pack build puma-sample --buildpack paketo-buildpacks/ruby --builder paketobuildpacks/builder:full\nfull: Pulling from paketobuildpacks/builder\ne58c18cab017: Already exists\n\n041be6a6e80e: Pull complete\n4612107b91b7: Pull complete\n273978fd99f5: Pull complete\n4f4fb700ef54: Pull complete\nDigest: sha256:6deb1900981341c9e0dd66537b20cb64fdb7f26111a46ceb3681ba6fdf9dfea0\nStatus: Downloaded newer image for paketobuildpacks/builder:full\nfull-cnb: Pulling from paketobuildpacks/run\ne58c18cab017: Already exists\ne5e9174b359f: Already exists\n5e91d2c23726: Pull complete\n9c0a2c4cca94: Pull complete\n05f59ae6b933: Pull complete\nec4575052267: Pull complete\n068822fb393b: Pull complete\nDigest: sha256:8338a316a476ca8d9c8f2b82b59816c21ffce1863de45150de1229c56c747d0d\nStatus: Downloaded newer image for paketobuildpacks/run:full-cnb\n===&gt; ANALYZING\nPrevious image with name \"puma-sample\" not found\n===&gt; DETECTING\n5 of 12 buildpacks participating\npaketo-buildpacks/ca-certificates 3.5.1\npaketo-buildpacks/mri             0.11.1\npaketo-buildpacks/bundler         0.7.4\npaketo-buildpacks/bundle-install  0.6.3\npaketo-buildpacks/puma            0.4.15\n===&gt; RESTORING\n===&gt; BUILDING\n\nPaketo Buildpack for CA Certificates 3.5.1\n  https://github.com/paketo-buildpacks/ca-certificates\n  Launch Helper: Contributing to layer\n    Creating /layers/paketo-buildpacks_ca-certificates/helper/exec.d/ca-certificates-helper\nPaketo Buildpack for MRI 0.11.1\n  Resolving MRI version\n    Candidate version sources (in priority order):\n      Gemfile   -&gt; \"3.2.1\"\n      &lt;unknown&gt; -&gt; \"\"\n\n    Selected MRI version (using Gemfile): 3.2.1\n\n  Executing build process\n    Installing MRI 3.2.1\n      Completed in 16.232s\n\n  Generating SBOM for /layers/paketo-buildpacks_mri/mri\n      Completed in 34ms\n\n  Configuring build environment\n    GEM_PATH         -&gt; \"/home/cnb/.local/share/gem/ruby/3.2.0:/layers/paketo-buildpacks_mri/mri/lib/ruby/gems/3.2.0\"\n    MALLOC_ARENA_MAX -&gt; \"2\"\n\n  Configuring launch environment\n    GEM_PATH         -&gt; \"/home/cnb/.local/share/gem/ruby/3.2.0:/layers/paketo-buildpacks_mri/mri/lib/ruby/gems/3.2.0\"\n    MALLOC_ARENA_MAX -&gt; \"2\"\n\nPaketo Buildpack for Bundler 0.7.4\n  Resolving Bundler version\n    Candidate version sources (in priority order):\n      Gemfile.lock -&gt; \"2.*.*\"\n      &lt;unknown&gt;    -&gt; \"\"\n\n    Selected bundler version (using Gemfile.lock): 2.4.6\n\n  Executing build process\n    Installing Bundler 2.4.6\n      Completed in 1.763s\n\n  Generating SBOM for /layers/paketo-buildpacks_bundler/bundler\n      Completed in 5ms\n\n  Configuring build environment\n    GEM_PATH -&gt; \"$GEM_PATH:/layers/paketo-buildpacks_bundler/bundler\"\n\n  Configuring launch environment\n    GEM_PATH -&gt; \"$GEM_PATH:/layers/paketo-buildpacks_bundler/bundler\"\n\nPaketo Buildpack for Bundle Install 0.6.3\n  Executing launch environment install process\n    Running 'bundle config --global clean true'\n    Running 'bundle config --global path /layers/paketo-buildpacks_bundle-install/launch-gems'\n    Running 'bundle config --global without development:test'\n    Running 'bundle config --global cache_path --parseable'\n\n    Running 'bundle install'\n      Fetching gem metadata from https://rubygems.org/..........\n      Resolving dependencies...\n\n      &lt;snip...&gt;\n\n      Fetching mysql2 0.5.5\n      Installing logstash-event 1.2.02\n      Fetching tzinfo 1.2.11\n      Installing mysql2 0.5.5 with native extensions\n\n      &lt;snip...&gt;\n\n      Installing puma 4.3.12 with native extensions\n      Fetching rails 6.0.6.1\n      Installing rails 6.0.6.1\n      Fetching bootsnap 1.16.0\n      Installing bootsnap 1.16.0 with native extensions\n      Bundle complete! 17 Gemfile dependencies, 70 gems now installed.\n      Gems in the groups 'development' and 'test' were not installed.\n      Bundled gems are installed into `/layers/paketo-buildpacks_bundle-install/launch-gems`\n      Completed in 1m51.974s\n\n  Generating SBOM for /layers/paketo-buildpacks_bundle-install/launch-gems\n      Completed in 11.999s\n\n  Configuring launch environment\n    BUNDLE_USER_CONFIG -&gt; \"/layers/paketo-buildpacks_bundle-install/launch-gems/config\"\n\nPaketo Buildpack for Puma 0.4.15\n  Assigning launch processes:\n    web (default): bash -c bundle exec puma --bind tcp://0.0.0.0:${PORT:-9292}\n\n===&gt; EXPORTING\nAdding layer 'paketo-buildpacks/ca-certificates:helper'\nAdding layer 'paketo-buildpacks/mri:mri'\nAdding layer 'paketo-buildpacks/bundler:bundler'\nAdding layer 'paketo-buildpacks/bundle-install:launch-gems'\nAdding layer 'launch.sbom'\nAdding 1/1 app layer(s)\nAdding layer 'launcher'\nAdding layer 'config'\nAdding layer 'process-types'\nAdding label 'io.buildpacks.lifecycle.metadata'\nAdding label 'io.buildpacks.build.metadata'\nAdding label 'io.buildpacks.project.metadata'\nSetting default process type 'web'\nSaving puma-sample...\n*** Images (8236d63db653):\n      puma-sample\nAdding cache layer 'paketo-buildpacks/mri:mri'\nAdding cache layer 'paketo-buildpacks/bundler:bundler'\nAdding cache layer 'cache.sbom'\nSuccessfully built image puma-sample\n</code></pre> </li> <li> <p>image\u78ba\u8a8d</p> <pre><code>$ docker images rails-api-server\nREPOSITORY         TAG       IMAGE ID       CREATED        SIZE\nrails-api-server   latest    635f3a7771df   43 years ago   956MB\n</code></pre> </li> </ol>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/","title":"Amazon VPC CNI Plugin for Kubernetes","text":"<ul> <li>Amazon VPC CNI(Container Networking Interface) Plugin for Kubernetes</li> <li> <p>https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/managing-vpc-cni.html</p> <p>Amazon VPC CNI plugin for Kubernetes \u30a2\u30c9\u30aa\u30f3\u306f Amazon EKS \u30af\u30e9\u30b9\u30bf\u30fc\u5185\u306e\u5404 Amazon EC2 \u30ce\u30fc\u30c9\u306b\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u307e\u3059\u3002\u30a2\u30c9\u30aa\u30f3\u306f Elastic Network Interface \u3092\u4f5c\u6210\u3057\u3001Amazon EC2 \u30ce\u30fc\u30c9\u306b\u30a2\u30bf\u30c3\u30c1\u3057\u307e\u3059\u3002\u307e\u305f\u30a2\u30c9\u30aa\u30f3\u306f\u3001VPC \u306e\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8IPv4 \u307e\u305f\u306f IPv6 \u30a2\u30c9\u30ec\u30b9\u3092\u5404 Pod \u304a\u3088\u3073\u30b5\u30fc\u30d3\u30b9\u306b\u5272\u308a\u5f53\u3066\u307e\u3059\u3002</p> </li> </ul>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/#reference","title":"Reference","text":"<ul> <li>https://github.com/aws/amazon-vpc-cni-k8s</li> <li>https://github.com/aws/amazon-vpc-cni-k8s/tree/master/docs</li> <li>https://github.com/aws/eks-charts/tree/master/stable/aws-vpc-cni</li> <li>https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html</li> <li>https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network.html</li> <li>https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/</li> <li>https://aws.github.io/aws-eks-best-practices/networking/custom-networking/</li> <li>https://repost.aws/ja/knowledge-center/eks-custom-subnet-for-pod</li> <li>https://repost.aws/knowledge-center/eks-multiple-cidr-ranges</li> <li>https://aws.amazon.com/jp/blogs/news/leveraging-cni-custom-networking-alongside-security-groups-for-pods-in-amazon-eks/</li> <li>https://speakerdeck.com/sshota0809/netutowakushi-dian-dexue-bu-amazon-eks-kurasutanosukerabiritei</li> <li>https://github.com/aws-ia/terraform-aws-eks-blueprints/tree/main/docs</li> </ul>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/#install","title":"Install","text":"<ul> <li>Amazon EKS Cluster\u3092\u4f5c\u6210\u3059\u308b\u3068\u81ea\u52d5\u7684\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u308b<ul> <li>https://aws-ia.github.io/terraform-aws-eks-blueprints/add-ons/managed-add-ons/</li> <li>https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/managing-vpc-cni.html</li> </ul> </li> <li>\u624b\u52d5\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb<ul> <li>https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#vpc-add-on-self-managed-update</li> <li>https://github.com/aws/amazon-vpc-cni-k8s/tree/master/charts/aws-vpc-cni</li> </ul> </li> </ul>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/#what-you-can-do","title":"What you can do?","text":"<ul> <li>Pod\u306bVPC\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u540c\u3058CIDR\u306eIP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059</li> <li>Pod\u306bNode\u3068\u7570\u306a\u308bENI\u3092\u30a2\u30bf\u30c3\u30c1\u3059\u308b\u304c\u3067\u304d\u307e\u3059(InstanceType\u306b\u3088\u308a1 Node\u3042\u305f\u308a\u30a2\u30bf\u30c3\u30c1\u53ef\u80fd\u306aENI\u6570\u304c\u6c7a\u307e\u308a\u307e\u3059)</li> <li>Pod\u306bNode\u3068\u7570\u306a\u308bSecurityGroup\u3092\u30a2\u30bf\u30c3\u30c1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059(v1.7.7\u4ee5\u964d)</li> <li>Pod\u306bNode\u3068\u7570\u306a\u308bSubnet\u30bb\u30b0\u30e1\u30f3\u30c8\u306eIP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059</li> <li>Prefix Mode\u3092\u30b5\u30dd\u30fc\u30c8</li> <li>Kubernetes NetworkPolicy\u306b\u3088\u308aPod\u9593\u306e\u7d30\u304b\u306a\u30c8\u30e9\u30d5\u30a3\u30c3\u30af\u30d5\u30ed\u30fc\u5236\u5fa1\u304c\u3067\u304d\u307e\u3059<ul> <li>https://aws.amazon.com/jp/blogs/news/amazon-vpc-cni-now-supports-kubernetes-network-policies/</li> </ul> </li> </ul>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/#eni-assign-pattern","title":"ENI Assign Pattern","text":"<p>Pod\u306bVPC\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u540c\u3058CIDR\u306eIP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u6642\u3001ENI\u306e\u5272\u308a\u5f53\u3066\u65b9\u6cd5\u306b\u3044\u3064\u304b\u306e\u30d1\u30bf\u30fc\u30f3\u304c\u5b58\u5728\u3057\u307e\u3059\u3002</p> <p>\u5927\u304d\u304f\u306f\u4ee5\u4e0b3\u3064\u306e\u30dd\u30a4\u30f3\u30c8\u304c\u3042\u308a\u307e\u3059\u3002</p> <ol> <li>ENI\u3092Worker Node\u3068\u5171\u6709\u3059\u308b (default)</li> <li>ENI\u3092Worker Node\u3068\u5171\u6709\u305b\u305a\u3001Pod\u9593\u3067\u5171\u6709\u3059\u308b (Custom Networking)</li> <li>ENI\u3092Worker Node\u3068\u3082Pod\u9593\u3067\u3082\u5171\u6709\u305b\u305a\u30011 Pod\u3067ENI\u3092\u5360\u6709\u3059\u308b (SecurityGroup for Pods)</li> </ol> <p>Custom Networking\u3084SecurityGroup for Pods\u306f\u6392\u4ed6\u5229\u7528\u3067\u306f\u306a\u3044\u306e\u3067\u3069\u3061\u3089\u3082\u6709\u52b9\u306b\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 \u3053\u306e\u5834\u5408(\u4ee5\u4e0b\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u8868\u306e4\u756a)\u3001SecurityGroupPolicy\u306eselector\u306bMatch\u3057\u306a\u3044Pod\u306fCustom Networking\u3092\u5229\u7528\u3059\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p> # Custom Networking SecurityGroup for Pods 1 N N 2 N Y 3 Y N 4 Y Y"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/#default","title":"Default","text":"<p>\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306fAmazon VPC CNI Plugin\u306fWorker Node\u306ePrimary Subnet\u304b\u3089IP\u30a2\u30c9\u30ec\u30b9\u3092\u53d6\u5f97\u3057Pod\u306b\u5272\u308a\u5f53\u3066\u307e\u3059\u3002 Primary Subnet\u3068\u306fWorker Node\u306ePrimary ENI\u304c\u63a5\u7d9a\u3057\u3066\u3044\u308bSubnet\u3092\u6307\u3057\u3001Primary ENI\u3068\u306fWorker Node\u306b\u30a2\u30bf\u30c3\u30c1\u3055\u308c\u3066\u3044\u308b1\u3064\u76ee\u306eENI(eth0)\u3092\u6307\u3057\u307e\u3059\u3002 Pod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306fPrimary ENI\u306b\u5272\u308a\u5f53\u3066\u3089\u308c\u308bSecondary IP\u30a2\u30c9\u30ec\u30b9\u304c\u4f7f\u7528\u3055\u308c\u3001Pod\u306b\u30a2\u30bf\u30c3\u30c1\u3055\u308c\u308bSecurityGroup\u306fWorker Node\u306ePrimary ENI\u306e\u3082&gt;\u306e\u3068\u540c\u3058\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u52d5\u4f5c\u3067\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u554f\u984c\u304c\u61f8\u5ff5\u3055\u308c\u307e\u3059\u3002</p> <ol> <li>Subnet\u306eIP\u30a2\u30c9\u30ec\u30b9\u304c\u67af\u6e07\u3057Pod\u306bIP\u30a2\u30c9\u30ec\u30b9\u304c\u5272\u308a\u5f53\u3066\u3089\u308c\u306a\u3044<ul> <li>Worker Node\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u3084\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3Pod\u306f\u3082\u3061\u308d\u3093\u3001\u69d8\u3005\u306aWorkloads\u306b\u3088\u308aIP\u30a2\u30c9\u30ec\u30b9\u304c\u6d88\u8cbb\u3055\u308c\u308b</li> <li>\u7279\u306bAddons\u306a\u3069\u3067DaemonSet Workloads\u304c\u3042\u308b\u5834\u5408\u3001Worker Node\u3054\u3068\u306bPod\u304c\u8d77\u52d5\u3059\u308b\u305f\u3081\u3088\u308a\u591a\u304f\u306eIP\u30a2\u30c9\u30ec\u30b9\u3092\u6d88\u8cbb\u3059\u308b\u3053\u3068\u306b\u306a\u308b</li> </ul> </li> <li>(\u904b\u7528\u4e0a) Worker Node\u3068Pod\u3067Subnet\u3092\u5206\u3051\u305f\u3044\u5834\u5408<ul> <li>IP\u30a2\u30c9\u30ec\u30b9\u306e\u67af\u6e07\u306e\u30b1\u30fc\u30b9\u3082\u3067\u3059\u304c\u3001\u904b\u7528\u4e0aWorker Node\u3068Pod\u3067Subnet\u3092\u5206\u3051\u305f\u3044\u5834\u5408</li> </ul> </li> <li>(\u904b\u7528\u4e0a) Node\u3068Pod\u3067SecurityGroup\u3092\u5206\u3051\u305f\u3044\u5834\u5408<ul> <li>SecurityGroup\u306fWorker Node\u3084\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304c\u7a3c\u50cd\u3059\u308bPod\u306a\u3069\u3067\u30a2\u30af\u30bb\u30b9\u5236\u5fa1\u306esource/dest\u304c\u7570\u306a\u308a\u307e\u3059\u3002   Worker Node SG\u3067\u5927\u304d\u304f\u8a31\u53ef\u3059\u308b\u3053\u3068\u3067Pod\u306e\u30a2\u30af\u30bb\u30b9\u5236\u5fa1\u3092\u517c\u306d\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u304c\u3001Security\u306e\u89b3\u70b9\u3067\u306f\u3088\u304f\u306a\u3044\u3067\u3059</li> </ul> </li> </ol> <p>Custom Networking\u306f\u3053\u308c\u3089\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/#custom-networking","title":"Custom Networking","text":"<p>Custom Networking\u3068\u306fPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u3092\u53d6\u5f97\u3059\u308bSubnet\u3084Pod\u306b\u30a2\u30bf\u30c3\u30c1\u3055\u308c\u308bSecurityGroup\u3092Primary ENI\u3068\u533a\u5225\u3057\u3066\u6307\u5b9a\u3067\u304d\u308b\u6a5f\u80fd\u3067\u3059\u3002</p> <p>Custom Networking</p>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/#prefix-mode","title":"Prefix Mode","text":"<p>Prefix Mode</p>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/#network-policy","title":"Network Policy","text":"<p>Network Policy</p>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/#securitygroup-for-pods","title":"SecurityGroup for Pods","text":"<p>SecurityGroup for Pods</p>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/custom-networking/","title":"Custom Networking","text":""},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/custom-networking/#custom-networking","title":"Custom Networking","text":"<p>\u53c2\u8003\u306b\u306a\u308b\u30da\u30fc\u30b8</p> <ul> <li>https://aws.amazon.com/jp/blogs/news/leveraging-cni-custom-networking-alongside-security-groups-for-pods-in-amazon-eks/</li> <li>https://aws.github.io/aws-eks-best-practices/networking/custom-networking/</li> <li>https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network.html</li> <li>https://www.eksworkshop.com/docs/networking/custom-networking/configure-vpc-cni</li> </ul> <p>hostNetworking\u3092\u6307\u5b9a\u3057\u3066\u3044\u308bPod\u306fPrimary ENI\u306b\u30a2\u30bf\u30c3\u30c1\u3055\u308c\u3066\u3044\u308bSecondary IP\u30a2\u30c9\u30ec\u30b9\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/custom-networking/#enabled-custom-networking","title":"Enabled Custom Networking","text":"<p>Custom Networking\u3092\u6709\u52b9\u306b\u3059\u308b\u3068 <code>ENIConfig</code> \u30ab\u30b9\u30bf\u30e0\u30ea\u30bd\u30fc\u30b9\u3067\u5b9a\u7fa9\u3057\u305fSubnet\u3068SecurityGroup\u3092Pod\u306b\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p> <ol> <li> <p><code>AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG</code> \u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067Custom Networking\u3092\u6709\u52b9\u306b\u3067\u304d\u307e\u3059\u3002     <pre><code>kubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true\n</code></pre></p> </li> <li> <p><code>ENIConfig</code> \u30ab\u30b9\u30bf\u30e0\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210</p> <ul> <li>https://github.com/aws/amazon-vpc-cni-k8s/blob/b01e57f96567b192d96f04d239f58c3d5337ad22/pkg/apis/crd/v1alpha1/eniconfig_types.go#L26-L30</li> <li>https://aws.github.io/aws-eks-best-practices/networking/custom-networking/<ul> <li><code>AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true</code> \u306e\u5834\u5408\u3001CNI \u306f ENIConfig \u3067\u5b9a\u7fa9\u3055\u308c\u305f\u30b5\u30d6\u30cd\u30c3\u30c8\u304b\u3089 Pod \u306e IP \u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u307e\u3059\u3002   ENIConfig \u30ab\u30b9\u30bf\u30e0\u30ea\u30bd\u30fc\u30b9\u306f\u3001Pod \u304c\u30b9\u30b1\u30b8\u30e5\u30fc\u30ea\u30f3\u30b0\u3055\u308c\u308b\u30b5\u30d6\u30cd\u30c3\u30c8\u3092\u5b9a\u7fa9\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002     <pre><code>apiVersion : crd.k8s.amazonaws.com/v1alpha1\nkind : ENIConfig\nmetadata:\n  name: us-west-2a\nspec: \n  securityGroups:\n    - sg-0dff111a1d11c1c11\n  subnet: subnet-011b111c1f11fdf11\n</code></pre></li> </ul> </li> </ul> </li> <li></li> </ol>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/network-policy/","title":"Network Policy","text":""},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/network-policy/#references","title":"References","text":"<ul> <li>https://kubernetes.io/docs/concepts/services-networking/network-policies/</li> <li>https://docs.aws.amazon.com/eks/latest/userguide/cni-network-policy.html</li> <li>https://github.com/aws/amazon-vpc-cni-k8s/blob/master/README.md#network-policies</li> <li>https://aws.amazon.com/jp/blogs/news/amazon-vpc-cni-now-supports-kubernetes-network-policies/</li> <li>https://aws.github.io/aws-eks-best-practices/security/docs/network/</li> <li>https://github.com/aws-samples/eks-network-policy-examples</li> <li>https://www.linkedin.com/pulse/how-enable-network-policies-eks-using-aws-vpc-cni-plugin-engin-diri</li> </ul>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/network-policy/#about","title":"About","text":"<ul> <li> <p>Kubernetes\u306eNetworkPolicy</p> <ul> <li>Kubernetes Cluster\u5185\u306ePod\u9593\u306enetwork traffic\u3092\u5236\u5fa1\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd(default: \u5168\u3066\u306ePod\u9593\u3067\u901a\u4fe1\u304c\u8a31\u53ef\u3055\u308c\u307e\u3059)</li> </ul> <p>Warn</p> <p>SecurityGroup for Pod\u306e\u4ee3\u66ff\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> <ul> <li>https://aws.amazon.com/jp/blogs/news/amazon-vpc-cni-now-supports-kubernetes-network-policies/ <p>Security Groups for Pods \u3068 NetworkPolicy \u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u6d3b\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u4f53\u5236\u3092\u5f37\u5316\u3067\u304d\u307e\u3059\u3002NetworkPolicy \u304c\u6709\u52b9\u306b\u306a\u3063\u3066\u3044\u308b\u5834\u5408\u3001Security Groups for Pods \u306f\u591a\u5c64\u9632\u5fa1\u6226\u7565\u306e\u8ffd\u52a0\u306e\u30ec\u30a4\u30e4\u30fc\u3068\u3057\u3066\u6a5f\u80fd\u3057\u307e\u3059\u3002NetworkPolicy \u3092\u4f7f\u3046\u3068\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u5185\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30c8\u30e9\u30d5\u30a3\u30c3\u30af\u306e\u6d41\u308c\u3092\u304d\u3081\u7d30\u304b\u304f\u5236\u5fa1\u3067\u304d\u307e\u3059\u3002\u4e00\u65b9\u3067\u3001Security Groups for Pods \u306f Amazon \u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30c3\u30af\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u3092\u6d3b\u7528\u3057 Amazon RDS \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306a\u3069\u306e Virtual Private Cloud (VPC) \u5185\u306e\u30ea\u30bd\u30fc\u30b9\u3068\u306e\u901a\u4fe1\u3092\u7ba1\u7406\u3059\u308b\u3053\u3068\u3067\u4fdd\u8b77\u3092\u5f37\u5316\u3057\u307e\u3059\u3002</p> </li> </ul> <p>Info</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/cni-network-policy.html</p> <ul> <li> <p>You can use network policies with security groups for Pods. With network policies, you can control all in-cluster communication. With security groups for Pods, you can control access to AWS services from applications within a Pod.</p> </li> <li> <p>You can use network policies with custom networking and prefix delegation.</p> </li> </ul> <ul> <li>amazon-vpc-cni-plugin\u3067\u306eNetworkPolicy\u306f\u4ee5\u4e0b\u74b0\u5883\u3067\u4f7f\u7528\u53ef\u80fd\u3067\u3059<ul> <li>SecurityGroup for Pod</li> <li>Custom Networking\u304a\u3088\u3073Prefix Mode</li> </ul> </li> </ul> </li> </ul>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/prefix-mode/","title":"Prefix mode","text":""},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/prefix-mode/#references","title":"References","text":"<ul> <li>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-prefix-eni.html</li> <li>https://aws.amazon.com/jp/blogs/containers/amazon-vpc-cni-increases-pods-per-node-limits/</li> <li>https://aws.amazon.com/jp/blogs/news/amazon-vpc-cni-increases-pods-per-node-limits/</li> <li>https://aws.github.io/aws-eks-best-practices/networking/prefix-mode/index_linux/</li> <li>https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html</li> <li>https://aws.amazon.com/jp/blogs/containers/automating-custom-networking-to-solve-ipv4-exhaustion-in-amazon-eks/</li> <li>https://github.com/aws/amazon-vpc-cni-k8s#enable_prefix_delegation-v190</li> <li>https://www.eksworkshop.com/docs/networking/prefix/</li> </ul>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/prefix-mode/#about","title":"About","text":"<ul> <li> <p>Worker Node\u306b\u30a2\u30bf\u30c3\u30c1\u3055\u308c\u308bENI\u306eSecondary IP\u30a2\u30c9\u30ec\u30b9\u7528\u30b9\u30ed\u30c3\u30c8\u306b/28 \u306eIP Prefix(16 IP addresses)\u3092\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059</p> <p>Info</p> <p> IP addresses per network interface per instance type \u306b\u3042\u308b\u901a\u308a c5.large \u3067\u306f1\u3064\u306eENI\u306b\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u304c\u3067\u304d\u308bIP\u30a2\u30c9\u30ec\u30b9\u6570\u306f10\u500b\u3068\u306a\u308a\u307e\u3059\u3002\u305d\u306e\u3046\u3061\u3001Pod\u306b\u5272\u308a\u5f53\u3066\u53ef\u80fd\u306aSecondary IP\u30a2\u30c9\u30ec\u30b9\u6570\u306f9\u500b\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>Prefix Mode\u3092\u6709\u52b9\u306b\u3057\u3066\u3044\u308b\u5834\u5408\u3001ENI\u306eSecondary IP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u30b9\u30ed\u30c3\u30c8\u306b /28 \u306eIP Prefix(16 IP addresses)\u3092\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308a\u3001\u3088\u308a\u591a\u304f\u306eIP\u30a2\u30c9\u30ec\u30b9\u3092\u6301\u3064\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>1 Woker Node\u3067\u8d77\u52d5\u53ef\u80fd\u306aPod\u306e\u6700\u5927\u6570\u306fEC2\u306b\u30a2\u30bf\u30c3\u30c1\u53ef\u80fd\u306aENI\u6570\u3068\u5272\u308a\u5f53\u3066\u53ef\u80fd\u306aIP\u30a2\u30c9\u30ec\u30b9\u6570\u306b\u4f9d\u5b58\u3059\u308b\u305f\u3081Prefix Mode\u3067\u306f\u3088\u308a\u591a\u304f\u306ePod\u30921 Worker Node\u4e0a\u3067\u8d77\u52d5\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>Warning</p> <ul> <li> <p>/28 \u306eIP Prefix(16 IP addresses)\u306e\u9023\u7d9a\u3057\u305f\u672a\u4f7f\u7528\u306eIP\u30a2\u30c9\u30ec\u30b9\u3092\u78ba\u4fdd\u3067\u304d\u306a\u3044\u5834\u5408\u306fPrefix Mode\u306e\u4f7f\u7528\u306f\u63a7\u3048\u3066\u304f\u3060\u3055\u3044</p> <ul> <li>Subnet\u3067\u6255\u3044\u51fa\u3057\u53ef\u80fd\u306aIP\u30a2\u30c9\u30ec\u30b9\u304c\u65ad\u7247\u5316\u3055\u308c\u3001/28 \u306e\u9023\u7d9a\u3057\u305f\u672a\u4f7f\u7528\u306eIP\u30a2\u30c9\u30ec\u30b9\u3092\u78ba\u4fdd\u3067\u304d\u306a\u3044\u6642\u306fCNI Plugin\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u3092\u8a18\u9332\u3057\u307e\u3059     <pre><code>failed to allocate a private IP/Prefix address: InsufficientCidrBlocks: There are not enough free cidr blocks in the specified subnet to satisfy the request.\n</code></pre></li> <li>/28 \u306eIP Prefix\u3092\u78ba\u4fdd\u3067\u304d\u305a\u306b\u30a8\u30e9\u30fc\u3068\u306a\u308b\u3053\u3068\u3092\u56de\u907f\u3059\u308b\u305f\u3081\u306b\u306f VPC Subnet CIDR reservations \u3067Subnet\u5185\u306bPrefix Mode\u7528\u306b\u4e88\u7d04\u9818\u57df\u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002CNI Plugin\u306fEC2 API\u3092\u547c\u3073\u51fa\u3057\u4e88\u7d04\u9818\u57df\u304b\u3089\u81ea\u52d5\u7684\u306b\u5272\u308a\u5f53\u3066\u3089\u308c\u308bPrefix\u3092\u5272\u308a\u5f53\u3066\u307e\u3059\u3002</li> </ul> </li> <li> <p>Prefix Mode\u3067\u306fSecurityGroup\u306fWorker Node\u4e0a\u306ePod\u3067\u5171\u6709\u3055\u308c\u307e\u3059\u3002SecurityGroup\u3092Pod\u3054\u3068\u306b\u5206\u3051\u305f\u3044\u5834\u5408\u306fSecurityGroup for Pod\u3092\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044</p> </li> <li>SecurityGroup for Pods\u3067\u306fPrefix Mode\u3092\u4f7f\u7528\u3067\u304d\u307e\u305b\u3093</li> </ul> </li> </ul>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/security_group_for_pod/","title":"SecurityGroup For Pod","text":""},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/security_group_for_pod/#about","title":"About","text":"<ul> <li>EKS\u3067\u306fSecurityGroup\u306b\u3088\u308b\u30a2\u30af\u30bb\u30b9\u5236\u5fa1\u3092Pod\u306b\u9069\u7528\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059</li> <li>https://aws.github.io/aws-eks-best-practices/networking/sgpp/</li> <li>https://aws.amazon.com/jp/blogs/containers/introducing-security-groups-for-pods/</li> <li>https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/managing-vpc-cni.html</li> <li>https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/cni-custom-network.html</li> <li>https://pages.awscloud.com/rs/112-TZM-766/images/20220804-AWS-kubernetes_2_ZOZO.pdf</li> <li>https://archive.eksworkshop.com/beginner/115_sg-per-pod/</li> </ul>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/security_group_for_pod/#why","title":"Why","text":"<p>\u306a\u305c <code>SecurityGroup For Pod</code> \u304c\u5fc5\u8981\u306a\u306e\u304b?</p> <p>\u524d\u63d0</p> <ul> <li>Amazon VPC CNI plugin for Kubernetes<ul> <li>Node ENI(Primary NetworkInterface)\u306eSecondary IP\u30a2\u30c9\u30ec\u30b9\u3092Instance Type\u3054\u3068\u306e\u4e0a\u9650\u6570\u5206(refs) \u78ba\u4fdd\u3059\u308b(\u78ba\u4fdd\u6570\u306f<code>WARM_IP_TARGET</code>\u306b\u3088\u308a\u5909\u66f4\u53ef\u80fd)</li> <li><code>SecurityGroup For Pod</code> \u304c\u7121\u52b9\u306e\u5834\u5408\u306fPod\u306eIP\u30a2\u30c9\u30ec\u30b9\u306fNode ENI\u306b\u5272\u308a\u5f53\u3066\u3089\u308c\u3066\u3044\u308bSecondary IP\u30a2\u30c9\u30ec\u30b9\u304b\u3089\u5272\u308a\u5f53\u3066\u3089\u308c\u307e\u3059</li> </ul> </li> </ul> <ol> <li>Node\u4e0a\u306b\u8d77\u52d5\u53ef\u80fd\u306aPod\u6570\u3092Node ENI(Primary NetworkInterface)\u3067\u6255\u3044\u51fa\u3057\u53ef\u80fd\u306aIP\u30a2\u30c9\u30ec\u30b9\u6570\u4ee5\u4e0a\u3068\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd</li> <li>Node \u3068 Pod \u306e\u30a2\u30af\u30bb\u30b9\u5236\u5fa1\u3092\u5206\u96e2\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd<ul> <li><code>SecurityGroup For Pod</code>\u3092\u4f7f\u308f\u306a\u3044\u5834\u5408\u306fNode ENI\u306eSecondary IP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b\u3001\u3064\u307e\u308aNode SG\u3067\u306e\u30a2\u30af\u30bb\u30b9\u5236\u5fa1\u304c\u9069\u7528\u3055\u308c\u307e\u3059</li> </ul> </li> </ol>"},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/security_group_for_pod/#usage","title":"Usage","text":""},{"location":"eks/amazon-vpc-cni-plugin-for-kubernetes/security_group_for_pod/#preparation","title":"Preparation","text":"<ul> <li><code>ENABLE_POD_ENI=true</code></li> </ul>"},{"location":"eks/aws-efs-csi-driver/about/","title":"aws-efs-csi-driver","text":"<ul> <li>Amazon EFS CSI Driver</li> <li>https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/efs-csi.html <p>Amazon Elastic File System (Amazon EFS) \u306f\u3001\u30b5\u30fc\u30d0\u30fc\u30ec\u30b9\u3067\u4f38\u7e2e\u81ea\u5728\u306a\u30d5\u30a1\u30a4\u30eb\u30b9\u30c8\u30ec\u30fc\u30b8\u3092\u63d0\u4f9b\u3059\u308b\u305f\u3081\u3001\u30b9\u30c8\u30ec\u30fc\u30b8\u5bb9\u91cf\u304a\u3088\u3073\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3084\u7ba1\u7406\u3092\u884c\u3046\u3053\u3068\u306a\u304f\u30d5\u30a1\u30a4\u30eb\u30c7\u30fc\u30bf\u3092\u5171\u6709\u3067\u304d\u307e\u3059\u3002Amazon EFS Container Storage Interface (CSI) \u30c9\u30e9\u30a4\u30d0\u30fc\u306f\u3001AWS \u3067\u52d5\u4f5c\u3059\u308b Kubernetes \u30af\u30e9\u30b9\u30bf\u30fc\u304c Amazon EFS \u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u3092\u7ba1\u7406\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b CSI \u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> </li> </ul>"},{"location":"eks/aws-efs-csi-driver/about/#reference","title":"Reference","text":"<ul> <li>https://github.com/kubernetes-sigs/aws-efs-csi-driver</li> <li>https://github.com/kubernetes-sigs/aws-efs-csi-driver/tree/master/docs</li> <li>https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html</li> <li>https://aws.amazon.com/jp/blogs/news/amazon-efs-csi-dynamic-provisioning/</li> <li>https://www.eksworkshop.com/docs/fundamentals/storage/efs/efs-csi-driver/</li> <li>aws blog\u306etags<ul> <li>https://aws.amazon.com/jp/blogs/news/category/storage/amazon-elastic-file-system-efs/</li> <li>https://aws.amazon.com/jp/blogs/storage/category/storage/amazon-elastic-file-system-efs/</li> <li>https://repost.aws/tags/TAmTqmXysORgKO6xcE3oh0MA/amazon-elastic-file-system</li> </ul> </li> </ul>"},{"location":"eks/aws-efs-csi-driver/about/#install","title":"Install","text":"<ul> <li>https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html#efs-install-driver</li> <li>https://github.com/kubernetes-sigs/aws-efs-csi-driver/tree/master?tab=readme-ov-file#deploy-the-driver</li> <li>https://github.com/kubernetes-sigs/aws-efs-csi-driver/tree/master/charts/aws-efs-csi-driver</li> </ul>"},{"location":"eks/aws-efs-csi-driver/about/#about","title":"About","text":"<ul> <li>aws-efs-csi-driver\u306fEKS Cluster\u4e0a\u3067EFS Volume\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u3092\u7ba1\u7406\u3059\u308b\u305f\u3081\u306eCSI Interface\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002<ul> <li>EKS Pod\u3067EFS Volume\u3092\u30de\u30a6\u30f3\u30c8\u3057\u3066\u8aad\u307f\u66f8\u304d\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</li> <li>https://docs.aws.amazon.com/ja_jp/prescriptive-guidance/latest/patterns/run-stateful-workloads-with-persistent-data-storage-by-using-amazon-efs-on-amazon-eks-with-aws-fargate.html<ul> <li></li> </ul> </li> </ul> </li> <li>EKS Cluster\u4e0a\u3067EFS Volume\u3092\u4f7f\u3046\u305f\u3081\u306b\u306fEFS Volume\u3092PersistentVolume Storage\u3068\u3057\u3066\u8868\u73fe\u3057\u307e\u3059\u3002</li> </ul>"},{"location":"eks/aws-efs-csi-driver/about/#efs-volume","title":"EFS Volume","text":"<p>aws-efs-csi-driver\u3067PV\u3092\u4f5c\u6210\u3059\u308b\u65b9\u6cd5\u306fStatic Provisioning\u3068Dynamic Provisioning\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u3069\u3061\u3089\u306e\u65b9\u6cd5\u3067\u3042\u3063\u3066\u3082\u4e8b\u524d\u306bEFS Volume\u3092\u4f5c\u6210\u3057\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>EFS Volume\u4f5c\u6210\u6642\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044\u3068PersistentVolume\u3092\u4f5c\u6210\u3067\u304d\u307e\u305b\u3093</p> <ul> <li>https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/docs/efs-create-filesystem.md</li> <li> <p>https://repost.aws/ja/knowledge-center/eks-troubleshoot-efs-volume-mount-issues</p> <p>EFS \u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u306e\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30b0\u30eb\u30fc\u30d7\u306b\u306f\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u306e VPC \u306e CIDR \u304b\u3089\u306e NFS \u30c8\u30e9\u30d5\u30a3\u30c3\u30af\u3092\u8a31\u53ef\u3059\u308b\u30a4\u30f3\u30d0\u30a6\u30f3\u30c9\u30eb\u30fc\u30eb\u304c\u5fc5\u8981\u3067\u3059\u3002\u30a4\u30f3\u30d0\u30a6\u30f3\u30c9\u30c8\u30e9\u30d5\u30a3\u30c3\u30af\u306b\u30dd\u30fc\u30c8 2049 \u3092\u8a31\u53ef\u3057\u307e\u3059\u3002    - EFS Volume\u304cEKS Node\u4e0a\u306b\u30de\u30a6\u30f3\u30c8\u3057\u3066\u3044\u308b\u305f\u3081</p> <p>\u30dd\u30c3\u30c9\u304c EFS \u30dc\u30ea\u30e5\u30fc\u30e0\u306e\u30de\u30a6\u30f3\u30c8\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u30ef\u30fc\u30ab\u30fc\u30ce\u30fc\u30c9\u306b\u95a2\u9023\u4ed8\u3051\u3089\u308c\u3066\u3044\u308b\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30b0\u30eb\u30fc\u30d7\u306b\u306f\u3001\u30a2\u30a6\u30c8\u30d0\u30a6\u30f3\u30c9\u30eb\u30fc\u30eb\u304c\u5fc5\u8981\u3067\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u3053\u306e\u30a2\u30a6\u30c8\u30d0\u30a6\u30f3\u30c9\u30eb\u30fc\u30eb\u3067\u306f\u3001EFS \u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3078\u306e NFS \u30c8\u30e9\u30d5\u30a3\u30c3\u30af (\u30dd\u30fc\u30c8 2049) \u3092\u8a31\u53ef\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002    - EFS Volume\u304cEKS Node\u4e0a\u306b\u30de\u30a6\u30f3\u30c8\u3057\u3066\u3044\u308b\u305f\u3081</p> <p>EKS \u30ce\u30fc\u30c9\u304c\u5b9f\u884c\u3055\u308c\u3066\u3044\u308b\u5404\u30a2\u30d9\u30a4\u30e9\u30d3\u30ea\u30c6\u30a3\u30fc\u30be\u30fc\u30f3\u306b EFS \u30de\u30a6\u30f3\u30c8\u30bf\u30fc\u30b2\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u305f\u3068\u3048\u3070\u3001\u30ef\u30fc\u30ab\u30fc\u30ce\u30fc\u30c9\u304c us-east-1a \u3068 us-east-1b \u306b\u5206\u6563\u3057\u3066\u3044\u308b\u3068\u3057\u307e\u3059\u3002\u3053\u306e\u5834\u5408\u3001\u30de\u30a6\u30f3\u30c8\u3059\u308b EFS \u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u306e\u4e21\u65b9\u306e\u30a2\u30d9\u30a4\u30e9\u30d3\u30ea\u30c6\u30a3\u30fc\u30be\u30fc\u30f3\u306b\u30de\u30a6\u30f3\u30c8\u30bf\u30fc\u30b2\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> </li> </ul>"},{"location":"eks/aws-efs-csi-driver/about/#provisioning","title":"Provisioning","text":""},{"location":"eks/aws-efs-csi-driver/about/#static-provisioning","title":"Static Provisioning","text":"<ul> <li>https://github.com/kubernetes-sigs/aws-efs-csi-driver/tree/master/examples/kubernetes/static_provisioning</li> <li>Static Provisioning\u306f\u4e8b\u524d\u306bPersistentVolume\u3092\u4f5c\u6210\u3057\u3066\u304a\u304f\u7ba1\u7406\u65b9\u6cd5\u3067\u3059\u3002</li> </ul>"},{"location":"eks/aws-efs-csi-driver/about/#dynamic-provisioning","title":"Dynamic Provisioning","text":"<ul> <li>https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/</li> <li>https://aws.amazon.com/jp/blogs/news/amazon-efs-csi-dynamic-provisioning/</li> <li>https://github.com/kubernetes-sigs/aws-efs-csi-driver/tree/master/examples/kubernetes/dynamic_provisioning</li> <li>Dynamic Provisioning\u306fPersistentVolumeClaim\u304c\u4f5c\u6210\u3055\u308c\u305f\u3089\u8981\u6c42\u3055\u308c\u305f\u6761\u4ef6\u306b\u5408\u81f4\u3059\u308bStorageClass\u304b\u3089PersistentVolume\u3092\u52d5\u7684\u306b\u4f5c\u6210\u3055\u308c\u308b\u7ba1\u7406\u65b9\u6cd5\u3067\u3059\u3002</li> </ul>"},{"location":"eks/aws-efs-csi-driver/troubleshoot/","title":"Troubleshoot","text":"<p>https://repost.aws/ja/knowledge-center/eks-troubleshoot-efs-volume-mount-issues</p>"},{"location":"failure_test/failure_scenario_of_node/","title":"Node","text":""},{"location":"failure_test/failure_scenario_of_node/#worker-node","title":"Worker Node","text":""},{"location":"failure_test/failure_scenario_of_node/#resources","title":"Resources","text":""},{"location":"failure_test/failure_scenario_of_node/#oom-killer","title":"OOM Killer","text":""},{"location":"failure_test/failure_scenario_of_node/#_1","title":"\u53c2\u8003","text":"<ul> <li>k8s.af \u304b\u3089<ul> <li>https://www.bluematador.com/blog/post-mortem-kubernetes-node-oom</li> </ul> </li> <li>https://www.scsk.jp/sp/sysdig/blog/sysdig_monitor/kubernetes_oomcpu.html</li> <li>Goldstine\u7814\u7a76\u6240</li> <li>https://blog.mosuke.tech/entry/2020/03/31/kubernetes-resource/</li> <li>https://blog.mosuke.tech/entry/2021/03/11/kubernetes-node-down/</li> <li>\u5916\u9053\u7236\u306e\u5320<ul> <li>http://blog.father.gedow.net/2019/11/28/eks-kubernetes-ouf-of-memory/</li> </ul> </li> </ul>"},{"location":"failure_test/failure_scenario_of_node/#impact","title":"Impact","text":"<ul> <li>Memory\u304cNode\u4e0a\u9650\u306b\u9054\u3057\u305f\u5834\u5408OOM Killer\u304cPod\u3092\u5f37\u5236\u505c\u6b62\u3059\u308b<ul> <li>\u5f37\u5236\u505c\u6b62\u3059\u308bPod\u306fQoS Class\u306e\u512a\u5148\u5ea6\u3067\u6c7a\u307e\u308b (\u53c2\u8003)</li> <li><code>Note: The kubelet also sets an oom_score_adj value of -997 for containers in Pods that have system-node-critical Priority</code></li> <li>\u540c\u3058QoS Class(Burstable) \u306e\u5834\u5408\u306fPod\u306erequest\u3057\u305f\u30e1\u30e2\u30ea\u30fc\u91cf\u3068\u30ce\u30fc\u30c9\u306e\u30ad\u30e3\u30d1\u30b7\u30c6\u30a3\u306e\u5272\u5408\u306b\u3088\u3063\u3066\u30b9\u30b3\u30a2\u4ed8\u3051\u3055\u308c\u308b</li> </ul> </li> </ul>"},{"location":"failure_test/failure_scenario_of_node/#recommend","title":"Recommend","text":"<ul> <li><code>Requests Memory = Limits Memory</code> \u3067\u8a2d\u5b9a\u3059\u308b</li> <li>Limits Memory\u304cRequests Memory\u3088\u308a\u591a\u3044\u5834\u5408\u3001Requests Memory\u304cAllocatable\u306b\u53ce\u307e\u308b\u3088\u3046\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u3001Allocatable\u3092\u8d85\u3048\u305f\u5834\u5408\u306bNode\u306eOOM Killer\u304c\u767a\u52d5\u3059\u308b\u306e\u3067\u30b7\u30b9\u30c6\u30e0\u5168\u4f53\u306e\u30d7\u30ed\u30bb\u30b9\u304b\u3089Kill\u5bfe\u8c61\u304b\u3089\u9078\u3070\u308c\u308b</li> </ul>"},{"location":"failure_test/failure_scenario_of_node/#node-","title":"Node - \u505c\u6b62","text":""},{"location":"failure_test/failure_scenario_of_node/#_2","title":"\u53c2\u8003","text":"<ul> <li>Goldstine\u7814\u7a76\u6240: Kubernetes\u306e\u30ce\u30fc\u30c9\u969c\u5bb3\u6642\u306ePod\u306e\u52d5\u304d\u306b\u3064\u3044\u3066\u306e\u691c\u8a3c</li> </ul>"},{"location":"failure_test/failure_scenario_of_node/#impact_1","title":"Impact","text":"<ul> <li> <p>ReplicaSet Pod\u306e\u518d\u914d\u7f6e\u304c\u884c\u308f\u308c\u308b</p> </li> <li> <p><code>node_lifecycle_controller</code></p> <ol> <li>Kubelet\u304cNode\u60c5\u5831\u3092\u66f4\u65b0\u3057\u306a\u304f\u306a\u3063\u305f\u3053\u3068\u3092\u691c\u77e5\u3057\u3066Node\u306eStatus\u3092\u5909\u66f4</li> <li><code>key: node.kubernetes.io/unreachable</code> \u306eTaint\u3092\u4ed8\u4e0e</li> </ol> </li> <li> <p>Pod\u306e\u518d\u914d\u7f6e</p> <ul> <li>Pod\u304c\u4f5c\u6210\u3055\u308c\u308b\u6642\u306bDefaultTolerationSeconds AdmissionController\u306b\u3088\u3063\u3066\u4ee5\u4e0b\u306etolerations\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u3044\u308b<ul> <li><code>node.kubernetes.io/not-ready:NoExecute</code></li> <li><code>node.kubernetes.io/unreachable:NoExecute</code></li> </ul> </li> <li>\u3053\u308c\u3089\u306etolerations\u304c <code>tolerationSeconds: 300</code> \u3092\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001300\u79d2\u7d4c\u904e\u5f8c\u306bNode\u304c\u5fa9\u65e7\u305b\u305a<code>node.kubernetes.io/unreachable</code> Taint\u304c\u5916\u308c\u306a\u3044\u5834\u5408Pod\u304cEviction\uff08\u5f37\u5236\u9000\u53bb\uff09\u3055\u308c\u5225\u306e\u30ce\u30fc\u30c9\u306b\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3055\u308c\u308b</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaulttolerationseconds</li> <li>\u5225\u30ce\u30fc\u30c9\u3078\u306e\u518d\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u307e\u3067\u6700\u59275\u5206\u306e\u30bf\u30a4\u30e0\u30e9\u30b0\u304c\u767a\u751f\u3059\u308b\u5834\u5408\u304c\u3042\u308b</li> <li>DefaultTolerationSeconds</li> </ul> </li> </ul>"},{"location":"failure_test/failure_story/","title":"Failure Story","text":""},{"location":"failure_test/failure_story/#failure-stories","title":"Failure Stories","text":"<ul> <li>https://k8s.af/<ul> <li>https://codeberg.org/hjacobs/kubernetes-failure-stories</li> </ul> </li> </ul>"},{"location":"failure_test/point_of_failure/","title":"Point of failure","text":""},{"location":"failure_test/point_of_failure/#point-of-failure","title":"Point of failure","text":""},{"location":"failure_test/point_of_failure/#kubernetes-component","title":"Kubernetes Component","text":""},{"location":"failure_test/point_of_failure/#controll-plane","title":"Controll Plane","text":"<ul> <li>etcd</li> <li>kube-apiserver</li> <li>kube-controller-manager</li> <li>kube-scheduler</li> </ul>"},{"location":"failure_test/point_of_failure/#worker-node","title":"Worker Node","text":"<ul> <li>resources(cpu, memory, disk, network...)</li> <li>OOM Killer</li> <li>kubelet</li> <li>kube-proxy</li> <li>cni</li> <li>container-runtime</li> </ul>"},{"location":"failure_test/point_of_failure/#addons","title":"Addons","text":"<ul> <li> <p>\u53c2\u8003</p> <ul> <li>https://github.com/kubernetes/kubernetes/tree/master/cluster/addons</li> <li>https://registry.terraform.io/modules/particuleio/addons/kubernetes/latest</li> <li>https://aws.amazon.com/jp/blogs/news/introducing-amazon-eks-add-ons-jp/</li> <li>https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/eks-add-ons.html</li> </ul> </li> <li> <p>CoreDNS</p> </li> <li>Amazon VPC CNI(EKS)</li> </ul>"},{"location":"failure_test/point_of_failure/#controller","title":"Controller","text":"<ul> <li>Ingress Controller</li> <li>External-dns</li> </ul>"},{"location":"failure_test/point_of_failure/#autoscale","title":"AutoScale","text":"<ul> <li>MetricsServer</li> <li> <p>Pod</p> <ul> <li>HPA</li> <li>VPA</li> </ul> </li> <li> <p>Node(Horizontal Scale)</p> <ul> <li>Cluster Autoscaler</li> </ul> </li> </ul>"},{"location":"kubernetes/architecture/garbage-collection/","title":"Garbage Collection","text":""},{"location":"kubernetes/architecture/garbage-collection/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://kubernetes.io/ja/docs/concepts/workloads/controllers/garbage-collection/</li> <li>https://kubernetes.io/docs/concepts/architecture/garbage-collection/</li> <li>https://kubernetes.io/docs/tasks/administer-cluster/use-cascading-deletion</li> <li>https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/</li> <li>https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/</li> <li>https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/</li> <li>https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/object-meta</li> </ul>"},{"location":"kubernetes/architecture/garbage-collection/#about-garbage-collection","title":"About Garbage Collection","text":"<ul> <li>Kubernetes\u304cCluster Resources\u306ecleanup\u3092\u884c\u3046\u4ed5\u7d44\u307f<ul> <li>cleanup\u5bfe\u8c61\u3068\u306a\u308bResources\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e<ol> <li>\u7d42\u4e86\u3057\u305fPod</li> <li>\u5b8c\u4e86\u3057\u305fJob</li> <li>owner reference\u306e\u306a\u3044\u30aa\u30d6\u30b8\u30a7\u30af\u30c8</li> <li>\u672a\u4f7f\u7528\u306e\u30b3\u30f3\u30c6\u30ca\u3068\u30b3\u30f3\u30c6\u30ca\u30a4\u30e1\u30fc\u30b8</li> <li>StorageClass\u306e\u518d\u5229\u7528\u30dd\u30ea\u30b7\u30fc\u304cDelete\u3067\u3042\u308b\u52d5\u7684\u306b\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3055\u308c\u305fPersistentVolume</li> <li>\u5931\u52b9\u307e\u305f\u306f\u671f\u9650\u5207\u308c\u306eCertificateSigningRequests (CSRs)</li> <li>\u6b21\u306e\u30b7\u30ca\u30ea\u30aa\u3067\u524a\u9664\u3055\u308c\u305fNode:</li> <li>\u30af\u30e9\u30a6\u30c9\u4e0a\u3067\u30af\u30e9\u30b9\u30bf\u30fc\u304c\u30af\u30e9\u30a6\u30c9\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9\u30fc\u30de\u30cd\u30fc\u30b8\u30e3\u30fc\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408</li> <li>\u30aa\u30f3\u30d7\u30ec\u30df\u30b9\u3067\u30af\u30e9\u30b9\u30bf\u30fc\u304c\u30af\u30e9\u30a6\u30c9\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9\u30fc\u30de\u30cd\u30fc\u30b8\u30e3\u30fc\u3068\u540c\u69d8\u306e\u30a2\u30c9\u30aa\u30f3\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408</li> <li>Node Lease\u30aa\u30d6\u30b8\u30a7\u30af\u30c8</li> </ol> </li> </ul> </li> </ul>"},{"location":"kubernetes/architecture/garbage-collection/#owner-references","title":"Owner References","text":"<p>Kubernetes\u3067\u306f\u4f9d\u5b58\u95a2\u4fc2\u306b\u3042\u308bResource\u304c\u5b58\u5728\u3057\u307e\u3059\u3002 \u4f8b\u3048\u3070\u3001<code>ReplicaSet</code> \u306e\u5b9a\u7fa9\u306b\u57fa\u3065\u3044\u3066Pod\u304c\u4f5c\u6210\u3055\u308c\u308b\u969b\u3001 <code>metadata.ownerReferences</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u306bReplicaSet\u3092\u7279\u5b9a\u3059\u308b\u60c5\u5831\u3092\u4fdd\u6301\u3057\u307e\u3059\u3002 \u4ed6\u306b\u3082\u3001Service\u5b9a\u7fa9\u306b\u57fa\u3065\u3044\u3066EndpointSlice\u304c\u4f5c\u6210\u3055\u308c\u305f\u308a\u3001KEDA\u306escaledObject\u4f5c\u6210\u6642\u306bHPA\u304c\u5b58\u5728\u3057\u306a\u3051\u308c\u3070\u4f75\u305b\u3066\u4f5c\u6210\u3059\u308b (refs About KEDA)\u306a\u3069Kubernetes\u306e\u591a\u304f\u306eObject\u306f\u3001<code>metadata.ownerReferences</code> \u3092\u4ecb\u3057\u3066\u76f8\u4e92\u306b\u30ea\u30f3\u30af\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>Kubernetes\u306f\u3001ReplicaSet\u3092\u524a\u9664\u3057\u305f\u3068\u304d\u306b\u6b8b\u3055\u308c\u305fPod\u306a\u3069\u3001owner reference\u304c\u306a\u304f\u306a\u3063\u305fObject\u3092\u30c1\u30a7\u30c3\u30af\u3057\u3066\u524a\u9664\u3057\u307e\u3059\u3002</p>"},{"location":"kubernetes/architecture/garbage-collection/#cascading-deletion","title":"Cascading Deletion","text":"<p>Kubernetes\u306fReplicaSet\u3092\u524a\u9664\u3057\u305f\u6642\u306b\u6b8b\u3055\u308c\u305fPod\u306a\u3069owner reference\u304c\u306a\u304f\u306a\u3063\u305fObject\u3092\u30c1\u30a7\u30c3\u30af\u3057\u3066\u524a\u9664\u3057\u307e\u3059\u3002 owner reference\u304c\u306a\u304f\u306a\u3063\u305fObject\u3092\u524a\u9664\u3059\u308b\u3068\u304d\u3001\u30ab\u30b9\u30b1\u30fc\u30c9\u524a\u9664\u3068\u547c\u3070\u308c\u308b\u30d7\u30ed\u30bb\u30b9\u3067\u4f9d\u5b58\u95a2\u4fc2\u306b\u3042\u308bobject\u3092\u81ea\u52d5\u7684\u306b\u524a\u9664\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u5236\u5fa1\u3067\u304d\u307e\u3059\u3002</p> <ul> <li>\u30ab\u30b9\u30b1\u30fc\u30c9\u524a\u9664\u306f2\u7a2e\u985e\u3042\u308a\u307e\u3059<ol> <li>foreground cascading deletion<ul> <li>Owner Object\u304c\u524a\u9664\u9032\u884c\u4e2d\u3068\u306a\u308a\u3001\u5148\u306b\u4f9d\u5b58Resources\u3092cleanup\u3057\u305f\u5f8c\u3067Owner Object\u3092\u524a\u9664\u3059\u308b<ul> <li><code>metadata.deletionTimestamp</code> \u3092\u524a\u9664\u9032\u884c\u4e2d\u3068\u306a\u3063\u305f\u6642\u70b9\u306etimestamp\u3067\u66f4\u65b0</li> <li><code>metadata.finalizers</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u3092 <code>foregroundDeletion</code> \u306b\u8a2d\u5b9a</li> <li>Owner Object\u306f\u524a\u9664\u51e6\u7406\u304c\u5b8c\u4e86\u3059\u308b\u307e\u3067\u306fkubernetes api-server\u3092\u4ecb\u3057\u3066\u8868\u793a\u3055\u308c\u308b</li> </ul> </li> <li>https://kubernetes.io/docs/tasks/administer-cluster/use-cascading-deletion/#use-foreground-cascading-deletion</li> </ul> </li> <li>background cascading deletion<ul> <li>default\u52d5\u4f5c</li> <li>kube-apiserver\u304cOwner Resources\u3092\u3059\u3050\u306b\u524a\u9664\u3057\u3001kube-constroller-manager\u304cbackground\u3067\u4f9d\u5b58Resources\u3092cleanup\u3059\u308b</li> <li>https://kubernetes.io/docs/tasks/administer-cluster/use-cascading-deletion/#use-background-cascading-deletion</li> </ul> </li> </ol> </li> </ul>"},{"location":"kubernetes/architecture/garbage-collection/#metadatafinalizers-field","title":"<code>metadata.finalizers</code> field","text":"<ul> <li>https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/</li> <li>https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/</li> <li>https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/</li> </ul> <p>foreground cascading deletion\u3067\u306f <code>metadata.finalizers</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u3092 <code>foregroundDeletion</code> \u306b\u8a2d\u5b9a\u3057\u307e\u3059\u3002 \u3053\u306e <code>metadata.finalizers</code> field\u306f\u524a\u9664\u5bfe\u8c61\u3068\u3057\u3066\u30de\u30fc\u30af\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u3092\u5b8c\u5168\u306b\u524a\u9664\u3059\u308b\u524d\u306b\u3001\u7279\u5b9a\u306e\u6761\u4ef6\u304c\u6e80\u305f\u3055\u308c\u308b\u307e\u3067Kubernetes\u3092\u5f85\u6a5f\u3055\u305b\u308b\u305f\u3081\u306e\u540d\u524d\u7a7a\u9593\u4ed8\u304d\u306e\u30ad\u30fc\u3067\u3059\u3002</p> <p>finalizers\u306e\u4ee3\u8868\u7684\u306a\u5229\u7528\u65b9\u6cd5\u3068\u3057\u3066\u3001<code>kubernetes.io/pv-protection</code> \u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u308c\u306f <code>PersistentVolume</code> \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u304c\u8aa4\u3063\u3066\u524a\u9664\u3055\u308c\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u306e\u3082\u306e\u3067\u3059\u3002 Kubernetes\u306fPod\u304c<code>PersistentVolume</code> \u3092\u5229\u7528\u4e2d\u306e\u5834\u5408\u306f <code>kubernetes.io/pv-protection</code> finalizers\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002 <code>kubernetes.io/pv-protection</code> finalizers\u304c\u8ffd\u52a0\u3055\u308c\u305f<code>PersistentVolume</code>\u3092\u524a\u9664\u3057\u3088\u3046\u3068\u3057\u305f\u5834\u5408 <code>Terminating</code> Status\u306e\u72b6\u614b\u3067\u5f85\u6a5f\u3057\u307e\u3059\u3002 <code>PersistentVolume</code> \u3092\u5229\u7528\u3057\u3066\u3044\u308bPod\u304c<code>PersistentVolume</code>\u306e\u5229\u7528\u3092\u505c\u6b62\u3059\u308b\u3068Kubernetes\u306f <code>kubernetes.io/pv-protection</code> finalizers\u3092\u524a\u9664\u3057\u3001<code>PersistentVolume</code>\u304c\u524a\u9664\u3055\u308c\u307e\u3059\u3002</p> <p>Garbage Collection\u306b\u304a\u3044\u3066owner object\u306e <code>metadata.finalizers</code> \u306b <code>foregroundDeletion</code> \u304c\u8a2d\u5b9a\u3055\u308c\u305f\u5834\u5408\u3001 owner object\u306f\u3059\u3050\u306b\u524a\u9664\u3055\u308c\u305a\u3001owner reference\u306bowner object\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b(\u4f9d\u5b58\u95a2\u4fc2\u306b\u3042\u308b)object \u306e\u524a\u9664\u304c\u5b8c\u4e86\u3057\u305f\u3089 <code>metadata.finalizers</code> \u304b\u3089 <code>foregroundDeletion</code> \u304c\u524a\u9664\u3055\u308c\u3001owner object\u304c\u524a\u9664\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"kubernetes/architecture/gateway-api/","title":"Gateway API","text":""},{"location":"kubernetes/architecture/gateway-api/#references","title":"References","text":"<ul> <li>https://gateway-api.sigs.k8s.io/</li> <li>https://github.com/kubernetes-sigs/gateway-api/releases</li> <li>https://medium.com/google-cloud-jp/gke-gateway-4150649d8c37</li> <li>https://medium.com/google-cloud-jp/gke-gateway2-317ed62f727f</li> <li>https://amsy810.hateblo.jp/entry/2023/12/01/090000</li> <li>https://qiita.com/ipppppei/items/6edb78b513c7bbe55d91</li> <li>https://thinkit.co.jp/article/19625</li> <li>https://zenn.dev/pkkudo/articles/cb1942011d81ae</li> </ul>"},{"location":"kubernetes/architecture/gateway-api/#about","title":"About","text":"<p>Gateway API\u306fCluster\u306e\u5916\u306b\u3044\u308bClient\u304b\u3089Cluster\u306e\u4e2d\u306b\u3042\u308bService\u3078\u306etraffic routing\u3092\u7ba1\u7406\u3059\u308b\u76ee\u7684\u3067\u7b56\u5b9a\u3055\u308c\u307e\u3057\u305f\u3002</p> <p>Gateway\u30ea\u30bd\u30fc\u30b9(\u57fa\u790e\u3068\u306a\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30b2\u30fc\u30c8\u30a6\u30a7\u30a4/\u30d7\u30ed\u30ad\u30b7\u30b5\u30fc\u30d0) \u3092\u4e2d\u5fc3\u3068\u3057\u305f\u30ea\u30bd\u30fc\u30b9\u306e\u30b3\u30ec\u30af\u30b7\u30e7\u30f3\u3067\u3042\u308a\u3001\u30b7\u30f3\u30d7\u30eb\u306a\u6a5f\u80fd\u63d0\u4f9b\u306b\u3068\u3069\u307e\u308bIngress\u306b\u5bfe\u3057\u3066\u30d7\u30ed\u30c8\u30b3\u30eb\u3084traffic routing\u306a\u3069\u904b\u7528\u9762\u3067\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a5f\u80fd\u306a\u3069\u304c\u8ffd\u52a0\u3055\u308c\u305fNetwork\u6a5f\u80fd\u3092\u63d0\u4f9b\u3059\u308b\u3082\u306e\u3067\u3059\u3002</p> <p>2023/10/31\u306bv1.0: GA Release \u3055\u308c\u305fKubernetes Project\u3067\u3059\u3002</p> <p>Gateway API is an official Kubernetes project focused on L4 and L7 routing in Kubernetes. This project represents the next generation of Kubernetes Ingress, Load Balancing, and Service Mesh APIs. From the outset, it has been designed to be generic, expressive, and role-oriented. -- Introduction \u304b\u3089\u5f15\u7528</p> <p>Ingress\u306b\u306f\u3044\u304f\u3064\u304b\u306e\u8ab2\u984c\u304c\u3042\u308a\u307e\u3059\u3002 refs https://blog.nginx.org/blog/5-reasons-to-try-the-kubernetes-gateway-api</p> <ol> <li>\u30a4\u30f3\u30d5\u30e9\u7ba1\u7406\u8005/Cluster\u7ba1\u7406\u8005/\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u958b\u767a\u8005 \u9593\u306e\u8cac\u4efb\u5206\u754c\u70b9\u304c\u660e\u78ba\u3067\u306f\u306a\u304b\u3063\u305f</li> <li>L7\u3057\u304b\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u306a\u3044</li> <li>\u7d30\u304b\u306atraffic routing\u306fingress controller(Nginx IngressController\u3084AWS load-balancer-controller\u306a\u3069) \u3054\u3068\u306b\u63d0\u4f9b\u3055\u308c\u308bannotation\u3067\u7ba1\u7406\u3059\u308b\u305f\u3081\u74b0\u5883\u4f9d\u5b58\u304c\u767a\u751f\u3059\u308b</li> </ol> <p>Gateway API\u306f\u3053\u308c\u3089\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3059\u308b\u3053\u3068\u3082\u8003\u616e\u3057\u7b56\u5b9a\u3055\u308c\u307e\u3057\u305f\u3002</p> <ol> <li>\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30ea\u30bd\u30fc\u30b9\u30e2\u30c7\u30eb\u306b\u3088\u308a\u5f79\u5272\u304c\u5206\u304b\u308c\u3066\u3044\u308b<ul> <li></li> </ul> </li> <li>L4\u3068L7\u3092\u30b5\u30dd\u30fc\u30c8</li> <li>HTTPS/HTTP \u3060\u3051\u3067\u306a\u304fTLS\u3084TCP/UDP\u306a\u3069\u69d8\u3005\u306atraffic routing\u3092\u30b5\u30dd\u30fc\u30c8<ul> <li>https://gateway-api.sigs.k8s.io/guides/</li> </ul> </li> </ol>"},{"location":"kubernetes/architecture/gateway-api/#architecture","title":"Architecture","text":"<ul> <li>https://gateway-api.sigs.k8s.io/concepts/api-overview/</li> <li>https://docs.nginx.com/nginx-gateway-fabric/overview/gateway-architecture/</li> </ul>"},{"location":"kubernetes/architecture/gateway-api/#resources","title":"Resources","text":"<ul> <li>https://gateway-api.sigs.k8s.io/concepts/api-overview/<ul> <li><code>GatewayClass</code><ul> <li>https://gateway-api.sigs.k8s.io/api-types/gatewayclass/</li> </ul> </li> <li><code>Gateway</code><ul> <li>https://gateway-api.sigs.k8s.io/api-types/gateway/</li> </ul> </li> <li><code>xRoute</code><ul> <li><code>HTTPRoute</code><ul> <li>https://gateway-api.sigs.k8s.io/guides/http-routing/</li> <li>https://gateway-api.sigs.k8s.io/api-types/httproute/</li> <li>https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.HTTPRoute</li> </ul> </li> <li><code>TLSRoute</code><ul> <li>https://gateway-api.sigs.k8s.io/guides/tls/</li> <li>https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1alpha2.TLSRoute</li> </ul> </li> <li><code>TCPRoute</code> / <code>UDPRoute</code><ul> <li>https://gateway-api.sigs.k8s.io/guides/tcp/</li> <li>https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1alpha2.TCPRoute</li> </ul> </li> <li><code>GRPCRoute</code><ul> <li>v1.1.0 \u3067GA release</li> <li>https://gateway-api.sigs.k8s.io/guides/grpc-routing/</li> <li>https://gateway-api.sigs.k8s.io/api-types/grpcroute/</li> <li>https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1.GRPCRoute</li> </ul> </li> </ul> </li> <li><code>ReferenceGrant</code><ul> <li>https://gateway-api.sigs.k8s.io/api-types/referencegrant/</li> <li>https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io/v1alpha2.ReferenceGrant</li> </ul> </li> <li><code>BackendTLSPolicy</code></li> </ul> </li> </ul>"},{"location":"kubernetes/architecture/gateway-api/#backendtlspolicy","title":"BackendTLSPolicy","text":"<p>v1.1.0\u3067\u306f <code>v1alpha3</code> \u306e\u6a5f\u80fd\u3067\u3059\u3002</p> <p>Gateway \u304b\u3089 backend service\u306bHTTPS\u3067traffic routing\u3059\u308b\u5834\u5408\u306eTLS\u8a2d\u5b9a\u3067\u3059\u3002Cluster\u5916\u306eclient\u304b\u3089\u306eTLS\u6697\u53f7\u3092backend service\u3067\u7d42\u7aef\u3055\u305b\u308b <code>passthrough</code> \u3068 Cluster\u5916\u306eclient\u304b\u3089\u306eTLS\u6697\u53f7\u3092Gateway\u3067\u7d42\u7aef\u3055\u305b\u3066Gateway\u3068backend service\u3067\u65b0\u305f\u306aTLS\u6697\u53f7\u3092\u884c\u3046 <code>edge</code> \u304c\u3042\u308a\u307e\u3059\u3002</p> <ul> <li>https://gateway-api.sigs.k8s.io/guides/tls/</li> <li>https://gateway-api.sigs.k8s.io/api-types/backendtlspolicy/</li> <li>https://gateway-api.sigs.k8s.io/geps/gep-1897/</li> </ul>"},{"location":"kubernetes/architecture/gateway-api/#referencegrant","title":"ReferenceGrant","text":"<ul> <li>https://gateway-api.sigs.k8s.io/api-types/referencegrant/<ul> <li>https://qiita.com/ipppppei/items/8e6e1f0e77de10e2446d</li> <li>https://thinkit.co.jp/article/21458</li> </ul> </li> </ul>"},{"location":"kubernetes/architecture/gateway-api/#progressive-delivery","title":"Progressive delivery","text":"<p>xRoute\u30ea\u30bd\u30fc\u30b9\u306e <code>BackendRef</code> \u306b\u306f <code>weight</code> properties \u304c\u3042\u308a\u3001Progressive delivery\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u307e\u3059\u3002</p> <p>Argo Rollouts\u3082Gateway API\u3067\u306eProgressive delivery\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002</p> <ul> <li>https://gateway-api.sigs.k8s.io/implementations/#argo-rollouts</li> <li>https://argo-rollouts.readthedocs.io/en/stable/features/traffic-management/plugins/#gateway-API</li> <li>https://github.com/argoproj-labs/rollouts-plugin-trafficrouter-gatewayapi/</li> </ul>"},{"location":"kubernetes/architecture/gateway-api/#service-mesh","title":"Service Mesh","text":"<p>Gateway API\u306fCluster\u306e\u5916\u306b\u3044\u308bClient\u304b\u3089Cluster\u306e\u4e2d\u306b\u3042\u308bService\u3078\u306etraffic routing\u3092\u7ba1\u7406\u3059\u308b\u76ee\u7684\u3067\u7b56\u5b9a\u3055\u308c\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u6642\u304c\u7d4c\u3064\u306b\u3064\u308c\u3066Service Mesh\u306b\u5bfe\u3059\u308b\u95a2\u5fc3\u304c\u9ad8\u307e\u3063\u3066\u304d\u3066\u304a\u308a\u3001Gateway API\u3092\u540c\u4e00Cluster\u5185\u306eeast/west traffic routing\u306b\u5229\u7528\u3059\u308b\u65b9\u6cd5\u3092\u7b56\u5b9a\u3059\u308b\u76ee\u7684\u3067 GAMMA(Gateway API for Mesh Management and Administration) Initiative \u304c\u7acb\u3061\u4e0a\u304c\u308a\u307e\u3057\u305f\u3002</p> <p>2024/05/9\u306b v1.1.0 \u3067 Gateway API for Service Mesh \u304cGA Release\u3055\u308c\u307e\u3057\u305f\u3002\u305d\u308c\u306b\u4f34\u3044Istio v1.22\u3067 Gateway API Mesh Support Promoted To Stable \u3068\u306a\u308a\u307e\u3057\u305f\u3002</p> <ul> <li>https://istio.io/latest/docs/tasks/traffic-management/ingress/gateway-api/</li> <li>https://developer.mamezou-tech.com/blogs/2022/07/24/k8s-gateway-api-intro/</li> </ul>"},{"location":"kubernetes/architecture/gateway-api/#implementations","title":"Implementations","text":"<p>https://gateway-api.sigs.k8s.io/implementations/</p> <p>Gateway API\u3092\u5b9f\u88c5\u3057\u305f\u88fd\u54c1\u7fa4</p>"},{"location":"kubernetes/architecture/gateway-api/#argo-rollouts","title":"Argo Rollouts","text":"<ul> <li>https://rollouts-plugin-trafficrouter-gatewayapi.readthedocs.io/en/latest/</li> </ul>"},{"location":"kubernetes/architecture/gateway-api/#cloud-provider","title":"Cloud Provider","text":""},{"location":"kubernetes/architecture/gateway-api/#aws","title":"AWS","text":"<ul> <li>https://www.gateway-api-controller.eks.aws.dev/latest/</li> <li>AWS Gateway API Controller for VPC Lattice</li> </ul>"},{"location":"kubernetes/architecture/gateway-api/#google-cloud","title":"Google Cloud","text":"<ul> <li>https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-api?hl=ja</li> </ul>"},{"location":"kubernetes_dashboard/","title":"kubernetes dashboard","text":""},{"location":"kubernetes_dashboard/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://github.com/kubernetes/dashboard</li> <li>https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/</li> </ul>"},{"location":"load_test/locust/about/","title":"Locust","text":""},{"location":"load_test/locust/about/#reference","title":"Reference","text":"<ul> <li>https://locust.io/</li> <li>https://docs.locust.io/en/stable/</li> <li>https://github.com/locustio/locust</li> </ul>"},{"location":"load_test/locust/about/#about","title":"About","text":"<ul> <li>Python\u88fd\u306e\u8ca0\u8377\u8a66\u9a13\u30c4\u30fc\u30eb</li> <li>Python\u3067\u30c6\u30b9\u30c8\u30b1\u30fc\u30b9\u3092\u8868\u73fe\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd</li> <li>Web\u30d9\u30fc\u30b9\u306eUI\u3092\u5099\u3048\u3066\u3044\u308b</li> <li>CLI\u3060\u3051\u3067\u3082\u8ca0\u8377\u8a66\u9a13\u3092\u5b9f\u884c\u53ef\u80fd(headless)</li> <li>\u8ca0\u8377\u8a66\u9a13\u4e2d\u306eRPS\u9077\u79fb\u3092\u30b0\u30e9\u30d5\u3067\u53ef\u8996\u5316\u53ef\u80fd(CLI\u5358\u4f53\u5b9f\u884c\u3067\u3082summary\u3092HTML\u51fa\u529b\u53ef\u80fd)</li> <li>master/worker \u69cb\u6210\u3067\u9ad8\u3044\u8ca0\u8377(RPS)\u3092\u304b\u3051\u308b\u3053\u3068\u304c\u53ef\u80fd</li> </ul>"},{"location":"load_test/locust/about/#locustrps","title":"Locust\u306b\u304a\u3051\u308bRPS\u3092\u6c7a\u3081\u308b\u305f\u3081\u306e\u30d1\u30e9\u30e1\u30fc\u30bf","text":""},{"location":"load_test/locust/about/#users","title":"users","text":"<ul> <li>\u540c\u6642\u63a5\u7d9a\u30e6\u30fc\u30b6\u306e\u6700\u5927\u6570<ul> <li>CLI\u3067 <code>--users=100</code> \u306e\u5834\u5408\u3001\u6700\u5927\u540c\u6642\u63a5\u7d9a\u3059\u308b\u30e6\u30fc\u30b6\u3092100\u30e6\u30fc\u30b6\u307e\u3067\u5897\u3084\u3059\u3053\u3068\u304c\u3067\u304d\u308b</li> <li>1000RPS\u306e\u8ca0\u8377\u3092\u304b\u3051\u305f\u3044\u5834\u5408\u3001<code>--users=1000</code> \u3068\u306a\u308b</li> </ul> </li> </ul>"},{"location":"load_test/locust/about/#spawn-rate","title":"spawn rate","text":"<ul> <li>1\u79d2\u3054\u3068\u306b\u5897\u52a0\u3055\u305b\u308b\u540c\u6642\u63a5\u7d9a\u30e6\u30fc\u30b6\u6570<ul> <li>CLI\u3067 <code>--spawn-rate=1</code> \u306e\u5834\u5408\u3001\u540c\u6642\u63a5\u7d9a\u3059\u308b\u30e6\u30fc\u30b6\u3092\u79d2\u95931\u30e6\u30fc\u30b6\u305a\u3064\u5897\u3084\u3059\u3053\u3068\u306b\u306a\u308b</li> <li><code>--spawn-rate=10</code> \u306e\u5834\u5408\u30011000RPS\u306b\u5230\u9054\u3059\u308b\u306e\u306f100\u79d2\u5f8c\u3068\u306a\u308b</li> </ul> </li> </ul>"},{"location":"load_test/locust/about/#masterworker","title":"master/worker","text":"<ul> <li>https://docs.locust.io/en/stable/running-distributed.html<ul> <li>Python\u306f\u30d7\u30ed\u30bb\u30b9\u3054\u3068\u306b1\u3064\u4ee5\u4e0a\u306e\u30b3\u30a2\u3092\u5b8c\u5168\u306b\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u305b\u3093(GIL \u53c2\u7167)\u3002   Locust\u3092\u5b9f\u884c\u3059\u308b\u30de\u30b7\u30f3\u306ecompute resource\u3092\u5168\u3066\u5229\u7528\u3059\u308b\u305f\u3081\u306b\u8907\u6570\u30d7\u30ed\u30bb\u30b9(<code>master</code> \u30d7\u30ed\u30bb\u30b9\u3068 <code>worker</code> \u30d7\u30ed\u30bb\u30b9) \u3092\u8d77\u52d5\u30fb\u540c\u671f\u3057\u3066\u8ca0\u8377\u3092\u304b\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002   \u307e\u305f\u3001Locust\u3092\u5b9f\u884c\u3059\u308b\u30de\u30b7\u30f3\u3092\u8907\u6570\u53f0\u7528\u610f\u3057\u305d\u308c\u305e\u308c\u3067\u30d7\u30ed\u30bb\u30b9\u3092\u8d77\u52d5\u30fb\u540c\u671f\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002</li> <li><code>master</code> \u30d7\u30ed\u30bb\u30b9\u3068 <code>worker</code> \u30d7\u30ed\u30bb\u30b9 \u306f<code>5557/TCP</code> \u3067\u901a\u4fe1\u3057\u307e\u3059</li> <li>AWS\u306a\u3069\u306ecloud\u74b0\u5883\u306ecompute resource\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u306fSecurityGroup\u306a\u3069FW\u306e\u30dd\u30fc\u30c8\u3092\u89e3\u653e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</li> <li><code>--master-bind-port</code> \u3067\u30dd\u30fc\u30c8\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059(worker\u5074\u3082 <code>--master-port</code> \u3067\u5909\u66f4\u304c\u5fc5\u8981)</li> <li>Usage<ul> <li>master     <pre><code># with WebUI\nlocust -f locustfile.py --master\n\n# without WebUI\nlocust -f locustfile.py --master --headless\n\n# wait until worker process start completly with expected process count\n# (require with --headless)\nlocust -f locustfile.py --master --headless --expect-workers=5\n</code></pre></li> <li>worker     <pre><code>locust -f locustfile.py --worker --master-host=&lt;IP\u30a2\u30c9\u30ec\u30b9&gt;\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"load_test/locust/about/#cliheadless","title":"CLI\u5358\u4f53\u3067\u5b9f\u884c(headless)","text":"<ul> <li>https://docs.locust.io/en/stable/running-without-web-ui.html</li> <li>https://docs.locust.io/en/stable/quickstart.html#direct-command-line-usage-headless</li> </ul>"},{"location":"load_test/locust/about/#locust-http-client","title":"Locust HTTP Client","text":"<ul> <li> <p><code>HttpUser</code></p> <ul> <li>https://github.com/locustio/locust/blob/320cba0816e1ea38904d70b4672e173a9300747d/locust/user/users.py#L219</li> <li>https://docs.locust.io/en/stable/api.html#httpuser-class</li> <li><code>HttpUser</code> \u306f <code>request</code> \u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u305f\u3081   \u9ad8\u30b9\u30eb\u30fc\u30d7\u30c3\u30c8\u6642\u306bCPU\u5229\u7528\u52b9\u7387\u304c\u60aa\u304f\u60f3\u5b9a\u901a\u308a\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u304c\u3067\u306a\u3044\u5834\u5408\u304c\u3042\u308a\u307e\u3059</li> </ul> </li> <li> <p><code>FastHttpUser</code></p> <ul> <li>https://github.com/locustio/locust/blob/320cba0816e1ea38904d70b4672e173a9300747d/locust/contrib/fasthttp.py#L287</li> <li>https://docs.locust.io/en/stable/increase-performance.html</li> <li><code>HttpUser</code> Class\u3068\u9055\u3044 <code>geventhttpclient</code> \u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3067   CPU\u5229\u7528\u52b9\u7387\u304c\u3042\u304c\u308a <code>HttpUser</code> \u5229\u7528\u6642\u306b\u6bd4\u3079\u3066\u3088\u308a\u9ad8\u30b9\u30eb\u30fc\u30d7\u30c3\u30c8\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u51fa\u305b\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059</li> </ul> </li> </ul>"},{"location":"load_test/locust/about/#custom-client","title":"Custom Client","text":"<ul> <li>https://docs.locust.io/en/stable/testing-other-systems.html<ul> <li><code>User</code> \u3084 <code>HttpUser</code> / <code>FastHttpUser</code> \u3092\u7d99\u627f\u3055\u305b\u308b\u3053\u3068\u3067\u72ec\u81ea\u306eHTTP Client\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059</li> </ul> </li> </ul>"},{"location":"load_test/locust/about/#custom-load-shapes","title":"Custom load shapes","text":"<ul> <li>https://docs.locust.io/en/stable/custom-load-shape.html<ul> <li><code>locust</code> \u5b9f\u884c\u6642\u306b(WebUI\u3082\u3057\u304f\u306fCLI\u5f15\u6570)\u3067\u6307\u5b9a\u3059\u308bUser\u6570\u3084spawn rate\u3067\u306f\u5236\u5fa1\u3067\u304d\u306a\u3044\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u304c\u3042\u308a\u307e\u3059\u3002   \u4f8b\u3048\u3070\u3001\u8ca0\u8377\u306e\u30b9\u30d1\u30a4\u30af\u3092\u767a\u751f\u3055\u305b\u305f\u308a\u3001\u8a66\u9a13\u306e\u6642\u9593\u7d4c\u904e\u306b\u3088\u3063\u3066RPS\u3092\u5897\u3084\u3057\u305f\u308a\u6e1b\u3089\u3057\u305f\u308a\u3001\u3068\u3044\u3063\u305f\u30b1\u30fc\u30b9\u3067\u3059\u3002</li> <li> <p><code>LoadTestShape</code> Class\u3092\u7d99\u627f\u3057\u305fClass\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002Locust\u306f\u305d\u306eClass\u3092\u81ea\u52d5\u7684\u306b\u898b\u3064\u3051\u3066\u4f7f\u7528\u3057\u307e\u3059\u3002   \u3053\u306eClass\u5185\u3067 User\u6570\u3068spawn rate\u3092turple\u3067\u8fd4\u3059 <code>tick</code> Method\u3092\u5b9a\u7fa9\u3057\u307e\u3059(\u7d42\u4e86\u6642\u306f <code>None</code> \u3092\u8fd4\u3059)\u3002   Locust\u306f1\u79d2\u306b1\u56de\u7a0b\u5ea6 <code>tick</code> Method\u3092\u547c\u3073\u51fa\u3057\u307e\u3059\u3002</p> <p>sample <pre><code>from locust import FastHttpUser, task, constant_pacing, LoadTestShape\n\n\nclass SampleCustomShape(LoadTestShape):\n    stages = [\n        {\"duration\": 120, \"users\": 10, \"spawn_rate\": 1},\n        {\"duration\": 240, \"users\": 10, \"spawn_rate\": 0},\n    ]\n\n    def tick(self):\n        run_time = self.get_run_time()\n\n        for stage in self.stages:\n            if run_time &lt; stage[\"duration\"]:\n                tick_data = (stage[\"users\"], stage[\"spawn_rate\"])\n                return tick_data\n\n        return None\n\n\nclass SampleLoadTest(FastHttpUser):\n    wait_time = constant_pacing(1)\n\n    host = \"http://localhost:3000\"\n\n    @task\n    def task(self):\n        name = self.host\n        self.client.get(\"/\",)\n</code></pre> </p> </li> </ul> </li> </ul>"},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/","title":"bootstrapping kubernetes-dashboard","text":""},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/#about-kubernetes-dashboard","title":"about kubernetes-dashboard","text":"<p>https://github.com/kubernetes/dashboard</p> <p>Kubernetes Dashboard\u306f\u3001Kubernetes\u30af\u30e9\u30b9\u30bf\u7528\u306e\u6c4e\u7528\u7684\u306aWeb\u30d9\u30fc\u30b9\u306eUI\u3067\u3059\u3002\u30af\u30e9\u30b9\u30bf\u30fc\u3067\u52d5\u4f5c\u3059\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u7ba1\u7406\u3084\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u306e\u307b\u304b\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u81ea\u4f53\u306e\u7ba1\u7406\u3082\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://github.com/kubernetes/dashboard/blob/master/docs/common/dashboard-arguments.md</li> <li>https://itnext.io/how-to-expose-your-kubernetes-dashboard-with-cert-manager-422ab1e3bf30</li> <li>https://magda.io/docs/how-to-setup-https-to-local-cluster.html</li> <li>https://vmwire.com/2022/02/07/running-kubernetes-dashboard-with-signed-certificates/</li> <li>https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard</li> <li>https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard</li> </ul>"},{"location":"monitoring/kubernetes-dashboard/bootstrapping_kubernetes-dashboard/#install","title":"install","text":"<ol> <li> <p>create self signed certificate</p> <pre><code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout kubernetes-dashboard.key -out kubernetes-dashboard.crt -subj \"/CN=k8s-dashboard.local/O=k8s-dashboard.local\"\n</code></pre> </li> <li> <p>create ns and secret</p> <pre><code>kubectl create ns kubernetes-dashboard\nkubectl create secret generic kubernetes-dashboard-certs --from-file=./ -n kubernetes-dashboard\n</code></pre> </li> <li> <p>self-signed certificates regist to key chain(Mac OSX)</p> <ul> <li>Chrome\u3067\u30a2\u30af\u30bb\u30b9\u3057\u305f\u969b\u306b\u4e0d\u6b63\u306a\u8a3c\u660e\u66f8\u3068\u3057\u3066\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u81ea\u5df1\u7f72\u540d\u8a3c\u660e\u66f8\u3092\u4fe1\u983c\u6e08\u307f\u8a3c\u660e\u66f8\u3068\u3057\u3066\u767b\u9332\u3059\u308b</li> <li></li> </ul> </li> <li> <p>download kubernetes-dashboard manifests</p> <pre><code>sudo curl -o /etc/kubernetes/manifests/kubernetes-dashboard.yaml https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n</code></pre> </li> <li> <p>edit of kubernetes-dashboard manifests</p> <pre><code>sudo vim /etc/kubernetes/manifests/kubernetes-dashboard.yaml\n</code></pre> <ul> <li> <p>Service\u30ea\u30bd\u30fc\u30b9\u306eType\u3092 <code>NodePort</code> \u306b\u5909\u66f4</p> <p>diff <pre><code>@@ -37,6 +37,7 @@\n   name: kubernetes-dashboard\n   namespace: kubernetes-dashboard\n spec:\n+  type: NodePort\n   ports:\n     - port: 443\n       targetPort: 8443\n</code></pre> <li> <p>kubernetes-dashboard\u30b3\u30f3\u30c6\u30ca\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306bTLS\u8a3c\u660e\u66f8\u30d5\u30a1\u30a4\u30eb\u3068TLS\u9375\u30d5\u30a1\u30a4\u30eb\u3092\u6307\u5b9a\u3059\u308b</p> <p>diff <pre><code>@@ -198,6 +199,8 @@\n           args:\n             - --auto-generate-certificates\n             - --namespace=kubernetes-dashboard\n+            - --tls-cert-file=/tls.crt\n+            - --tls-key-file=/tls.key\n</code></pre> <li> <p>apply kubernetes-dashboard manifests</p> <pre><code>kubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard.yaml\n</code></pre> </li> <li> <p>create ingress</p> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/kubernetes/manifests/kubernetes-dashboard-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\nspec:\n  tls:\n    - hosts:\n      - k8s-dashboard.local\n      secretName: dashboard-secret-tls\n  rules:\n  - host: k8s-dashboard.local\n    http:\n      paths:\n        - pathType: Prefix\n          path: \"/\"\n          backend:\n            service:\n              name: kubernetes-dashboard\n              port:\n                number: 443\nEOF\n\nkubectl apply -f /etc/kubernetes/manifests/kubernetes-dashboard-ingress.yaml\n</code></pre> </li> <li> <p>adding fqdn and node ip address to <code>/etc/hosts</code></p> <ul> <li> <p>get node ip</p> <pre><code>kubectl get pods -n kubernetes-dashboard -l k8s-app=kubernetes-dashboard -o json | jq -r .items[].status.hostIP\n</code></pre> </li> <li> <p>adding entry to <code>/etc/hosts</code></p> <pre><code>&lt;Node IP ADDRESS&gt; `k8s-dashboard.local`\n</code></pre> </li> </ul> </li> <li> <p>get node port</p> <pre><code>kubectl get service -n kubernetes-dashboard kubernetes-dashboard -o json | jq -r .spec.ports[].nodePort\n</code></pre> </li> <li> <p>access kubernetes-dashboard with browser</p> <ul> <li><code>https://k8s-dashboard.local:&lt;node port of previous command result&gt;</code><ul> <li></li> </ul> </li> </ul> </li> <li> <p>create token strings</p> <pre><code>kubectl create serviceaccount dashboard -n default\nkubectl create clusterrolebinding dashboard-admin -n default --clusterrole=cluster-admin --serviceaccount=default:dashboard\nTOKEN=`kubectl get secret $(kubectl get serviceaccount dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode`\necho $TOKEN\n</code></pre> </li> <li> <p>input token and login</p> <ul> <li>input previous command result as token strings</li> <li></li> </ul> </li> <li> <p>login successed</p> <ul> <li></li> </ul> </li>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/","title":"metrics-server","text":"<p><code>metrics-server</code>\u3068\u306fkubelet\u3084kube-apiserver\u304b\u3089\u5404\u7a2e\u30ea\u30bd\u30fc\u30b9\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002 \u53ce\u96c6\u3057\u305f\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u60c5\u5831\u306fautoscaling(HPA \u3084 VPA)\u3092\u884c\u3046\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_1","title":"\u53c2\u8003","text":"<ul> <li>metrics-server<ul> <li>https://github.com/kubernetes-sigs/metrics-server<ul> <li>docs/command-line-flags.txt</li> </ul> </li> <li>https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/</li> </ul> </li> <li>kube-apiserver Aggregation Layer\u8a2d\u5b9a\u306b\u3064\u3044\u3066<ul> <li>https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/</li> <li>https://kubernetes.io/ja/docs/concepts/cluster-administration/proxies/</li> <li>https://github.com/ansilh/kubernetes-the-hardway-virtualbox/blob/master/15.Deploy-Metric-Server.md</li> </ul> </li> </ul>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_2","title":"\u8981\u4ef6","text":"<ul> <li>https://github.com/kubernetes-sigs/metrics-server#requirements<ul> <li>Metrics Server must be reachable from kube-apiserver by container IP address (or node IP if hostNetwork is enabled).</li> <li>The kube-apiserver must enable an aggregation layer.</li> <li>Nodes must have Webhook authentication and authorization enabled.</li> <li>Kubelet certificate needs to be signed by cluster Certificate Authority (or disable certificate validation by passing --kubelet-insecure-tls to Metrics Server)</li> <li>Container runtime must implement a container metrics RPCs (or have cAdvisor support)</li> </ul> </li> </ul>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_3","title":"\u69cb\u7bc9\u624b\u9806","text":""},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#kube-apiserver-aggregation-layer","title":"kube-apiserver Aggregation Layer\u8a2d\u5b9a","text":"<p>Info</p> <p>https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server</p> <p>Metrics Server collects metrics from the Summary API, exposed by Kubelet on each node, and is registered with the main API server via Kubernetes aggregator.</p> <ul> <li>kube-apiserver\u3067Aggregation Layer\u3092\u6709\u52b9\u306b\u3059\u308b<ul> <li>Aggregation Layer\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408\u306fmetrics-server\u3067\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u30ed\u30b0\u304c\u51fa\u3066\u3044\u308b     <pre><code>E0217 15:13:53.378655       1 webhook.go:224] Failed to make webhook authorizer request: Post \"https://10.32.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s\": cont\next canceled\nE0217 15:13:53.378917       1 errors.go:77] Post \"https://10.32.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s\": context canceled\nE0217 15:13:53.379124       1 timeout.go:137] post-timeout activity - time-elapsed: 121.389\u00b5s, GET \"/apis/metrics.k8s.io/v1beta1\" result: &lt;nil&gt;\n</code></pre></li> <li>kube-apiserver front-proxy(for aggregation layer)\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8<ul> <li>front-proxy\u7528CA\u8a3c\u660e\u66f8\u304a\u3088\u3073\u30b5\u30fc\u30d0\u8a3c\u660e\u66f8\u3068\u79d8\u5bc6\u9375\u3092\u751f\u6210\u3059\u308b<ul> <li><code>/var/lib/kubernetes/front-proxy-ca.pem</code></li> <li><code>/var/lib/kubernetes/front-proxy.pem</code></li> <li><code>/var/lib/kubernetes/front-proxy-key.pem</code></li> </ul> </li> </ul> </li> <li>/setup/06_master/03_bootstrapping_kube-apiserver/<ul> <li>kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u8ffd\u52a0     <pre><code>--enable-aggregator-routing=true\n--requestheader-client-ca-file=/var/lib/kubernetes/front-proxy-ca.pem\n--requestheader-allowed-names=front-proxy-ca\n--requestheader-extra-headers-prefix=X-Remote-Extra\n--requestheader-group-headers=X-Remote-Group\n--requestheader-username-headers=X-Remote-User\n--proxy-client-cert-file=/var/lib/kubernetes/front-proxy.pem\n--proxy-client-key-file=/var/lib/kubernetes/front-proxy-key.pem\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#metics-server","title":"metics-server\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ol> <li> <p>manifests\u3092deploy</p> <pre><code>$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n</code></pre> </li> <li> <p>metrics-server \u304c\u8d77\u52d5\u306f\u3059\u308b\u304cTLS\u95a2\u9023\u306e\u30a8\u30e9\u30fc\u3067\u6b63\u5e38\u306b\u52d5\u4f5c\u3057\u306a\u3044</p> <ul> <li><code>kubectl logs</code> \u30b3\u30de\u30f3\u30c9     <pre><code>Failed to scrape node\" err=\"Get \\\"https://192.168.10.51:10250/metrics/resource\\\": x509: certificate is valid for 192.168.10.50, not 192.168.10.51\" node=\"k8s-node1\n</code></pre></li> </ul> </li> <li> <p>TLS\u8a8d\u8a3c\u8a2d\u5b9a\u306e\u5909\u66f4</p> <ul> <li>metrics-server\u306e\u8d77\u52d5\u5f15\u6570\u306b <code>--kubelet-insecure-tls</code> \u3092\u8ffd\u52a0<ul> <li>https://github.com/kubernetes-sigs/metrics-server/issues/131</li> <li>https://github.com/kubernetes-sigs/metrics-server/issues/300</li> <li>kubectl logs\u30b3\u30de\u30f3\u30c9\u3067\u4ee5\u4e0b\u30a8\u30e9\u30fc\u304c\u51fa\u7d9a\u3051\u3066\u3044\u308b\u5834\u5408\u306e\u5bfe\u51e6</li> <li>TLS\u8a3c\u660e\u66f8\u306e\u691c\u8a3c\u3092\u884c\u308f\u306a\u3044\u3088\u3046\u306b\u3059\u308b(\u8a3c\u660e\u66f8\u306e\u7f72\u540d\u304cmetrics-serverg\u304c\u60f3\u5b9a\u3059\u308bCA\u3067\u306f\u306a\u3044\u305f\u3081)     <pre><code>kubectl patch deploy metrics-server -n kube-system --patch \"\nspec:\n  template:\n    spec:\n      containers:\n      - args:\n        - --cert-dir=/tmp\n        - --secure-port=4443\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        - --kubelet-insecure-tls\n        name: metrics-server\n\"\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>\u8d77\u52d5\u3057\u305f\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>I0217 14:54:41.737667       1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)\nI0217 14:54:42.981853       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController\nI0217 14:54:42.981913       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController\nI0217 14:54:42.981901       1 configmap_cafile_content.go:201] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::client-ca-file\"\nI0217 14:54:42.981970       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\nI0217 14:54:42.981993       1 configmap_cafile_content.go:201] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\"\nI0217 14:54:42.982036       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\nI0217 14:54:42.985611       1 secure_serving.go:266] Serving securely on [::]:4443\nI0217 14:54:42.985841       1 tlsconfig.go:240] \"Starting DynamicServingCertificateController\"\nW0217 14:54:42.986181       1 shared_informer.go:372] The sharedIndexInformer has started, run more than once is not allowed\nI0217 14:54:42.987234       1 dynamic_serving_content.go:131] \"Starting controller\" name=\"serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key\"\nI0217 14:54:43.082779       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\nI0217 14:54:43.082892       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController\nI0217 14:54:43.082896       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\n</code></pre></p> </li> </ol>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#_4","title":"\u52d5\u4f5c\u78ba\u8a8d","text":""},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#api-path","title":"API Path","text":"<ul> <li>pods     <pre><code>$ kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/pods\" | jq '.items[] | select(.metadata.name == \"coredns-675db8b7cc-hbzb2\")'\n{\n  \"metadata\": {\n    \"name\": \"coredns-675db8b7cc-hbzb2\",\n    \"namespace\": \"kube-system\",\n    \"creationTimestamp\": \"2022-02-17T16:00:21Z\",\n    \"labels\": {\n      \"k8s-app\": \"kube-dns\",\n      \"pod-template-hash\": \"675db8b7cc\"\n    }\n  },\n  \"timestamp\": \"2022-02-17T16:00:03Z\",\n  \"window\": \"15.692s\",\n  \"containers\": [\n    {\n      \"name\": \"coredns\",\n      \"usage\": {\n        \"cpu\": \"7990556n\",\n        \"memory\": \"14064Ki\"\n      }\n    }\n  ]\n}\n</code></pre></li> <li>nodes     <pre><code>$ kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | jq .                                                                                              [22/47786]\n{\n  \"kind\": \"NodeMetricsList\",\n  \"apiVersion\": \"metrics.k8s.io/v1beta1\",\n  \"metadata\": {},\n  \"items\": [\n    {\n      \"metadata\": {\n        \"name\": \"k8s-master\",\n        \"creationTimestamp\": \"2022-02-17T14:58:58Z\",\n        \"labels\": {\n          \"beta.kubernetes.io/arch\": \"arm64\",\n          \"beta.kubernetes.io/os\": \"linux\",\n          \"kubernetes.io/arch\": \"arm64\",\n          \"kubernetes.io/hostname\": \"k8s-master\",\n          \"kubernetes.io/os\": \"linux\"\n        }\n      },\n      \"timestamp\": \"2022-02-17T14:58:49Z\",\n      \"window\": \"10.198s\",\n      \"usage\": {\n        \"cpu\": \"273214048n\",\n        \"memory\": \"1024976Ki\"\n      }\n    },\n    {\n      \"metadata\": {\n        \"name\": \"k8s-node1\",\n        \"creationTimestamp\": \"2022-02-17T14:58:58Z\",\n        \"labels\": {\n          \"beta.kubernetes.io/arch\": \"arm64\",\n          \"beta.kubernetes.io/os\": \"linux\",\n          \"kubernetes.io/arch\": \"arm64\",\n          \"kubernetes.io/hostname\": \"k8s-node1\",\n          \"kubernetes.io/os\": \"linux\"\n        }\n      },\n      \"timestamp\": \"2022-02-17T14:58:51Z\",\n      \"window\": \"10.094s\",\n      \"usage\": {\n        \"cpu\": \"141038629n\",\n        \"memory\": \"548580Ki\"\n      }\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#kubectl-top-pod","title":"<code>kubectl top pod</code>","text":"<pre><code>$ kubectl top nodes\nNAME         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nk8s-master   260m         7%     999Mi           80%\nk8s-node1    130m         3%     527Mi           42%\n</code></pre>"},{"location":"monitoring/metrics-server/bootstrapping_metrics_server/#kubectl-top-pod_1","title":"<code>kubectl top pod</code>","text":"<pre><code>$ kubectl top pods -A\nNAMESPACE     NAME                                 CPU(cores)   MEMORY(bytes)\nkube-system   coredns-675db8b7cc-hbzb2             7m           13Mi\nkube-system   etcd-k8s-master                      31m          108Mi\nkube-system   kube-apiserver-k8s-master            62m          220Mi\nkube-system   kube-controller-manager-k8s-master   24m          72Mi\nkube-system   kube-proxy-2kmcf                     1m           23Mi\nkube-system   kube-proxy-fxcgv                     1m           12Mi\nkube-system   kube-scheduler-k8s-master            3m           26Mi\nkube-system   metrics-server-8bb87844c-v67lj       12m          15Mi\n</code></pre>"},{"location":"monitoring/prometheus/install_cadvisor/","title":"Install cadvisor","text":""},{"location":"monitoring/prometheus/install_cadvisor/#about-cadvisor","title":"About cadvisor","text":"<p>https://github.com/google/cadvisor</p> <p>cadvisor(Container Advisor) \u306f\u5b9f\u884c\u4e2d\u306e\u30b3\u30f3\u30c6\u30ca\u306e\u30ea\u30bd\u30fc\u30b9\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u53ce\u96c6\u3059\u308b\u305f\u3081\u306e\u30c4\u30fc\u30eb\u3067\u3059\u3002 kube-state-metrics \u306fKubernetes Object\u306b\u5bfe\u3059\u308b\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u53ce\u96c6\u3092\u884c\u3046\u3082\u306e\u306a\u306e\u3067\u3001<code>Pod</code> \u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306f\u53ce\u96c6\u3055\u308c\u307e\u3059\u304c\u30b3\u30f3\u30c6\u30ca\u3054\u3068\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306f\u53ce\u96c6\u3055\u308c\u307e\u305b\u3093\u3002</p> <p><code>cadvisor</code> \u3067\u53ce\u96c6\u3055\u308c\u308b\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u306b\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u30da\u30fc\u30b8\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md</p>"},{"location":"monitoring/prometheus/install_cadvisor/#install","title":"Install","text":"<ul> <li> <p>install kustomize    <pre><code>curl -s -o /tmp/install_kustomize.sh \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\"\n\n# Raspberry Pi 4 is aarch64 (ARM 64-bit architecture)\n# refs https://github.com/kubernetes-sigs/kustomize/issues/4696\nsed -i -e 's/arm64/aarch64/g' /tmp/install_kustomize.sh\n\nbash -x ./tmp/install_kustomize.sh\nsudo mv ./kustomize /usr/local/bin/\n</code></pre></p> </li> <li> <p>install cadvisor</p> <ul> <li>https://github.com/google/cadvisor/tree/master/deploy/kubernetes#cadvisor-kubernetes-daemonset <pre><code># https://github.com/google/cadvisor/releases\nVERSION=v0.46.0\n\ngit clone https://github.com/google/cadvisor.git ~/work/cadvisor\ncd deploy/kubernetes/base &amp;&amp; kustomize edit set image gcr.io/cadvisor/cadvisor:${VERSION} &amp;&amp; cd ../../..\nkubectl kustomize deploy/kubernetes/base | kubectl apply -f -\n</code></pre></li> </ul> </li> </ul>"},{"location":"monitoring/prometheus/install_cadvisor/#dashboard","title":"Dashboard","text":"<ul> <li>https://grafana.com/grafana/dashboards/14282-cadvisor-exporter/</li> <li>Install Grafana &gt; Install Node Exporter Full Dashboards \u53c2\u7167</li> </ul>"},{"location":"monitoring/prometheus/install_grafana/","title":"Install Grafana","text":""},{"location":"monitoring/prometheus/install_grafana/#install-grafana","title":"Install Grafana","text":"<ul> <li>https://github.com/grafana/helm-charts/tree/main/charts/grafana</li> </ul>"},{"location":"monitoring/prometheus/install_grafana/#install","title":"Install","text":"<ol> <li> <p>values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63</p> <ul> <li> <p>Ephemeral Storage\u306e <code>storageClass</code> \u3092 OpenEBS\u306ejiva storage\u306e\u3082\u306e\u3068\u3059\u308b( e.g. <code>openebs-jiva-csi-default</code>)</p> <p>Info</p> <ul> <li>Installing OpenEBS \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0</li> </ul> </li> <li> <p>Grafana WebUI\u3078\u306e\u30a2\u30af\u30bb\u30b9\u306b<code>MetalLB</code>\u306eexternal IP\u30a2\u30c9\u30ec\u30b9\u3092\u4f7f\u7528\u3059\u308b</p> <ul> <li><code>service.type: LoadBalancer</code></li> <li><code>annotations</code> \u306b <code>metallb.universe.tf/address-pool: ip-pool</code></li> </ul> </li> </ul> <pre><code>mkdir -p ~/work/prometheus/grafana\ncurl -so ~/work/prometheus/grafana/grafana.yaml https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml\nvim ~/work/prometheus/grafana/grafana.yaml\n</code></pre> <p>grafana.yaml \u4fee\u6b63\u5f8c\u306ediff <pre><code>$ diff -u &lt;(curl -s https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml) &lt;(cat ~/work/prometheus/grafana/grafana.yaml)\n--- /dev/fd/63  2022-10-30 15:05:32.554153834 +0000\n+++ /dev/fd/62  2022-10-30 15:05:32.566153656 +0000\n@@ -157,12 +157,13 @@\n ##\n service:\n   enabled: true\n-  type: ClusterIP\n+  type: LoadBalancer\n   port: 80\n   targetPort: 3000\n     # targetPort: 4181 To be used with a proxy extraContainer\n   ## Service annotations. Can be templated.\n-  annotations: {}\n+  annotations:\n+    metallb.universe.tf/address-pool: ip-pool\n   labels: {}\n   portName: service\n   # Adds the appProtocol field to the service. This allows to work with istio protocol selection. Ex: \"http\" or \"tcp\"\n@@ -297,10 +297,10 @@\n persistence:\n   type: pvc\n   enabled: false\n-  # storageClassName: default\n+  storageClassName: openebs-jiva-csi-default\n   accessModes:\n     - ReadWriteOnce\n-  size: 10Gi\n+  size: 5Gi\n   # annotations: {}\n   finalizers:\n     - kubernetes.io/pvc-protection\n@@ -507,15 +507,14 @@\n ## Configure grafana datasources\n ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n ##\n-datasources: {}\n-#  datasources.yaml:\n-#    apiVersion: 1\n-#    datasources:\n-#    - name: Prometheus\n-#      type: prometheus\n-#      url: http://prometheus-prometheus-server\n-#      access: proxy\n-#      isDefault: true\n+datasources:\n+  datasources.yaml:\n+    apiVersion: 1\n+    datasources:\n+    - name: Prometheus\n+      type: prometheus\n+      url: http://prometheus-server.monitoring.svc.cluster.local\n+      isDefault: true\n #    - name: CloudWatch\n #      type: cloudwatch\n #      access: proxy\n</code></pre> </p> </li> <li> <p>install</p> <pre><code>helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml\n</code></pre> <p>\u5b9f\u884c\u30ed\u30b0 <pre><code>$ helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml\nRelease \"grafana\" does not exist. Installing it now.\nW1030 15:08:59.041312  221007 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\nW1030 15:08:59.265293  221007 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\nNAME: grafana\nLAST DEPLOYED: Sun Oct 30 15:08:54 2022\nNAMESPACE: monitoring\nSTATUS: deployed\nREVISION: 1\nNOTES:\n1. Get your 'admin' user password by running:\n\n   kubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\n2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:\n\n   grafana.monitoring.svc.cluster.local\n\n   Get the Grafana URL to visit by running these commands in the same shell:\nNOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        You can watch the status of by running 'kubectl get svc --namespace monitoring -w grafana'\n     export SERVICE_IP=$(kubectl get svc --namespace monitoring grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n     http://$SERVICE_IP:80\n\n3. Login with the password from step 1 and the username: admin\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Grafana pod is terminated.                            #####\n#################################################################################\n</code></pre> </p> </li> <li> <p>grafana Service\u306eMetalLB\u3067\u6255\u3044\u51fa\u3055\u308c\u305fExternal IP\u30a2\u30c9\u30ec\u30b9\u3092\u78ba\u8a8d\u3059\u308b     <pre><code>kubectl get service -n monitoring grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre></p> </li> <li> <p>\u30d6\u30e9\u30a6\u30b6\u304b\u3089\u30a2\u30af\u30bb\u30b9</p> <ol> <li> <p>\u30ed\u30b0\u30a4\u30f3\u753b\u9762</p> username <code>admin</code> <code>password</code> grafana\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6642\u306e\u8868\u793a\u3092\u53c2\u8003\u306b\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067<code>admin</code> \u306e <code>password</code> \u3092\u53d6\u5f97<code>kubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo</code> </li> <li> <p>\u30ed\u30b0\u30a4\u30f3\u6210\u529f</p> <ul> <li></li> </ul> </li> </ol> </li> </ol>"},{"location":"monitoring/prometheus/install_grafana/#install-node-exporter-full-dashboards","title":"Install <code>Node Exporter Full</code> Dashboards","text":""},{"location":"monitoring/prometheus/install_grafana/#grafana","title":"grafana\u3068\u4e00\u7dd2\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408","text":"<p>Info</p> <ul> <li>grafana helm chart\u3067\u306fgrafana dashboard import\u8a2d\u5b9a\u304c\u53ef\u80fd\u3067\u3059\u3002   <code>.Values.dashboards</code> \u306b\u8a2d\u5b9a\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u3001configmap.yaml \u306b\u3042\u308b <code>download_dashboards.sh</code> \u3067\u6307\u5b9a\u3057\u305fdashboard import\u8a2d\u5b9a\u304c\u5165\u308a\u307e\u3059\u3002\u305d\u3057\u3066 Pod\u306einitContainers \u3067 <code>download-dashboards.sh</code> \u3092\u5b9f\u884c\u3057\u3066\u3044\u307e\u3059\u3002<ul> <li>https://github.com/grafana/helm-charts/blob/grafana-6.43.5/charts/grafana/README.md</li> <li>https://github.com/grafana/helm-charts/blob/grafana-6.43.5/charts/grafana/values.yaml#L629-L658</li> </ul> </li> </ul> <ol> <li> <p>values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63     <pre><code>vim ~/work/prometheus/grafana/grafana.yaml\n</code></pre></p> <p>grafana.yaml \u4fee\u6b63\u5f8c\u306ediff(<code>cadvisor-exporter</code> Dashboard\u3082\u5165\u308c\u3066\u3044\u308b\u4f8b\u3067\u3059) <pre><code>$ diff -u &lt;(curl -s https://raw.githubusercontent.com/grafana/helm-charts/main/charts/grafana/values.yaml) &lt;(cat ~/work/prometheus/grafana/grafana.yaml)\n\n~ snip ~\n\n@@ -632,7 +632,17 @@\n ##\n ## dashboards per provider, use provider name as key.\n ##\n-dashboards: {}\n+dashboards:\n+  default:\n+    node-exporter-full:\n+      # https://grafana.com/grafana/dashboards/1860-node-exporter-full/\n+      gnetId: 1860\n+      datasource: Prometheus\n+    cadvisor-exporter:\n+      # https://grafana.com/grafana/dashboards/14282-cadvisor-exporter/\n+      gnetId: 14282\n+      datasource: Prometheus\n+\n   # default:\n   #   some-dashboard:\n   #     json: |\n</code></pre> </p> </li> <li> <p>install     <pre><code>helm upgrade -i grafana grafana/grafana -n monitoring -f ~/work/prometheus/grafana/grafana.yaml\n</code></pre></p> </li> <li> <p><code>grafana\u30b3\u30f3\u30c6\u30ca:/var/lib/grafana/dashboards/default/</code> \u306b Dashboard\u5b9a\u7fa9\u3067\u3042\u308bjson\u30d5\u30a1\u30a4\u30eb\u304c\u914d\u7f6e\u6e08\u307f\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>$ kubectl exec -it -c grafana -n monitoring grafana-5f4c7d46db-xtktb -- ls -l /var/lib/grafana/dashboards/default/\ntotal 232\n-rw-r--r--    1 grafana  472          18484 Nov  8 16:03 cadvisor-exporter.json\n-rw-r--r--    1 grafana  472         215154 Nov  8 16:03 node-exporter-full.json\n</code></pre></p> </li> </ol>"},{"location":"monitoring/prometheus/install_grafana/#_1","title":"\u624b\u52d5\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408","text":"<ol> <li>Dashboards\u30da\u30fc\u30b8\u306b\u30a2\u30af\u30bb\u30b9<ul> <li></li> </ul> </li> <li><code>New</code> -&gt; <code>New Dashboard</code> -&gt; <code>Import</code> \u3092\u9078\u629e<ul> <li></li> </ul> </li> <li><code>Node Exporter Full</code> \u306e Dashboard ID\u3092\u5165\u529b<ul> <li>https://grafana.com/grafana/dashboards/1860-node-exporter-full/</li> <li></li> </ul> </li> <li><code>Import</code> \u3092\u5b9f\u884c\u3059\u308b<ul> <li></li> </ul> </li> <li><code>Node Exporter Full</code> \u304c\u8868\u793a\u3055\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li></li> </ul> </li> </ol>"},{"location":"monitoring/prometheus/install_prometheus/","title":"Install Prometheus","text":""},{"location":"monitoring/prometheus/install_prometheus/#prometheus-installing","title":"Prometheus Installing","text":"<ul> <li>https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus</li> </ul>"},{"location":"monitoring/prometheus/install_prometheus/#install","title":"Install","text":"<ol> <li> <p>values\u30d5\u30a1\u30a4\u30eb\u306e\u4fee\u6b63</p> <ul> <li> <p>Ephemeral Storage\u306e <code>storageClass</code> \u3092 OpenEBS\u306ejiva storage\u306e\u3082\u306e\u3068\u3059\u308b( e.g. <code>openebs-jiva-csi-default</code>)</p> <p>Info</p> <ul> <li>Installing OpenEBS \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0</li> </ul> </li> <li> <p><code>Prometheus</code> \u3084 <code>Alertmanager</code> \u306e WebUI\u3092\u5229\u7528\u3057\u305f\u3044\u5834\u5408</p> <ul> <li>NodeIp:NodePort \u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u5834\u5408<ul> <li><code>service.type: NodePort</code></li> </ul> </li> <li> <p><code>MetalLB</code> \u3067\u6255\u3044\u51fa\u3057\u305fexternal ip\u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u5834\u5408</p> <ul> <li><code>service.type: LoadBalancer</code></li> <li><code>annotations</code> \u306b <code>metallb.universe.tf/address-pool: ip-pool</code></li> </ul> <p>Info</p> <ul> <li>MetalLB \u3092\u5b9f\u65bd\u6e08\u307f\u306e\u524d\u63d0</li> </ul> </li> </ul> </li> </ul> <pre><code>mkdir -p ~/work/prometheus\ncurl -so ~/work/prometheus/prometheus.yaml https://raw.githubusercontent.com/prometheus-community/helm-charts/main/charts/prometheus/values.yaml\nvim ~/work/prometheus/prometheus.yaml\n</code></pre> <p>prometheus.yaml \u4fee\u6b63\u5f8c\u306ediff <pre><code>$ diff -u &lt;(curl -s https://raw.githubusercontent.com/prometheus-community/helm-charts/main/charts/prometheus/values.yaml) &lt;(cat ~/work/prometheus/prometheus.yaml)\n--- /dev/fd/63  2022-10-30 14:07:09.532561438 +0000\n+++ /dev/fd/62  2022-10-30 14:07:09.540561310 +0000\n@@ -231,7 +231,7 @@\n     ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n     ##   GKE, AWS &amp; OpenStack)\n     ##\n-    # storageClass: \"-\"\n+    storageClass: \"openebs-jiva-csi-default\"\n\n     ## alertmanager data Persistent Volume Binding Mode\n     ## If defined, volumeBindingMode: &lt;volumeBindingMode&gt;\n@@ -947,7 +947,7 @@\n     ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n     ##   GKE, AWS &amp; OpenStack)\n     ##\n-    # storageClass: \"-\"\n+    storageClass: \"openebs-jiva-csi-default\"\n\n     ## Prometheus server data Persistent Volume Binding Mode\n     ## If defined, volumeBindingMode: &lt;volumeBindingMode&gt;\n@@ -1403,7 +1403,7 @@\n     ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n     ##   GKE, AWS &amp; OpenStack)\n     ##\n-    # storageClass: \"-\"\n+    storageClass: \"openebs-jiva-csi-default\"\n\n     ## pushgateway data Persistent Volume Binding Mode\n     ## If defined, volumeBindingMode: &lt;volumeBindingMode&gt;\n</code></pre> </p> </li> <li> <p>install</p> <pre><code>helm upgrade -i prometheus -n monitoring --create-namespace prometheus-community/prometheus -f ~/work/prometheus/prometheus.yaml\n</code></pre> </li> <li> <p>\u4f5c\u6210\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u8a8d\u3059\u308b     Service <pre><code>$ kubectl get services -n monitoring\nNAME                            TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)        AGE\nprometheus-alertmanager         ClusterIP      10.32.0.114   &lt;none&gt;          80/TCP         33h\nprometheus-kube-state-metrics   ClusterIP      10.32.0.246   &lt;none&gt;          8080/TCP       33h\nprometheus-node-exporter        ClusterIP      10.32.0.31    &lt;none&gt;          9100/TCP       33h\nprometheus-pushgateway          ClusterIP      10.32.0.138   &lt;none&gt;          9091/TCP       33h\nprometheus-server               ClusterIP      10.32.0.75    &lt;none&gt;          80/TCP         33h\n</code></pre> </p> <p>Deployment <pre><code>$ kubectl get deployments -n monitoring\nNAME                            READY   UP-TO-DATE   AVAILABLE   AGE\nprometheus-alertmanager         1/1     1            1           33h\nprometheus-kube-state-metrics   1/1     1            1           33h\nprometheus-pushgateway          1/1     1            1           33h\nprometheus-server               1/1     1            1           33h\n</code></pre> </p> <p>DaemonSet <pre><code>$ kubectl get DaemonSet -n monitoring -l app=prometheus\nNAME                       DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\nprometheus-node-exporter   2         2         2       2            2           &lt;none&gt;          35h\n</code></pre> </p> <p>PersistentVolumeClaim <pre><code>$ kubectl get PersistentVolumeClaim -n monitoring\nNAME                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS               AGE\nprometheus-alertmanager   Bound    pvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd   2Gi        RWO            openebs-jiva-csi-default   33h\nprometheus-server         Bound    pvc-836ef65a-da18-4453-96a6-7b909d0c668b   8Gi        RWO            openebs-jiva-csi-default   33h\n</code></pre> </p> <p>PersistentVolume <pre><code>$ kubectl get PersistentVolume -n monitoring pvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd pvc-836ef65a-da18-4453-96a6-7b909d0c668b\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                STORAGECLASS               REASON   AGE\npvc-93589533-c5b1-4dc4-8089-8dcccd42b8cd   2Gi        RWO            Delete           Bound    monitoring/prometheus-alertmanager   openebs-jiva-csi-default            33h\npvc-836ef65a-da18-4453-96a6-7b909d0c668b   8Gi        RWO            Delete           Bound    monitoring/prometheus-server         openebs-jiva-csi-default            33h\n</code></pre> </p> <p>ServiceAccount <pre><code>$ kubectl get ServiceAccount -n monitoring -l app=prometheus\nNAME                                   SECRETS   AGE\ndefault                                1         9d\nprometheus-alertmanager                1         35h\nprometheus-kube-prometheus-admission   1         4d23h\nprometheus-kube-state-metrics          1         35h\nprometheus-node-exporter               1         35h\nprometheus-pushgateway                 1         35h\nprometheus-server                      1         35h\n</code></pre> </p> <p>ClusterRole <pre><code>$ kubectl get ClusterRole -n monitoring -l app=prometheus\nNAME                      CREATED AT\nprometheus-alertmanager   2022-10-30T03:25:38Z\nprometheus-pushgateway    2022-10-30T03:25:38Z\nprometheus-server         2022-10-30T03:25:38Z\n</code></pre> </p> <p>ClusterRoleBinding <pre><code>$ kubectl get ClusterRoleBinding -n monitoring -l app=prometheus\nNAME                      ROLE                                  AGE\nprometheus-alertmanager   ClusterRole/prometheus-alertmanager   35h\nprometheus-pushgateway    ClusterRole/prometheus-pushgateway    35h\nprometheus-server         ClusterRole/prometheus-server         35h\n</code></pre> </p> <p>ConfigMap <pre><code>$ kubectl get ConfigMap -n monitoring -l app=prometheus\nNAME                      DATA   AGE\nprometheus-alertmanager   2      35h\nprometheus-server         6      35h\n</code></pre> </p> <p>Info</p> <ul> <li>ConfigMap\u306b\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b <code>prometheus.yml</code> \u306e\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u305f\u3044\u5834\u5408\u306f\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067\u78ba\u8a8d     <pre><code>kubectl get ConfigMap -n monitoring prometheus-server -o jsonpath=\"{.data.prometheus\\.yml}\"\n</code></pre></li> </ul> </li> <li> <p>Confirm FQDN of prometheus-server service A Record</p> <ul> <li>Grafana\u3092helm install\u3059\u308b\u969b\u306b <code>datasources.datasources[0].url</code> \u306b\u8a2d\u5b9a\u3059\u308bFQDN\u3092\u78ba\u8a8d\u3059\u308b</li> <li> <p><code>prometheus-server.monitoring.svc.cluster.local</code></p> <pre><code>$ nslookup prometheus-server.monitoring.svc.cluster.local `kubectl get ep -n kube-system kube-dns -o jsonpath=\"{.subsets[0].addresses[0].ip}\"`\nServer:         10.200.2.235\nAddress:        10.200.2.235#53\n\nName:   prometheus-server.monitoring.svc.cluster.local\nAddress: 10.32.0.75\n</code></pre> </li> </ul> </li> </ol>"},{"location":"operations/backup_etcd/","title":"Backup etcd","text":""},{"location":"operations/backup_etcd/#etcdctl","title":"<code>etcdctl</code> \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<pre><code>sudo apt install etcd-client\n</code></pre>"},{"location":"operations/backup_etcd/#ercd","title":"ercd \u758e\u901a\u78ba\u8a8d","text":"<ul> <li>https://github.com/etcd-io/etcd/tree/main/etcdctl#endpoint-health <pre><code>ETCD_ENDPOINT=https://192.168.10.50:2379\nETCD_CACERT=/etc/etcd/ca.pem\nETCD_CERT=/etc/etcd/kubernetes.pem\nETCD_KEY=/etc/etcd/kubernetes-key.pem\n\nETCDCTL_API=3 etcdctl endpoint health \\\n  --endpoints=${ETCD_ENDPOINT} \\\n  --cacert=${ETCD_CACERT} \\\n  --cert=${ETCD_CERT} \\\n  --key=${ETCD_KEY}\n</code></pre><ul> <li><code>https://192.168.10.50:2379 is healthy: successfully committed proposal: took = 32.557549ms</code> \u306a\u3069\u306e\u6a19\u6e96\u51fa\u529b\u304c\u78ba\u8a8d\u3067\u304d\u308c\u3070\u758e\u901a\u3067\u304d\u3066\u3044\u308b</li> </ul> </li> </ul>"},{"location":"operations/backup_etcd/#etcd","title":"etcd \u30d0\u30c3\u30af\u30a2\u30c3\u30d7","text":"<ul> <li>https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-save-filename <pre><code>ETCD_ENDPOINT=https://192.168.10.50:2379\nETCD_CACERT=/etc/etcd/ca.pem\nETCD_CERT=/etc/etcd/kubernetes.pem\nETCD_KEY=/etc/etcd/kubernetes-key.pem\n\nETCDCTL_API=3 etcdctl snapshot save snapshot.db \\\n  --endpoints=${ETCD_ENDPOINT} \\\n  --cacert=${ETCD_CACERT} \\\n  --cert=${ETCD_CERT} \\\n  --key=${ETCD_KEY}\n</code></pre></li> </ul>"},{"location":"operations/backup_etcd/#_1","title":"\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30d5\u30a1\u30a4\u30eb\u306e\u60c5\u5831\u3092\u78ba\u8a8d","text":"<ul> <li>https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-status-filename <pre><code>ETCDCTL_API=3 etcdctl snapshot status snapshot.db\n</code></pre><ul> <li>\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u51fa\u529b\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059       <pre><code>+----------+----------+------------+------------+---------+\n|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE | VERSION |\n+----------+----------+------------+------------+---------+\n| c9a815d0 |     3356 |        488 |     1.1 MB |         |\n+----------+----------+------------+------------+---------+\n</code></pre></li> </ul> </li> </ul>"},{"location":"operations/backup_etcd/#etcd_1","title":"etcd \u30ea\u30b9\u30c8\u30a2","text":"<ul> <li>https://github.com/etcd-io/etcd/tree/main/etcdctl#snapshot-restore-options-filename <pre><code>ETCD_ENDPOINT=https://192.168.10.50:2379\nETCD_CACERT=/etc/etcd/ca.pem\nETCD_CERT=/etc/etcd/kubernetes.pem\nETCD_KEY=/etc/etcd/kubernetes-key.pem\n\nETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\\n  --endpoints=${ETCD_ENDPOINT} \\\n  --cacert=${ETCD_CACERT} \\\n  --cert=${ETCD_CERT} \\\n  --key=${ETCD_KEY}\n</code></pre></li> </ul>"},{"location":"operations/cluster-upgrade/","title":"Cluster Upgrade","text":""},{"location":"operations/cluster-upgrade/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/</li> <li>https://kubernetes.io/docs/reference/using-api/deprecation-guide/</li> <li>https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/update-cluster.html</li> <li>https://speakerdeck.com/shmurata/strategy-to-upgrade-kubernetes-clusters-in-production</li> <li>https://repost.aws/ja/knowledge-center/eks-plan-upgrade-cluster</li> <li>https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar</li> </ul>"},{"location":"operations/cluster-upgrade/#_2","title":"\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u6226\u7565","text":"<ol> <li>Live Upgrade(in-place upgrade)<ul> <li>https://speakerdeck.com/shmurata/strategy-to-upgrade-kubernetes-clusters-in-production?slide=23</li> </ul> </li> <li>Blue/Green Upgrade<ul> <li>https://speakerdeck.com/shmurata/strategy-to-upgrade-kubernetes-clusters-in-production?slide=32</li> </ul> </li> </ol>"},{"location":"operations/cluster-upgrade/#_3","title":"\u6e96\u5099","text":"<ol> <li>Kubernetes\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u5909\u66f4\u70b9\u306e\u78ba\u8a8d<ul> <li>https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG</li> <li>https://github.com/kubernetes/kubernetes/releases</li> <li>https://kubernetes.io/releases/</li> <li>AWS EKS</li> <li>https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html</li> <li>https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/update-cluster.html</li> </ul> </li> <li>\u975e\u63a8\u5968\u3084\u524a\u9664\u3055\u308c\u305fAPI\u3092\u4f7f\u3063\u3066\u3044\u308baddon\u3092\u898b\u3064\u3051\u308b<ul> <li>https://kubernetes.io/docs/reference/using-api/deprecation-guide/</li> <li>Pluto<ul> <li>https://github.com/FairwindsOps/pluto</li> <li>Kubernetes\u306e\u975e\u63a8\u5968(deplicated api)\u3092\u81ea\u52d5\u691c\u51fa\u3059\u308b</li> <li>https://kakakakakku.hatenablog.com/entry/2022/07/20/091424</li> <li>https://zenn.dev/johnn26/articles/detect-kubernetes-deplicated-api-automatically</li> </ul> </li> <li>e.g.<ul> <li>Kyverno<ul> <li>https://kyverno.io/policies/best-practices/check_deprecated_apis/check_deprecated_apis/</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Node</p> </li> <li> <p>Addon\u306e\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u8a08\u753b</p> </li> </ol>"},{"location":"operations/cluster-upgrade/#_4","title":"\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9","text":""},{"location":"operations/cluster-upgrade/#aws-eks","title":"AWS EKS","text":"<ol> <li>EKS Cluster Upgrade</li> <li>ManagedNodeGroup Upgrade</li> <li>eks-optimized AMI\u3092Upgreade\u5bfe\u8c61\u306ecluster version\u3068\u4e00\u81f4\u3059\u308b\u3082\u306e\u306b\u3059\u308b</li> </ol>"},{"location":"paketo/about/","title":"About","text":"<p>https://paketo.io/</p> <p>https://paketo.io/docs/howto/python/ https://github.com/paketo-buildpacks/python https://github.com/paketo-buildpacks/jam</p>"},{"location":"pod_security/","title":"Index","text":""},{"location":"pod_security/#pod","title":"Pod","text":"<p>https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</p>"},{"location":"secrets/about_secrets/","title":"About Secrets","text":""},{"location":"secrets/about_secrets/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://kubernetes.io/ja/docs/concepts/configuration/secret/</li> <li>https://kubernetes.io/ja/docs/tasks/configmap-secret/managing-secret-using-kubectl/</li> </ul>"},{"location":"secrets/about_secrets/#overview","title":"Overview","text":"<ul> <li>API Key\u3001Token\u3001SSH Key\u306a\u3069\u6a5f\u5bc6\u6027\u306e\u9ad8\u3044\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u305f\u3081\u306e\u30ea\u30bd\u30fc\u30b9</li> <li>\u53c2\u7167\u65b9\u6cd5</li> <li>Volume\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u30de\u30a6\u30f3\u30c8</li> <li>\u74b0\u5883\u5909\u6570</li> <li>imagePullSecrets</li> <li>\u5024\u306e\u683c\u7d0d\u30d5\u30a9\u30fc\u30de\u30c3\u30c8</li> <li><code>date</code>: base64\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3055\u308c\u305f\u6587\u5b57\u5217</li> <li><code>stringData</code>: \u5e73\u6587\u306a\u3069\u305d\u306e\u307e\u307e\u683c\u7d0d\u3057\u305f\u3044\u5834\u5408</li> </ul>"},{"location":"secrets/about_secrets/#kind-of-secret","title":"Kind of secret","text":""},{"location":"secrets/about_secrets/#builtin-type","title":"Builtin Type","text":"<ul> <li>Opaque</li> <li>kubernetes.io/service-account-token</li> <li>kubernetes.io/dockercfg</li> <li>kubernetes.io/dockerconfigjson</li> <li>kubernetes.io/basic-auth</li> <li>kubernetes.io/ssh-auth</li> <li>kubernetes.io/tls</li> <li>bootstrap.kubernetes.io/token</li> </ul>"},{"location":"setup/01_setup_RaspberryPi/","title":"01. Raspberry Pi \u69cb\u6210","text":""},{"location":"setup/01_setup_RaspberryPi/#raspberry-pi","title":"Raspberry Pi \u69cb\u6210","text":""},{"location":"setup/01_setup_RaspberryPi/#_1","title":"\u8cfc\u5165\u3057\u305f\u3082\u306e","text":"<ul> <li>Raspberry Pi 4 Model B/2GB x3</li> <li>GeeekPi Raspberry Pi4 \u30af\u30e9\u30b9\u30bf\u30fc\u30b1\u30fc\u30b9(\u51b7\u5374\u30d5\u30a1\u30f3\u4ed8\u304d) x1</li> <li>Anker PowerPort I PD - 1 PD &amp; 4 PowerIQ x1</li> <li>Amazon\u30d9\u30fc\u30b7\u30c3\u30af HDMI\u30b1\u30fc\u30d6\u30eb 0.9m (\u30bf\u30a4\u30d7A\u30aa\u30b9 - \u30de\u30a4\u30af\u30ed\u30bf\u30a4\u30d7D\u30aa\u30b9) x1</li> <li>Samsung EVO Plus 32GB microSDHC x3</li> <li>Anker USB Type C \u30b1\u30fc\u30d6\u30eb PowerLine USB-C &amp; USB-A 3.0 \u30b1\u30fc\u30d6\u30eb x3</li> </ul>"},{"location":"setup/02_setup_ubuntu20-04-LTS/","title":"02. OS Install","text":""},{"location":"setup/02_setup_ubuntu20-04-LTS/#os-setup","title":"OS Setup","text":""},{"location":"setup/02_setup_ubuntu20-04-LTS/#ubuntuserver20042lts-setup","title":"UbuntuServer20.04.2LTS Setup","text":"<ol> <li>GeeekPi\u30b1\u30fc\u30b9\u306e\u7d44\u307f\u7acb\u3066 &amp; \u7d50\u7dda &amp; \u8d77\u52d5\u78ba\u8a8d</li> <li>Raspberry Pi Imager \u3067SD\u30ab\u30fc\u30c9\u306bUbuntuServer20.04.2LTS(64bit) \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb</li> <li>Raspberry Pi 4\u3078SD\u30ab\u30fc\u30c9\u3092\u633f\u5165\u3057\u8d77\u52d5</li> <li> <p>wifi\u8a2d\u5b9a    <pre><code>sudo vim /etc/netplan/50-cloud-init.yaml\n\n# config check\nsudo netplan --debug try\nsudo netplan --debug generate\n\n# \u9069\u7528\nsudo netplan --debug apply\n</code></pre></p> <p>/etc/netplan/50-cloud-init.yaml (master\u306e\u5834\u5408) <pre><code>network:\n  ethernets:\n      eth0:\n          dhcp4: true\n          optional: true\n  version: 2\n  wifis:\n    wlan0:\n      optional: true\n      dhcp4: false\n      addresses:\n      - 192.168.3.50/24\n      gateway4: 192.168.3.1\n      nameservers:\n        addresses:\n        - 8.8.8.8\n        - 8.8.4.4\n        search: []\n      access-points:\n        \"&lt;SSID\u540d&gt;\":\n          password: \"&lt;\u30d1\u30b9\u30ef\u30fc\u30c9&gt;\"\n</code></pre> </p> </li> <li> <p>package\u66f4\u65b0    <pre><code>sudo apt update\nsudo apt upgrade -y\n</code></pre></p> </li> <li>\u65e5\u672c\u8a9e\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u5909\u66f4\u3057\u518d\u8d77\u52d5    <pre><code>sudo dpkg-reconfigure keyboard-configuration\nsudo reboot\n</code></pre><ul> <li><code>Generic 105-key (Intl) PC</code> \u3092\u9078\u629e</li> <li><code>Japanese</code> \u3092\u9078\u629e</li> <li><code>Japanese</code> \u3092\u9078\u629e</li> <li><code>The default for the keyboard layout</code> \u3092\u9078\u629e</li> <li><code>No compose key</code> \u3092\u9078\u629e</li> </ul> </li> <li>LOCALE    <pre><code>sudo apt install -y language-pack-ja\nsudo update-locale LANG=ja_JP.UTF-8\n</code></pre></li> <li>hostname    <pre><code>#name=k8s-node1\n#name=k8s-node2\nname=k8s-master\necho ${name} | sudo tee /etc/hostname\nsudo sed -i -e 's/127.0.1.1.*/127.0.1.1\\t'$name'/' /etc/hosts\n</code></pre></li> <li><code>/etc/hosts</code></li> <li>(\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u5909\u308f\u3063\u3066\u3082\u758e\u901a\u3057\u305f\u3044\u305f\u3081) k8s\u5185\u3067\u306f\u30db\u30b9\u30c8\u540d\u901a\u4fe1\u3059\u308b\u305f\u3081\u306bhost\u3092\u66f8\u304d\u8fbc\u3080<ul> <li><code>k8s-master</code></li> <li><code>k8s-node1</code></li> <li><code>k8s-node2</code></li> </ul> </li> </ol>"},{"location":"setup/03_common_settings/","title":"03. master/node\u3067\u5171\u901a\u624b\u9806","text":""},{"location":"setup/03_common_settings/#swap","title":"swap\u3092\u7121\u52b9\u306b\u3059\u308b","text":"<ol> <li> <p>swap\u304c\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d    <pre><code>$ free -h\n              total        used        free      shared  buff/cache   available\nMem:          1.8Gi        54Mi       1.5Gi       8.0Mi       244Mi       1.7Gi\nSwap:          99Mi          0B        99Mi\n</code></pre></p> </li> <li> <p>swap\u3092\u7121\u52b9\u306b\u8a2d\u5b9a\u3059\u308b    <pre><code>sudo swapoff --all\n\n# Ubuntu Desktop 22.04 LTS (for RPi4)\nsudo systemctl stop dphys-swapfile\nsudo systemctl disable dphys-swapfile\n</code></pre></p> </li> <li> <p>swap\u304c\u7121\u52b9\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b    <pre><code>$ free -h\n              total        used        free      shared  buff/cache   available\nMem:          1.8Gi        57Mi       1.5Gi       8.0Mi       251Mi       1.7Gi\nSwap:            0B          0B          0B\n\n$ systemctl status dphys-swapfile\n\u25cf dphys-swapfile.service - dphys-swapfile - set up, mount/unmount, and delete a swap file\n   Loaded: loaded (/lib/systemd/system/dphys-swapfile.service; disabled; vendor preset: enabled)\n   Active: inactive (dead)\n     Docs: man:dphys-swapfile(8)\n\n12\u6708 30 20:48:54 k8s-master1 systemd[1]: Starting dphys-swapfile - set up, mount/unmount, and delete a swap file...\n12\u6708 30 20:48:55 k8s-master1 dphys-swapfile[330]: want /var/swap=100MByte, checking existing: keeping it\n12\u6708 30 20:48:55 k8s-master1 systemd[1]: Started dphys-swapfile - set up, mount/unmount, and delete a swap file.\n12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopping dphys-swapfile - set up, mount/unmount, and delete a swap file...\n12\u6708 31 06:57:57 k8s-master1 systemd[1]: dphys-swapfile.service: Succeeded.\n12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopped dphys-swapfile - set up, mount/unmount, and delete a swap file.\n</code></pre></p> </li> </ol>"},{"location":"setup/03_common_settings/#cgroupfs-memory","title":"cgroupfs \u306ememory\u3092\u6709\u52b9\u306b\u3059\u308b","text":"<ol> <li> <p>kernel\u306eboot option\u306b <code>cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</code> \u3092\u8ffd\u8a18\u3059\u308b    <pre><code>sudo vim /boot/firmware/cmdline.txt\n</code></pre></p> <ul> <li> <p><code>cmdline.txt</code> \u306e\u6709\u52b9\u884c\u306e\u78ba\u8a8d</p> <pre><code>$ sudo sed -e 's/\\s/\\n/g' /boot/firmware/cmdline.txt\nconsole=serial0,115200\nconsole=tty1\nroot=PARTUUID=fb7271c3-02\nrootfstype=ext4\nelevator=deadline\nfsck.repair=yes\nrootwait\nquiet\nsplash\nplymouth.ignore-serial-consoles\ncgroup_enable=cpuset\ncgroup_memory=1\ncgroup_enable=memory\n\n$ sudo reboot\n\n$ cat /proc/cgroups\n#subsys_name    hierarchy       num_cgroups     enabled\ncpuset  9       1       1\ncpu     5       34      1\ncpuacct 5       34      1\nblkio   10      34      1\nmemory  8       80      1\ndevices 4       34      1\nfreezer 7       1       1\nnet_cls 2       1       1\nperf_event      6       1       1\nnet_prio        2       1       1\npids    3       39      1\n</code></pre> </li> </ul> </li> </ol>"},{"location":"setup/03_common_settings/#container-runtime","title":"Container RunTime\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ul> <li><code>containerd</code> \u3092\u63a1\u7528</li> <li>\u521d\u671f\u306f <code>cri-o</code> \u3092\u63a1\u7528\u3057\u3066\u3044\u305f\u304c\u4ee5\u4e0b\u7406\u7531\u3067<code>containerd</code>\u3078\u5909\u66f4<ul> <li>CNCF\u3067Graduated Project(cri-o\u306fincubating)</li> <li>AWS eks-optimized AMI\u3067\u306fcontainerd\u304c\u6a19\u6e96\u3068\u306a\u308a\u305d\u3046(eks 1.22 \u3067\u306fdocker\u304cdefault)</li> <li><code>buildkit</code> \u3068\u7d44\u307f\u5408\u308f\u305b\u3066image build\u304c\u53ef\u80fd (cri-o\u3067\u306e\u53ef\u5426\u306f\u672a\u78ba\u8a8d)<ul> <li>https://speakerdeck.com/ktock/dockerkaracontainerdhefalseyi-xing?slide=7</li> </ul> </li> </ul> </li> </ul>"},{"location":"setup/03_common_settings/#_1","title":"\u524d\u63d0\u4f5c\u696d","text":"<ul> <li>https://kubernetes.io/docs/setup/production-environment/container-runtimes/ <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\n# sysctl params required by setup, params persist across reboots\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n\n# Apply sysctl params without reboot\nsudo sysctl --system\n</code></pre> </li> </ul>"},{"location":"setup/03_common_settings/#containerd","title":"Containerd","text":"<p>https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd https://github.com/containerd/containerd/blob/main/docs/getting-started.md</p> <ol> <li>\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb<ul> <li>for apt package<ul> <li>refs https://docs.docker.com/engine/install/ubuntu/ <pre><code>sudo apt update\nsudo apt install -y ca-certificates curl gnupg lsb-release\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt update\nsudo apt install -y containerd.io\n</code></pre></li> </ul> </li> </ul> </li> <li><code>/etc/containerd/config.toml</code> <pre><code>sudo containerd config default | sudo tee /etc/containerd/config.toml\nsudo vim /etc/containerd/config.toml\n</code></pre><ul> <li>refs https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd<ul> <li>Configuring the systemd cgroup driver</li> <li>Overriding the sandbox (pause) image</li> </ul> </li> </ul> </li> <li>restart containerd     <pre><code>sudo systemctl restart containerd\n</code></pre></li> </ol>"},{"location":"setup/03_common_settings/#cri-o","title":"CRI-O","text":"<p>https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o</p> <ol> <li>kernel module\u306eload<ul> <li>overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module <code>overlay</code></li> <li>iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module <code>br_netfilter</code></li> </ul> </li> <li> <p>kernel parameter\u306eset</p> <ul> <li>iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a</li> </ul> </li> <li> <p>kernel module\u306eload</p> <ul> <li>overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module <code>overlay</code></li> <li>iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module <code>br_netfilter</code> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/crio.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n</code></pre></li> </ul> </li> <li> <p>kernel parameter\u306eset</p> <ul> <li>iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a   <pre><code>cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\n\n# https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n\nsudo sysctl --system\n</code></pre></li> </ul> </li> <li> <p>system\u8d77\u52d5\u6642\u306b kernel parameter \u3092\u518d\u8aad\u307f\u8fbc\u307f\u3055\u305b\u308b</p> <ul> <li>kube-proxy\u306b\u3066\u5fc5\u8981\u306akernel parameter\u8a2d\u5b9a(kubelet\u8a2d\u5b9a\u624b\u9806\u306b\u3066\u5f8c\u8ff0) \u304ciptables\u8d77\u52d5\u6642\u306ekernel module load\u3067\u4e0a\u66f8\u304d\u3055\u308c\u308b\u305f\u3081</li> <li>\u5229\u7528\u74b0\u5883\u304c <code>/etc/sysconfig/iptables-config</code> \u3092\u5229\u7528\u53ef\u80fd\u306a\u3089 <code>IPTABLES_MODULES_UNLOAD=\"no\"</code> \u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u672c\u624b\u9806\u306f\u4e0d\u8981\u3067\u3059     <pre><code>egrep  \"sysctl\\s+--system\" /etc/rc.local &gt; /dev/null || sudo bash -c \"echo \\\"sysctl --system\\\" &gt;&gt; /etc/rc.local\"\negrep  \"sysctl\\s+--system\" /etc/rc.local\n</code></pre></li> </ul> </li> <li> <p>CRI-O\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb</p> <ul> <li>https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.21/xUbuntu_20.04/arm64/ <pre><code>VERSION=1.21\nOS=xUbuntu_20.04\n\nsudo bash -c \"echo \\\"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /\\\" &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\"\nsudo bash -c \"echo \\\"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /\\\" &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list\"\n\ncurl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key add -\ncurl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key add -\n\nsudo apt update\nsudo apt install -y cri-o cri-o-runc\n\nsudo apt install -y conntrack\n</code></pre></li> </ul> </li> <li> <p>storage driver\u3092 <code>overlay2</code> \u3078\u5909\u66f4\u3059\u308b    <pre><code>sudo vim /etc/containers/storage.conf\nsudo vim /etc/crio/crio.conf\n</code></pre></p> <ul> <li><code>/etc/crio/crio.conf</code> \u3078graph driver\u8a2d\u5b9a\u3092\u5165\u308c\u308b<ul> <li>podman\u3084buildah\u3067build\u3057\u305flocal image\u3092\u53c2\u7167\u3059\u308b\u305f\u3081</li> <li><code>[crio]</code> \u30bb\u30af\u30b7\u30e7\u30f3\u306b\u5165\u308c\u308b    <pre><code>graphroot = \"/var/lib/containers/storage\"\n</code></pre> /etc/containers/storage.conf <pre><code>[storage]\ndriver = \"overlay2\"\nrunroot = \"/run/containers/storage\"\ngraphroot = \"/var/lib/containers/storage\"\n\n[storage.options]\nadditionalimagestores = [\n]\n\n[storage.options.overlay]\nmountopt = \"nodev\"\n\n[storage.options.thinpool]\n</code></pre> /etc/crio/crio.conf <pre><code>[crio]\nstorage_driver = \"overlay2\"\ngraphroot = \"/var/lib/containers/storage\"\nlog_dir = \"/var/log/crio/pods\"\nversion_file = \"/var/run/crio/version\"\nversion_file_persist = \"/var/lib/crio/version\"\nclean_shutdown_file = \"/var/lib/crio/clean.shutdown\"\n\n[crio.api]\nlisten = \"/var/run/crio/crio.sock\"\nstream_address = \"127.0.0.1\"\nstream_port = \"0\"\nstream_enable_tls = false\nstream_idle_timeout = \"\"\nstream_tls_cert = \"\"\nstream_tls_key = \"\"\nstream_tls_ca = \"\"\ngrpc_max_send_msg_size = 16777216\ngrpc_max_recv_msg_size = 16777216\n\n[crio.runtime]\nno_pivot = false\ndecryption_keys_path = \"/etc/crio/keys/\"\nconmon = \"\"\nconmon_cgroup = \"system.slice\"\nconmon_env = [\n        \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n]\ndefault_env = [\n]\nseccomp_profile = \"\"\nseccomp_use_default_when_empty = false\napparmor_profile = \"crio-default\"\nirqbalance_config_file = \"/etc/sysconfig/irqbalance\"\ncgroup_manager = \"systemd\"\nseparate_pull_cgroup = \"\"\ndefault_capabilities = [\n        \"CHOWN\",\n        \"DAC_OVERRIDE\",\n        \"FSETID\",\n        \"FOWNER\",\n        \"SETGID\",\n        \"SETUID\",\n        \"SETPCAP\",\n        \"NET_BIND_SERVICE\",\n        \"KILL\",\n]\ndefault_sysctls = [\n]\nadditional_devices = [\n]\nhooks_dir = [\n        \"/usr/share/containers/oci/hooks.d\",\n]pids_limit = 1024\nlog_size_max = -1\nlog_to_journald = false\ncontainer_exits_dir = \"/var/run/crio/exits\"\ncontainer_attach_socket_dir = \"/var/run/crio\"\nbind_mount_prefix = \"\"\nread_only = false\nlog_level = \"info\"\nlog_filter = \"\"\nuid_mappings = \"\"\ngid_mappings = \"\"\nctr_stop_timeout = 30\ndrop_infra_ctr = false\ninfra_ctr_cpuset = \"\"\nnamespaces_dir = \"/var/run\"\npinns_path = \"\"\ndefault_runtime = \"runc\"\n\n[crio.runtime.runtimes.runc]\nruntime_path = \"\"\nruntime_type = \"oci\"\nruntime_root = \"/run/runc\"\nallowed_annotations = [\n        \"io.containers.trace-syscall\",\n]\n\n[crio.image]\ndefault_transport = \"docker://\"\nglobal_auth_file = \"\"\npause_image = \"k8s.gcr.io/pause:3.2\"\npause_image_auth_file = \"\"\npause_command = \"/pause\"\nsignature_policy = \"\"\nimage_volumes = \"mkdir\"\nbig_files_temporary_dir = \"\"\n\n[crio.network]\nnetwork_dir = \"/etc/cni/net.d/\"\nplugin_dirs = [\n        \"/opt/cni/bin/\",\n]\n[crio.metrics]\nenable_metrics = false\nmetrics_port = 9090\nmetrics_socket = \"\"\n</code></pre> </li> </ul> </li> </ul> </li> <li> <p>crio\u3092\u518d\u8d77\u52d5\u3059\u308b    <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart crio\n</code></pre></p> </li> </ol>"},{"location":"setup/03_common_settings/#cli-tool","title":"CLI TOOL","text":"<ol> <li> <p>nerdctl</p> <ul> <li>containerd\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u516c\u958b\u3057\u3066\u3044\u308bdocker-cli\u4e92\u63db\u306eCLI</li> <li>https://github.com/containerd/nerdctl</li> <li>https://speakerdeck.com/ktock/dockerkaracontainerdhefalseyi-xing?slide=22 <pre><code>NERDCTL_VERSION=`curl -s -L https://api.github.com/repos/containerd/nerdctl/releases/latest | jq -r .tag_name`\ncurl -L -s https://github.com/containerd/nerdctl/releases/download/${NERDCTL_VERSION}/nerdctl-`echo ${NERDCTL_VERSION} | sed -e 's/^v//'`-linux-arm64.tar.gz | sudo tar -zxC /usr/local/bin/\n\nls -l /usr/local/bin\n</code></pre></li> </ul> </li> <li> <p>buildkit</p> <ul> <li>https://github.com/moby/buildkit</li> <li><code>nerdctl build</code> \u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981     <pre><code>BUILDKIT_VERSION=`curl -s -L https://api.github.com/repos/moby/buildkit/releases/latest | jq -r .tag_name`\ncurl -L -s https://github.com/moby/buildkit/releases/download/${BUILDKIT_VERSION}/buildkit-${BUILDKIT_VERSION}.linux-arm64.tar.gz | sudo tar -zxC /tmp/\nsudo mv /tmp/bin/* /usr/local/bin/\nls -l /usr/local/bin\n\nsudo curl -sL https://raw.githubusercontent.com/moby/buildkit/${BUILDKIT_VERSION}/examples/systemd/system/buildkit.socket -o /etc/systemd/system/buildkit.socket\nsudo curl -sL https://raw.githubusercontent.com/moby/buildkit/${BUILDKIT_VERSION}/examples/systemd/system/buildkit.service -o /etc/systemd/system/buildkit.service\nsudo systemctl enable buildkit.socket buildkit.service\nsudo systemctl start buildkit.socket buildkit.service\n</code></pre></li> </ul> </li> <li> <p>buildah </p> <ul> <li>cri-o \u5c0e\u5165\u6642\u671f\u306bimage build\u3067\u4f7f\u7528(containerd\u3067\u306f\u524d\u8ff0\u306enerdctl\u3078\u79fb\u884c\u6e08\u307f)</li> <li>https://github.com/containers/buildah/blob/master/install.md <pre><code>sudo apt-get -qq -y install buildah\n</code></pre></li> </ul> </li> <li> <p>cri-tools(crictl) \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb</p> <ul> <li>https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md <pre><code>VERSION=\"v1.30.1\"\nARCH=\"arm64\"\nDOWNLOAD_URL=\"https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-${VERSION}-linux-${ARCH}.tar.gz\"\ncurl -L ${DOWNLOAD_URL} | sudo tar -zxC /usr/local/bin\n\nls -l /usr/local/bin\n</code></pre></li> </ul> </li> <li> <p>podman \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(optional)</p> <ul> <li>https://podman.io/getting-started/installation <pre><code>sudo apt-get -y install podman\nsudo rm -f /etc/cni/net.d/87-podman-bridge.conflist\n</code></pre></li> </ul> </li> </ol>"},{"location":"setup/03_common_settings/#cni-plugin","title":"CNI Plugin\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ul> <li>https://github.com/containernetworking/plugins <pre><code>sudo mkdir -p /opt/cni/bin\nCNI_PLUGIN_VERSION=`curl -s -L https://api.github.com/repos/containernetworking/plugins/releases/latest | jq -r .tag_name`\nARCH=\"arm64\"\nDOWNLOAD_URL=\"https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGIN_VERSION}/cni-plugins-linux-${ARCH}-${CNI_PLUGIN_VERSION}.tgz\"\ncurl -L ${DOWNLOAD_URL} | sudo tar -zxC /opt/cni/bin\n\nls -l /opt/cni/bin/\n</code></pre></li> <li>cni config\u3092\u4f5c\u6210\u3059\u308b<ul> <li>https://www.cni.dev/plugins/current/main/bridge/ <pre><code>POD_CIDR=\"10.200.0.0/24\"\ncat &lt;&lt;EOF | sudo tee /etc/cni/net.d/10-bridge.conf\n{\n    \"cniVersion\": \"0.4.0\",\n    \"name\": \"bridge\",\n    \"type\": \"bridge\",\n    \"bridge\": \"cnio0\",\n    \"isGateway\": true,\n    \"ipMasq\": true,\n    \"ipam\": {\n        \"type\": \"host-local\",\n        \"ranges\": [\n          [{\"subnet\": \"${POD_CIDR}\"}]\n        ],\n        \"routes\": [{\"dst\": \"0.0.0.0/0\"}]\n    }\n}\nEOF\n\ncat &lt;&lt;EOF | sudo tee /etc/cni/net.d/20-loopback.conf\n{\n    \"cniVersion\": \"0.4.0\",\n    \"name\": \"lo\",\n    \"type\": \"loopback\"\n}\nEOF\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/03_common_settings/#kubectl","title":"kubectl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ul> <li>https://kubernetes.io/ja/docs/tasks/tools/install-kubectl/</li> </ul>"},{"location":"setup/04_creation_certificate/","title":"\u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210","text":""},{"location":"setup/04_creation_certificate/#_1","title":"\u624b\u9806","text":""},{"location":"setup/04_creation_certificate/#cfssl","title":"cfssl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<ul> <li>\u8a3c\u660e\u66f8\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e cfssl \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b<ul> <li>https://qiita.com/iaoiui/items/fc2ea829498402d4a8e3</li> <li>https://coreos.com/os/docs/latest/generate-self-signed-certificates.html <pre><code>sudo apt install -y golang-cfssl\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#ca","title":"CA(\u8a8d\u8a3c\u5c40) \u4f5c\u6210","text":"<pre><code>cat &lt;&lt; EOF &gt; ca-config.json\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"8760h\"\n        },\n        \"profiles\": {\n            \"kubernetes\": {\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ],\n                \"expiry\": \"8760h\"\n            }\n        }\n    }\n}\nEOF\n\ncat &lt;&lt; EOF &gt; ca-csr.json\n{\n    \"CN\": \"Kubernetes\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"Kubernetes\",\n            \"OU\": \"CA\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>ca-key.pem</code></li> <li><code>ca.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#_2","title":"\u8a3c\u660e\u66f8\u306e\u4f5c\u6210","text":""},{"location":"setup/04_creation_certificate/#_3","title":"\u7ba1\u7406\u8005\u30e6\u30fc\u30b6 \u8a3c\u660e\u66f8","text":"<pre><code>cat &lt;&lt; EOF &gt; admin-csr.json\n{\n    \"CN\": \"admin\",\n    \"hosts\": [],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"system:masters\",\n            \"OU\": \"Kubernetes The HardWay\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  admin-csr.json | cfssljson -bare admin\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>admin-key.pem</code></li> <li><code>admin.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kubelet","title":"kubelet\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8","text":"<ul> <li><code>EXTERNAL_IP</code><ul> <li>master\u30b5\u30fc\u30d0\u306ehostname</li> <li>master\u304c\u8907\u6570\u30b5\u30fc\u30d0\u69cb\u6210\u306e\u5834\u5408\u306f\u4e0a\u4f4d\u306eLB IP</li> </ul> </li> </ul> <pre><code>for instance in k8s-master k8s-node1 k8s-node2; do\n\ncat &lt;&lt; EOF &gt; ${instance}-csr.json\n{\n   \"CN\": \"system:node:${instance}\",\n   \"key\": {\n       \"algo\": \"rsa\",\n       \"size\": 2048\n   },\n   \"names\": [\n       {\n           \"C\": \"JP\",\n           \"L\": \"Tokyo\",\n           \"O\": \"system:nodes\",\n           \"OU\": \"Kubernetes The HardWay\",\n           \"ST\": \"Sample\"\n       }\n   ]\n}\nEOF\n\nEXTERNAL_IP=k8s-master\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=${instance},${EXTERNAL_IP} \\\n  -profile=kubernetes \\\n  ${instance}-csr.json | cfssljson -bare ${instance}\n\ndone\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>k8s-node1-key.pem</code></li> <li><code>k8s-node1.pem</code></li> <li><code>k8s-node2-key.pem</code></li> <li><code>k8s-node2.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kube-proxy","title":"kube-proxy\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8","text":"<pre><code>cat &lt;&lt; EOF &gt; kube-proxy-csr.json\n{\n    \"CN\": \"system:kube-proxy\",\n    \"hosts\": [],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"system:node-proxier\",\n            \"OU\": \"Kubernetes The Hard Way\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  kube-proxy-csr.json | cfssljson -bare kube-proxy\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>kube-proxy-key.pem</code></li> <li><code>kube-proxy.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kube-controller-manage","title":"kube-controller-manage\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8","text":"<pre><code>cat &lt;&lt; EOF &gt; kube-controller-manager-csr.json\n{\n  \"CN\": \"system:kube-controller-manager\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"JP\",\n      \"L\": \"Tokyo\",\n      \"O\": \"system:kube-controller-manager\",\n      \"OU\": \"Kubernetes The Hard Way\",\n      \"ST\": \"Sample\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>kube-controller-manager-key.pem</code></li> <li><code>kube-controller-manager.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kube-scheduler","title":"kube-scheduler\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8","text":"<pre><code>cat &lt;&lt; EOF &gt; kube-scheduler-csr.json\n{\n  \"CN\": \"system:kube-scheduler\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"Portland\",\n      \"O\": \"system:kube-scheduler\",\n      \"OU\": \"Kubernetes The Hard Way\",\n      \"ST\": \"Sample\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  kube-scheduler-csr.json | cfssljson -bare kube-scheduler\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>kube-scheduler-key.pem</code></li> <li><code>kube-scheduler.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kube-apiserver","title":"kube-apiserver\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8","text":"<ul> <li><code>10.32.0.1</code><ul> <li>Cluster IP</li> </ul> </li> </ul> <pre><code>KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local\n\ncat &lt;&lt; EOF &gt; kubernetes-csr.json\n{\n    \"CN\": \"Kubernetes\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"Kubernetes\",\n            \"OU\": \"Kubernetes The Hard Way\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=10.32.0.1,k8s-master,k8s-node1,k8s-node2,127.0.0.1,${KUBERNETES_HOSTNAMES} \\\n  -profile=kubernetes \\\n  kubernetes-csr.json | cfssljson -bare kubernetes\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>kubernetes-key.pem</code></li> <li><code>kubernetes.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#kube-apiserver-front-proxyfor-aggregation-layer","title":"kube-apiserver front-proxy(for aggregation layer)\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8","text":"<ol> <li> <p>CA(\u8a8d\u8a3c\u5c40)\u4f5c\u6210     <pre><code>cat &lt;&lt; EOF &gt; front-proxy-ca-config.json\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"8760h\"\n        },\n        \"profiles\": {\n            \"kubernetes\": {\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ],\n                \"expiry\": \"8760h\"\n            }\n        }\n    }\n}\nEOF\n\ncat &lt;&lt; EOF &gt; front-proxy-ca-csr.json\n{\n    \"CN\": \"Kubernetes\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"Kubernetes\",\n            \"OU\": \"CA\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca\n</code></pre></p> </li> <li> <p>front-proxy\u7528\u8a3c\u660e\u66f8\u306e\u4f5c\u6210     <pre><code>cat &lt;&lt; EOF &gt; front-proxy-csr.json\n{\n    \"CN\": \"front-proxy-ca\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"JP\",\n            \"L\": \"Tokyo\",\n            \"O\": \"Kubernetes\",\n            \"OU\": \"Kubernetes The Hard Way\",\n            \"ST\": \"Sample\"\n        }\n    ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=front-proxy-ca.pem \\\n  -ca-key=front-proxy-ca-key.pem \\\n  -config=front-proxy-ca-config.json \\\n  -profile=kubernetes \\\n  front-proxy-csr.json | cfssljson -bare front-proxy\n</code></pre></p> </li> <li> <p>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> <ul> <li><code>front-proxy-key.pem</code></li> <li><code>front-proxy.pem</code></li> </ul> </li> </ol>"},{"location":"setup/04_creation_certificate/#service-account","title":"service-account\u306e\u8a3c\u660e\u66f8","text":"<pre><code>cat &lt;&lt; EOF &gt; service-account-csr.json\n{\n  \"CN\": \"service-accounts\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"JP\",\n      \"L\": \"Tokyo\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes The Hard Way\",\n      \"ST\": \"Sample\"\n    }\n  ]\n}\nEOF\n\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  service-account-csr.json | cfssljson -bare service-account\n</code></pre> <ul> <li>\u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d<ul> <li><code>service-account-key.pem</code></li> <li><code>service-account.pem</code></li> </ul> </li> </ul>"},{"location":"setup/04_creation_certificate/#masternode","title":"\u8a3c\u660e\u66f8\u3092master/node\u3078\u30b3\u30d4\u30fc\u3059\u308b","text":"<ul> <li>master</li> <li>node</li> </ul>"},{"location":"setup/04_creation_certificate/#_4","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://kubernetes.io/ja/docs/setup/best-practices/certificates/</li> <li>https://kubernetes.io/ja/docs/concepts/cluster-administration/certificates/</li> <li>https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf</li> </ul>"},{"location":"setup/05_creating_config/","title":"\u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210","text":"<ul> <li>Controll Plane\u3068Node\u306e\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e <code>.kubeconfig</code> \u3092\u4f5c\u6210\u3059\u308b</li> </ul>"},{"location":"setup/05_creating_config/#_1","title":"\u624b\u9806","text":""},{"location":"setup/05_creating_config/#kubelet","title":"kubelet","text":"<pre><code>KUBERNETES_PUBLIC_ADDRESS=k8s-master\n\nfor instance in k8s-master k8s-node1 k8s-node2; do\n    kubectl config set-cluster kubernetes \\\n        --certificate-authority=ca.pem \\\n        --embed-certs=true \\\n        --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\\n        --kubeconfig=${instance}.kubeconfig\n\n    kubectl config set-credentials system:node:${instance} \\\n        --client-certificate=${instance}.pem \\\n        --client-key=${instance}-key.pem \\\n        --embed-certs=true \\\n        --kubeconfig=${instance}.kubeconfig\n\n    kubectl config set-context default \\\n        --cluster=kubernetes \\\n        --user=system:node:${instance} \\\n        --kubeconfig=${instance}.kubeconfig\n\n    kubectl config use-context default --kubeconfig=${instance}.kubeconfig\ndone\n</code></pre>"},{"location":"setup/05_creating_config/#kube-proxy","title":"kube-proxy","text":"<pre><code>KUBERNETES_PUBLIC_ADDRESS=k8s-master\n\nkubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\\n    --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config set-credentials system:kube-proxy \\\n    --client-certificate=kube-proxy.pem \\\n    --client-key=kube-proxy-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config set-context default \\\n    --cluster=kubernetes \\\n    --user=system:kube-proxy \\\n    --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig\n</code></pre>"},{"location":"setup/05_creating_config/#kube-controller-manager","title":"kube-controller-manager","text":"<pre><code>KUBE_API_SERVER_ADDRESS=k8s-master\n\nkubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-credentials system:kube-controller-manager \\\n    --client-certificate=kube-controller-manager.pem \\\n    --client-key=kube-controller-manager-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-context default \\\n    --cluster=kubernetes \\\n    --user=system:kube-controller-manager \\\n    --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig\n</code></pre>"},{"location":"setup/05_creating_config/#kube-scheduler","title":"kube-scheduler","text":"<pre><code>KUBE_API_SERVER_ADDRESS=k8s-master\n\nkubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\\n    --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config set-credentials system:kube-scheduler \\\n    --client-certificate=kube-scheduler.pem \\\n    --client-key=kube-scheduler-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config set-context default \\\n    --cluster=kubernetes \\\n    --user=system:kube-scheduler \\\n    --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig\n</code></pre>"},{"location":"setup/05_creating_config/#admin","title":"admin","text":"<pre><code>KUBERNETES_PUBLIC_ADDRESS=k8s-master\n\nkubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\\n    --kubeconfig=admin.kubeconfig\n\nkubectl config set-credentials admin \\\n    --client-certificate=admin.pem \\\n    --client-key=admin-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=admin.kubeconfig\n\nkubectl config set-context default \\\n    --cluster=kubernetes \\\n    --user=admin \\\n    --kubeconfig=admin.kubeconfig\n\nkubectl config use-context default --kubeconfig=admin.kubeconfig\n\nsudo mkdir -p /var/lib/kubernetes/\nsudo cp admin.kubeconfig /var/lib/kubernetes/admin.kubeconfig\n</code></pre>"},{"location":"setup/05_creating_config/#_2","title":"\u53c2\u8003\u8cc7\u6599","text":"<ul> <li>https://github.com/kelseyhightower/kubernetes/blob/master/docs/05-kubernetes-configuration-files.md</li> <li>https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf</li> <li>https://h3poteto.hatenablog.com/entry/2020/08/20/180552</li> <li><code>kubectl config set-cluster</code><ul> <li>https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-cluster/</li> <li>https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-cluster-em-</li> </ul> </li> <li><code>kubectl config set-credentials</code><ul> <li>https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-credentials/</li> <li>https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-credentials-em-</li> </ul> </li> <li><code>kubectl config set-context</code><ul> <li>https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-context/</li> <li>https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-context-em-</li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/","title":"bootstrapping kubelet(master/worker \u5171\u901a)","text":"<ul> <li><code>kubelet</code> \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002</li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#worker-node","title":"worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206","text":"<ul> <li>Reserve Compute Resources for System Daemons<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/</li> <li> <p><code>Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9</code> = <code>Node resource - system-reserved - kube-reserved - eviction-threshold</code> \u3089\u3057\u3044</p> name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi </li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_1","title":"\u624b\u9806","text":""},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubelet","title":"<code>kubelet</code> \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9","text":"<pre><code>VERSION=\"v1.30.1\"\nARCH=\"arm64\"\n\nsudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet\nsudo chmod +x /usr/bin/kubelet\n</code></pre>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeconfig","title":"kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e","text":"<pre><code># host=\"k8s-node2\"\n# host=\"k8s-node1\"\nhost=\"k8s-master\"\n\nsudo install -o root -g root -m 755 -d /etc/kubelet.d\nsudo install -o root -g root -m 755 -d /var/lib/kubernetes\nsudo install -o root -g root -m 755 -d /var/lib/kubelet\nsudo cp ca.pem /var/lib/kubernetes/\nsudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/\nsudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig\n</code></pre>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#varlibkubeletkubelet-configyaml","title":"<code>/var/lib/kubelet/kubelet-config.yaml</code> \u3092\u4f5c\u6210\u3059\u308b","text":"<ul> <li><code>clusterDNS</code> \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b</li> <li><code>podCIDR</code> \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b   <pre><code># host=\"k8s-node2\"\n# host=\"k8s-node1\"\nhost=\"k8s-master\"\n\ncat &lt;&lt; EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml\n---\nkind: KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\n\n# https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/\nstaticPodPath: /etc/kubelet.d\n\n# kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f\n#   - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b\n#   - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981\nauthentication:\n  anonymous:\n    enabled: true\n  webhook:\n    enabled: false\n    cacheTTL: \"2m\"\n  x509:\n    clientCAFile: \"/var/lib/kubernetes/ca.pem\"\n\n# kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a\n#   - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow\n#   - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981\nauthorization:\n  mode: AlwaysAllow\n\nclusterDomain: \"cluster.local\"\nclusterDNS:\n  - \"10.32.0.10\"\nresolvConf: \"/run/systemd/resolve/resolv.conf\"\npodCIDR: \"10.200.0.0/24\"\nruntimeRequestTimeout: \"15m\"\ntlsCertFile: \"/var/lib/kubelet/${host}.pem\"\ntlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\"\n\n# Reserve Compute Resources for System Daemons\n# https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/\n#\n# Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044\n#\n# system-reserved\n#   - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b\n#\n# kube-reserved\n#   - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b\nenforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"]\ncgroupsPerQOS: true\ncgroupDriver: systemd\ncgroupRoot: /\nsystemCgroups: /systemd/system.slice\nsystemReservedCgroup: /system.slice\nsystemReserved:\n  cpu: 256m\n  memory: 256Mi\nruntimeCgroups: /kube.slice/containerd.service\nkubeletCgroups: /kube.slice/kubelet.service\nkubeReservedCgroup: /kube.slice\nkubeReserved:\n  cpu: 1024m\n  memory: 1024Mi\nEOF\n</code></pre></li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#etcsystemdsystemkubeletservice","title":"<code>/etc/systemd/system/kubelet.service</code> \u3092\u914d\u7f6e","text":"<pre><code>cat &lt;&lt; 'EOF' | sudo tee /etc/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=containerd.service\nRequires=containerd.service\n\n[Service]\nRestart=on-failure\nRestartSec=5\n\nExecStartPre=/usr/bin/mkdir -p \\\n  /sys/fs/cgroup/kube.slice \\\n  /sys/fs/cgroup/system.slice \\\n  /sys/fs/cgroup/systemd/kube.slice \\\n  /sys/fs/cgroup/cpuset/kube.slice \\\n  /sys/fs/cgroup/cpuset/system.slice \\\n  /sys/fs/cgroup/pids/kube.slice \\\n  /sys/fs/cgroup/pids/system.slice \\\n  /sys/fs/cgroup/memory/kube.slice \\\n  /sys/fs/cgroup/memory/system.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/system.slice \\\n  /sys/fs/cgroup/hugetlb/system.slice \\\n  /sys/fs/cgroup/hugetlb/kube.slice\n\nExecStart=/usr/bin/kubelet \\\n  --config=/var/lib/kubelet/kubelet-config.yaml \\\n  --kubeconfig=/var/lib/kubelet/kubeconfig \\\n  --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\\n  --register-node=true \\\n  --v=2\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeletservice","title":"<code>kubelet.service</code> \u3092\u8d77\u52d5","text":"<pre><code>sudo systemctl enable kubelet.service\nsudo systemctl start kubelet.service\n</code></pre>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_2","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":""},{"location":"setup/06_master/01_bootstrapping_kubelet/#cgroup","title":"cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408","text":"<pre><code>kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist\n</code></pre> <ul> <li> <p><code>kubelet</code> \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( <code>--v 10</code> )</p> <pre><code>cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice]\n</code></pre> </li> <li> <p>\u5bfe\u5fdc     <code>kubelet.service</code> \u306e <code>ExecStartPre</code> \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b <pre><code>ExecStartPre=/usr/bin/mkdir -p \\\n  /sys/fs/cgroup/systemd/kube.slice \\\n  /sys/fs/cgroup/cpuset/kube.slice \\\n  /sys/fs/cgroup/cpuset/system.slice \\\n  /sys/fs/cgroup/pids/kube.slice \\\n  /sys/fs/cgroup/pids/system.slice \\\n  /sys/fs/cgroup/memory/kube.slice \\\n  /sys/fs/cgroup/memory/system.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice\n</code></pre>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cgroupsystemreserved-memory-size","title":"cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f","text":"<ul> <li>\u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f    <pre><code>kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy\n</code></pre></li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubeconfig-cn-node","title":"kubeconfig \u306e\u8a3c\u660e\u66f8\u306e <code>CN</code> \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b","text":"<pre><code>360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\"\n</code></pre> <ul> <li>kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b       <pre><code>sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject:\n</code></pre><ul> <li><code>k8s-master</code> \u304c\u6b63\u3057\u3044\u306e\u306b <code>CN = system:node:k8s-node1</code> \u3068\u306a\u3063\u3066\u3044\u305f    <pre><code>root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject:\n    Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#node-specpodcidr-cidr","title":"<code>Node</code> \u30ea\u30bd\u30fc\u30b9\u306e <code>spec.podCIDR</code> \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044","text":"<ul> <li> <p>\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044</p> <ul> <li>flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f...  <pre><code>kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'\n</code></pre></li> </ul> </li> <li> <p><code>kube-controller-manager</code> \u306e\u30ed\u30b0</p> <ul> <li><code>Set node k8s-node1 PodCIDR to [10.200.0.0/24]</code> \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8<ul> <li><code>kube-controller-manager</code> \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b <code>--allocate-node-cidrs=true</code> \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... <pre><code>actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist\nrange_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24]\nttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\"\ncontroller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}]\ncontroller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes\nnode_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\"\ncontroller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1\nnode_lifecycle_controller.go:1429] Initializing eviction metric for zone:\nnode_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp.\nevent.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\"\nnode_lifecycle_controller.go:1245] Controller detected that zone  is now in state Normal.\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#webhook-authentication","title":"Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044","text":"<pre><code>I0214 07:03:56.822586       1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\"\nF0214 07:03:56.822637       1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication\ngoroutine 1 [running]:\n</code></pre> <ul> <li>https://kubernetes.io/docs/reference/access-authn-authz/webhook/</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication</li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cni-plugin-etccninetd-cni-plugin","title":"CNI Plugin\u3092 <code>/etc/cni/net.d</code> \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044","text":"<pre><code>kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\ncni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d\nkubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\n</code></pre> <ul> <li>CNI Plugin\u3092 <code>/etc/cni/net.d</code> \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b<ul> <li>https://github.com/containernetworking/plugins/releases</li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#kubelet-cannot-determine-cpu-online-state","title":"Kubelet cannot determine CPU online state","text":"<pre><code>sysinfo.go:203] Nodes topology is not available, providing CPU topology\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory\ngce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping\nmachine.go:72] Cannot read number of physical cores correctly, number of cores set to 0\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping\nmachine.go:86] Cannot read number of sockets correctly, number of sockets set to 0\ncontainer_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name:\n</code></pre> <ul> <li>\u65e2\u77e5\u3089\u3057\u3044<ul> <li>https://github.com/kubernetes/kubernetes/issues/95039</li> </ul> </li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cni-plugin-not-initialized","title":"cni plugin not initialized","text":"<ul> <li><code>/opt/cni/bin</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f <code>/etc/cni/net.d</code> \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b     <pre><code>\"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\n</code></pre></li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#cni-config-uninitialized","title":"cni config uninitialized","text":"<ul> <li><code>/opt/cni/bin</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f <code>/etc/cni/net.d</code> \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b     <pre><code>\"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni config uninitialized\"\n</code></pre></li> </ul>"},{"location":"setup/06_master/01_bootstrapping_kubelet/#_3","title":"\u53c2\u8003","text":"<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/</li> <li>https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</li> <li>https://cyberagent.ai/blog/tech/4036/<ul> <li>kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b</li> </ul> </li> <li>https://downloadkubernetes.com/</li> <li> <p>https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/</p> </li> <li> <p>Node Authorization</p> <ul> <li>https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/node/</li> <li>https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/</li> </ul> </li> <li> <p>static pod</p> <ul> <li>https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/</li> <li>https://kubernetes.io/docs/concepts/policy/pod-security-policy/</li> <li>https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver</li> <li><code>PodSecurityPolicy</code> \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf(<code>false</code> \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f <code>true</code> \u306b\u76f4\u3059)</li> <li>https://github.com/kubernetes/kubernetes/issues/70952</li> </ul> </li> </ul>"},{"location":"setup/06_master/02_bootstrapping_etcd/","title":"bootstrapping etcd","text":"<p>coreos\u304cetcd docker image \u3092\u63d0\u4f9b \u3057\u3066\u3044\u307e\u3059\u304c\u3001Raspberry Pi\u306b\u642d\u8f09\u3055\u308c\u3066\u3044\u308bARM CPU\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3 <code>armv8(64bit)</code> \u3067\u5229\u7528\u53ef\u80fd\u306aimage\u306f\u63d0\u4f9b\u3057\u3066\u3044\u306a\u3044\u305f\u3081\u3001image\u3092build\u3057\u307e\u3059\u3002</p>"},{"location":"setup/06_master/02_bootstrapping_etcd/#_1","title":"\u624b\u9806","text":"<ol> <li> <p><code>Dockerfile_etcd.armhf</code> \u3092\u4f5c\u6210\u3059\u308b   Dockerfile_etcd.armhf <pre><code>cat &lt;&lt; 'EOF' &gt; Dockerfile_etcd.armhf\nFROM arm64v8/ubuntu:bionic AS etcd-builder\n\nRUN set -ex \\\n  &amp;&amp; apt update \\\n  &amp;&amp; apt install -y git tar zip curl \\\n  &amp;&amp; apt clean\n\nRUN set -ex \\\n  &amp;&amp; curl -L https://go.dev/dl/go1.22.3.linux-arm64.tar.gz | tar -zxC /usr/local\n\nRUN set -ex \\\n  &amp;&amp; git clone https://github.com/etcd-io/etcd.git -b v3.5.13 /tmp/etcd\\\n  &amp;&amp; cd /tmp/etcd \\\n  &amp;&amp; PATH=$PATH:/usr/local/go/bin:~/go/bin ./build\n\n\n\nFROM arm64v8/ubuntu:bionic\n\nCOPY --from=etcd-builder /tmp/etcd/bin/etcd /usr/local/bin/\nCOPY --from=etcd-builder /tmp/etcd/bin/etcdctl /usr/local/bin/\n\nRUN set -ex \\\n  &amp;&amp; apt update \\\n  &amp;&amp; apt clean \\\n  &amp;&amp; install -o root -g root -m 700 -d /var/lib/etcd \\\n  &amp;&amp; install -o root -g root -m 644 -d /etc/etcd\n\nCOPY ca.pem /etc/etcd/\nCOPY kubernetes-key.pem /etc/etcd/\nCOPY kubernetes.pem /etc/etcd/\n\nENV ETCD_UNSUPPORTED_ARCH=arm64\n\nEXPOSE 2379 2380\n\nENTRYPOINT [\"/usr/local/bin/etcd\"]\nEOF\n</code></pre> </p> </li> <li> <p>image build    <pre><code>sudo mkdir -p /etcd-data\nsudo nerdctl build --namespace k8s.io -t k8s-etcd --file=Dockerfile_etcd.armhf ./\n</code></pre></p> </li> <li> <p>pod manifests\u3092 <code>/etc/kubelet.d</code> \u3078\u4f5c\u6210\u3059\u308b   /etc/kubelet.d/etcd.yaml <pre><code>cat &lt;&lt; EOF | sudo tee /etc/kubelet.d/etcd.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://k8s-master:2379\n  name: etcd\n  namespace: kube-system\n  labels:\n    tier: control-plane\n    component: etcd\n\nspec:\n  # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n  priorityClassName: system-node-critical\n  hostNetwork: true\n  volumes:\n  - name: etcd-data-volume\n    hostPath:\n      path: /etcd-data\n      type: Directory\n  containers:\n    - name: etcd\n      image: k8s-etcd:latest\n      imagePullPolicy: IfNotPresent\n      volumeMounts:\n      - mountPath: /etcd-data\n        name: etcd-data-volume\n      env:\n      - name: ETCD_UNSUPPORTED_ARCH\n        value: \"arm64\"\n      resources:\n        requests:\n          cpu: 0.5\n          memory: \"384Mi\"\n        limits:\n          cpu: 1\n          memory: \"384Mi\"\n      command:\n        - /usr/local/bin/etcd\n        - --data-dir=/etcd-data\n        - --advertise-client-urls=https://k8s-master:2379,https://k8s-master:2380\n        - --listen-client-urls=https://0.0.0.0:2379\n        - --initial-advertise-peer-urls=https://k8s-master:2380\n        - --listen-peer-urls=https://0.0.0.0:2380\n        - --name=etcd0\n        - --cert-file=/etc/etcd/kubernetes.pem\n        - --key-file=/etc/etcd/kubernetes-key.pem\n        - --peer-cert-file=/etc/etcd/kubernetes.pem\n        - --peer-key-file=/etc/etcd/kubernetes-key.pem\n        - --trusted-ca-file=/etc/etcd/ca.pem\n        - --peer-trusted-ca-file=/etc/etcd/ca.pem\n        - --peer-client-cert-auth\n        - --client-cert-auth\n        - --initial-cluster-token=etcd-cluster-1\n        - --initial-cluster=etcd0=https://k8s-master:2380\n        - --initial-cluster-state=new\nEOF\n</code></pre> </p> </li> <li> <p><code>crictl</code> \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b    <pre><code>$ sudo crictl ps --name etcd\nCONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID\n72f58248ec087       6e8b8110dc13cfe61d75f867a22c39766a397989413570500f51dedf94be7a12   25 seconds ago       Running             etcd                0                   206c5b952097a\n</code></pre></p> </li> </ol>"},{"location":"setup/06_master/02_bootstrapping_etcd/#_2","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://etcd.io/docs/v2/docker_guide/</li> <li>https://quay.io/repository/coreos/etcd?tag=latest&amp;tab=tags</li> <li>https://github.com/etcd-io/etcd</li> </ul>"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/","title":"bootstrapping kube-apiserver","text":""},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_1","title":"\u624b\u9806","text":"<ol> <li> <p><code>Dockerfile_kube-apiserver.armhf</code> \u3092\u4f5c\u6210\u3059\u308b   Dockerfile_kube-apiserver.armhf <pre><code>cat &lt;&lt; 'EOF' &gt; Dockerfile_kube-apiserver.armhf\nFROM arm64v8/ubuntu:bionic\n\nARG VERSION=\"v1.30.1\"\nARG ARCH=\"arm64\"\n\nRUN set -ex \\\n  &amp;&amp; apt update \\\n  &amp;&amp; apt install -y wget \\\n  &amp;&amp; apt clean \\\n  &amp;&amp; wget --quiet -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-apiserver \\\n  &amp;&amp; chmod +x /usr/bin/kube-apiserver \\\n  &amp;&amp; install -o root -g root -m 755 -d /var/lib/kubernetes \\\n  &amp;&amp; install -o root -g root -m 755 -d /etc/kubernetes/config \\\n  &amp;&amp; install -o root -g root -m 755 -d /etc/kubernetes/webhook\n\nCOPY ca.pem \\\n     ca-key.pem \\\n     kubernetes-key.pem \\\n     kubernetes.pem \\\n     service-account-key.pem \\\n     service-account.pem \\\n     encryption-config.yaml \\\n     front-proxy-ca.pem \\\n     front-proxy.pem \\\n     front-proxy-key.pem \\\n     /var/lib/kubernetes/\n\nCOPY authorization-config.yaml /etc/kubernetes/webhook/\n\nEXPOSE 6443\n\nENTRYPOINT [\"/usr/bin/kube-apiserver\"]\nEOF\n</code></pre> </p> </li> <li> <p>encryption-provider-config \u3092\u4f5c\u6210\u3059\u308b</p> <ul> <li><code>--encryption-provider-config</code> \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u3066secret\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u6697\u53f7\u5316\u3059\u308b\u305f\u3081\u306e\u9375\u3092\u5b9a\u7fa9\u3059\u308b<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#encrypting-your-data</li> <li>https://access.redhat.com/documentation/ja-jp/openshift_container_platform/3.11/html/cluster_administration/admin-guide-encrypting-data-at-datastore encryption-config.yaml <pre><code>ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)\n\ncat &lt;&lt; EOF &gt; encryption-config.yaml\n---\nkind: EncryptionConfig\napiVersion: v1\nresources:\n  - resources:\n      - secrets\n    providers:\n      - aescbc:\n          keys:\n            - name: key1\n              secret: ${ENCRYPTION_KEY}\n      - identity: {}\nEOF\n</code></pre> </li> </ul> </li> </ul> </li> <li> <p>webbhook config\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b</p> <ul> <li><code>--authorization-webhook-config-file</code> \u3067\u6307\u5b9a\u3059\u308b\u30d5\u30a1\u30a4\u30eb    authorization-config.yaml <pre><code>KUBE_API_SERVER_ADDRESS=k8s-master\n\ncat &lt;&lt; EOF &gt; authorization-config.yaml\n---\napiVersion: v1\n# kind of the API object\nkind: Config\n# clusters refers to the remote service.\nclusters:\n  - name: kubernetes\n    cluster:\n      certificate-authority: /var/lib/kubernetes/ca.pem       # CA for verifying the remote service.\n      server: https://${KUBE_API_SERVER_ADDRESS}:6443/authenticate # URL of remote service to query. Must use 'https'.\n\n# users refers to the API server's webhook configuration.\nusers:\n  - name: api-server-webhook\n    user:\n      client-certificate: /var/lib/kubernetes/kubernetes.pem  # cert for the webhook plugin to use\n      client-key: /var/lib/kubernetes/kubernetes-key.pem      # key matching the cert\n\n# kubeconfig files require a context. Provide one for the API server.\ncurrent-context: webhook\ncontexts:\n- context:\n    cluster: kubernetes\n    user: api-server-webhook\n  name: webhook\nEOF\n</code></pre> </li> </ul> </li> <li> <p>image build    <pre><code>sudo nerdctl build --namespace k8s.io -t k8s-kube-apiserver --file=Dockerfile_kube-apiserver.armhf ./\n</code></pre></p> </li> <li> <p>pod manifests\u3092 <code>/etc/kubelet.d</code> \u3078\u4f5c\u6210\u3059\u308b</p> <ul> <li><code>--advertise-address</code> \u30aa\u30d7\u30b7\u30e7\u30f3\u306fIP\u30a2\u30c9\u30ec\u30b9\u3067\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b(hostname\u3067\u306f\u8d77\u52d5\u3057\u306a\u304b\u3063\u305f)   /etc/kubelet.d/kube-api-server.yaml <pre><code>KUBE_API_SERVER_ADDRESS=192.168.3.50\n\ncat &lt;&lt; EOF | sudo tee /etc/kubelet.d/kube-api-server.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: runtime/default\n  labels:\n    tier: control-plane\n    component: kube-apiserver\n\nspec:\n  # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n  priorityClassName: system-node-critical\n  hostNetwork: true\n  containers:\n    - name: kube-apiserver\n      image: k8s-kube-apiserver:latest\n      imagePullPolicy: IfNotPresent\n      resources:\n        requests:\n          memory: \"512Mi\"\n        limits:\n          memory: \"1024Mi\"\n      command:\n        - /usr/bin/kube-apiserver\n        - --advertise-address=k8s-master\n        - --allow-privileged=true\n        - --anonymous-auth=false\n        - --apiserver-count=1\n        - --audit-log-maxage=30\n        - --audit-log-maxbackup=3\n        - --audit-log-maxsize=100\n        - --audit-log-path=/var/log/audit.log\n        - --authorization-mode=Node,RBAC,Webhook\n        - --authorization-webhook-config-file=/etc/kubernetes/webhook/authorization-config.yaml\n        - --authentication-token-webhook-cache-ttl=2m\n        - --authentication-token-webhook-version=v1\n        - --bind-address=0.0.0.0\n        - --client-ca-file=/var/lib/kubernetes/ca.pem\n        - --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,RuntimeClass\n        - --etcd-cafile=/var/lib/kubernetes/ca.pem\n        - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem\n        - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem\n        - --etcd-servers=https://k8s-master:2379\n        - --event-ttl=1h\n        - --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml\n        - --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem\n        - --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem\n        - --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem\n        - --runtime-config=authentication.k8s.io/v1beta1=true\n        - --feature-gates=APIPriorityAndFairness=false\n        - --service-account-key-file=/var/lib/kubernetes/service-account.pem\n        - --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem\n        - --service-account-issuer=api\n        - --service-account-api-audiences=api\n        - --service-cluster-ip-range=10.32.0.0/24\n        - --service-node-port-range=30000-32767\n        - --tls-cert-file=/var/lib/kubernetes/kubernetes.pem\n        - --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem\n        - --http2-max-streams-per-connection=3000\n        - --max-requests-inflight=3000\n        - --max-mutating-requests-inflight=1000\n        - --enable-aggregator-routing=true\n        - --requestheader-client-ca-file=/var/lib/kubernetes/front-proxy-ca.pem\n        - --requestheader-allowed-names=front-proxy-ca\n        - --requestheader-extra-headers-prefix=X-Remote-Extra\n        - --requestheader-group-headers=X-Remote-Group\n        - --requestheader-username-headers=X-Remote-User\n        - --proxy-client-cert-file=/var/lib/kubernetes/front-proxy.pem\n        - --proxy-client-key-file=/var/lib/kubernetes/front-proxy-key.pem\n        - --v=2\nEOF\n</code></pre> </li> </ul> </li> <li> <p><code>crictl</code> \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b    <pre><code>$ sudo crictl ps --name kube-apiserver\nCONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID\n82c371fd9d99e       83e685a0b921ef5dd91eb3cdf208ba70690c1dd7decfc39bb3903be6ede752e6   24 seconds ago      Running             kube-apiserver      0                   6af4d1b99fa37\n</code></pre></p> </li> <li> <p>master node\u306bPod\u304cschedule\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u308b</p> <ul> <li>taint\u3092\u8a2d\u5b9a\u3059\u308b<ul> <li>https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/</li> <li>https://kubernetes.io/ja/docs/concepts/scheduling-eviction/taint-and-toleration/ <pre><code>kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule\n</code></pre> <pre><code>$ kubectl get node k8s-master -o=jsonpath='{.spec.taints}'\n[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\"}]\n</code></pre></li> </ul> </li> </ul> </li> </ol>"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_2","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":"<ol> <li> <p><code>failed creating mandatory flowcontrol settings: failed getting mandatory FlowSchema exempt due to the server was unable to return a response in the time allotted, but may still be processing the request</code></p> <ul> <li>https://github.com/kubernetes/kubernetes/issues/97525#issuecomment-753022219<ul> <li>kube-apiserver\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u4ee5\u4e0b2\u3064\u3092\u4ed8\u52a0\u3059\u308b<ul> <li><code>--feature-gates=APIPriorityAndFairness=false</code></li> <li><code>--runtime-config=flowcontrol.apiserver.k8s.io/v1beta1=false</code></li> </ul> </li> </ul> </li> </ul> </li> <li> <p><code>failed to run Kubelet: no client provided, cannot use webhook authentication</code></p> <ul> <li>kubelet\u304cWebhook\u8a8d\u8a3c\u3092\u671f\u5f85\u3057\u3066\u3044\u308b\u306e\u306bkube-api-server\u3067Webhook\u8a8d\u8a3c\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408<ul> <li>Webhook\u8a8d\u8a3c\u3092\u6709\u52b9\u306b\u3059\u308b</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/webhook/<ul> <li>https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication</li> </ul> </li> </ul> </li> </ul> </li> <li> <p><code>Failed creating a mirror pod for \"kube-scheduler-k8s-master_kube-system(a4a914cd05761a5a4335e2510ca075aa)\": pods \"kube-scheduler-k8s-master\" is forbidden: PodSecurityPolicy: no providers available to validate pod request</code></p> <ul> <li>StaticPod\u3092\u8d77\u52d5\u3057\u305f\u969b\u306bkubelet\u304b\u3089kube-apiserver\u3078mirror pod\u60c5\u5831\u3092\u767b\u9332\u3057\u3088\u3046\u3068\u3057\u3066PodSecurityPolicy\u306b\u3088\u308a\u62d2\u5426\u3055\u308c\u305f</li> </ul> </li> </ol>"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#_3","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/webhook/</li> </ul>"},{"location":"setup/06_master/03_bootstrapping_kube-apiserver/#old","title":"old","text":"<ol> <li>default\u306ePodSecurityPolicy(PSP)\u3092\u4f5c\u6210\u3059\u308b<ul> <li> <p><code>staticPod</code> \u3092\u4f5c\u6210\u3059\u308b\u969b\u306bkubelet\u304b\u3089mirror pod\u4f5c\u6210\u30ea\u30af\u30a8\u30b9\u30c8\u304c\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3059 (\u53c2\u8003)   PSP / ClusterRole / ClusterRoleBinding <pre><code>cat &lt;&lt; EOF | kubectl apply --kubeconfig admin.kubeconfig -f -\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  annotations:\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'\n    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'\n    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'\n  name: default\nspec:\n  # allowedCapabilities: []  # default set of capabilities are implicitly allowed\n  allowedCapabilities:\n    - '*'\n    # - NET_ADMIN\n    # - NET_RAW\n    # - SYS_ADMIN\n  fsGroup:\n    rule: 'MustRunAs'\n    ranges:\n      # Forbid adding the root group.\n      - min: 1\n        max: 65535\n  hostIPC: true\n  hostNetwork: true\n  hostPID: true\n  privileged: true\n  allowPrivilegeEscalation: true\n  readOnlyRootFilesystem: true\n  runAsUser:\n    rule: 'MustRunAsNonRoot'\n  seLinux:\n    rule: 'RunAsNonRoot'\n  supplementalGroups:\n    rule: 'RunAsNonRoot'\n    ranges:\n      # Forbid adding the root group.\n      - min: 1\n        max: 65535\n  volumes:\n  - 'configMap'\n  - 'downwardAPI'\n  - 'emptyDir'\n  - 'persistentVolumeClaim'\n  - 'projected'\n  - 'secret'\n  - 'hostPath'\n  hostNetwork: true\n  runAsUser:\n    rule: 'RunAsAny'\n  seLinux:\n    rule: 'RunAsAny'\n  supplementalGroups:\n    rule: 'RunAsAny'\n  fsGroup:\n    rule: 'RunAsAny'\n\n---\n\n# Cluster role which grants access to the default pod security policy\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: default-psp\nrules:\n- apiGroups:\n  - policy\n  resourceNames:\n  - default\n  resources:\n  - podsecuritypolicies\n  verbs:\n  - use\n\n---\n\n# Cluster role binding for default pod security policy granting all authenticated users access\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: default-psp\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: default-psp\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:authenticated\nEOF\n</code></pre> </p> <pre><code>$ cat &lt;&lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f -\n\n  &lt;\u7701\u7565&gt;\n\npodsecuritypolicy.policy/default created\nclusterrole.rbac.authorization.k8s.io/default-psp created\nclusterrolebinding.rbac.authorization.k8s.io/default-psp created\n</code></pre> </li> </ul> </li> </ol>"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/","title":"bootstrapping kube-controller-manager","text":""},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_1","title":"\u624b\u9806","text":"<ol> <li> <p><code>Dockerfile_kube-controller-manager.armhf</code> \u3092\u4f5c\u6210\u3059\u308b   Dockerfile_kube-controller-manager.armhf <pre><code>cat &lt;&lt; 'EOF' &gt; Dockerfile_kube-controller-manager.armhf\nFROM arm64v8/ubuntu:bionic\n\nARG VERSION=\"v1.30.1\"\nARG ARCH=\"arm64\"\n\nRUN set -ex \\\n  &amp;&amp; apt update \\\n  &amp;&amp; apt install -y wget \\\n  &amp;&amp; apt clean \\\n  &amp;&amp; wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-controller-manager \\\n  &amp;&amp; chmod +x /usr/bin/kube-controller-manager \\\n  &amp;&amp; install -o root -g root -m 755 -d /var/lib/kubernetes \\\n  &amp;&amp; install -o root -g root -m 755 -d /etc/kubernetes/config\n\nCOPY ca.pem \\\n     ca-key.pem \\\n     service-account-key.pem \\\n     kube-controller-manager.kubeconfig \\\n     /var/lib/kubernetes/\n\nENTRYPOINT [\"/usr/bin/kube-controller-manager\"]\nEOF\n</code></pre> </p> </li> <li> <p>image build    <pre><code>sudo nerdctl build --namespace k8s.io -t k8s-kube-controller-manager --file=Dockerfile_kube-controller-manager.armhf ./\n</code></pre></p> </li> <li> <p>pod manifests\u3092 <code>/etc/kubelet.d</code> \u3078\u4f5c\u6210\u3059\u308b</p> </li> <li> <p><code>--allocate-node-cidrs=true</code></p> <ul> <li> <p>Node resource\u306e <code>spec.podCIDR</code> \u3078CIDR\u304c\u8a2d\u5b9a\u3055\u308c\u308b      <pre><code>kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'\n</code></pre></p> <ul> <li><code>spec.podCIDR</code> \u306e\u5024\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044node instance\u3067\u306fCNI Plugin(flannel)\u304c\u6b63\u5e38\u52d5\u4f5c\u3057\u306a\u304b\u3063\u305f</li> </ul> </li> </ul> <p>/etc/kubelet.d/kube-controller-manager.yaml <pre><code>cat &lt;&lt; EOF | sudo tee /etc/kubelet.d/kube-controller-manager.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\n  namespace: kube-system\n  labels:\n    tier: control-plane\n    component: kube-controller-manager\n\nspec:\n  # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n  priorityClassName: system-node-critical\n  hostNetwork: true\n  containers:\n    - name: kube-controller-manager\n      image: k8s-kube-controller-manager:latest\n      imagePullPolicy: IfNotPresent\n      resources:\n        requests:\n          cpu: \"256m\"\n          memory: \"128Mi\"\n        limits:\n          cpu: \"384m\"\n          memory: \"128Mi\"\n      command:\n        - /usr/bin/kube-controller-manager\n        - --bind-address=0.0.0.0\n        - --cluster-cidr=10.200.0.0/16\n        - --allocate-node-cidrs=true\n        - --node-cidr-mask-size=24\n        - --cluster-name=kubernetes\n        - --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem\n        - --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem\n        - --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig\n        - --leader-elect=false\n        - --root-ca-file=/var/lib/kubernetes/ca.pem\n        - --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem\n        - --service-cluster-ip-range=10.32.0.0/24\n        - --use-service-account-credentials=true\n        - --v=2\nEOF\n</code></pre> </p> </li> <li> <p><code>crictl</code> \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b    <pre><code>$ sudo crictl ps --name kube-controller-manager\nCONTAINER           IMAGE                                                              CREATED             STATE               NAME                      ATTEMPT             POD ID\na72cec7323686       4ada5d332b2c795b6333b8b6c538491dec96fb80f81b600359615651725b0ccf   20 seconds ago      Running             kube-controller-manager   0                   526d7f2e9d3cb\n</code></pre></p> </li> </ol>"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_2","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":"<ol> <li>Client.Timeout\u3092\u8d85\u3048\u305f\u305f\u3081\u3001kube-control-manager\u3068kube-scheduler\u304c\u30ed\u30c3\u30af\u3092\u53d6\u5f97\u3067\u304d\u306a\u3044<ul> <li>\u767a\u751f\u3057\u305f\u3089\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u518d\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067\u56de\u5fa9\u3059\u308b</li> <li>kube-apiserver\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u304c\u4e0a\u304c\u308b\u3068\u767a\u751f\u3057\u6613\u304f\u306a\u308b     <pre><code>E0325 11:08:47.205570       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://192.168.10.50:6443/apis/coordination.k8s.io/v1/namespaces/kube-\nsystem/leases/kube-controller-manager?timeout=10s\": context deadline exceeded\nI0325 11:08:47.205695       1 leaderelection.go:278] failed to renew lease kube-system/kube-controller-manager: timed out waiting for the condition\nF0325 11:08:47.205929       1 controllermanager.go:294] leaderelection lost\n</code></pre></li> </ul> </li> </ol>"},{"location":"setup/06_master/04_bootstrapping_kube-controller-manager/#_3","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/</li> <li>node(flannel)\u306e <code>Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned</code> \u30a8\u30e9\u30fc\u306b\u95a2\u3057\u3066</li> <li>https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/</li> <li>https://devops.stackexchange.com/questions/5898/how-to-get-kubernetes-pod-network-cidr</li> </ul>"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/","title":"bootstrapping kube-scheduler","text":""},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/#_1","title":"\u624b\u9806","text":"<ol> <li> <p><code>Dockerfile_kube-scheduler.armhf</code> \u3092\u4f5c\u6210\u3059\u308b   Dockerfile_kube-scheduler.armhf <pre><code>cat &lt;&lt; 'EOF' &gt; Dockerfile_kube-scheduler.armhf\nFROM arm64v8/ubuntu:bionic\n\nARG VERSION=\"v1.30.1\"\nARG ARCH=\"arm64\"\n\nRUN set -ex \\\n  &amp;&amp; apt update \\\n  &amp;&amp; apt install -y wget \\\n  &amp;&amp; apt clean \\\n  &amp;&amp; wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-scheduler \\\n  &amp;&amp; chmod +x /usr/bin/kube-scheduler \\\n  &amp;&amp; install -o root -g root -m 755 -d /var/lib/kubernetes \\\n  &amp;&amp; install -o root -g root -m 755 -d /etc/kubernetes/config\n\nCOPY kube-scheduler.yaml /etc/kubernetes/config/\nCOPY kube-scheduler.kubeconfig /var/lib/kubernetes/\n\nENTRYPOINT [\"/usr/bin/kube-scheduler\"]\nEOF\n</code></pre> </p> </li> <li> <p>kube-scheduler\u306econfig\u751f\u6210</p> </li> <li> <p>k8s 1.19.0 \u3067 <code>KubeSchedulerConfiguration</code> \u304c beta\u306bupdate\u3055\u308c\u3066\u3044\u307e\u3059</p> <ul> <li>https://qiita.com/everpeace/items/7dbf14773db82e765370 kube-scheduler.yaml <pre><code>cat &lt;&lt; EOF &gt; kube-scheduler.yaml\n---\napiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nclientConnection:\n  kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\"\nleaderElection:\n  leaderElect: false\nEOF\n</code></pre> </li> </ul> </li> <li> <p>image build    <pre><code>sudo nerdctl build --namespace k8s.io -t k8s-kube-scheduler --file=Dockerfile_kube-scheduler.armhf ./\n</code></pre></p> </li> <li> <p>pod manifests\u3092 <code>/etc/kubelet.d</code> \u3078\u4f5c\u6210\u3059\u308b   /etc/kubelet.d/kube-scheduler.yaml <pre><code>cat &lt;&lt; EOF | sudo tee /etc/kubelet.d/kube-scheduler.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-scheduler\n  namespace: kube-system\n  labels:\n    tier: control-plane\n    component: kube-scheduler\n\nspec:\n  # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n  priorityClassName: system-node-critical\n  hostNetwork: true\n  containers:\n    - name: kube-scheduler\n      image: k8s-kube-scheduler:latest\n      imagePullPolicy: IfNotPresent\n      resources:\n        requests:\n          cpu: \"256m\"\n          memory: \"128Mi\"\n        limits:\n          cpu: \"384m\"\n          memory: \"128Mi\"\n      command:\n        - /usr/bin/kube-scheduler\n        - --config=/etc/kubernetes/config/kube-scheduler.yaml\n        - --v=2\nEOF\n</code></pre> </p> </li> <li> <p><code>crictl</code> \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b    <pre><code>$ sudo crictl ps --name kube-scheduler\nCONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID\na19648dec2d54       70e852515b3c74175bb3ad4855287cb81101921b2b1f5a890fa4ebd0eeeee684   15 seconds ago      Running             kube-scheduler      0                   da1d0572bc2b1\n</code></pre></p> </li> </ol>"},{"location":"setup/06_master/05_bootstrapping_kube-scheduler/#_2","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/</li> </ul>"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/","title":"kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b","text":"<p><code>kubectl</code> \u3084\u4ed6Client tool\u3067\u306fkube-apiserver\u3078\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u6295\u3052\u307e\u3059\u3002<code>kube-apiserver</code> \u3067\u306fetcd\u306b\u683c\u7d0d\u3055\u308c\u305f\u60c5\u5831\u3092\u57fa\u306b\u5404worker node(\u306e <code>kubelet</code>) \u3068\u3084\u308a\u3068\u308a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002(\u4f8b\u3048\u3070exec,top,logs\u306a\u3069) <code>kube-apiserver</code> \u304b\u3089 <code>kubelet</code> \u306e\u5fc5\u8981\u306a\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u3092\u4ed8\u4e0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/#_1","title":"\u624b\u9806","text":"<ol> <li> <p>ClusterRole <code>system:kube-apiserver-to-kubelet</code> \u3092\u4f5c\u6210</p> <ul> <li><code>rbac.authorization.kubernetes.io/autoupdate</code> annotations<ul> <li>\u8d77\u52d5\u3059\u308b\u305f\u3073\u306b\u3001API\u30b5\u30fc\u30d0\u30fc\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRole\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a29\u9650\u3067\u66f4\u65b0\u3057\u3001   \u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRoleBinding\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308bsubjects\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002   \u3053\u308c\u306b\u3088\u308a\u3001\u8aa4\u3063\u305f\u5909\u66f4\u3092\u30af\u30e9\u30b9\u30bf\u304c\u4fee\u5fa9\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001   \u65b0\u3057\u3044Kubernetes\u30ea\u30ea\u30fc\u30b9\u3067\u6a29\u9650\u3068subjects\u304c\u5909\u66f4\u3055\u308c\u3066\u3082\u3001   Role\u3068RoleBinding\u3092\u6700\u65b0\u306e\u72b6\u614b\u306b\u4fdd\u3064\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</li> </ul> </li> <li><code>kubernetes.io/bootstrapping: rbac-defaults</code> labels<ul> <li>k8s\u306e\u65e2\u5b9a\u30af\u30e9\u30b9\u30bf\u30ed\u30fc\u30eb\u3068\u65e2\u5b9a\u30ed\u30fc\u30eb\u30d0\u30a4\u30f3\u30c9\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3059</li> </ul> </li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl apply --kubeconfig admin.kubeconfig -f -\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: system:kube-apiserver-to-kubelet\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/proxy\n      - nodes/stats\n      - nodes/log\n      - nodes/spec\n      - nodes/metrics\n    verbs:\n      - \"*\"\nEOF\n</code></pre> </li> <li> <p><code>Kubernetes</code> \u30e6\u30fc\u30b6\u3078<code>system:kube-apiserver-to-kubelet</code> ClusterRole\u3092\u7d10\u4ed8\u3051\u308b</p> <ul> <li><code>roleRef</code> \u3067\u7d10\u4ed8\u3051\u305fRole\u3092\u6307\u5b9a\u3059\u308b</li> <li> <p><code>subjects</code> \u3067Role\u3092\u7d10\u4ed8\u3051\u308bAccount\u3092\u6307\u5b9a\u3059\u308b   <pre><code>cat &lt;&lt; EOF | kubectl apply --kubeconfig admin.kubeconfig -f -\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:kube-apiserver\n  namespace: \"\"\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:kube-apiserver-to-kubelet\nsubjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: User\n    name: Kubernetes\nEOF\n</code></pre></p> <ul> <li>\u3053\u306e\u7d10\u4ed8\u3051\u304c\u6b63\u3057\u304f\u306a\u3044\u3001\u3082\u3057\u304f\u306f\u672a\u8a2d\u5b9a\u306e\u5834\u5408\u3001token\u4ed8\u304d\u3067kubectl\u3092\u5229\u7528\u3057\u305f\u5834\u5408\u306b\u4ee5\u4e0b\u30a8\u30e9\u30fc\u3068\u306a\u308b   <pre><code>Error from server (Forbidden): Forbidden (user=Kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-proxy)\n</code></pre></li> </ul> </li> </ul> </li> </ol>"},{"location":"setup/06_master/06_configuration_rbac_to_access_from-apiserver-to-kubelet/#_2","title":"\u53c2\u8003\u8cc7\u6599","text":"<ul> <li>https://kubernetes.io/ja/docs/reference/access-authn-authz/rbac/</li> <li>https://qiita.com/sheepland/items/67a5bb9b19d8686f389d</li> </ul>"},{"location":"setup/06_master/07_controller_health_check/","title":"Kubernetes API \u306e\u30d8\u30eb\u30b9\u30c1\u30a7\u30c3\u30af","text":""},{"location":"setup/06_master/07_controller_health_check/#_1","title":"\u624b\u9806","text":""},{"location":"setup/06_master/07_controller_health_check/#_2","title":"\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u8d77\u52d5\u78ba\u8a8d","text":"<p><pre><code>kubectl get pods -n kube-system\n</code></pre> \u5b9f\u884c\u4f8b <pre><code>$ kubectl get pods -n kube-system\nNAME                                 READY   STATUS    RESTARTS   AGE\netcd-k8s-master                      1/1     Running   0          5m56s\nkube-apiserver-k8s-master            1/1     Running   0          6m7s\nkube-controller-manager-k8s-master   1/1     Running   0          4m2s\nkube-scheduler-k8s-master            1/1     Running   0          2m48s\n</code></pre> </p>"},{"location":"setup/06_master/07_controller_health_check/#master-noderesource","title":"master node\u4e0a\u306eresource\u78ba\u8a8d","text":"<p><pre><code>kubectl get nodes\nkubectl describe node &lt;pod_name&gt;\n</code></pre> \u5b9f\u884c\u4f8b <pre><code>$ kubectl get nodes\nNAME         STATUS   ROLES    AGE     VERSION\nk8s-master   Ready    &lt;none&gt;   7m57s   v1.20.1\n\n$ kubectl describe node k8s-master\nName:               k8s-master\nRoles:              &lt;none&gt;\nLabels:             beta.kubernetes.io/arch=arm64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=arm64\n                    kubernetes.io/hostname=k8s-master\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 17 Apr 2021 15:13:42 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-master\n  AcquireTime:     &lt;unset&gt;\n  RenewTime:       Sat, 17 Apr 2021 16:34:29 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 17 Apr 2021 16:34:09 +0000   Sat, 17 Apr 2021 15:13:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 17 Apr 2021 16:34:09 +0000   Sat, 17 Apr 2021 15:13:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 17 Apr 2021 16:34:09 +0000   Sat, 17 Apr 2021 15:13:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 17 Apr 2021 16:34:09 +0000   Sat, 17 Apr 2021 15:13:52 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.10.50\n  Hostname:    k8s-master\nCapacity:\n  cpu:                4\n  ephemeral-storage:  30459624Ki\n  memory:             1892528Ki\n  pods:               110\nAllocatable:\n  cpu:                3400m\n  ephemeral-storage:  28071589432\n  memory:             1380528Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 58f6de70444c4198b56b30122b6c77dc\n  System UUID:                58f6de70444c4198b56b30122b6c77dc\n  Boot ID:                    79af3428-cf70-4189-a447-0b917a035a42\n  Kernel Version:             5.4.0-1032-raspi\n  OS Image:                   Ubuntu 20.04.2 LTS\n  Operating System:           linux\n  Architecture:               arm64\n  Container Runtime Version:  cri-o://1.20.2\n  Kubelet Version:            v1.20.1\n  Kube-Proxy Version:         v1.20.1\nPodCIDR:                      10.200.0.0/24\nPodCIDRs:                     10.200.0.0/24\nNon-terminated Pods:          (4 in total)\n  Namespace                   Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                  ------------  ----------  ---------------  -------------  ---\n  kube-system                 etcd-k8s-master                       500m (14%)    1 (29%)     256Mi (18%)      384Mi (28%)    78m\n  kube-system                 kube-apiserver-k8s-master             500m (14%)    1 (29%)     256Mi (18%)      384Mi (28%)    78m\n  kube-system                 kube-controller-manager-k8s-master    100m (2%)     300m (8%)   128Mi (9%)       256Mi (18%)    76m\n  kube-system                 kube-scheduler-k8s-master             100m (2%)     300m (8%)   128Mi (9%)       256Mi (18%)    75m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1200m (35%)  2600m (76%)\n  memory             768Mi (56%)  1280Mi (94%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:              &lt;none&gt;\n</code></pre> </p>"},{"location":"setup/06_master/07_controller_health_check/#health-checks","title":"health checks","text":"<p>kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067 <code>--anonymous-auth=false</code> \u3092\u4ed8\u52a0\u3057\u3066\u3044\u308b\u305f\u3081 <code>https://localhost:6443</code> \u3078\u306eanonymous\u30a2\u30ab\u30a6\u30f3\u30c8\u3067\u306e\u78ba\u8a8d\u306f\u884c\u308f\u305a\u306b <code>kubectl</code> \u3067\u78ba\u8a8d\u3059\u308b</p> <ol> <li> <p>API endpoints for health     <pre><code>kubectl get --raw='/readyz?verbose'\n</code></pre> \u5b9f\u884c\u4f8b <pre><code>$ kubectl get --raw='/readyz?verbose'\n[+]ping ok\n[+]log ok\n[+]etcd ok\n[+]informer-sync ok\n[+]poststarthook/start-kube-apiserver-admission-initializer ok\n[+]poststarthook/generic-apiserver-start-informers ok\n[+]poststarthook/max-in-flight-filter ok\n[+]poststarthook/start-apiextensions-informers ok\n[+]poststarthook/start-apiextensions-controllers ok\n[+]poststarthook/crd-informer-synced ok\n[+]poststarthook/bootstrap-controller ok\n[+]poststarthook/rbac/bootstrap-roles ok\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\n[+]poststarthook/priority-and-fairness-config-producer ok\n[+]poststarthook/start-cluster-authentication-info-controller ok\n[+]poststarthook/start-kube-aggregator-informers ok\n[+]poststarthook/apiservice-registration-controller ok\n[+]poststarthook/apiservice-status-available-controller ok\n[+]poststarthook/kube-apiserver-autoregistration ok\n[+]autoregister-completion ok\n[+]poststarthook/apiservice-openapi-controller ok\n[+]shutdown ok\nreadyz check passed\n</code></pre> </p> </li> <li> <p>Individual health checks     <pre><code>kubectl get --raw='/livez/etcd'\n</code></pre> \u5b9f\u884c\u4f8b <pre><code>$ kubectl get --raw='/livez/etcd'\nok\n\n$ kubectl get --raw='/livez/poststarthook/start-apiextensions-controllers'\nok\n</code></pre> </p> </li> </ol>"},{"location":"setup/06_master/07_controller_health_check/#_3","title":"\u53c2\u8003\u8cc7\u6599","text":"<ul> <li>https://kubernetes.io/docs/reference/using-api/health-checks/</li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/","title":"bootstrapping kubelet(master/worker \u5171\u901a)","text":"<ul> <li><code>kubelet</code> \u3092host\u4e0a\u306esystemd service\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u3002</li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#worker-node","title":"worker node\u306e\u30ea\u30bd\u30fc\u30b9\u914d\u5206","text":"<ul> <li>Reserve Compute Resources for System Daemons<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/</li> <li> <p><code>Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9</code> = <code>Node resource - system-reserved - kube-reserved - eviction-threshold</code> \u3089\u3057\u3044</p> name description default SystemReserved OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil KubeReserved k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b nil EvictionHard \u30e1\u30e2\u30ea\u30fc\u306e\u53ef\u7528\u6027\u304c\u95be\u5024\u3092\u8d85\u3048\u305f\u5834\u5408\u30b7\u30b9\u30c6\u30e0\u304cOOM\u306e\u72b6\u614b\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u306bOut Of Resource Handling(\u30ea\u30bd\u30fc\u30b9\u4e0d\u8db3\u306e\u51e6\u7406)\u3092\u5b9f\u65bd\u3057\u307e\u3059 100Mi </li> </ul> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_1","title":"\u624b\u9806","text":""},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubelet","title":"<code>kubelet</code> \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9","text":"<pre><code>VERSION=\"v1.30.1\"\nARCH=\"arm64\"\n\nsudo wget -P /usr/bin/ https://dl.k8s.io/${VERSION}/bin/linux/${ARCH}/kubelet\nsudo chmod +x /usr/bin/kubelet\n</code></pre>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeconfig","title":"kubeconfig \u3068 \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8\u3092\u914d\u7f6e","text":"<pre><code># host=\"k8s-node2\"\n# host=\"k8s-node1\"\nhost=\"k8s-master\"\n\nsudo install -o root -g root -m 755 -d /etc/kubelet.d\nsudo install -o root -g root -m 755 -d /var/lib/kubernetes\nsudo install -o root -g root -m 755 -d /var/lib/kubelet\nsudo cp ca.pem /var/lib/kubernetes/\nsudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/\nsudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig\n</code></pre>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#varlibkubeletkubelet-configyaml","title":"<code>/var/lib/kubelet/kubelet-config.yaml</code> \u3092\u4f5c\u6210\u3059\u308b","text":"<ul> <li><code>clusterDNS</code> \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b</li> <li><code>podCIDR</code> \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b   <pre><code># host=\"k8s-node2\"\n# host=\"k8s-node1\"\nhost=\"k8s-master\"\n\ncat &lt;&lt; EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml\n---\nkind: KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\n\n# https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/\nstaticPodPath: /etc/kubelet.d\n\n# kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f\n#   - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b\n#   - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981\nauthentication:\n  anonymous:\n    enabled: true\n  webhook:\n    enabled: false\n    cacheTTL: \"2m\"\n  x509:\n    clientCAFile: \"/var/lib/kubernetes/ca.pem\"\n\n# kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a\n#   - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow\n#   - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981\nauthorization:\n  mode: AlwaysAllow\n\nclusterDomain: \"cluster.local\"\nclusterDNS:\n  - \"10.32.0.10\"\nresolvConf: \"/run/systemd/resolve/resolv.conf\"\npodCIDR: \"10.200.0.0/24\"\nruntimeRequestTimeout: \"15m\"\ntlsCertFile: \"/var/lib/kubelet/${host}.pem\"\ntlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\"\n\n# Reserve Compute Resources for System Daemons\n# https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/\n#\n# Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044\n#\n# system-reserved\n#   - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b\n#\n# kube-reserved\n#   - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b\nenforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"]\ncgroupsPerQOS: true\ncgroupDriver: systemd\ncgroupRoot: /\nsystemCgroups: /systemd/system.slice\nsystemReservedCgroup: /system.slice\nsystemReserved:\n  cpu: 256m\n  memory: 256Mi\nruntimeCgroups: /kube.slice/containerd.service\nkubeletCgroups: /kube.slice/kubelet.service\nkubeReservedCgroup: /kube.slice\nkubeReserved:\n  cpu: 1024m\n  memory: 1024Mi\nEOF\n</code></pre></li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#etcsystemdsystemkubeletservice","title":"<code>/etc/systemd/system/kubelet.service</code> \u3092\u914d\u7f6e","text":"<pre><code>cat &lt;&lt; 'EOF' | sudo tee /etc/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=containerd.service\nRequires=containerd.service\n\n[Service]\nRestart=on-failure\nRestartSec=5\n\nExecStartPre=/usr/bin/mkdir -p \\\n  /sys/fs/cgroup/kube.slice \\\n  /sys/fs/cgroup/system.slice \\\n  /sys/fs/cgroup/systemd/kube.slice \\\n  /sys/fs/cgroup/cpuset/kube.slice \\\n  /sys/fs/cgroup/cpuset/system.slice \\\n  /sys/fs/cgroup/pids/kube.slice \\\n  /sys/fs/cgroup/pids/system.slice \\\n  /sys/fs/cgroup/memory/kube.slice \\\n  /sys/fs/cgroup/memory/system.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/system.slice \\\n  /sys/fs/cgroup/hugetlb/system.slice \\\n  /sys/fs/cgroup/hugetlb/kube.slice\n\nExecStart=/usr/bin/kubelet \\\n  --config=/var/lib/kubelet/kubelet-config.yaml \\\n  --kubeconfig=/var/lib/kubelet/kubeconfig \\\n  --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\\n  --register-node=true \\\n  --v=2\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeletservice","title":"<code>kubelet.service</code> \u3092\u8d77\u52d5","text":"<pre><code>sudo systemctl enable kubelet.service\nsudo systemctl start kubelet.service\n</code></pre>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_2","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":""},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cgroup","title":"cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408","text":"<pre><code>kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist\n</code></pre> <ul> <li> <p><code>kubelet</code> \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( <code>--v 10</code> )</p> <pre><code>cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice]\n</code></pre> </li> <li> <p>\u5bfe\u5fdc     <code>kubelet.service</code> \u306e <code>ExecStartPre</code> \u3067mkdir\u3092\u5b9f\u884c\u3059\u308b <pre><code>ExecStartPre=/usr/bin/mkdir -p \\\n  /sys/fs/cgroup/systemd/kube.slice \\\n  /sys/fs/cgroup/cpuset/kube.slice \\\n  /sys/fs/cgroup/cpuset/system.slice \\\n  /sys/fs/cgroup/pids/kube.slice \\\n  /sys/fs/cgroup/pids/system.slice \\\n  /sys/fs/cgroup/memory/kube.slice \\\n  /sys/fs/cgroup/memory/system.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice \\\n  /sys/fs/cgroup/cpu,cpuacct/kube.slice\n</code></pre>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cgroupsystemreserved-memory-size","title":"cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f","text":"<ul> <li>\u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f    <pre><code>kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy\n</code></pre></li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubeconfig-cn-node","title":"kubeconfig \u306e\u8a3c\u660e\u66f8\u306e <code>CN</code> \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b","text":"<pre><code>360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\"\n</code></pre> <ul> <li>kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b       <pre><code>sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject:\n</code></pre><ul> <li><code>k8s-master</code> \u304c\u6b63\u3057\u3044\u306e\u306b <code>CN = system:node:k8s-node1</code> \u3068\u306a\u3063\u3066\u3044\u305f    <pre><code>root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject:\n    Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#node-specpodcidr-cidr","title":"<code>Node</code> \u30ea\u30bd\u30fc\u30b9\u306e <code>spec.podCIDR</code> \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044","text":"<ul> <li> <p>\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044</p> <ul> <li>flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f...  <pre><code>kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'\n</code></pre></li> </ul> </li> <li> <p><code>kube-controller-manager</code> \u306e\u30ed\u30b0</p> <ul> <li><code>Set node k8s-node1 PodCIDR to [10.200.0.0/24]</code> \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8<ul> <li><code>kube-controller-manager</code> \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b <code>--allocate-node-cidrs=true</code> \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... <pre><code>actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist\nrange_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24]\nttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\"\ncontroller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}]\ncontroller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes\nnode_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\"\ncontroller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1\nnode_lifecycle_controller.go:1429] Initializing eviction metric for zone:\nnode_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp.\nevent.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\"\nnode_lifecycle_controller.go:1245] Controller detected that zone  is now in state Normal.\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#webhook-authentication","title":"Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044","text":"<pre><code>I0214 07:03:56.822586       1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\"\nF0214 07:03:56.822637       1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication\ngoroutine 1 [running]:\n</code></pre> <ul> <li>https://kubernetes.io/docs/reference/access-authn-authz/webhook/</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication</li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cni-plugin-etccninetd-cni-plugin","title":"CNI Plugin\u3092 <code>/etc/cni/net.d</code> \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044","text":"<pre><code>kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\ncni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d\nkubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\n</code></pre> <ul> <li>CNI Plugin\u3092 <code>/etc/cni/net.d</code> \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b<ul> <li>https://github.com/containernetworking/plugins/releases</li> </ul> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#kubelet-cannot-determine-cpu-online-state","title":"Kubelet cannot determine CPU online state","text":"<pre><code>sysinfo.go:203] Nodes topology is not available, providing CPU topology\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory\nsysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory\ngce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping\nmachine.go:72] Cannot read number of physical cores correctly, number of cores set to 0\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping\nmachine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping\nmachine.go:86] Cannot read number of sockets correctly, number of sockets set to 0\ncontainer_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name:\n</code></pre> <ul> <li>\u65e2\u77e5\u3089\u3057\u3044<ul> <li>https://github.com/kubernetes/kubernetes/issues/95039</li> </ul> </li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cni-plugin-not-initialized","title":"cni plugin not initialized","text":"<ul> <li><code>/opt/cni/bin</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f <code>/etc/cni/net.d</code> \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b     <pre><code>\"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\n</code></pre></li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#cni-config-uninitialized","title":"cni config uninitialized","text":"<ul> <li><code>/opt/cni/bin</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306eCNI Plugin\u3082\u3057\u304f\u306f <code>/etc/cni/net.d</code> \u4ee5\u4e0b\u306eCNI Config\u306b\u8a2d\u5b9a\u4e0d\u5099\u304c\u3042\u308b\u53ef\u80fd\u6027\u304c\u8003\u3048\u3089\u308c\u308b     <pre><code>\"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni config uninitialized\"\n</code></pre></li> </ul>"},{"location":"setup/07_worker/01_bootstrapping_kubelet/#_3","title":"\u53c2\u8003","text":"<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/</li> <li>https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</li> <li>https://cyberagent.ai/blog/tech/4036/<ul> <li>kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b</li> </ul> </li> <li>https://downloadkubernetes.com/</li> <li> <p>https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/</p> </li> <li> <p>Node Authorization</p> <ul> <li>https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/node/</li> <li>https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/</li> </ul> </li> <li> <p>static pod</p> <ul> <li>https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/</li> <li>https://kubernetes.io/docs/concepts/policy/pod-security-policy/</li> <li>https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver</li> <li><code>PodSecurityPolicy</code> \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf(<code>false</code> \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f <code>true</code> \u306b\u76f4\u3059)</li> <li>https://github.com/kubernetes/kubernetes/issues/70952</li> </ul> </li> </ul>"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/","title":"bootstrapping kube-proxy","text":""},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/#kube-proxy","title":"kube-proxy \u3068\u306f","text":"<p>kube-proxy \u3068\u306f\u5404worker node\u3067\u52d5\u4f5c\u3059\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30d7\u30ed\u30ad\u30b7\u3092\u5b9f\u73fe\u3059\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3067\u3059\u3002\u5177\u4f53\u7684\u306b\u306fSertvice\u30ea\u30bd\u30fc\u30b9\u3067\u4f5c\u6210\u3055\u308c\u308bCluster IP\u3084Node Port\u306e\u7ba1\u7406\u3068\u305d\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30c6\u30fc\u30d6\u30eb\u306e\u7ba1\u7406\u3001\u307e\u305fnginx ingress controller\u3092\u5229\u7528\u3057\u305fIngress\u30ea\u30bd\u30fc\u30b9\u3067\u306fPod\u3078\u306e\u8ca0\u8377\u5206\u6563\u306bkube-proxy\u3092\u6d3b\u7528\u6307\u5b9a\u305f\u308a\u3059\u308b\u305d\u3046\u3067\u3059(<code>With NGINX, we\u2019ll use the DNS name or virtual IP address to identify the service, and rely on kube-proxy to perform the internal load-balancing across the pool of pods.</code>)</p>"},{"location":"setup/07_worker/02_bootstrapping_kube-proxy/#_1","title":"\u624b\u9806","text":"<ol> <li> <p><code>Dockerfile_kube-proxy.armhf</code> \u3092\u4f5c\u6210\u3059\u308b  Dockerfile_kube-proxy.armhf <pre><code>cat &lt;&lt; 'EOF' &gt; Dockerfile_kube-proxy.armhf\nFROM arm64v8/ubuntu:bionic\n\nARG VERSION=\"v1.30.1\"\nARG ARCH=\"arm64\"\n\nRUN set -ex \\\n  &amp;&amp; apt update \\\n  &amp;&amp; apt install -y wget \\\n  &amp;&amp; apt clean \\\n  &amp;&amp; wget -P /usr/bin/ https://dl.k8s.io/$VERSION/bin/linux/$ARCH/kube-proxy \\\n  &amp;&amp; chmod +x /usr/bin/kube-proxy \\\n  &amp;&amp; install -o root -g root -m 755 -d /var/lib/kube-proxy \\\n  &amp;&amp; install -o root -g root -m 755 -d /etc/kubernetes/config\n\nCOPY kube-proxy.kubeconfig /var/lib//kube-proxy/kubeconfig\n\nENTRYPOINT [\"/usr/bin/kube-proxy\"]\nEOF\n</code></pre> </p> </li> <li> <p>image build    <pre><code>sudo nerdctl build --namespace k8s.io -f Dockerfile_kube-proxy.armhf -t k8s-kube-proxy ./\n</code></pre></p> </li> <li> <p>kernel parameter    <pre><code>cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/kubelet.conf\n# kube-proxy\nnet.ipv4.conf.all.route_localnet = 1\nnet.netfilter.nf_conntrack_max = 131072\nnet.netfilter.nf_conntrack_tcp_timeout_established = 86400\nnet.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600\nEOF\n\nsudo sysctl --system\n\ncat &lt;&lt;EOF | sudo tee /etc/modprobe.d/kube-proxy.conf\noptions nf_conntrack hashsize=32768\nEOF\n\nsudo /sbin/modprobe nf_conntrack hashsize=32768\n</code></pre></p> </li> <li> <p>pod manifests\u3092 <code>/etc/kubernetes/manifests/</code> \u3078\u4f5c\u6210\u3059\u308b  /etc/kubernetes/manifests/kube-proxy.yaml <pre><code>cluster_cidr=\"10.200.0.0/16\"\nsudo mkdir -p /etc/kubernetes/manifests\n\ncat &lt;&lt; EOF | sudo tee /etc/kubernetes/manifests/kube-proxy.yaml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app: kube-proxy\n  name: kube-proxy-configuration\n  namespace: kube-system\ndata:\n  config.conf: |-\n    ---\n    apiVersion: kubeproxy.config.k8s.io/v1alpha1\n    kind: KubeProxyConfiguration\n    clientConnection:\n      kubeconfig: \"/var/lib/kube-proxy/kubeconfig\"\n    mode: \"iptables\"\n    clusterCIDR: \"${cluster_cidr}\"\n\n    # https://kubernetes.io/docs/reference/config-api/kube-proxy-config.v1alpha1/\n    # metricsBindAddress: 127.0.0.1:10249\n    metricsBindAddress: 0.0.0.0:10249\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\n  labels:\n    component: kube-proxy\n    # TODO\n    # master node\u306baddon-manager\u3092\u5c0e\u5165\u3057\u305f\u3089\u30b3\u30e1\u30f3\u30c8\u5916\u3059\n    # addonmanager.kubernetes.io/mode=Reconcile\nspec:\n  selector:\n    matchLabels:\n      name: kube-proxy\n  # https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#performing-a-rolling-update\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    # template \u4ee5\u4e0b\u306fpod templates\n    #   (apiVersion\u3084kind\u3092\u3082\u305f\u306a\u3044\u3053\u3068\u3092\u9664\u3044\u3066\u306f\u3001Pod\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3068\u540c\u3058\u30b9\u30ad\u30fc\u30de)\n    #   https://kubernetes.io/ja/docs/concepts/workloads/controllers/daemonset/\n    metadata:\n      labels:\n        name: kube-proxy\n    spec:\n      # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n      priorityClassName: system-node-critical\n      hostNetwork: true\n      containers:\n        - name: kube-proxy\n          image: k8s-kube-proxy:latest\n          securityContext:\n            capabilities:\n              add:\n                - SYS_ADMIN\n                - NET_ADMIN\n                - NET_RAW\n          command:\n            - /usr/bin/kube-proxy\n            - --config=/var/lib/kube-proxy/kube-proxy-config.yaml\n          imagePullPolicy: IfNotPresent\n          resources:\n            requests:\n              cpu: \"256m\"\n          volumeMounts:\n          - name: kube-proxy-configuration\n            mountPath: /var/lib/kube-proxy/kube-proxy-config.yaml\n          - name: conntrack-command\n            mountPath: /usr/sbin/conntrack\n          - name: iptables-command\n            mountPath: /usr/sbin/iptables\n          - name: iptables-restore-command\n            mountPath: /usr/sbin/iptables-restore\n          - name: iptables-save-command\n            mountPath: /usr/sbin/iptables-save\n          - name: xtables-lock-file\n            mountPath: /run/xtables.lock\n          - name: usr-lib-dir\n            mountPath: /usr/lib\n          - name: lib-dir\n            mountPath: /lib\n          - name: sys-dir\n            mountPath: /sys\n      volumes:\n      - name: kube-proxy-configuration\n        configMap:\n          name: kube-proxy-configuration\n      - name: conntrack-command\n        hostPath:\n          path: /usr/sbin/conntrack\n      - name: iptables-command\n        hostPath:\n          path: /usr/sbin/iptables\n      - name: iptables-restore-command\n        hostPath:\n          path: /usr/sbin/iptables-restore\n      - name: iptables-save-command\n        hostPath:\n          path: /usr/sbin/iptables-save\n      - name: xtables-lock-file\n        hostPath:\n          path: /run/xtables.lock\n      - name: usr-lib-dir\n        hostPath:\n          path: /usr/lib\n      - name: lib-dir\n        hostPath:\n          path: /lib\n      - name: sys-dir\n        hostPath:\n          path: /sys\nEOF\n</code></pre> </p> </li> <li> <p>pod\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b    <pre><code>kubectl apply -f /etc/kubernetes/manifests/kube-proxy.yaml\n</code></pre></p> </li> </ol>"},{"location":"setup/08_flannel/bootstrapping_flannel/","title":"bootstrapping flannel","text":""},{"location":"setup/08_flannel/bootstrapping_flannel/#_1","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://github.com/flannel-io/flannel/</li> <li>https://github.com/flannel-io/flannel/blob/master/Documentation/troubleshooting.md</li> <li><code>/opt/bin/flanneld</code> \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3</li> <li>https://github.com/flannel-io/flannel/blob/master/main.go#L110-L132</li> </ul>"},{"location":"setup/08_flannel/bootstrapping_flannel/#_2","title":"\u624b\u9806","text":"<ol> <li> <p>vxlan module\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b     <pre><code>sudo apt install -y linux-modules-extra-raspi locate\nsudo updatedb\nsudo modprobe vxlan\nsudo lsmod | grep vxlan\n</code></pre></p> </li> <li> <p>flannel k8s manifests\u3092\u516c\u5f0f\u304b\u3089\u53d6\u5f97\u3059\u308b</p> </li> <li> <p>master\u30d6\u30e9\u30f3\u30c1\u304b\u3089\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u304c2021/03/06 \u6642\u70b9\u3067\u306f release tag <code>v0.13.1-rc2</code> \u306e\u5185\u5bb9</p> <pre><code>sudo mkdir -p /etc/kubernetes/manifests\nsudo curl -o /etc/kubernetes/manifests/kube-flannel.yml -sSL https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml\n</code></pre> </li> <li> <p>manifests\u3092\u4fee\u6b63\u3059\u308b</p> <ul> <li><code>net-conf.json</code> \u306e <code>Network</code> \u3092controller-manager\u306e<code>--cluster-cidr</code>\u3067\u6307\u5b9a\u3057\u305f\u5024\u306b\u5909\u66f4\u3059\u308b</li> <li>etcd\u306b\u7528\u3044\u305fcertificate\u3084admin\u306ekubeconfig\u3092kube-flannel\u30b3\u30f3\u30c6\u30ca\u3078bind mount\u3059\u308b</li> </ul> <pre><code>        volumeMounts:\n        - name: var-lib-kubernetes-dir\n          mountPath: /var/lib/kubernetes/\n\n      volumes:\n      - name: var-lib-kubernetes-dir\n        hostPath:\n          path: /var/lib/kubernetes\n</code></pre> <ul> <li><code>/opt/bin/flanneld</code> \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3<ul> <li><code>--kubeconfig-file</code></li> <li><code>--etcd-endpoints</code></li> <li><code>--etcd-prefix</code></li> <li><code>--etcd-keyfile</code></li> <li><code>--etcd-certfile</code></li> <li><code>--etcd-cafile</code></li> <li> <p><code>--v</code></p> <p><pre><code>sudo vim /etc/kubernetes/manifests/kube-flannel.yml\n</code></pre> \u4fee\u6b63\u5f8c\u306e<code>/etc/kubernetes/manifests/kube-flannel.yml</code> <pre><code>---\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: psp.flannel.unprivileged\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default\n    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default\n    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default\nspec:\n  privileged: false\n  volumes:\n  - configMap\n  - secret\n  - emptyDir\n  - hostPath\n  allowedHostPaths:\n  - pathPrefix: \"/etc/cni/net.d\"\n  - pathPrefix: \"/etc/kube-flannel\"\n  - pathPrefix: \"/run/flannel\"\n  readOnlyRootFilesystem: false\n  # Users and groups\n  runAsUser:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n  # Privilege Escalation\n  allowPrivilegeEscalation: false\n  defaultAllowPrivilegeEscalation: false\n  # Capabilities\n  allowedCapabilities: ['NET_ADMIN', 'NET_RAW']\n  defaultAddCapabilities: []\n  required\n  seLinux:\n    # SELinux is unused in CaaSP\n    rule: 'RunAsAny'\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n- apiGroups: ['extensions']\n  resources: ['podsecuritypolicies']\n  verbs: ['use']\n  resourceNames: ['psp.flannel.unprivileged']\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  verbs:\n  - get\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes/status\n  verbs:\n  - patch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      \"name\": \"cbr0\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"flannel\",\n          \"delegate\": {\n            \"hairpinMode\": true,\n            \"isDefaultGateway\": true\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"capabilities\": {\n            \"portMappings\": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"10.200.0.0/16\",\n      \"Backend\": {\n        \"Type\": \"vxlan\"\n      }\n    }\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/os\n                operator: In\n                values:\n                - linux\n      hostNetwork: true\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni-plugin\n       #image: flannelcni/flannel-cni-plugin:v1.0.1 for ppc64le and mips64le (dockerhub limitations may apply)\n        image: rancher/mirrored-flannelcni-flannel-cni-plugin:v1.0.1\n        command:\n        - cp\n        args:\n        - -f\n        - /flannel\n        - /opt/cni/bin/flannel\n        volumeMounts:\n        - name: cni-plugin\n          mountPath: /opt/cni/bin\n      - name: install-cni\n       #image: flannelcni/flannel:v0.16.3 for ppc64le and mips64le (dockerhub limitations may apply)\n        image: rancher/mirrored-flannelcni-flannel:v0.16.3\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n       #image: flannelcni/flannel:v0.16.3 for ppc64le and mips64le (dockerhub limitations may apply)\n        image: rancher/mirrored-flannelcni-flannel:v0.16.3\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        - --kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig\n        - --etcd-endpoints=https://k8s-master:4001\n        - --etcd-prefix=/coreos.com/network\n        - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem\n        - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem\n        - --etcd-cafile=/var/lib/kubernetes/ca.pem\n        - --v=10\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: false\n          capabilities:\n            add: [\"NET_ADMIN\", \"NET_RAW\"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run/flannel\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: var-lib-kubernetes-dir\n          mountPath: /var/lib/kubernetes/\n      volumes:\n      - name: run\n        hostPath:\n          path: /run/flannel\n      - name: cni-plugin\n        hostPath:\n          path: /opt/cni/bin\n      - name: cni\n        hostPath:\n          path: /etc/cni/net.d\n      - name: flannel-cfg\n        configMap:\n          name: kube-flannel-cfg\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: var-lib-kubernetes-dir\n        hostPath:\n          path: /var/lib/kubernetes\n</code></pre> <li> <p>flannel Pod\u3092\u30c7\u30d7\u30ed\u30a4</p> </li> <pre><code>kubectl apply -f /etc/kubernetes/manifests/kube-flannel.yml\n</code></pre>"},{"location":"setup/08_flannel/bootstrapping_flannel/#_3","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":"<ol> <li> <p>flannel\u304c\u53c2\u7167\u3059\u308bkubeconfig\u304c\u6b63\u3057\u304f\u306a\u3044</p> <ul> <li>flannel-io/flannel Documentation/kube-flannel.yml \u305d\u306e\u307e\u307e\u3060\u3068\u767a\u751f\u3057\u305f     <pre><code>Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\nerror creating inClusterConfig, falling back to default config: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory\nFailed to create SubnetManager: fail to create kubernetes config: invalid configuration: no configuration has been provided, try setting KUBERNETES_MASTER environment variable\n</code></pre></li> </ul> </li> <li> <p><code>Failed to create SubnetManager: fail to create kubernetes config: stat \"/var/lib/kubernetes/admin.kubeconfig\": no such file or directory</code></p> <ul> <li><code>--kubeconfig-file</code> \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u305fkubeconfig path\u304c\u6b63\u3057\u304f\u306a\u3044<ul> <li>\u79c1\u306e\u30b1\u30fc\u30b9\u3067\u306f <code>--kubeconfig-file=\"/var/lib/kubernetes/admin.kubeconfig\"</code> \u3068\u3057\u3066\u3044\u308b\u3068\u30c0\u30d6\u30eb\u30af\u30a9\u30fc\u30c8(\") \u304c\u30d1\u30b9\u306b\u542b\u307e\u308c\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6c17\u4ed8\u304f\u306e\u306b\u6642\u9593\u304b\u304b\u308a\u307e\u3057\u305f(\u6b63\u3057\u304f\u306f <code>--kubeconfig-file=/var/lib/kubernetes/admin.kubeconfig</code>)</li> </ul> </li> </ul> </li> <li> <p><code>Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned</code></p> <ul> <li>Node\u30ea\u30bd\u30fc\u30b9\u306e <code>.spec.podCIDR</code> \u304c\u767b\u9332\u3055\u308c\u3066\u3044\u306a\u3044</li> <li>\u78ba\u8a8d\u65b9\u6cd5     <pre><code>kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'\n</code></pre></li> <li> <p><code>kube-controller-manager</code> \u306e\u8a2d\u5b9a\u4e0d\u5099\u306e\u53ef\u80fd\u6027</p> <ul> <li>\u4ee5\u4e0b\u8a2d\u5b9a\u5909\u66f4\u5f8c\u3001kubelet\u306e\u8d77\u52d5\u524d\u306b <code>kubectl delete node &lt;NODE&gt;</code> \u3092\u5b9f\u884c\u3059\u308b<ul> <li>https://github.com/flannel-io/flannel/issues/728#issuecomment-325347810<ul> <li>podCIDR\u5272\u308a\u5f53\u3066\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044<ul> <li><code>--cluster-cidr=&lt;CIDR&gt;</code></li> <li><code>--allocate-node-cidrs=true</code></li> </ul> </li> </ul> </li> <li>https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/<ul> <li><code>--service-cluster-ip-range=&lt;CIDR&gt;</code> \u3068 (flannnel\u306e)<code>--pod-cidr=&lt;CIDR&gt;</code> \u304c\u88ab\u3063\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>(kube-controller-manager \u304c\u6b63\u3057\u3044\u5834\u5408) Node\u30ea\u30bd\u30fc\u30b9\u306e <code>.spec.podCIDR</code> \u3092\u624b\u52d5\u8a2d\u5b9a\u3057\u3066\u56de\u907f\u3059\u308b     <pre><code>kubectl patch node &lt;NODE_NAME&gt; -p '{\"spec\":{\"podCIDR\":\"&lt;SUBNET&gt;\"}}'\n</code></pre></p> </li> </ul> </li> <li> <p><code>Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again.</code></p> <ul> <li><code>kubectl delete pod kube-flannel-...</code> \u3067 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9 <code>flannel.1</code> \u304c\u958b\u653e\u3055\u308c\u306a\u3044<ul> <li>https://github.com/flannel-io/flannel/issues/1060</li> <li>\u53e4\u3044 <code>flannel.1</code> \u3092\u524a\u9664     <pre><code>ip a\nsudo ip link delete flannel.1\n</code></pre></li> </ul> </li> </ul> </li> </ol>"},{"location":"setup/09_coredns/bootstrapping_coredns/","title":"bootstrapping CoreDNS","text":""},{"location":"setup/09_coredns/bootstrapping_coredns/#coredns","title":"CoreDNS \u3068\u306f","text":"<p>CoreDNS \u306fCNCF\u306egraduated project \u3068\u3057\u3066\u30db\u30b9\u30c8\u3055\u308c\u3066\u3044\u308bDNS\u30b5\u30fc\u30d0\u3067\u3001Kubernetes 1.13 \u4ee5\u964d\u306b\u3066\u30c7\u30d5\u30a9\u30eb\u30c8DNS\u30b5\u30fc\u30d0\u3068\u3057\u3066\u63a1\u7528 \u3055\u308c\u3066\u304a\u308a\u3001Cluster\u5185\u3067\u306eService\u30ea\u30bd\u30fc\u30b9\u306e\u540d\u524d\u89e3\u6c7a\u306b\u5229\u7528\u3057\u3066\u3044\u307e\u3059 (Kubernetes DNS-Based Service Discovery)\u3002AWS Load Balancer Controller\u306a\u3069Cluster\u5916\u3078\u30a8\u30f3\u30c9\u30dd\u30a4\u30f3\u30c8\u3092\u516c\u958b\u3059\u308b\u5834\u5408\u306f\u5225\u9014\u5916\u90e8DNS\u30b5\u30fc\u30d0(Route53\u306a\u3069)\u3092\u5229\u7528\u3057\u307e\u3059(CoreDNS\u3092Cluster\u5916\u90e8\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd)\u3002</p> <p>CoreDNS\u306f\u3042\u3089\u3086\u308b\u51e6\u7406\u3092Plugin\u3068\u3057\u3066\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002CoreDNS\u5358\u4f53\u306fDNS\u30af\u30a8\u30ea\u30fc\u3092\u89e3\u91c8\u3057\u3066\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb(<code>./Corefile</code>)\u306b\u8a18\u8ff0\u3055\u308c\u305fPlugin\u306b\u51e6\u7406\u3092\u53d7\u3051\u6e21\u3057\u307e\u3059\u3002(\u53c2\u8003)</p> <p>kubernetes plugin \u306fKubernetes DNS-Based Service Discovery Specification.\u306e\u5b9f\u88c5\u3067\u3059\u3002 <code>Corefile</code> \u3067kubernetes plugin\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u5229\u7528\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u8a2d\u5b9a\u306f kubernetes/coredns.yaml.sed \u306e\u5185\u5bb9\u3067\u3059\u3002<code>fallthrough</code> \u3068\u306fNXDOMAIN(\u4e0d\u5728\u5fdc\u7b54)\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408\u306b\u51e6\u7406\u3092\u4e0b\u6d41\u306ePlugin\u306b\u6e21\u3057\u3066\u304f\u308c\u307e\u3059(\u3053\u306e\u8a2d\u5b9a\u3067\u306f\u9006\u5f15\u304d\u3067NXDOMAIN\u304c\u8fd4\u3063\u3066\u304d\u305f\u5834\u5408)\u3002</p> <pre><code>.:53 {\n\n  kubernetes cluster.local in-addr.arpa ip6.arpa {\n    fallthrough in-addr.arpa ip6.arpa\n  }\n\n}\n</code></pre>"},{"location":"setup/09_coredns/bootstrapping_coredns/#_1","title":"\u53c2\u8003\u6587\u732e","text":"<ul> <li>https://coredns.io/</li> <li>https://github.com/coredns/coredns</li> <li>https://github.com/coredns/deployment</li> <li>https://github.com/coredns/helm</li> <li>https://engineer.retty.me/entry/2020/12/15/161544</li> <li>https://www.netone.co.jp/knowledge-center/netone-blog/20191226-1/</li> <li>https://www.scsk.jp/sp/sysdig/blog/container_monitoring/coredns.html</li> </ul>"},{"location":"setup/09_coredns/bootstrapping_coredns/#_2","title":"\u624b\u9806","text":"<p>sudo sed -i \"s/^nameserver\\s127.0.0.53$/nameserver ${NAMESERVER_ADDRESSES}/\" /run/systemd/resolve/stub-resolv.conf sudo systemctl restart systemd-resolved sudo systemctl restart kubelet sudo systemctl restart containerd</p> <ol> <li>coredns k8s manifests\u3092\u516c\u5f0f\u304b\u3089\u53d6\u5f97\u3059\u308b     <pre><code>git clone https://github.com/coredns/deployment.git coredns_deployment\ncd coredns_deployment/kubernetes/\n\nbash deploy.sh -i 10.32.0.10  -s -t coredns.yaml.sed | kubectl apply -f -\n</code></pre></li> </ol>"},{"location":"setup/09_coredns/bootstrapping_coredns/#coredns_1","title":"CoreDNS\u306e\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u53d6\u5f97\u306b\u3064\u3044\u3066","text":"<ul> <li><code>Corefile</code> \u3067\u4ee5\u4e0b\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u3066\u304a\u304f\u3053\u3068\u3067Prometheus\u7528\u306b <code>9153/TCP</code> \u3067\u30e1\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u516c\u958b\u3067\u304d\u307e\u3059    <pre><code>.:53 {\n\n  prometheus :9153\n\n}\n</code></pre></li> </ul>"},{"location":"setup/09_coredns/bootstrapping_coredns/#_3","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":"<ol> <li> <p><code>kubectl get pods -n kube-system</code> \u3067\u3044\u3064\u307e\u3067\u7d4c\u3063\u3066\u3082<code>ContainerCreating</code> \u306e\u307e\u307e</p> <ul> <li><code>kubectl describe pod -n kube-system &lt;POD_ID&gt;</code> <pre><code>Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"&lt;CONTAINER_ID&gt;\" network for pod \"&lt;POD_ID&gt;\": networkPlugin cni failed to set up pod \"&lt;&lt;POD_NAME&gt;\" network: failed to Statfs \"/proc/15875/ns/net\": no such file or directory, failed to clean up sandbox container \"&lt;CONTAINER_ID&gt;\" network for pod \"&lt;POD_ID&gt;\": networkPlugin cni failed to teardown pod \"&lt;POD_NAME&gt;\" network: neither iptables nor ip6tables usable]\n</code></pre></li> <li>controller-manager<ul> <li>https://github.com/kubernetes/kubernetes/blob/v1.20.2/pkg/controller/endpointslice/utils.go#L407-L415 <pre><code>couldn't find ipfamilies for headless service: kube-system/kube-dns. This could happen if controller manager is connected to an old apiserver that does not support ip families yet. EndpointSlices for this Service will use IPv4 as the IP Family based on familyOf(ClusterIP:10.32.0.10).\n</code></pre> <pre><code>$ kubectl get service -n kube-system -o jsonpath='{.items[*].spec.clusterIP}'\n10.32.0.10\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>cni0(flannel)\u306e\u8d77\u52d5\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b     <pre><code>failed to set bridge addr: \"cni0\" already has an IP address different from 10.200.1.1/24\n</code></pre></p> </li> <li> <p>\u540d\u524d\u89e3\u6c7a\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b</p> <ul> <li>https://coredns.io/plugins/loop/#troubleshooting <pre><code>[FATAL] plugin/loop: Loop (127.0.0.1:36286 -&gt; :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 1048258276942848743.906062863108256161.\"\n</code></pre></li> </ul> </li> <li> <p><code>i/o timeout</code></p> <pre><code>[ERROR] plugin/errors: 2 1233258971421873826.4416823678189275919. HINFO: read udp 10.200.0.5:35249-&gt;8.8.4.4:53: i/o timeout\n</code></pre> <ul> <li> <p>Pod\u306e\u30b3\u30f3\u30c6\u30ca\u304b\u3089\u540d\u524d\u89e3\u6c7a\u304c\u3067\u304d\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b</p> <ul> <li><code>/etc/resolv.conf</code> \u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u3044\u304b\u78ba\u8a8d\u3059\u308b</li> </ul> <pre><code>kubectl run nginx --image=nginx\nPOD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl exec -it $POD_NAME -- bash\n\ncat /etc/resolv.conf\n\napt-get update              # \u5916\u306b\u3059\u3089\u51fa\u308c\u306a\u3044\u5834\u5408\u306f\u5931\u6557\u3059\u308b\napt-get install dnsutils\nnslookup kubernetes\n</code></pre> </li> <li> <p>kube-apiserver \u3078\u306e\u758e\u901a\u304c\u3067\u304d\u3066\u3044\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b</p> <ul> <li>kube-proxy -&gt; flannel \u306e\u9806\u3067pod\u306e\u518d\u8d77\u52d5\u3092\u884c\u3063\u3066\u307f\u308b</li> </ul> </li> </ul> </li> </ol>"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/","title":"bootstrapping nginx ingress controller","text":""},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_1","title":"\u53c2\u8003","text":"<ul> <li>https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/</li> <li>https://kubernetes.github.io/ingress-nginx/</li> <li>https://github.com/nginxinc/kubernetes-ingress</li> </ul>"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_2","title":"\u624b\u9806","text":""},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_3","title":"\u69cb\u7bc9","text":"<ul> <li>https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal-clusters</li> </ul>"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_4","title":"\u52d5\u4f5c\u78ba\u8a8d","text":""},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#nginx-ingressclass","title":"nginx IngressClass \u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d","text":"<pre><code>$ kubectl describe ingressclasses nginx\nName:         nginx\nLabels:       app.kubernetes.io/component=controller\n              app.kubernetes.io/instance=ingress-nginx\n              app.kubernetes.io/name=ingress-nginx\n              app.kubernetes.io/part-of=ingress-nginx\n              app.kubernetes.io/version=1.3.0\nAnnotations:  &lt;none&gt;\nController:   k8s.io/ingress-nginx\nEvents:       &lt;none&gt;\n</code></pre>"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#ingressworker-nodeip","title":"Ingress\u3092\u4f5c\u6210\u3057\u3066worker node\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d","text":"<ol> <li> <p>manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210</p> <p>/etc/kubernetes/manifests/04_nginx_ingress_controller.yaml <pre><code>sudo tee /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml &lt;&lt; EOF &gt; /dev/null\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  type: NodePort\n  ports:\n    - name: node-port\n      protocol: TCP\n      port: 8080\n      targetPort: 80\n      nodePort: 30011\n  selector:\n    app: nginx\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx-test-ingress\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: &lt;Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN&gt;\nspec:\n  ingressClassName: nginx\n  defaultBackend:\n    service:\n      name: nginx-service\n      port:\n        number: 8080\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-service\n            port:\n              number: 8080\n  # external-dns.alpha.kubernetes.io/hostname annotations\u3092\u6307\u5b9a\u305b\u305arule\u6bce\u306b\u8a2d\u5b9a\u3059\u308b\u4f8b\n  # - host: &lt;Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN&gt;\n  #   http:\n  #     paths:\n  #       - path: /\n  #         pathType: Prefix\n  #         backend:\n  #           service:\n  #             name: nginx-service\n  #             port:\n  #               number: 8080\nEOF\n</code></pre> </p> </li> <li> <p><code>&lt;Route53\u306bA record\u3068\u3057\u3066\u767b\u9332\u3057\u305f\u3044FQDN&gt;</code> \u306e\u7b87\u6240\u3092\u4fee\u6b63\u3059\u308b     <pre><code>sudo vim /etc/kubernetes/manifests/04_nginx_ingress_controller.yaml\n</code></pre></p> </li> <li> <p>\u30ea\u30bd\u30fc\u30b9\u4f5c\u6210     <pre><code>kubectl apply -f /etc.kubernetes/manifests/04_nginx_ingress_controller.yaml\n</code></pre></p> </li> <li> <p>service\u3067node port(30011/TCP) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>$ kubectl describe services nginx-service\nName:                     nginx-service\nNamespace:                default\nLabels:                   &lt;none&gt;\nAnnotations:              &lt;none&gt;\nSelector:                 app=nginx\nType:                     NodePort\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.32.0.145\nIPs:                      10.32.0.145\nPort:                     node-port  8080/TCP\nTargetPort:               80/TCP\nNodePort:                 node-port  30011/TCP\nEndpoints:                10.200.1.184:80\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n</code></pre></p> </li> <li> <p>ingress\u3067worker node\u306eIP\u30a2\u30c9\u30ec\u30b9(wlan0: <code>192.168.10.51</code>) \u3067\u516c\u958b\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>$ kubectl describe ingresses nginx-test-ingress\nName:             nginx-test-ingress\nNamespace:        default\nAddress:          192.168.10.51\nDefault backend:  nginx-service:8080 (10.200.1.184:80)\nRules:\n  Host        Path  Backends\n  ----        ----  --------\n  *\n              /   nginx-service:8080 (10.200.1.184:80)\nAnnotations:  &lt;none&gt;\nEvents:\n  Type    Reason  Age                  From                      Message\n  ----    ------  ----                 ----                      -------\n  Normal  Sync    5m39s (x7 over 25h)  nginx-ingress-controller  Scheduled for sync\n</code></pre></p> </li> <li> <p>MacBookPro\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u3067curl\u306b\u3066\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>$ curl --include http://192.168.10.51:30011/\nHTTP/1.1 200 OK\nDate: Tue, 21 Sep 2021 16:35:07 GMT\nContent-Type: text/html\nContent-Length: 612\nConnection: keep-alive\nLast-Modified: Tue, 04 Dec 2018 14:44:49 GMT\nETag: \"5c0692e1-264\"\nAccept-Ranges: bytes\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> </li> </ol>"},{"location":"setup/10_nginx_ingress_controller/bootstrapping_nginx_ingress_controller/#_5","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b","text":"<ul> <li> <p><code>Error when getting IngressClass nginx: the server could not find the requested resource</code></p> <ul> <li>https://github.com/nginxinc/kubernetes-ingress/issues/1906</li> <li>https://qiita.com/smallpalace/items/7a6844651d1d7b43b411<ul> <li>https://github.com/kubernetes/ingress-nginx/issues/7448</li> <li>https://github.com/kubernetes/ingress-nginx/blob/3c0bfc1ca3eb48246b12e77d40bde1162633efae/deploy/static/provider/baremetal/deploy.yaml</li> </ul> </li> </ul> </li> <li> <p><code>error validating data: ValidationError</code></p> <ul> <li><code>--validate=false</code> \u3092\u4ed8\u52a0\u3059\u308b</li> <li>ValidatingWebhookConfiguration \u3092\u524a\u9664\u3059\u308b (issue comment)</li> </ul> </li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/","title":"bootstrapping external-dns","text":""},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_1","title":"\u53c2\u8003\u60c5\u5831","text":"<ul> <li>https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/integrations/external_dns/</li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#kubernetes-122","title":"kubernetes 1.22 \u5bfe\u5fdc\u72b6\u6cc1\u306b\u3064\u3044\u3066","text":"<ul> <li>v0.10.0 \u3067\u5bfe\u5fdc\u6e08\u307f</li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#aws-route53external-dns-controller","title":"\u30aa\u30f3\u30d7\u30ec\u3067AWS Route53\u5411\u3051external-dns controller\u306e\u8d77\u52d5\u65b9\u6cd5\u306b\u3064\u3044\u3066","text":"<ul> <li>https://stackoverflow.com/questions/60267737/is-it-possible-to-use-aws-route-53-as-a-dns-provider-for-a-bare-metal-k8s-cluste</li> <li>https://github.com/kubernetes-sigs/external-dns/issues/539</li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#wan-ipdns-provider","title":"\u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306eWAN IP\u30a2\u30c9\u30ec\u30b9\u3092DNS Provider\u306b\u901a\u77e5\u3057\u305f\u3044","text":"<ul> <li>https://github.com/kubernetes-sigs/external-dns/issues/1394</li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#public-iproute53","title":"\u30aa\u30f3\u30d7\u30ec\u306a\u3069\u81ea\u5b85\u74b0\u5883\u306b\u304a\u3051\u308bPublic IP\u3092\u6255\u3044\u51fa\u3057\u3064\u3064route53\u3078\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3059\u308b","text":"<ul> <li>k8s cluster\u5185\u90e8\u306eIP\u30a2\u30c9\u30ec\u30b9\u3067\u306f\u306a\u304f\u3001worker node\u306einterface(wlan0)\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u308bIP\u30a2\u30c9\u30ec\u30b9(\u4fbf\u5b9c\u7684\u306bpublic ip\u3068\u4eee\u5b9a)\u3068\u540c\u3058\u30ec\u30f3\u30b8\u3067IP\u30a2\u30c9\u30ec\u30b9\u3092\u6255\u3044\u51fa\u3059\u3088\u3046\u306a\u69cb\u6210<ul> <li>\u3053\u308c\u306f\u5c06\u6765\u7528</li> <li>metallb \u3067k8s cluster\u306e\u5916\u5074\u306bLoadBalancer\u3092\u4f5c\u6210\u3057Public IP\u30a2\u30c9\u30ec\u30b9\u3092\u5272\u308a\u5f53\u3066\u308b</li> <li>https://blog.web-apps.tech/type-loadbalancer_by_metallb/</li> </ul> </li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_2","title":"\u69cb\u7bc9\u624b\u9806","text":""},{"location":"setup/11_external_dns/bootstrapping_external_dns/#1-aws-iam-policy","title":"1. AWS IAM Policy\u4f5c\u6210","text":"<ol> <li> <p><code>external-dns-controller-policy-document.json</code> \u3092\u4f5c\u6210</p> <p>external-dns-controller-policy-document.json <pre><code>```\nsudo tee external-dns-controller-policy-document.json &lt;&lt; EOF &gt; /dev/null\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\nEOF\n```\n</code></pre> <li> <p>policy\u3092\u4f5c\u6210     <pre><code>aws iam create-policy --policy-name k8s-external-dns-policy  --policy-document file://external-dns-controller-policy-document.json\n</code></pre></p> </li>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#2-aws-iam-user","title":"2. AWS IAM User\u4f5c\u6210","text":"<ol> <li> <p>user\u3092\u4f5c\u6210     <pre><code>aws iam create-user --user-name k8s-external-dns\n</code></pre></p> </li> <li> <p>\u4f5c\u6210\u3057\u305fIAM Policy\u3092\u30a2\u30bf\u30c3\u30c1\u3059\u308b     <pre><code>aws iam attach-user-policy --user-name k8s-external-dns --policy-arn arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:policy/k8s-external-dns-policy\n</code></pre></p> </li> <li> <p>\u4f5c\u6210\u3057\u305fIAM User\u306ecredential\u3092\u78ba\u8a8d\u3059\u308b(Deployments\u30de\u30cb\u30d5\u30a7\u30b9\u30c8\u3067\u74b0\u5883\u5909\u6570\u3068\u3057\u3066\u30bb\u30c3\u30c8\u3059\u308b)</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> </ul> </li> </ol>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#3-external-dns-controller","title":"3. external-dns controller\u3092\u30c7\u30d7\u30ed\u30a4","text":"<ul> <li> <p>point</p> <ul> <li>https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md \u3092\u30d9\u30fc\u30b9\u306bbare-metal\u5411\u3051\u306b\u4fee\u6b63</li> <li>namespace\u306f <code>kube-system</code></li> <li>aws credential\u306f <code>k8s-external-dns</code> iam user\u306e\u3082\u306e</li> <li>hosted_zone_id \u306fexternal-dns\u3067\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3055\u305b\u305f\u3044Route53 zone</li> </ul> </li> <li> <p>manifests\u306b\u4ee3\u5165\u3059\u308b\u5909\u6570\u3092\u5b9a\u7fa9</p> variable name description <code>DOMAIN</code> external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306e\u30c9\u30e1\u30a4\u30f3 <code>HOSTED_ZONE_ID</code> external-dns\u3067\u767b\u9332\u3057\u305f\u3044\u30be\u30fc\u30f3\u306eHosted Zone ID <code>AWS_ACCESS_KEY_ID</code> external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 <code>AWS_SECRET_ACCESS_KEY</code> external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 <code>AWS_DEFAULT_REGION</code> external-dns\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u306b\u4f7f\u7528\u3059\u308bAWS IAM User\u306ecredential\u60c5\u5831 <pre><code>DOMAIN=\"XXXXXXX.com\"\nHOSTED_ZONE_ID=\"XXXXXXX\"\nAWS_ACCESS_KEY_ID=\"XXXXXXX\"\nAWS_SECRET_ACCESS_KEY=\"XXXXXXX\"\nAWS_DEFAULT_REGION=\"XXXXXXX\"\n</code></pre> </li> <li> <p>manifests\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210     /etc/kubernetes/manifests/external-dns.yaml <pre><code>```\nAWS_DEFAULT_REGION=\"ap-north-east-1\"\n\nsudo tee /etc/kubernetes/manifests/external-dns.yaml &lt;&lt;  EOF &gt; /dev/null\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: kube-system\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: kube-system\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: k8s.gcr.io/external-dns/external-dns:v0.10.0\n        env:\n        - name: AWS_ACCESS_KEY_ID\n          value: &lt;k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_ACCESS_KEY_ID&gt;\n        - name: AWS_SECRET_ACCESS_KEY\n          value:  &lt;k8s-external-dns AWS\u30a2\u30ab\u30a6\u30f3\u30c8\u306eAWS_SECRET_ACCESS_KEY&gt;\n        - name: AWS_DEFAULT_REGION\n          value: \"${AWS_DEFAULT_REGION}\"\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=${DOMAIN} # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=aws\n        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=&lt;HOSTED_ZONE_ID&gt;\n        - --txt-prefix=prefix_\n        - --log-level=debug\n      securityContext:\n        fsGroup: 65534 # For ExternalDNS to be able to read Kubernetes and AWS token files\nEOF\n```\n</code></pre> <li> <p>\u30c7\u30d7\u30ed\u30a4     <pre><code>kubectl apply -f /etc/kubernetes/manifests/external-dns.yaml\n</code></pre></p> </li>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#_3","title":"\u52d5\u4f5c\u78ba\u8a8d","text":"<ul> <li> <p>external-dns \u30b3\u30f3\u30c6\u30ca\u30ed\u30b0</p> <ul> <li>\u30c7\u30d7\u30ed\u30a4\u6e08\u307fingress\u306ehostname\u3067route53\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u767b\u9332\u3092\u78ba\u8a8d     <pre><code>time=\"2021-09-26T04:59:24Z\" level=info msg=\"Instantiating new Kubernetes client\"\ntime=\"2021-09-26T04:59:24Z\" level=debug msg=\"apiServerURL: \"\ntime=\"2021-09-26T04:59:24Z\" level=debug msg=\"kubeConfig: \"\ntime=\"2021-09-26T04:59:24Z\" level=info msg=\"Using inCluster-config based on serviceaccount-token\"\ntime=\"2021-09-26T04:59:24Z\" level=info msg=\"Created Kubernetes client https://10.32.0.1:443\"\ntime=\"2021-09-26T04:59:30Z\" level=debug msg=\"Refreshing zones list cache\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/&lt;HOSTED_ZONE_ID&gt; (domain: example.com.)\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/kube-dns\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service kube-system/metrics-server\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/kubernetes\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service default/nginx-service\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"No endpoints could be generated from service ingress-nginx/ingress-nginx-controller-admission\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN A  192.168.10.51 []]\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/&lt;HOSTED_ZONE_ID&gt; (domain: example.com.)\"\ntime=\"2021-09-26T04:59:31Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Refreshing zones list cache\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Considering zone: /hostedzone/&lt;HOSTED_ZONE_ID&gt; (domain: example.com.)\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-09-26T04:59:31Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com A [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-09-26T04:59:31Z\" level=info msg=\"Desired change: CREATE dev1.example.com TXT [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-09-26T04:59:32Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;] were successfully updated\"\n</code></pre></li> </ul> </li> <li> <p>route53 record set</p> <ul> <li>\u5bfe\u8c61\u306ehosted zone\u306bingress\u306ehost\u3067\u6307\u5b9a\u3057\u305fhostname\u3067\u30ec\u30b3\u30fc\u30c9\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d     <pre><code>$ aws route53 list-resource-record-sets --output json --hosted-zone-id &lt;HOSTED_ZONE_ID&gt; | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\"))'\n[\n  {\n    \"Name\": \"dev1.example.com.\",\n    \"Type\": \"A\",\n    \"TTL\": 300,\n    \"ResourceRecords\": [\n      {\n        \"Value\": \"192.168.10.51\"\n      }\n    ]\n  },\n  {\n    \"Name\": \"dev1.example.com.\",\n    \"Type\": \"TXT\",\n    \"TTL\": 300,\n    \"ResourceRecords\": [\n      {\n        \"Value\": \"\\\"heritage=external-dns,external-dns/owner=&lt;HOSTED_ZONE_ID&gt;,external-dns/resource=ingress/default/nginx-test-ingress\\\"\"\n      }\n    ]\n  }\n]\n</code></pre></li> </ul> </li> <li> <p>\u767b\u9332\u3055\u308c\u305fA\u30ec\u30b3\u30fc\u30c9\u306ehostname\u304c\u540d\u524d\u89e3\u6c7a\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d</p> <ul> <li>k8s service\u30ea\u30bd\u30fc\u30b9\u304b\u3089\u898b\u308b\u3068node port\u306b\u5bfe\u3059\u308bnode address\u306f\u81ea\u5b85\u74b0\u5883\u306eWifi\u30eb\u30fc\u30bf\u3067\u6255\u3044\u51fa\u3059\u30ec\u30f3\u30b8(192.168.10.0/24)\u306a\u306e\u3067\u60f3\u5b9a\u901a\u308a     <pre><code>$ dig +noall +answer dev1.example.com\ndev1.example.com.      283     IN      A       192.168.10.51\n</code></pre></li> </ul> </li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#appendix","title":"Appendix","text":""},{"location":"setup/11_external_dns/bootstrapping_external_dns/#ingressannotations","title":"Ingress\u30ea\u30bd\u30fc\u30b9\u3067\u4f7f\u7528\u53ef\u80fd\u306aannotations","text":"<ul> <li> <p>https://github.com/kubernetes-sigs/external-dns/blob/v0.10.0/source/source.go#L40-L68</p> annotations describe <code>external-dns.alpha.kubernetes.io/controller</code> \u8907\u6570\u306eDNS Controller\u304c\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u3069\u306eDNS Controller\u304c\u8cac\u4efb\u3092\u8ca0\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u628a\u63e1\u3059\u308b\u305f\u3081\u306b\u6307\u5b9a\u3059\u308b <code>external-dns.alpha.kubernetes.io/hostname</code> \u4f7f\u7528\u3059\u308b\u30db\u30b9\u30c8\u540d\u3092\u6307\u5b9a\u3059\u308bService\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u3067\u6307\u5b9a\u3059\u308bIngress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306f\u3053\u306eannotations\u304brule\u306ehost attr\u3067\u6307\u5b9a\u3059\u308b <code>external-dns.alpha.kubernetes.io/access</code> \u30d1\u30d6\u30ea\u30c3\u30af\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u3068\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u30a2\u30c9\u30ec\u30b9\u306e\u3069\u3061\u3089\u3092\u4f7f\u7528\u3059\u308b\u304b\u3092\u6307\u5b9a\u3059\u308b <code>external-dns.alpha.kubernetes.io/target</code> CNAME record\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\u306bCNAME record\u306evalue\u3068\u306a\u308b\u5024\u3092\u6307\u5b9a\u3059\u308b <code>external-dns.alpha.kubernetes.io/ttl</code> DNS record\u306eTTL\u3092\u6307\u5b9a\u3059\u308b(default: 300) <code>external-dns.alpha.kubernetes.io/alias</code> <code>true</code> \u3067ALIAS record\u3092\u4f5c\u6210\u3059\u308b <code>external-dns.alpha.kubernetes.io/ingress-hostname-source</code> Ingress\u30ea\u30bd\u30fc\u30b9\u306e\u5834\u5408\u306bhostname\u306e\u6307\u5b9a\u65b9\u6cd5\u3092annotations\u304brule\u306ehost attr\u304b\u3092\u9650\u5b9a\u3067\u304d\u308b <code>external-dns.alpha.kubernetes.io/internal-hostname</code> Target IP\u30a2\u30c9\u30ec\u30b9\u3092Cluster IP\u30a2\u30c9\u30ec\u30b9\u3068\u3059\u308b\u5834\u5408\u306b\u6307\u5b9a\u3059\u308b <code>external-dns.alpha.kubernetes.io/set-identifier</code> AWS Route53\u306b\u304a\u3044\u3066DNS Name\u3068Type\u304c\u540c\u3058\u5834\u5408\u306b\u91cd\u307f\u4ed8\u3051\u306a\u3069\u306b\u3088\u308b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30dd\u30ea\u30b7\u30fc\u3092\u5b9a\u7fa9\u3059\u308b\u969b\u306e\u8b58\u5225\u5b50Record ID\u3068\u306a\u308b\u5024 </li> </ul>"},{"location":"setup/11_external_dns/bootstrapping_external_dns/#external-dnsacname","title":"external-dns\u3067A\u30ec\u30b3\u30fc\u30c9\u3067\u306f\u306a\u304fCNAME\u30ec\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3059\u308b","text":"<ol> <li> <p>controller\u306e\u8d77\u52d5\u5f15\u6570\u306b <code>--txt-prefix=&lt;prefix\u6587\u5b57\u5217&gt;</code> \u3092\u8ffd\u52a0</p> <ul> <li>\u6307\u5b9a\u3057\u305f\u6587\u5b57\u5217\u304cTXT record\u306e\u30ec\u30b3\u30fc\u30c9\u540d(A\u30ec\u30b3\u30fc\u30c9\u306e\u5834\u5408\u306fA\u30ec\u30b3\u30fc\u30c9\u3068\u540c\u540d)\u306eprefix\u3068\u3057\u3066\u8ffd\u52a0\u3055\u308c\u307e\u3059</li> <li>CNAME\u30ec\u30b3\u30fc\u30c9\u306f(TXT\u30ec\u30b3\u30fc\u30c9\u3067\u3042\u3063\u3066\u3082)\u4ed6\u306e\u30ec\u30b3\u30fc\u30c9\u3068\u5171\u5b58\u3067\u304d\u306a\u3044\u4ed5\u69d8\u3067\u3059(RFC 1034\u30bb\u30af\u30b7\u30e7\u30f33.6.2\u3001RFC 1912\u30bb\u30af\u30b7\u30e7\u30f32.4</li> <li>https://github.com/kubernetes-sigs/external-dns/blob/master/docs/faq.md#im-using-an-elb-with-txt-registry-but-the-cname-record-clashes-with-the-txt-record-how-to-avoid-this</li> </ul> </li> <li> <p>Ingress\u30ea\u30bd\u30fc\u30b9\u306eannotations\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b</p> <ul> <li><code>external-dns.alpha.kubernetes.io/hostname</code> \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eFQDN\u3092\u6307\u5b9a</li> <li> <p><code>external-dns.alpha.kubernetes.io/target</code> \u306bCNAME\u30ec\u30b3\u30fc\u30c9\u306eValue\u3068\u306a\u308b\u53c2\u7167\u5148\u306eFQDN\u307e\u305f\u306fIP\u30a2\u30c9\u30ec\u30b9\u306a\u3069\u6307\u5b9a</p> <pre><code>external-dns.alpha.kubernetes.io/hostname: dev1.example.com\nexternal-dns.alpha.kubernetes.io/target: alias1.example.com\n</code></pre> </li> </ul> </li> <li> <p><code>--txt-prefix=prefix_</code> \u3067\u52d5\u4f5c\u78ba\u8a8d</p> <ol> <li> <p>Ingress\u30ea\u30bd\u30fc\u30b9      <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx-test-ingress\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: dev1.example.com\n    external-dns.alpha.kubernetes.io/target: alias1.example.com\nspec:\n  ingressClassName: nginx\n  defaultBackend:\n    service:\n      name: nginx-service\n      port:\n        number: 8080\n  rules:\n    - host: dev1.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: nginx-service\n                port:\n                  number: 8080\n</code></pre> <li> <p>external-dns log      <pre><code>time=\"2021-10-01T09:05:18Z\" level=debug msg=\"Endpoints generated from ingress: default/nginx-test-ingress: [dev1.example.com 0 IN CNAME  alias1.example.com [] dev1.example.com 0 IN CNAME  alias1.example.com []]\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Removing duplicate endpoint dev1.example.com 0 IN CNAME  alias1.example.com []\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/&lt;HOSTED_ZONE_ID&gt; (domain: example.com.)\"\ntime=\"2021-10-01T09:05:18Z\" level=info msg=\"Applying provider record filter for domains: [example.com. .example.com.]\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Refreshing zones list cache\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Considering zone: /hostedzone/&lt;HOSTED_ZONE_ID&gt; (domain: example.com.)\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding dev1.example.com. to zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-10-01T09:05:18Z\" level=debug msg=\"Adding prefix_dev1.example.com. to zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE dev1.example.com CNAME [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-10-01T09:05:18Z\" level=info msg=\"Desired change: CREATE prefix_dev1.example.com TXT [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;]\"\ntime=\"2021-10-01T09:05:19Z\" level=info msg=\"2 record(s) in zone example.com. [Id: /hostedzone/&lt;HOSTED_ZONE_ID&gt;] were successfully updated\"\n</code></pre> <li> <p>route53 \u30ec\u30b3\u30fc\u30c9\u78ba\u8a8d      <pre><code>$ aws route53 list-resource-record-sets --output json --hosted-zone-id &lt;HOSTED_ZONE_ID&gt; | jq '.ResourceRecordSets | map(select(.Name == \"dev1.example.com.\" or .Name == \"prefix_dev1.example.com.\"))'\n[\n  {\n    \"Name\": \"prefix_dev1.example.com.\",\n    \"Type\": \"TXT\",\n    \"TTL\": 300,\n    \"ResourceRecords\": [\n      {\n        \"Value\": \"\\\"heritage=external-dns,external-dns/owner=&lt;HOSTED_ZONE_ID&gt;,external-dns/resource=ingress/default/nginx-test-ingress\\\"\"\n      }\n    ]\n  },\n  {\n    \"Name\": \"dev1.example.com.\",\n    \"Type\": \"CNAME\",\n    \"TTL\": 300,\n    \"ResourceRecords\": [\n      {\n        \"Value\": \"alias1.example.com\"\n      }\n    ]\n  }\n]\n\n$ dig +noall +answer dev1.example.com\ndev1.example.com.      300     IN      CNAME   alias1.example.com.\n</code></pre>"},{"location":"static_pod/","title":"Index","text":"<p>8\u6708 31 12:57:50 k8s-master kubelet[1912]: E0831 12:57:50.411795    1912 kubelet.go:1635] Failed creating a mirror pod for \"etcd-k8s-master_kube-system(8fff3e1f31b52ebeb520767ae50cc739)\": pods \"etcd-k8s-master\" is forbidden: PodSecurityPolicy: no providers available to validate pod request</p> <p>8\u6708 31 12:58:55 k8s-master kubelet[1912]: I0831 12:58:55.446865    1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-apiserver-k8s-master_kube-system(b2f9759c-9ffe-436c-9ebb-0b5f9c68a452)\" 8\u6708 31 12:58:58 k8s-master kubelet[1912]: I0831 12:58:58.433779    1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-scheduler-k8s-master_kube-system(b502a669-2571-41f7-a705-3aa194ade526)\" 8\u6708 31 12:59:00 k8s-master kubelet[1912]: I0831 12:59:00.432199    1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"kube-controller-manager-k8s-master_kube-system(f834a62b-d802-40c5-90df-24a49a38efb6)\" 8\u6708 31 12:59:11 k8s-master kubelet[1912]: I0831 12:59:11.421459    1912 kubelet.go:1885] SyncLoop (ADD, \"api\"): \"etcd-k8s-master_kube-system(e612e1ab-a907-4955-8b4c-97d5a37b11fa)\" 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849447    1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/etcd-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849581    1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-apiserver-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849654    1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-controller-manager-k8s-master\" status=Running 8\u6708 31 12:59:13 k8s-master kubelet[1912]: I0831 12:59:13.849719    1912 kubelet_getters.go:176] \"Pod status updated\" pod=\"kube-system/kube-scheduler-k8s-master\" status=Running</p>"}]}